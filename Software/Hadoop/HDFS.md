# HDFS

[TOC]

Hadoop Distributed File System，分布式文件系统。

## 架构

![](../../Image/h/hdfs.jpg)

- **Block数据块**

  1. 基本存储单位，一般大小为64M。配置大的块主要是因为：

     * 减少搜寻时间，一般硬盘传输速率比寻道时间要快，大的块可以减少寻道时间。
* 减少管理块的数据开销，每个块都需要在NameNode上有对应的记录。
     * 对数据块进行读写，减少建立网络的连接成本。
     
  2. 一个大文件会被拆分成一个个的块，然后存储于不同的机器。如果一个文件少于Block大小，那么实际占用的空间为其文件的大小。

  3. 基本的读写单位，类似于磁盘的页，每次都是读写一个块。

  4. 每个块都会被复制到多台机器，默认复制3份。

- **NameNode**

  1. 存储文件的metadata，运行时所有数据都保存到内存，整个HDFS可存储的文件数受限于NameNode的内存大小。
  2. 一个Block在NameNode中对应一条记录（一般一个block占用150字节），如果是大量的小文件，会消耗大量内存。同时map  task的数量是由splits来决定的，所以用MapReduce处理大量的小文件时，就会产生过多的map  task，线程管理开销将会增加作业时间。处理大量小文件的速度远远小于处理同等大小的大文件的速度。因此Hadoop建议存储大文件。
  3. 数据会定时保存到本地磁盘，但不保存block的位置信息，而是由DataNode注册时上报和运行时维护（NameNode中与DataNode相关的信息并不保存到NameNode的文件系统中，而是NameNode每次重启后，动态重建）。
  4. NameNode失效则整个HDFS都失效了，所以要保证NameNode的可用性。

- **Secondary NameNode**

  1. 定时与NameNode进行同步（定期合并文件系统镜像和编辑日志，然后把合并后的传给NameNode，替换其镜像，并清空编辑日志，类似于CheckPoint机制），但NameNode失效后仍需要手工将其设置成主机。

- **DataNode**

  1. 保存具体的block数据。
  2. 负责数据的读写操作和复制操作。
  3. DataNode启动时会向NameNode报告当前存储的数据块信息，后续也会定时报告修改信息。
  4. DataNode之间会进行通信，复制数据块，保证数据的冗余性。

## 写文件

![img](../../Image/h/hdfs-write.png)

1.客户端将文件写入本地磁盘的 HDFS Client 文件中。

2.当临时文件大小达到一个 block 大小时，HDFS client 通知 NameNode，申请写入文件。

3.NameNode 在 HDFS 的文件系统中创建一个文件，并把该 block id 和要写入的 DataNode 的列表返回给客户端。

4.客户端收到这些信息后，将临时文件写入 DataNodes。

- 客户端将文件内容写入第一个 DataNode（一般以 4kb 为单位进行传输）
- 第一个 DataNode 接收后，将数据写入本地磁盘，同时也传输给第二个 DataNode
- 依此类推到最后一个 DataNode，数据在 DataNode 之间是通过 pipeline 的方式进行复制的
- 后面的 DataNode 接收完数据后，都会发送一个确认给前一个 DataNode，最终第一个 DataNode 返回确认给客户端
- 当客户端接收到整个 block 的确认后，会向 NameNode 发送一个最终的确认信息
- 如果写入某个 DataNode 失败，数据会继续写入其他的 DataNode。然后 NameNode 会找另外一个好的 DataNode 继续复制，以保证冗余性
- 每个 block 都会有一个校验码，并存放到独立的文件中，以便读的时候来验证其完整性

5.文件写完后（客户端关闭），NameNode 提交文件（这时文件才可见，如果提交前，NameNode 垮掉，那文件也就丢失了。fsync：只保证数据的信息写到 NameNode 上，但并不保证数据已经被写到DataNode 中）

**Rack aware（机架感知）**

通过配置文件指定机架名和 DNS 的对应关系。

假设复制参数是3，在写入文件时，会在本地的机架保存一份数据，然后在另外一个机架内保存两份数据（同机架内的传输速度快，从而提高性能）

整个 HDFS 的集群，最好是负载平衡的，这样才能尽量利用集群的优势。

## 读文件

![img](../../Image/h/hdfs-read.png)

1. 客户端向NameNode发送读取请求
2. NameNode返回文件的所有block和这些block所在的DataNodes（包括复制节点）
3. 客户端直接从DataNode中读取数据，如果该DataNode读取失败（DataNode失效或校验码不对），则从复制节点中读取（如果读取的数据就在本机，则直接读取，否则通过网络读取）

## 可靠性

1.冗余副本策略

　　可以在 hdfs-site.xml 中设置复制因子指定副本数量。所有数据块都可副本。DataNode 启动时,遍历本地文件系统,产生一份 HDFS 数据块和本地文件的对应关系列表 (blockreport) 汇报给 Namenode

2.机架策略

　　HDFS 的"机架感知",通过节点之间发送一个数据包,来感应它们是否在同一个机架。一般在本机架放一个副本,在其他机架再存放一个副本,这样可以防止机架失效时丢失数据,也可以提高带宽利用率。

3.心跳机制

　　NameNode 周期性从 DataNode 接受心跳信息和块报告。NameNode 根据块报告验证元数据。没有按时发送心跳的 DataNode 会被标记为宕机,不会再给他任何 I/O 请求。如果 DataNode 失效造成副本数量下降,并且低于预先设定的值,NameNode 会检测出这些数据库,并在合适的时机重新复制。引发重新复制的原因还包括数据副本本身损坏,磁盘错误,复制因子被增大等。

4.安全模式

　　NameNode 启动时会先经过一个 "安全模式" 阶段。安全模式阶段不会产生数据写。在此阶段 NameNode 收集各个 DataNode 的报告, 当数据块达到最小副本数以上时,会被认为是"安全"的。在一定比例(可设置) 的数据块被确定为"安全" 后 ,在过若干时间,安全模式结束。当检测到副本数不足的数据块时,该块会被复制,直到达到最小副本数。

5.效验和　　

　　在文件创立时,每个数据块都产生效验和。效验和会作为单独一个隐藏文件保存在命名空间下。客户端获取数据时可以检查效验和是否相同,从而发现数据块是否损坏。如果正在读取的数据块损坏,则可以继续读取其他副本。

6.回收站

　　删除文件时,其实是放入回收站 /trash。回收站里的文件是可以快速恢复的。可以设置一个时间值,当回收站里文件的存放时间超过了这个值,就被彻底删除,并且释放占用的数据块。

7.元数据保护

　　映像文件和事物日志是 NameNode 的核心数据.可以配置为拥有多个副本。副本会降低 NameNode 的处理速度,但增加安全性。NameNode 依然是单点,如果发生故障要手工切换。

8.快照机制



## 命令工具

fsck: 检查文件的完整性

start-balancer.sh: 重新平衡HDFS

hdfs dfs -copyFromLocal 从本地磁盘复制文件到HDFS