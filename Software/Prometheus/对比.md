# 与常见监控系统比较

[TOC]

## Graphite

### 范围

[Graphite](http://graphite.readthedocs.org/en/latest/) 专注于成为一个具有查询语言和绘图功能的被动时间序列数据库。任何其他问题都由外部组件解决。

Prometheus  是一个完整的监控和趋势系统，that includes built-in and active scraping, storing, querying, graphing, and alerting based on time series data. 包括基于时间序列数据的内置和主动抓取、存储、查询、绘图和警报。它了解世界应该是什么样子（应该存在哪些端点，什么时间序列模式意味着麻烦，等等），并积极尝试发现错误。

### 数据模型

Graphite stores numeric samples for named time series, much like Prometheus does. However, Prometheus's metadata model is richer: while Graphite metric names consist of dot-separated components which implicitly encode dimensions, Prometheus encodes dimensions explicitly as key-value pairs, called labels, attached to a metric name. This allows easy filtering, grouping, and matching by these labels via the query language.

石墨存储命名时间序列的数字样本，就像普罗米修斯一样。然而，普罗米修斯的元数据模型更丰富：虽然Graphite度量名称由点分隔的组件组成，这些组件隐式编码维度，但普罗米修斯将维度显式编码为附加到度量名称的键值对，称为标签。这允许通过查询语言通过这些标签进行简单的筛选、分组和匹配。

Further, especially when Graphite is used in combination with [StatsD](https://github.com/etsy/statsd/), it is common to store only aggregated data over all monitored instances, rather than preserving the instance as a dimension and being able to drill down into individual problematic instances.

此外，特别是当Graphite与Stats D结合使用时，通常只存储所有监控实例的聚合数据，而不是将实例保留为一个维度，并能够深入到单个有问题的实例。

例如，storing the number of HTTP requests to API servers with the response code `500` and the method `POST` to the `/tracks` endpoint would commonly be encoded like this in Graphite/StatsD:

```bash
stats.api-server.tracks.post.500 -> 93
```

在 Prometheus 中，相同的数据可以这样编码（假设有三个 api 服务器实例）：

```bash
api_server_http_requests_total{method="POST",handler="/tracks",status="500",instance="<sample1>"} -> 34
api_server_http_requests_total{method="POST",handler="/tracks",status="500",instance="<sample2>"} -> 28
api_server_http_requests_total{method="POST",handler="/tracks",status="500",instance="<sample3>"} -> 31
```

### 存储

Graphite stores time series data on local disk in the [Whisper](http://graphite.readthedocs.org/en/latest/whisper.html) format, an RRD-style database that expects samples to arrive at regular intervals. Every time series is stored in a separate file, and new samples overwrite old ones after a certain amount of time.

Graphite以Whisper格式将时间序列数据存储在本地磁盘上，这是一个RRD风格的数据库，预计样本会定期到达。每个时间序列都存储在一个单独的文件中，经过一定时间后，新的样本会覆盖旧的样本。

Prometheus also creates one local file per time series, but allows storing samples at arbitrary intervals as scrapes or rule evaluations occur. Since new samples are simply appended, old data may be kept arbitrarily long. Prometheus also works well for many short-lived, frequently changing sets of time series.

Prometheus还为每个时间序列创建一个本地文件，但允许在发生刮擦或规则评估时以任意间隔存储样本。由于新样本被简单地附加，所以旧数据可以被任意地保留很长时间。普罗米修斯也适用于许多短暂的、经常变化的时间序列。

### 总结

Prometheus 除了更易于运行和集成到您的环境中之外，还提供了更丰富的数据模型和查询语言。如果想要一个能够长期保存历史数据的集群解决方案，Graphite  可能是更好的选择。

## InfluxDB

[InfluxDB](https://influxdata.com/) 是一个开源的时间序列数据库，具有用于扩展和集群的商业选项。The InfluxDB project was released almost a year after Prometheus development began, so we were unable to consider it as an alternative at the time. Still, there are significant differences between Prometheus and InfluxDB, and both systems are geared towards slightly different use cases.

InfluxDB 项目是在 Prometheus 开发开始近一年后发布的，所以当时无法将其作为替代方案。尽管如此，Prometheus 和 InfluxDB 之间仍存在显著差异，并且这两个系统针对的用例略有不同。

### 范围

For a fair comparison, we must also consider [Kapacitor](https://github.com/influxdata/kapacitor) together with InfluxDB, as in combination they address the same problem space as Prometheus and the Alertmanager.

为了进行公平的比较，我们还必须将Kapacitor与Influx DB一起考虑，因为它们的组合解决了与Prometheus和Alertmanager相同的问题空间。

The same scope differences as in the case of [Graphite](https://prometheus.io/docs/introduction/comparison/#prometheus-vs-graphite) apply here for InfluxDB itself. In addition InfluxDB offers continuous queries, which are equivalent to Prometheus recording rules.

与Graphite相同的范围差异适用于Influx DB本身。此外，Influx DB提供了连续查询，这相当于普罗米修斯的记录规则。

Kapacitor’s scope is a combination of Prometheus recording rules, alerting rules, and the Alertmanager's notification functionality. Prometheus offers [a more powerful query language for graphing and alerting](https://www.robustperception.io/translating-between-monitoring-languages/). The Prometheus Alertmanager additionally offers grouping, deduplication and silencing functionality.

Kapacitor的作用域是普罗米修斯记录规则、警报规则和警报管理器的通知功能的组合。普罗米修斯提供了一种更强大的查询语言，用于绘图和报警。Prometheus Alertmanager还提供分组、重复数据消除和静音功能。

### 数据模型/存储

Like Prometheus, the InfluxDB data model has key-value pairs as labels, which are called tags. In addition, InfluxDB has a second level of labels called fields, which are more limited in use. InfluxDB supports timestamps with up to nanosecond resolution, and float64, int64, bool, and string data types. Prometheus, by contrast, supports the float64 data type with limited support for strings, and millisecond resolution timestamps.

与Prometheus一样，Influx DB数据模型也有键值对作为标签，称为标签。此外，Influx DB还有一个称为字段的第二级标签，这些标签的使用更为有限。Influx  DB支持高达纳秒分辨率的时间戳，以及float64、int64、bool和字符串数据类型。相比之下，Prometheus支持float64数据类型，但对字符串和毫秒分辨率时间戳的支持有限。

InfluxDB uses a variant of a [log-structured merge tree for storage with a write ahead log](https://docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/), sharded by time. This is much more suitable to event logging than Prometheus's append-only file per time series approach.

Influx DB使用日志结构合并树的变体进行存储，其中包含按时间划分的预写日志。这比Prometheus的按时间序列只追加文件的方法更适合用于事件日志记录。

[Logs and Metrics and Graphs, Oh My!](https://grafana.com/blog/2016/01/05/logs-and-metrics-and-graphs-oh-my/) describes the differences between event logging and metrics recording.

日志、度量和图表，天哪！描述了事件日志记录和度量记录之间的差异。

### 架构

Prometheus servers run independently of each other and only rely on their local storage for their core functionality: scraping, rule processing, and alerting. The open source version of InfluxDB is similar.

Prometheus服务器彼此独立运行，其核心功能仅依赖于本地存储：抓取、规则处理和警报。Influx DB的开源版本与此类似。

The commercial InfluxDB offering is, by design, a distributed storage cluster with storage and queries being handled by many nodes at once.

从设计上讲，商业Influx DB产品是一个分布式存储集群，其存储和查询由多个节点同时处理。

This means that the commercial InfluxDB will be easier to scale horizontally, but it also means that you have to manage the complexity of a distributed storage system from the beginning. Prometheus will be simpler to run, but at some point you will need to shard servers explicitly along scalability boundaries like products, services, datacenters, or similar aspects. Independent servers (which can be run redundantly in parallel) may also give you better reliability and failure isolation.

这意味着商业Influx  DB将更容易横向扩展，但也意味着您必须从一开始就管理分布式存储系统的复杂性。Prometheus将更容易运行，但在某些时候，您需要明确地按照可扩展性边界划分服务器，如产品、服务、数据中心或类似方面。独立服务器（可以并行冗余运行）也可以为您提供更好的可靠性和故障隔离。

Kapacitor's open-source release has no built-in distributed/redundant options for  rules,  alerting, or notifications.  The open-source release of Kapacitor can  be scaled via manual sharding by the user, similar to Prometheus itself. Influx offers [Enterprise Kapacitor](https://docs.influxdata.com/enterprise_kapacitor), which supports an  HA/redundant alerting system.

Kapacitor的开源版本没有内置的分布式/冗余规则、警报或通知选项。Kapacitor的开源版本可以通过用户手动分片来扩展，类似于普罗米修斯本身。Influx提供Enterprise Kapacitor，它支持HA/冗余警报系统。

Prometheus and the Alertmanager by contrast offer a fully open-source redundant  option via running redundant replicas of Prometheus and using the Alertmanager's  [High Availability](https://github.com/prometheus/alertmanager#high-availability) mode. 

相比之下，Prometheus和Alertmanager通过运行Prometheus的冗余副本并使用Alertmanager的高可用性模式，提供了一个完全开源的冗余选项。

### 总结

这些系统之间有许多相似之处。Both have labels (called tags in InfluxDB) to efficiently support multi-dimensional metrics. Both use basically the same data compression algorithms. Both have extensive integrations, including with each other. Both have hooks allowing you to extend them further, such as analyzing data in statistical tools or performing automated actions.

两者都有标签（在 InfluxDB中称为标签），可以有效地支持多维指标。两者使用基本相同的数据压缩算法。两者都有广泛的集成，包括彼此之间的集成。两者都有钩子，允许您进一步扩展它们，例如在统计工具中分析数据或执行自动化操作。

InfluxDB 更好的地方：

- 如果正在进行事件日志记录。
- 商业选项为 InfluxDB 提供集群，这也更适合长期数据存储。
- Eventually consistent view of data between replicas.最终实现副本之间数据的一致性视图。

Prometheus 更好的地方：

- If you're primarily doing metrics.如果你主要是在做度量。
- 更强大的查询语言、警报和通知功能。
- Higher availability and uptime for graphing and alerting.图形和警报的可用性和正常运行时间更高。

InfluxDB 由一家商业公司按照开放核心模式进行维护，提供闭源集群、托管和支持等高级功能。

Prometheus 是一个完全开源和独立的项目，由多家公司和个人维护，其中一些公司和个人还提供商业服务和支持。

## OpenTSDB

[OpenTSDB](http://opentsdb.net/) 是一个基于 Hadoop 和 HBase 的分布式时间序列数据库。

### 范围

The same scope differences as in the case of [Graphite](https://prometheus.io/docs/introduction/comparison/#prometheus-vs-graphite) apply here.此处适用与石墨情况相同的范围差异。

### 数据模型

OpenTSDB 的数据模型与 Prometheus 的数据模型几乎相同：time series are identified by a set of arbitrary key-value pairs (OpenTSDB tags are Prometheus labels). All data for a metric is  [stored together](http://opentsdb.net/docs/build/html/user_guide/writing/index.html#time-series-cardinality), limiting the cardinality of metrics.时间序列由一组任意的键值对标识（Open  TSDB标签是Prometheus标签）。度量的所有数据都存储在一起，从而限制了度量的基数。不过也有一些小的区别：Prometheus 允许在标签值中使用任意字符，而OpenTSDB 的限制更大。OpenTSDB 也缺乏完整的查询语言，只允许通过其 API 进行简单的聚合和计算。

### 存储

OpenTSDB 的存储是在 Hadoop 和 HBase 之上实现的。这意味着水平扩展 OpenTSDB 很容易，但必须从一开始就接受运行 Hadoop / HBase 集群的总体复杂性。

Prometheus 最初运行起来会更简单，但一旦超过单个节点的容量，就需要显式分片。

### 总结

Prometheus 提供了更丰富的查询语言，可以处理更高的基数指标，并构成了完整监控系统的一部分。如果已经在运行 Hadoop ，并且重视长期存储而不是这些优势，那么 OpenTSDB 是一个不错的选择。

## Nagios

[Nagios](https://www.nagios.org/) 是一个监控系统，起源于 20 世纪 90 年代的 Net Saint 。

### 范围

Nagios 主要是基于脚本的退出代码发出警报。这些被称为“支票”。These are  called “checks”. There is silencing of individual alerts, however no grouping,  routing or deduplication.有单独警报的静音功能，但没有分组、路由或重复数据消除功能。

有各种各样的插件。例如，piping the few kilobytes of perfData plugins are allowed to return [to a time series database such as Graphite](https://github.com/shawn-sterling/graphios) or using NRPE to [run checks on remote machines](https://exchange.nagios.org/directory/Addons/Monitoring-Agents/NRPE--2D-Nagios-Remote-Plugin-Executor/details).管道传输几千字节的perf数据插件可以返回到时间序列数据库（如Graphite），或者使用NRPE在远程机器上运行检查。

### 数据模型

Nagios 是基于主机的。每个主机可以有一个或多个服务，每个服务可以执行一次检查。

没有标签或查询语言的概念。

### 存储

Nagios 本身没有存储，超出了当前的检查状态。有一些插件可以存储数据，例如 [for visualisation](https://docs.pnp4nagios.org/) 。

### 架构

Nagios 服务器是独立的。所有检查配置都是通过文件进行的。

### 总结

Nagios 适用于黑盒探测就足够了的小型和/或静态系统的基本监控。

如果想进行白盒监控，或者有一个动态或基于云的环境，那么 Prometheus 是一个不错的选择。

## Sensu

[Sensu](https://sensu.io) is an open source monitoring and observability pipeline with a commercial distribution which offers  additional features for scalability.是一个开源的监控和可观察性管道，具有商业发行版，为可扩展性提供了额外的功能。它可以重用现有的 Nagios 插件。

### 范围

Sensu is an observability pipeline that focuses on processing and alerting of observability data as a stream of [Events](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-events/events/). It provides an extensible framework for event [filtering](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-filter/), aggregation, [transformation](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-transform/), and [processing](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-process/) – including sending alerts to other systems and storing events in  third-party systems. Sensu's event processing capabilities are similar  in scope to Prometheus alerting rules and Alertmanager. 

Sensu是一个可观测性管道，专注于将可观测性数据作为事件流进行处理和报警。它为事件过滤、聚合、转换和处理提供了一个可扩展的框架，包括向其他系统发送警报和将事件存储在第三方系统中。Sensu的事件处理功能在范围上与Prometheus警报规则和Alertmanager类似。

### 数据模型

Sensu [Events](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-events/events/) represent service health and/or [metrics](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-events/events/#metric-attributes) in a structured data format identified by an [entity](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-entities/entities/) name (e.g. server, cloud compute instance, container, or service), an event name, and optional [key-value metadata](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-events/events/#metadata-attributes) called "labels" or "annotations". The Sensu Event payload may include one or more metric [`points`](https://docs.sensu.io/sensu-go/latest/observability-pipeline/observe-events/events/#points-attributes), represented as a JSON object containing a `name`, `tags` (key/value pairs), `timestamp`, and `value` (always a float).

Sensu事件以结构化数据格式表示服务运行状况和/或指标，该格式由实体名称（例如服务器、云计算实例、容器或服务）、事件名称和称为“标签”或“注释”的可选键值元数据标识。Sensu Event有效载荷可以包括一个或多个度量点，表示为JSON对象，该JSON对象包含名称、标签（键/值对）、时间戳和值（始终为浮点值）。

### 存储

Sensu 将当前和最近的事件状态信息以及实时库存数据存储在嵌入式数据库（etcd）或外部 RDBMS（PostgreSQL）中。

### 架构

Sensu 部署的所有组件都可以集群化，以实现高可用性和改进的事件处理吞吐量。

### 总结

Sensu 和 Prometheus 有一些共同的功能，但它们在监控方面采取了截然不同的方法。两者都为基于云的动态环境和临时的计算平台提供了可扩展的发现机制，尽管底层机制截然不同。两者都支持通过标签和注释收集多维指标。两者都有广泛的集成，Sensu 本身支持从所有 Prometheus 导出器那里收集指标。两者都能够将可观测性数据转发到第三方数据平台（例如事件存储或 TSDB）。Sensu 和 Prometheus 的最大区别在于它们的用例。

Sensu 更好的地方：

- If you're collecting and processing hybrid observability data (including metrics *and/or* events)

  如果您正在收集和处理混合可观测性数据（包括度量和/或事件）

- If you're consolidating multiple monitoring tools and need support for metrics *and* Nagios-style plugins or check scripts

  如果您正在整合多个监控工具，并且需要支持指标和 Nagios 风格的插件或检查脚本。

- 更强大的事件处理平台。

Prometheus 更好的地方：

- 如果主要收集和评估指标。
- If you're monitoring homogeneous Kubernetes infrastructure (if 100%  of the workloads you're monitoring are in K8s, Prometheus offers better  K8s integration)
- 如果正在监控同质的Kubernetes基础设施（如果您监控的工作负载100%在K8s中，Prometheus提供了更好的K8s集成）
- More powerful query language, and built-in support for historical data analysis 
- 更强大的查询语言，以及对历史数据分析的内置支持。

Sensu is maintained by a single commercial company following the  open-core business model, offering premium features like closed-source  event correlation and aggregation, federation, and support. Sensu 由一家遵循开放核心商业模式的商业公司维护，提供闭源事件关联和聚合、联合和支持等高级功能。

Prometheus 是一个完全开源和独立的项目，由多家公司和个人维护，其中一些公司和个人还提供商业服务和支持。