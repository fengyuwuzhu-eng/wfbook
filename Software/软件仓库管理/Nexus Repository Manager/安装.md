# 安装

[TOC]

# Download

 

*Nexus Repository OSS is distributed with Sencha Ext JS pursuant to a FLOSS  Exception agreed upon between Sonatype, Inc. and Sencha Inc. Sencha Ext  JS is licensed under GPL v3 and cannot be redistributed as part of a  closed source work.*

Distributions for Nexus Repository 3 are available here for the 64-bit versions for Apple macOS, Microsoft Windows and Unix/Linux. They contain all necessary resources to install and run the repository manager.

The download is used for both Nexus Repository PRO and OSS.

See [Installing and Updating Licenses](https://help.sonatype.com/repomanager3/nexus-repository-administration/installing-and-updating-licenses) for information on getting your OSS version to PRO with your professional license.

## Download the Latest Version

 

As of 3.44.0, Nexus Repository 3 instances using PostgreSQL databases now use the [pg_trgm (trigram) module](https://www.postgresql.org/docs/current/pgtrgm.html). This module may not be installed with PostgreSQL by default on all  Linux distributions, which will result in an exception when attempting  to upgrade. If you find yourself in this situation, you will need to  install the postgresql-contrib package available from your Linux  distribution. In order to install it, the PostgreSQL user must have CREATE privileges on the current database. See our [knowledge base article](https://support.sonatype.com/hc/en-us/articles/12607156575507-Enhanced-search-performance-with-Postgres-for-3-44-0-) for further information.



 

The previously released 3.47.0 contains a bug that is fixed in 3.47.1. 

If you have not upgraded to 3.47.0, upgrade to 3.47.1 instead. If you have already upgraded to 3.47.0, upgrade to 3.47.1 as soon as possible.

Download **Nexus Repository 3.47.1** for your respective operating system:

| Operating System | Link for Download                                            |
| :--------------- | :----------------------------------------------------------- |
| Unix archive     | https://download.sonatype.com/nexus/3/nexus-3.47.1-01-unix.tar.gz  (  [ASC](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-unix.tar.gz.asc) ,  [MD5](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-unix.tar.gz.md5) ,  [SHA1](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-unix.tar.gz.sha1) ) |
| Windows archive  | https://download.sonatype.com/nexus/3/nexus-3.47.1-01-win64.zip (  [ASC](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-win64.zip.asc) ,   [MD5](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-win64.zip.md5) ,   [SHA1](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-win64.zip.sha1) ) |
| OSX archive      | https://download.sonatype.com/nexus/3/nexus-3.47.1-01-mac.tgz (  [ASC](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-mac.tgz.asc), [MD5](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-mac.tgz.md5) ,  [SHA1](https://download.sonatype.com/nexus/3/nexus-3.47.1-01-mac.tgz.sha1) ) |

If you are performing a new installation, consult    [Installation and Upgrades](https://help.sonatype.com/repomanager3/installation-and-upgrades)    and    [System Requirements](https://help.sonatype.com/repomanager3/product-information/system-requirements)   for further information about next steps.

## Next Steps

If you are upgrading an existing Nexus Repository 3 instance, see  [Upgrading a Standalone Instance](https://help.sonatype.com/repomanager3/installation-and-upgrades/upgrading-a-standalone-instance) . 

If you are upgrading from Nexus Repository 2 to Nexus Repository 3, see the  [2.x to 3.y subsection](https://help.sonatype.com/repomanager3/installation-and-upgrades/upgrading-from-nexus-repository-manager-2#UpgradingfromNexusRepositoryManager2-Upgradingfrom2.xto3.y) and further subpages in the  [Upgrading from Nexus Repository Manager 2](https://help.sonatype.com/repomanager3/installation-and-upgrades/upgrading-from-nexus-repository-manager-2)  section.

## Related Links

| Software                                                     | Link for Download                                            |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Docker Images (from latest to version 3.0.0)                 | https://hub.docker.com/r/sonatype/nexus3/                    |
| Cloud Templates                                              | [Cloud Deployments](https://help.sonatype.com/repomanager3/integrations/cloud-deployments) |
| Jenkins Plugin                                               | [Nexus Platform Plugin for Jenkins](https://help.sonatype.com/integrations/nexus-and-continuous-integration/nexus-platform-plugin-for-jenkins#NexusPlatformPluginforJenkins-RepositoryManager3Integration) |
| Sonatype GPG Key                                             | [0374CF2E8DD1BDFD](https://keys.openpgp.org/search?q=0374CF2E8DD1BDFD) |
| [Database Migrator](https://help.sonatype.com/repomanager3/new-database-options/migrating-to-a-new-database) | https://download.sonatype.com/nexus/nxrm3-migrator/nexus-db-migrator-3.47.1-01.jar   ( [ASC](https://download.sonatype.com/nexus/nxrm3-migrator/nexus-db-migrator-3.47.1-01.jar.asc) ,  [SHA1](https://download.sonatype.com/nexus/nxrm3-migrator/nexus-db-migrator-3.47.1-01.jar.sha1) ,  [SHA256](https://download.sonatype.com/nexus/nxrm3-migrator/nexus-db-migrator-3.47.1-01.jar.sha256) ,  [SHA512](https://download.sonatype.com/nexus/nxrm3-migrator/nexus-db-migrator-3.47.1-01.jar.sha512) ) |

## Download Older Versions

While we recommend the latest, the [download archive](https://help.sonatype.com/repomanager3/product-information/download/download-archives---repository-manager-3) also contains older versions should you need.

# System Requirements

 

 

## Supported Versions

Sonatype fully supports versions of repository manager for one year after  the release date. Older releases are supported on a best effort basis  and the release dates are listed in our download archives. The terms of support are explained in section 3 of the [Support Policy](https://www.sonatype.com/Usage/Software-Support-Policy).

## Host Operating System

Any Windows, Linux or Macintosh operating system that can run a supported Java version will work. Other operating systems may work, but they are not tested by Sonatype.

The most widely used operating system for Nexus Repository Manager (NXRM)  is Linux and therefore customers should consider it the best tested  platform.

## Dedicated Operating System User Account

Unless you are just testing the repository manager or running it only for  personal use, a dedicated operating system user account is strongly  recommended to run each unique process on a given host.

 

The NXRM process user is typically named 'nexus' and must be able to create a valid shell.

Important

 

As a security precaution, do not run Nexus Repository Manager 3 as the `root` user.

## Adequate File Handle Limits

NXRM3 will most likely want to consume more file handles than the per user  default value allowed by your Linux or OSX operating system.

Running out of file descriptors can be disastrous and will most probably lead  to data loss. Make sure to increase the limit on the number of open  files descriptors for the user running Nexus Repository  Manager permanently to 65,536 or higher prior to starting.

See [https://issues.sonatype.org/browse/NEXUS-12041](https://issues.sonatype.org/browse/NEXUS-12041?__hstc=31049440.8ede5f702ac4d1e3e4b219a2c6292ddd.1676943147945.1676943147945.1676947588654.2&__hssc=31049440.4.1676947588654&__hsfp=1931082791) for additional background.

### Linux  * *

On most Linux systems, persistent limits can be set for a particular user by editing the `/etc/security/limits.conf` file. To set the maximum number of open files for both soft and hard limits for the `nexus` user to 65536, add the following line to the `/etc/security/limits.conf` file, where "nexus" should be replaced with the user ID that is being used to run the repository manager:

```
nexus - nofile 65536
```

This change will only take effect the next time the `nexus` process user opens a new session. Which essentially means that you will need to restart NXRM.

On Ubuntu systems there is a caveat: Ubuntu ignores the `/etc/security/limits.conf` file for processes started by `init.d`.

So if NXRM is started using init.d there, edit `/etc/pam.d/common-session` and uncomment the following line ( remove the hash # and space at the beginning of the line):

```
# session    required   pam_limits.so
```

For more information refer to your specific operating system documentation.

If you're using `systemd` to launch the server the above won't work. Instead, modify the configuration file to add a `LimitNOFILE` line:

```
[Unit]
Description=nexus service
After=network.target

[Service]
Type=forking
LimitNOFILE=65536
ExecStart=/opt/nexus/bin/nexus start
ExecStop=/opt/nexus/bin/nexus stop
User=nexus
Restart=on-abort

[Install]
WantedBy=multi-user.target
```

### Mac OSX

The method to modify the file descriptor limits on OSX has changed a few  times over the years. Please note your OS X version and follow the  appropriate instructions.

**For OS X Yosemite (10.10) and newer
**

1. Create the file: `/Library/LaunchDaemons/limit.maxfiles.plist`

   ```
   <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
     <plist version="1.0">
       <dict>
         <key>Label</key>
           <string>limit.maxfiles</string>
         <key>ProgramArguments</key>
           <array>
             <string>launchctl</string>
             <string>limit</string>
             <string>maxfiles</string>
             <string>65536</string>
             <string>65536</string>
           </array>
         <key>RunAtLoad</key>
           <true/>
         <key>ServiceIPC</key>
           <false/>
       </dict>
     </plist>
   ```

   If this file already exists, then ensure the value is **at least** 65536 as shown.

   The file must be owned by `root:wheel` and have permissions `-rw-r--r--`. 

   ```
   sudo chmod 644 /Library/LaunchDaemons/limit.maxfiles.plist
   sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist
   ```

   **Reboot the operating system to activate the change.

   **

2. Add a new line to `        $install-dir/bin/nexus.vmoptions` containing:

   ```
   -XX:-MaxFDLimit
   ```

   **Restart Nexus Repository to activate the change.

   **

**For OS X Lion (10.7) up to OS X Mavericks (10.9)
**

1. Create and edit the system file `/etc/launchd.conf` using this command:

   ```
   sudo sh -c 'echo "limit maxfiles 65536 65536" >> /etc/launchd.conf'
   ```

   **Reboot the operating system to activate the change.

   **

2. Add a new line to `$install-dir/bin/nexus.vmoptions` containing:

   ```
   -XX:-MaxFDLimit
   ```

   **Restart Nexus Repository to activate the change.**

### Windows

Windows operating systems do not need file handle limit adjustments.

### Docker

The Nexus Repository Docker images are configured with adequate file  limits. Some container platforms such as Amazon ECS will override the  default limits. On these platforms it is recommended that the Docker  image be run with the following flags:

```
--ulimit nofile=65536:65536
```

##   Java

Nexus Repository Manager requires a Java 8 Runtime Environment (JRE). The  distributions for OSX and Windows include suitable runtime environments  for the specific operating system. The distributions for Unix do not  include the runtime environment. If you prefer to use an external  runtime or use a Unix operating system, you can choose to install the  full JDK or the JRE only. You can confirm the installed Java version  with the `java -version` command, for example:

```
$ java -version
openjdk version "1.8.0_191"
OpenJDK Runtime Environment (build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
```

When multiple JDK or JRE versions are installed, you need to ensure the  correct version is configured by running the above command as the  operating system user that is used to run the repository manager.

In the event you have a non-standard location you need to update the configuration to specify a specific JDK or JRE  installation path. To set the path for a specific Java location open  the `bin/nexus` script and locate the line `INSTALL4J_JAVA_HOME_OVERRIDE`. Remove the hash and specify the location of your JDK/JRE:

```
INSTALL4J_JAVA_HOME_OVERRIDE=/usr/lib/jvm/openjdk-8
```

The startup script verifies the runtime environment by checking for the existence of the nested `bin/java `command as well as major and minor version of the runtime to be the required `1.8`. If the configured runtime is not suitable, it will proceed with a best  effort to locate a suitable runtime configured on the path or via the `JAVA_HOME` environment variable. If successful, it will start up the repository manager with  this JVM. This allows you to have a dedicated runtime environment for  the repository manager installed that is not on the path and not used by other installed applications. Further, you can separate upgrades of the Java runtime used by the repository manager from upgrades of the  runtime used by other applications.

## CPU

Performance is primarily bounded by IO (disk and network) rather than CPU.  Available CPUs will impact longer running operations and also the [thread allocation algorithms of the web container](https://support.sonatype.com/hc/en-us/articles/360000744687-Understanding-Eclipse-Jetty-9-4-8-Thread-Allocation).

**Minimum CPUs:** 4

**Recommended CPUs:** 8+

## Memory

### Configurable Memory Types

Visit the [Configuring the Runtime Enviroment](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-the-runtime-environment#ConfiguringtheRuntimeEnvironment-ConfiguringtheRuntimeEnvironment-ConfiguringMemory) page to learn how to change the default memory settings.

#### JVM Heap Memory

Heap memory stores runtime application objects. A min ( -Xms ) and max  ( -Xmx ) value must be specified and the values should be identical.

Increasing the heap memory larger than recommendations or setting the min and max values to be different is **not recommended**. This will create performance issues causing the operating system to thrash needlessly.

#### JVM Direct Memory

 

Only required for OrientDB.

Direct memory is allocated outside of and distinct from heap memory. A max value must be configured if using OrientDB.

#### Host Physical Memory

The total memory allocated to the entire operating system or virtual hardware, commonly referred to as RAM.

### Memory Requirements

The requirements assume there are no other significant memory hungry processes running on the same host.

|                     | JVM Heap | JVM Direct                               | Host Physical/RAM |
| ------------------- | -------- | ---------------------------------------- | ----------------- |
| Minimum ( default ) | 2703MB   | 2703MB                                   | 8GB               |
| Maximum             | 4GB      | (host physical/RAM * 2/3) - JVM max heap | *no limit*        |

### General Memory Guidelines

- minimum physical/RAM memory on the host 8GB
- minimum heap ( -Xms ) must equal set maximum heap ( -Xmx )
- minimum heap size 2703MB
- maximum heap size <= 4GB
- minimum direct memory ( -XX:MaxDirectMemorySize ) size 2703MB
- minimum unallocated host physical/RAM memory should be no less than 1/3 of total physical RAM to allow for virtual memory swap
- max heap + max direct memory <= host physical/RAM * 2/3

### Instance Memory Sizing Profiles

These profiles help gauge the typical physical memory requirements needed for a dedicated server host running repository manager. *Due to the inherent complexities of use cases, one size does not fit all and this should only be interpreted as a guideline.*

| Profile Use Case                                             | Physical/RAM Memory |
| ------------------------------------------------------------ | ------------------- |
| small, personalrepositories < 20 total blobstore size < 20GB single repository format type | 8GB minimum         |
| medium, teamrepositories < 50 total blobstore size < 200GB a few repository formats | 16GB                |
| large, enterpriserepositories > 50 total blobstore size > 200GB diverse set of repository formats | 32GB+               |

### Example Maximum Memory Configurations 

 

`-XX:MaxDirectMemorySize` is for **OrientDB only**

| Physical/RAM Memory | Example Maximum Memory Configuration                      |
| ------------------- | --------------------------------------------------------- |
| 8GB                 | `-Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M `      |
| 12GB                | `-Xms4G -Xmx4G -XX:MaxDirectMemorySize=4014M `            |
| 16GB                | `-Xms4G -Xmx4G -XX:MaxDirectMemorySize=6717M `            |
| 32GB                | `-Xms6G -Xmx6G -XX:MaxDirectMemorySize=15530M `           |
| 64GB                | `-Xms8G -Xmx8G-XX:+UseG1GC-XX:MaxDirectMemorySize=35158M` |

### Advanced Database Memory Tuning

Refer to another article which outlines [additional memory tuning procedures](https://support.sonatype.com/hc/en-us/articles/115007093447).

## Database Limitations

 

Also see [System Requirements for H2 and PostgreSQL Databases](https://help.sonatype.com/repomanager3/product-information/system-requirements-for-h2-and-postgresql-databases) for further database requirement information, including how to ensure you have the required[ trigram module for PostgreSQL](https://help.sonatype.com/repomanager3/product-information/system-requirements-for-h2-and-postgresql-databases#SystemRequirementsforH2andPostgreSQLDatabases-trigram).



 

We highly recommend that you have your Nexus Repository 3 instance use an external PostgreSQL database. See our [documentation on configuring Nexus Repository Pro for an external PostgreSQL database](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-nexus-repository-pro-for-h2-or-postgresql#ConfiguringNexusRepositoryProforH2orPostgreSQL-ConfiguringforExternalPostgreSQL) or on [migrating an existing Nexus Repository 3 instance to a PostgreSQL database](https://help.sonatype.com/repomanager3/installation-and-upgrades/migrating-to-a-new-database).

 

We strongly recommend against running Nexus Repository 3 on an embedded database within container orchestration environments such as Kubernetes. Doing so can lead to severe data corruption.

Ongoing performance analysis has shown that OrientDB and H2 databases are  unsuitable for use beyond the limits below. If you need to exceed the  below limits, you should migrate to a [PostgreSQL database](https://help.sonatype.com/repomanager3/planning-your-implementation/database-options).

### H2

For H2, container-based deployments are not supported. H2 supported  workload limitation is 20K requests per day and 100K components;  workload beyond these limitations is not supported.

### OrientDB

For OrientDB, we also recommend not exceeding 20K requests per day and 100K components.

## Temporary Directory

The [temporary directory](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-the-runtime-environment#ConfiguringtheRuntimeEnvironment-ConfiguringtheTemporaryDirectory) at `$data-dir/tmp` must not be mounted with `noexec` or repository manager startup will fail with `java.lang.UnsatisfiedLinkError`   message of  `failed to map segment from shared object: Operation not permitted`.

## Disk Space

Application Directory - The size of this directory varies slightly each release. It currently around 330 MB. It is normal to have multiple application  directories installed on the same host over time as repository manager  is upgraded.

Data Directory - On first start, repository manager  creates the base files needed to operate. The bulk of disk space will be held by your deployed and proxied artifacts, as well as any search  indexes. This is highly installation specific, and will be dependent on  the repository formats used, the number of artifacts stored, the size of your teams and projects, etc.  It's best to plan for a lot though,  formats like Docker and Maven can use very large amounts of storage  (500GB easily). When available disk space drops below 4GB the database will switch to read-only mode.

## File Systems

Nexus Repository stores multiple kinds of data, with two primary storage requirements:

1. **Embedded data** (H2, OrientDB, Elastic Search) requires very responsive, fast storage, ideally local disk
2. **Blob storage** (component binaries), which requires moderately responsive, high-capacity storage

File system selection should be made bearing both of these in mind.

| File System            |  Embedded data   |                         Blob Stores                          | Comment                                                      |
| ---------------------- | :--------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
| **Local storage**      |  **Supported**   |                        **Supported**                         | Local storage is a good choice for both embedded data and binary storage. |
| **NFS v4**             | Not Recommended* |                        **Supported**                         | Most common protocol for network attached storage among Nexus Repository deployments. |
| **Amazon EBS**         |  **Supported**   |                        **Supported**                         | EBS is a viable choice for both embedded data and binary storage. |
| **Amazon EFS**         |   Unsupported    |                       **Supported\****                       | EFS isn't sufficiently responsive for embedded data, but is appropriate for binary storage. |
| **Amazon S3**          |       N/A        |                        **Supported**                         | S3 semantics aren't suitable for embedded data, but S3 is popular for binary storage. |
| **SMB, CIFS**          |   Unsupported    |                        **Supported**                         | Problems are common with SMB or CIFS-mounted devices for embedded data. |
| **Azure Blob Storage** |       N/A        |                        **Supported**                         | Available for blob storage from Nexus 3.30.0 Pro. For performance reasons, the  Azure blob store should be in the same Azure region as the Nexus Repo  installation. |
| **Azure Files**        |   Unsupported    |                        **Supported**                         | Issues with file handles have been observed when accessing embedded data over SMB. |
| **S3-Compatible**      |   Unsupported    | Some S3-compatible object stores do not support all the features required by Nexus Repository or have subtle compatibility issues with the AWS Java  SDK that NXRM uses. This includes providers such as Ceph S3. |                                                              |
| **NFS v3**             |   Unsupported    | Numerous customers have experienced inadequate performance with NFS v3. |                                                              |
| **GlusterFS**          |   Unsupported    |    Split-brain problems and slow performance are common.     |                                                              |
| **FUSE**               |   Unsupported    | FUSE based user-space filesystems are known to be unreliable for Nexus Repository. |                                                              |

** NFSv4.1 or higher can be used for the work directory in small lightly  loaded installations, but we have found that it does not provide  sufficient performance for anything larger. In general it should be  avoided for the work directory.*

*** EFS binary storage may not provide necessary throughput for heavy workloads in all configurations.*

### File System Optimization

We also have some [optimization suggestions to use at your discretion](https://support.sonatype.com/hc/en-us/articles/213465258-Optimizing-Nexus-Disk-IO-Performance). Also consider the `noatime` option for your Nexus Repository work directory mounts and limit the symbolic  links used as this will cause increased overhead whenever paths need to  be resolved to an absolute file path.

## Web Browser

Our general policy is to support the most recent modern browser version for your supported OS at time of NXRM release date.

| Vendor    | Browser           | Versions                                                     |
| --------- | ----------------- | ------------------------------------------------------------ |
| Google    | Chrome            | latest at NXRM release                                       |
| Mozilla   | Firefox           | latest and  [ESR](https://www.mozilla.org/en-US/firefox/organizations/)  at NXRM release |
| Apple     | Safari            | latest at NXRM release                                       |
| Microsoft | Edge              | latest at NXRM release                                       |
| Microsoft | Internet Explorer | No longer supported                                          |

## 系统需求

### 主机操作系统

Any Windows, Linux or Macintosh operating system that can run a supported Java version will work. Other operating systems may work, but they are not tested by Sonatype.

The most widely used operating system for Nexus Repository Manager (NXRM)  is Linux and therefore customers should consider it the best tested  platform.

任何可以运行受支持的Java版本的Windows、Linux或Macintosh操作系统都可以工作。其他操作系统可能可以工作，但Sonatype未对其进行测试。

Nexus Repository Manager（NXRM）使用最广泛的操作系统是Linux，因此客户应将其视为最佳测试平台。

## Dedicated Operating System User Account

Unless you are just testing the repository manager or running it only for  personal use, a dedicated operating system user account is strongly  recommended to run each unique process on a given host.

 

The NXRM process user is typically named 'nexus' and must be able to create a valid shell.

Important

 

As a security precaution, do not run Nexus Repository Manager 3 as the `root` user.

## Adequate File Handle Limits

NXRM3 will most likely want to consume more file handles than the per user  default value allowed by your Linux or OSX operating system.

Running out of file descriptors can be disastrous and will most probably lead  to data loss. Make sure to increase the limit on the number of open  files descriptors for the user running Nexus Repository  Manager permanently to 65,536 or higher prior to starting.

See https://issues.sonatype.org/browse/NEXUS-12041 for additional background.

### Linux  * *

On most Linux systems, persistent limits can be set for a particular user by editing the `/etc/security/limits.conf` file. To set the maximum number of open files for both soft and hard limits for the `nexus` user to 65536, add the following line to the `/etc/security/limits.conf` file, where "nexus" should be replaced with the user ID that is being used to run the repository manager:

```
nexus - nofile 65536
```

This change will only take effect the next time the `nexus` process user opens a new session. Which essentially means that you will need to restart NXRM.

On Ubuntu systems there is a caveat: Ubuntu ignores the `/etc/security/limits.conf` file for processes started by `init.d`.

So if NXRM is started using init.d there, edit `/etc/pam.d/common-session` and uncomment the following line ( remove the hash # and space at the beginning of the line):

```
# session    required   pam_limits.so
```

For more information refer to your specific operating system documentation.

If you're using `systemd` to launch the server the above won't work. Instead, modify the configuration file to add a `LimitNOFILE` line:

```
[Unit]
Description=nexus service
After=network.target

[Service]
Type=forking
LimitNOFILE=65536
ExecStart=/opt/nexus/bin/nexus start
ExecStop=/opt/nexus/bin/nexus stop
User=nexus
Restart=on-abort

[Install]
WantedBy=multi-user.target
```

### Mac OSX

The method to modify the file descriptor limits on OSX has changed a few  times over the years. Please note your OS X version and follow the  appropriate instructions.

**For OS X Yosemite (10.10) and newer
**

1. Create the file: `/Library/LaunchDaemons/limit.maxfiles.plist`

   ```
   <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
     <plist version="1.0">
       <dict>
         <key>Label</key>
           <string>limit.maxfiles</string>
         <key>ProgramArguments</key>
           <array>
             <string>launchctl</string>
             <string>limit</string>
             <string>maxfiles</string>
             <string>65536</string>
             <string>65536</string>
           </array>
         <key>RunAtLoad</key>
           <true/>
         <key>ServiceIPC</key>
           <false/>
       </dict>
     </plist>
   ```

   If this file already exists, then ensure the value is **at least** 65536 as shown.

   The file must be owned by `root:wheel` and have permissions `-rw-r--r--`. 

   ```
   sudo chmod 644 /Library/LaunchDaemons/limit.maxfiles.plist
   sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist
   ```

   **Reboot the operating system to activate the change.

   **

2. Add a new line to `        $install-dir/bin/nexus.vmoptions      `  containing:

   ```
   -XX:-MaxFDLimit
   ```

   **Restart NXRM to activate the change.

   **

**For OS X Lion (10.7) up to OS X Mavericks (10.9)
**

1. Create and edit the system file `/etc/launchd.conf` using this command:

   ```
   sudo sh -c 'echo "limit maxfiles 65536 65536" >> /etc/launchd.conf'
   ```

   **Reboot the operating system to activate the change.

   **

2. Add a new line to `$install-dir/bin/nexus.vmoptions` containing:

   ```
   -XX:-MaxFDLimit
   ```

   **Restart NXRM to activate the change.**

### Windows

Windows operating systems do not need file handle limit adjustments.

### Docker

The Nexus Repository Docker images are configured with adequate file  limits. Some container platforms such as Amazon ECS will override the  default limits. On these platforms it is recommended that the Docker  image be run with the following flags:

```
--ulimit nofile=65536:65536
```

##   Java

Nexus Repository Manager requires a Java 8 Runtime Environment (JRE). The  distributions for OSX and Windows include suitable runtime environments  for the specific operating system. The distributions for Unix do not  include the runtime environment. If you prefer to use an external  runtime or use a Unix operating system, you can choose to install the  full JDK or the JRE only. You can confirm the installed Java version  with the `java -version` command, for example:

```
$ java -version
openjdk version "1.8.0_191"
OpenJDK Runtime Environment (build 1.8.0_191-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)
```

When multiple JDK or JRE versions are installed, you need to ensure the  correct version is configured by running the above command as the  operating system user that is used to run the repository manager.

In the event you have a non-standard location you need to update the configuration to specify a specific JDK or JRE  installation path. To set the path for a specific Java location open  the `bin/nexus` script and locate the line `INSTALL4J_JAVA_HOME_OVERRIDE`. Remove the hash and specify the location of your JDK/JRE:

```
INSTALL4J_JAVA_HOME_OVERRIDE=/usr/lib/jvm/openjdk-8
```

The startup script verifies the runtime environment by checking for the existence of the nested `bin/java `command as well as major and minor version of the runtime to be the required `1.8`. If the configured runtime is not suitable, it will proceed with a best  effort to locate a suitable runtime configured on the path or via the `JAVA_HOME` environment variable. If successful, it will start up the repository manager with  this JVM. This allows you to have a dedicated runtime environment for  the repository manager installed that is not on the path and not used by other installed applications. Further, you can separate upgrades of the Java runtime used by the repository manager from upgrades of the  runtime used by other applications.

## CPU

Performance is primarily bounded by IO (disk and network) rather than CPU.  Available CPUs will impact longer running operations and also the [thread allocation algorithms of the web container](https://support.sonatype.com/hc/en-us/articles/360000744687-Understanding-Eclipse-Jetty-9-4-8-Thread-Allocation).

**Minimum CPUs:** 4

**Recommended CPUs:** 8+

## Memory

### Configurable Memory Types

Visit the [Configuring the Runtime Enviroment](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-the-runtime-environment#ConfiguringtheRuntimeEnvironment-ConfiguringtheRuntimeEnvironment-ConfiguringMemory) page to learn how to change the default memory settings.

#### JVM Heap Memory

Heap memory stores runtime application objects. A min ( -Xms ) and max  ( -Xmx ) value must be specified and the values should be identical.

Increasing the heap memory larger than recommendations or setting the min and max values to be different is **not recommended**. This will create performance issues causing the operating system to thrash needlessly.

#### JVM Direct Memory

 

Only required for OrientDB.

Direct memory is allocated outside of and distinct from heap memory. A max value must be configured if using OrientDB.

#### Host Physical Memory

The total memory allocated to the entire operating system or virtual hardware, commonly referred to as RAM.

### Memory Requirements

The requirements assume there are no other significant memory hungry processes running on the same host.

|                     | JVM Heap | JVM Direct                               | Host Physical/RAM |
| ------------------- | -------- | ---------------------------------------- | ----------------- |
| Minimum ( default ) | 2703MB   | 2703MB                                   | 8GB               |
| Maximum             | 4GB      | (host physical/RAM * 2/3) - JVM max heap | *no limit*        |

### General Memory Guidelines

- minimum physical/RAM memory on the host 8GB
- minimum heap ( -Xms ) must equal set maximum heap ( -Xmx )
- minimum heap size 2703MB
- maximum heap size <= 4GB
- minimum direct memory ( -XX:MaxDirectMemorySize ) size 2703MB
- minimum unallocated host physical/RAM memory should be no less than 1/3 of total physical RAM to allow for virtual memory swap
- max heap + max direct memory <= host physical/RAM * 2/3

### Instance Memory Sizing Profiles

These profiles help gauge the typical physical memory requirements needed for a dedicated server host running repository manager. *Due to the inherent complexities of use cases, one size does not fit all and this should only be interpreted as a guideline.*

| Profile Use Case                                             | Physical/RAM Memory |
| ------------------------------------------------------------ | ------------------- |
| small, personalrepositories < 20 total blobstore size < 20GB single repository format type | 8GB minimum         |
| medium, teamrepositories < 50 total blobstore size < 200GB a few repository formats | 16GB                |
| large, enterpriserepositories > 50 total blobstore size > 200GB diverse set of repository formats | 32GB+               |

### Example Maximum Memory Configurations 

 

`-XX:MaxDirectMemorySize` is for **OrientDB only**

| Physical/RAM Memory | Example Maximum Memory Configuration                      |
| ------------------- | --------------------------------------------------------- |
| 8GB                 | `-Xms2703M -Xmx2703M -XX:MaxDirectMemorySize=2703M `      |
| 12GB                | `-Xms4G -Xmx4G -XX:MaxDirectMemorySize=4014M `            |
| 16GB                | `-Xms4G -Xmx4G -XX:MaxDirectMemorySize=6717M `            |
| 32GB                | `-Xms6G -Xmx6G -XX:MaxDirectMemorySize=15530M `           |
| 64GB                | `-Xms8G -Xmx8G-XX:+UseG1GC-XX:MaxDirectMemorySize=35158M` |

### Advanced Database Memory Tuning

Refer to another article which outlines [additional memory tuning procedures](https://support.sonatype.com/hc/en-us/articles/115007093447).

## Database Limitations

 

We highly recommend that you have your Nexus Repository 3 instance use an external PostgreSQL database. See our [documentation on configuring Nexus Repository Pro for an external PostgreSQL database](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-nexus-repository-pro-for-h2-or-postgresql#ConfiguringNexusRepositoryProforH2orPostgreSQL-ConfiguringforExternalPostgreSQL) or on [migrating an existing Nexus Repository 3 instance to a PostgreSQL database](https://help.sonatype.com/repomanager3/installation-and-upgrades/migrating-to-a-new-database).

 

We strongly recommend against running Nexus Repository 3 on an embedded database within container orchestration environments such as Kubernetes. Doing so can lead to severe data corruption.

Ongoing performance analysis has shown that OrientDB and H2 databases are  unsuitable for use beyond the limits below. If you need to exceed the  below limits, you should migrate to a [PostgreSQL database](https://help.sonatype.com/repomanager3/planning-your-implementation/database-options).

### H2

For H2, container-based deployments are not supported. H2 supported  workload limitation is 20K requests per day and 100K components;  workload beyond these limitations is not supported.

### OrientDB

For OrientDB, we also recommend not exceeding 20K requests per day and 100K components.

## Temporary Directory

The [temporary directory](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-the-runtime-environment#ConfiguringtheRuntimeEnvironment-ConfiguringtheTemporaryDirectory) at `    $data-dir/tmp  ` must not be mounted with `noexec` or repository manager startup will fail with `java.lang.UnsatisfiedLinkError`   message of  `failed to map segment from shared object: Operation not permitted` .

## Disk Space

Application Directory - The size of this directory varies slightly each release. It currently around 330 MB. It is normal to have multiple application  directories installed on the same host over time as repository manager  is upgraded.

Data Directory - On first start, repository manager  creates the base files needed to operate. The bulk of disk space will be held by your deployed and proxied artifacts, as well as any search  indexes. This is highly installation specific, and will be dependent on  the repository formats used, the number of artifacts stored, the size of your teams and projects, etc.  It's best to plan for a lot though,  formats like Docker and Maven can use very large amounts of storage  (500Gb easily). When available disk space drops below 4GB the database will switch to read-only mode.

## File Systems

Nexus Repository stores multiple kinds of data, with two primary storage requirements:

1. **Embedded data** (H2, OrientDB, Elastic Search) requires very responsive, fast storage, ideally local disk
2. **Blob storage** (component binaries), which requires moderately responsive, high-capacity storage

File system selection should be made bearing both of these in mind.

| File System            |  Embedded data   |                         Blob Stores                          | Comment                                                      |
| ---------------------- | :--------------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
| **Local storage**      |  **Supported**   |                        **Supported**                         | Local storage is a good choice for both embedded data and binary storage. |
| **NFS v4**             | Not Recommended* |                        **Supported**                         | Most common protocol for network attached storage among Nexus Repository deployments. |
| **Amazon EBS**         |  **Supported**   |                        **Supported**                         | EBS is a viable choice for both embedded data and binary storage. |
| **Amazon EFS**         |   Unsupported    |                       **Supported\****                       | EFS isn't sufficiently responsive for embedded data, but is appropriate for binary storage. |
| **Amazon S3**          |       N/A        |                        **Supported**                         | S3 semantics aren't suitable for embedded data, but S3 is popular for binary storage. |
| **SMB, CIFS**          |   Unsupported    |                        **Supported**                         | Problems are common with SMB or CIFS-mounted devices for embedded data. |
| **Azure Blob Storage** |       N/A        |                        **Supported**                         | Available for blob storage from Nexus 3.30.0 Pro. For performance reasons, the  Azure blob store should be in the same Azure region as the Nexus Repo  installation. |
| **Azure Files**        |   Unsupported    |                        **Supported**                         | Issues with file handles have been observed when accessing embedded data over SMB. |
| **S3-Compatible**      |   Unsupported    | Some S3-compatible object stores do not support all the features required by Nexus Repository or have subtle compatibility issues with the AWS Java  SDK that NXRM uses. This includes providers such as Ceph S3. |                                                              |
| **NFS v3**             |   Unsupported    | Numerous customers have experienced inadequate performance with NFS v3. |                                                              |
| **GlusterFS**          |   Unsupported    |    Split-brain problems and slow performance are common.     |                                                              |
| **FUSE**               |   Unsupported    | FUSE based user-space filesystems are known to be unreliable for Nexus Repository. |                                                              |

** NFSv4.1 or higher can be used for the work directory in small lightly  loaded installations, but we have found that it does not provide  sufficient performance for anything larger. In general it should be  avoided for the work directory.*

*** EFS binary storage may not provide necessary throughput for heavy workloads in all configurations.*

### File System Optimization

We also have some [optimization suggestions to use at your discretion](https://support.sonatype.com/hc/en-us/articles/213465258-Optimizing-Nexus-Disk-IO-Performance). Also consider the `noatime` option for your Nexus Repository work directory mounts and limit the symbolic  links used as this will cause increased overhead whenever paths need to  be resolved to an absolute file path.

## Web Browser

Our general policy is to support the most recent modern browser version for your supported OS at time of NXRM release date.

| Vendor    | Browser           | Versions                                                     |
| --------- | ----------------- | ------------------------------------------------------------ |
| Google    | Chrome            | latest at NXRM release                                       |
| Mozilla   | Firefox           | latest and  [ESR](https://www.mozilla.org/en-US/firefox/organizations/)  at NXRM release |
| Apple     | Safari            | latest at NXRM release                                       |
| Microsoft | Edge              | latest at NXRM release                                       |
| Microsoft | Internet Explorer | No longer supported                                          |

# System Requirements for H2 and PostgreSQL Databases

 

**Only available in Nexus Repository Pro. Interested in a free trial? [Start here](https://www.sonatype.com/products/repository-pro/trial).**



When using a new database mode, the Nexus Repository Manager system requirements are the same as described in Nexus Repository [System Requirements](https://help.sonatype.com/repomanager3/product-information/system-requirements). Notably, only Java 8 is supported at this time regardless of deployment type chosen. 

## PostgreSQL

If you are planning to use external PostgreSQL mode, you will need a  compatible version of PostgreSQL, such as one of the following:

- Amazon RDS instance
- Amazon Aurora PostgreSQL
- Azure PostgreSQL Single Server (At this time, we do not recommend using Flexible Server, which is still in preview)
- An instance of PostgreSQL that you provide

**Nexus Repository requires version 11.9 or newer of PostgreSQL.**

 

PostgreSQL 15+ no longer allows global write privileges to public. If using  PostgreSQL 15+, you must explicitly grant permissions to users in order  for them to have permission to create something inside of the public  schema.

For best performance, Nexus Repository should have a low latency connection to the database. If you are using a cloud database, then the Nexus Repository node should also be run in the  cloud in the same region.

### PostgreSQL Database User Permissions

The Nexus Repository database user requires CREATE and USAGE permission on the specified schema, which also needs to be on the search_path for that user. Nexus Repository will then create and manage its own tables, making it the owner of those  objects; so, you do not need to specify any extra permissions.

### Trigram Module Requirement for PostgreSQL Database  

NEW IN 3.44.0

 Nexus Repository 3 instances using PostgreSQL databases must have the [pg_trgm (trigram) module](https://www.postgresql.org/docs/current/pgtrgm.html), which we will eventually be leveraging for search performance  improvements. This module may not be installed with PostgreSQL by  default on all Linux distributions, which will result in an exception  when attempting to upgrade. If you find yourself in this situation, you  will need to install the postgresql-contrib package available from your  Linux distribution. In order to install it, the PostgreSQL user must have CREATE privileges on the current database. This can be granted using a command like the following.

```
grant create on database <database_name> to <database_user>;
```

 After granting CREATE privileges for your database, you will then need to  install the postgresql-contrib package available from your Linux  distribution. For example, you might use the following for Fedora:

```
sudo dnf install postgresql-contrib
```

Once installed, execute the following command to create the extension:

```
create extension pg_trgm;
```

Once that is done, you should see the extension listed:

```
select * from pg_extension;
```

See our [knowledge base article](https://support.sonatype.com/hc/en-us/articles/12607156575507-Enhanced-search-performance-with-Postgres-for-3-44-0-) for further information.

## H2 (Embedded)

There are no additional requirements to run using the embedded database;  however, it is still necessary to store the database files on a local  disk.

## Supported File System

Ensure you are using a supported file system as documented in the [Nexus Repository System Requirements](https://help.sonatype.com/repomanager3/product-information/system-requirements#SystemRequirements-FileSystems).

# Repository Manager Pro Features

 

 

 

**Only available in Nexus Repository Pro. Interested in a free trial? [Start here](https://www.sonatype.com/products/repository-pro/trial).**

## Atlassian Crowd Support

Atlassian Crowd is a single sign-on and identity management product that many  organizations use to consolidate user accounts and control which users  and groups have access to which applications. Atlassian Crowd support is a feature preinstalled and ready to configure in Nexus Repository Pro.  Nexus Repository contains a security realm that allows you to configure  the repository manager to authenticate against an Atlassian Crowd  instance.

## Staging and Build Promotion

In modern software development, it is imperative to thoroughly test  software before it is deployed to a production system or externally  accessible repository. Mostly commonly a release candidate will first be deployed to a staging system which is a close copy of the production  system so it can undergo a series of rigorous tests before a decision is made to promote it to production or return it to development.

The staging functionality in Nexus Repository Pro supports promotion of  software components matching your organization's software  development life cycle phases by moving those components between  repositories. This allows the creation of isolated release candidates  that can be discarded or promoted to make it possible to support the  decisions that go into certifying a release.

### Typical Staging Workflow

![img](https://help.sonatype.com/repomanager3/files/16355986/16355991/5/1676638313465/simple-staging-workflow-cp.png)



## Tagging

Tagging provides the ability to mark a set of components with a tag so they can be logically associated to each other. The usage of the tags is up to  you but the most common scenarios would be a CI build ID for a project  (e.g. project-abc-build-142) or a higher level release train when you  are coordinating multiple projects together as a single unit (e.g.  release-train-13). Tagging is used extensively by the [Staging](https://help.sonatype.com/repomanager3/nexus-repository-administration/staging) feature.

## User Token Support

When using Apache Maven with Nexus Repository Pro, the user credentials for  accessing the repository manager have to be stored in the user’s `settings.xml` file. Like a `pom.xml` your `settings.xml` is file that contains your user preferences. The Maven framework has the ability to encrypt passwords within the `settings.xml`, but the need for it to be reversible in order to be used limits its security. 

Other build systems use similar approaches and can benefit from the usage of  user tokens as well. Nexus Repository Pro’s user token feature  establishes a two-part token for the user. Usage of the token acts as a  substitute method for authentication that would normally require passing your username and password in plain text.

This is especially  useful for scenarios where single sign-on solutions like LDAP are used  for authentication against the repository manager and other systems and  the plain text username and password cannot be stored in the `settings.xml` following security policies. In this scenario the generated user tokens can be used instead.

## External PostgreSQL Database Option

Nexus Repository Pro allows you to use an external PostgreSQL database. By  externalizing your database, you can take advantage of a number of  benefits:

- Performance and scalability improvements
- Leverage the benefits of managed, fault-tolerant cloud databases (e.g., AWS Aurora, RDS, and Azure)
- Improved compatibility with container orchestration (e.g., Kubernetes and OpenShift)
- Full availability for writes during backups
- Fault-tolerant cloud deployments with multi-Availability Zone cloud deployment models
- Simpler and easier disaster recovery procedures

## Resilient Architecture Options

We recognize that Nexus Repository is often mission-critical for your business. Be ready for the unexpected by using a [resilient deployment option](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability) that can protect your business and your data in the event of disaster or outages. With comprehensive [backup and restoration tasks](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore) and a growing list of deployment architecture examples and guidance, Nexus Repsitory Pro can help you reduce your Recovery Time Objectives and Recovery Point Objectives.

## Repository Health Check (RHC)

Nexus Repository users can now automatically identify open source security  risks at the earliest stages of their DevOps pipeline. Specifically, the [RHC feature](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/repository-health-check) empowers software development teams into important capabilities:

- Prioritizes the list of vulnerable components by severity and impact, detailing how many times each component was downloaded from the repository manager by developers in the past 30 days.
- Provides actionable guidance on which components housed in the repository manager should be upgraded or replaced.

### Example Health Check Overview

![Example health check screenshot comprising three sections. The top section tells the name of the repository, when the check was performed, and how old the check is. It also tells you the total number of identified components. The main section below this breaks out security vulnerabilities by number (number of critical, severe, and moderate) as well as the number of different license warnings (e.g., copyleft, non-standard, not provided). The right-most section suggests trying Nexus Firewall.](https://help.sonatype.com/repomanager3/files/16355986/56232997/1/1597964544698/Screen+Shot+2020-08-20+at+7.00.13+PM.png)

## Customer Success

Sonatype significantly invests in each customer through our Customer Success  team. This team is unique in that they will proactively and continuously collaborate with you to ensure you derive ongoing value from your  Sonatype investment.

To start, they always recommend customers  invest in workshops and training, which they deliver. These accelerate  time-to-value and provide a solid foundation for long-term success.  What's more, at no additional cost, they will coach you from the  beginning. They will create and collaborate with you on a joint roll-out plan. They will provide you with best-practices, curate relevant  materials for you, help you with designing architecture, and continue to monitor your progress towards your goals. And they will do this  throughout your lifecycle as a Sonatype customer.

## Enterprise Support

Nexus Repository Pro users also get access to our World Class Support  organization. This highly skilled group will work with you to resolve  issues encountered while using the product. Detailed support information can be found at https://www.sonatype.com/usage/software-support-policy.

## Group Blob Stores

A group blob store combines multiple blob stores together so that they act as a single blob store for repositories.  Members of the group can be added or removed. Fill Policies are  selectable algorithms that determine to which group member a blob is  written. These features significantly increase the flexibility customers have in using and upgrading their storage. More information can be  found in the [Storage Guide](https://help.sonatype.com/repomanager3/planning-your-implementation/storage-guide).

### Example Group Blob Store

![Diagram illustrating a group blob store. Two repositories outside of the group blob store box feed into one fill policy within the group blob store. This fill policy fills into an S3 blob store and a file blob store, both contained within the group blob store.](https://help.sonatype.com/repomanager3/files/16355986/34537770/2/1676638313397/group-diagram.png)



## SAML Authentication and Single Sign-On

Nexus Repository Pro integrates with SAML Identity Providers to allow  identity, authentication, and authorization to be managed centrally.  Using SAML, Nexus Repository acts as a service provider which receives users' authentication and  authorization information from external Identity Providers.  Administrators can manage users and roles in one place, and users can  sign in with Single Sign-On credentials.

## Deployment to Group Repositories

Group deployment allows developers and CI servers to use a single URL, the  URL of a group repository, to both push and pull content.

Without  this feature developers have to use two URLs; one for pushing content,  one for pulling content. For some formats, these URLs can't be saved to  configuration and have to be manually entered.

When a group  repository is being configured an administrator will be able to select a hosted repository that the group can delegate push requests to. When  content is pushed the group repository will automatically route that  content to the delegated hosted repository.

### Docker

Docker format has several important advantages related to the Group Deployment Feature.

1. **Reduced ports** - Docker uses ports rather than the repository URL. This means that  each repository, that needs to be accessible from the Docker client,  must open up at least one port (2 ports if HTTP and HTTPS are used). 
2. **Reduced storage** - Docker images are made up of multiple layers. Some of those layers  will be from public remotes and some will be internally created. When an image is pushed to a repository the Docker client checks to see which  layers are already stored and skips those. Before group deployment  developers would push to a hosted repository and the Docker client would not find the public layers and would push those as well. With group  deployment that will no longer happen and only proprietary layers will  be pushed.
3. **Simplified client configuration** -  Docker doesn't provide a way to specify different endpoints for pushing  and pulling content. A developer or someone creating a CI build would  have to remember them. Also because Docker relies on ports it is hard to remember which port relates to which repository and these need to be  looked up.
4. **Simplified reverse proxy configuration** - Most Docker users have a reverse proxy between the client and the  server. Reducing the number of repositories that need to be accessible  makes this setup easier to maintain.

## Import/Export

Import/Export provides the capability to copy components between repositories or Nexus Repository instances.

Some common use cases to utilize Import/Export include:

1. Gradual migration of content from Nexus Repository 2
2. Consolidation of Nexus Repository 3 instances
3. Transfer components between disconnected Nexus Repository instances

These tasks keep track of the last run results and if run again, they will  skip files that were processed previously. These tasks work with HA-C  setup. 
A temporary file system location is needed for these tasks to export to and import  from. Please also ensure the Nexus Repository instance running the task  has sufficient disk space for export and blob storage for import before  running this task to avoid disk full issues.

## Change Repository Blob Store

Change repository blob store is a task that allows changing the blob store of a given repository. It moves the blobs from the chosen repository to a  different blob store.

Some common use cases to utilize the Change Repository Blob Store task:

1. A blob store reaching maximum capacity. You could use this task to move  repository content freeing up space for other repositories in the  original blob store.
2. Decommissioning of a blob store.

## Azure Blob Store

The Azure Blob Store allows assets to be stored in an Azure Storage Account container.  Nexus Repository Pro can take advantage of the storage  features that Azure provides such as replication, configurable  performance profiles and access control when running from within the  Azure cloud.



# Repository Manager Feature Matrix

The table below outlines Nexus Repository features as well as which versions (2 or 3) and solutions (OSS or Pro license) you  need to access these features.

Whether you're looking to upgrade,  curious about a specific format, or just simply want a bird's-eye view  of features, check back often as we update this table whenever new  features are released.

If you are an existing Nexus Repository 2  user and are looking to understand the equivalent feature in Nexus  Repository 3, you can also check out our [Nexus Repository 2 to 3 Feature Equivalency topic](https://help.sonatype.com/repomanager3/installation-and-upgrades/upgrading-from-nexus-repository-manager-2/nexus-repository-2-vs.-nexus-repository-3-feature-equivalency-matrix). 

**Accessibility Note**: Unsupported items are represented with a red x "(error)" icon; supported items are represented with a green "(tick)" icon.

| Feature                                                      |                            2 OSS                             |                            2 Pro                             |                            3 OSS                             |                            3 Pro                             |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| **Formats**                                                  |                                                              |                                                              |                                                              |                                                              |
| [Bower](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/bower-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Docker](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Git Large File Storage (LFS)](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/git-lfs-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Go Modules](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/go-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Maven](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/maven-repositories) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [npm](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/npm-registry) |                           Limited                            |                           Limited                            | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [APT](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/apt-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [R](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/r-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [CocoaPods](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/cocoapods-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Conda](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/conda-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Conan](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/conan-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Helm](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/helm-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [NuGet](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/nuget-repositories) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| OBR                                                          | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | [See Below](https://help.sonatype.com/repomanager3/product-information/repository-manager-feature-matrix#RepositoryManagerFeatureMatrix-OBR) | [See Below](https://help.sonatype.com/repomanager3/product-information/repository-manager-feature-matrix#RepositoryManagerFeatureMatrix-OBR) |
| [p2](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/p2-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [PyPI](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/pypi-repositories) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [RubyGems](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/rubygems-repositories) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Yum](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/yum-repositories) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| **Features**                                                 |                                                              |                                                              |                                                              |                                                              |
| [Change Repository Blob Store](https://help.sonatype.com/repomanager3/nexus-repository-administration/tasks/change-repository-blob-store) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Component Search](https://help.sonatype.com/repomanager3/using-nexus-repository/searching-for-components) |                           Limited                            | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Crowd](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/atlassian-crowd-support) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Custom Access Controls](https://help.sonatype.com/repomanager3/nexus-repository-administration/access-control) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| Custom Metadata                                              | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | Replaced with [Component Tagging](https://help.sonatype.com/repomanager3/nexus-repository-administration/tagging) |
| [Deployment to Group Repositories](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry/pushing-images-to-a-group-repository) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Docker Subdomain Connector](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry/docker-subdomain-connector) NEW IN 3.40.0 | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [LDAP](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/ldap) |                           Limited                            | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Export Assets](https://help.sonatype.com/repomanager3/nexus-repository-administration/tasks/repository-export) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [External PostgreSQL Support](https://help.sonatype.com/repomanager3/planning-your-implementation/database-options) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Gradle](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/maven-repositories) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Import External FIles](https://help.sonatype.com/repomanager3/nexus-repository-administration/tasks/repository-import) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Improved Backup & Restore](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| Open Source Integration                                      | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| Plugins                                                      | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | [See Below](https://help.sonatype.com/repomanager3/product-information/repository-manager-feature-matrix#RepositoryManagerFeatureMatrix-Plugins) | [See Below](https://help.sonatype.com/repomanager3/product-information/repository-manager-feature-matrix#RepositoryManagerFeatureMatrix-Plugins) |
| Provisioning API                                             | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Remote Auth Token (RUT)](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/authentication-via-remote-user-token) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Repository Health Check](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/repository-health-check) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Repository Health Check Detailed Report](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/repository-health-check#RepositoryHealthCheck-DetailedRepositoryHealthCheckReport) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Repository Replication (Legacy)](https://help.sonatype.com/repomanager3/product-information/repository-manager-feature-matrix#) NEW IN 3.34.0 | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | Limited - [Smart Proxy](https://help.sonatype.com/repomanager2/smart-proxy) Only | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| Repo Targets / [Content Selectors](https://help.sonatype.com/repomanager3/nexus-repository-administration/access-control/content-selectors) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Resilient Deployment Options](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability) NEW IN 3.34.0 | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | [Legacy High Availability](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)) Only | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [REST](https://help.sonatype.com/repomanager3/integrations/rest-and-integration-api) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [SAML](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/saml) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Staging & Build Promotion](https://help.sonatype.com/repomanager3/nexus-repository-administration/staging) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| Support                                                      |      [Community](https://community.sonatype.com/) Only       | Enterprise-level and [Community](https://community.sonatype.com/) |      [Community](https://community.sonatype.com/) Only       | Enterprise-level and [Community](https://community.sonatype.com/) |
| Unlimited Deployment                                         | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Upgrading 2x to 3x](https://help.sonatype.com/repomanager3/installation-and-upgrades/upgrading-from-nexus-repository-manager-2) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [Upload Third-Party Artifacts via UI](https://help.sonatype.com/repomanager3/using-nexus-repository/uploading-components) |                           Limited                            |                           Limited                            | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |
| [User Token Support](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/security-setup-with-user-tokens) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) | ![(出错)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/error.svg) | ![(滴答)](https://help.sonatype.com/s/-scv42a/8703/51k4y0/_/images/icons/emoticons/check.svg) |

If you have questions about a specific feature, contact [nexus-feedback@sonatype.com](mailto:nexus-feedback@sonatype.com).

## OBR  

We aim to provide support for every format available. However, we also  have to prioritize according to the needs of our customers. If you want  to see OBR supported, check out the Nexus Repository JIRA project and  add a [comment with your support](https://issues.sonatype.org/browse/NEXUS-12349?__hstc=31049440.8ede5f702ac4d1e3e4b219a2c6292ddd.1676943147945.1676943147945.1676947588654.2&__hssc=31049440.8.1676947588654&__hsfp=1931082791).

## Custom Metadata  

The Custom Metadata feature has been replaced with the new [component tagging](https://help.sonatype.com/repomanager3/nexus-repository-administration/tagging) feature.

## Plugins  

There are are number of plugins available for Nexus Repository 3, and more  coming all the time. View the entire list of plugins at [Nexus Exchange](http://exchange.sonatype.com/). If you’ve created a Nexus Repository plugin, reach out to our [Community Advocate](mailto:community@sonatype.com), and we’ll help you through the process of getting it out to other Nexus Repository users.        

# Repository Security Vulnerabilities

Sonatype considers security extremely important and uses multiple processes and tooling to ensure that our products are secure.

## Our Security Practices

To ensure the security of our products, we use a comprehensive application security practice that includes transitive dependency analysis at  multiple points in the SLDC, static analysis of application code, as  well as automated and human review processes for all changes.

### Dependency Vulnerabilities

As with most modern software applications, Nexus Repository incorporates a number of open source components as dependencies. Nexus Lifecycle’s  continuous monitoring capabilities regularly detect vulnerabilities in  these components.

These may or may not be  exploitable, depending upon both the nature of the vulnerability and how the components are used within Nexus Repository. However, we consider  all dependency vulnerabilities to be *potentially exploitable* because of attack techniques such as vulnerability chaining. Therefore, our development teams upgrade the component to a non-vulnerable version as soon as one is available. We make these upgrades available to our  customers and users in later releases of Nexus Repository.

To benefit from this ongoing risk mitigation, we recommend our customers  and users regularly update their Nexus Repository instances to the most  recent version.

### Inquiring About a Dependency Vulnerability's Status

If you have concerns about a dependency vulnerability with unknown  exploitability, we can confirm whether we are aware of it and that it is queued for remediation as part of our normal development process.

For the protection of our customers and users, we do not disclose the  exploitability of suspected vulnerabilities before they are remediated  and an upgraded version of Nexus Repository is released.

You can subscribe to announcements of new releases and exploitable security vulnerabilities by signing up for the [Nexus Repository Pro announcements Google group](https://groups.google.com/a/glists.sonatype.com/g/nexus-pro-announcements).

## Reporting a Security Vulnerability

Sonatype responds to exploitable security vulnerabilities with the utmost  urgency and follows a responsible disclosure and notification process to protect our users and customers.

If you would like  to report a new vulnerability that you have discovered or reproduced,  please follow the steps for reporting a security issue to [security@sonatype.com](mailto:security@sonatype.com) as detailed on our [Contact Us page](https://www.sonatype.com/contactus).

### Related Resources

For more detail, please refer to the following resources:

- [Nexus Repository Security Advisories](https://support.sonatype.com/hc/en-us/sections/203012668-Security-Advisories)
- [Nexus Repository Dependency Security Advisories and Non-Vulnerable Findings](https://help.sonatype.com/repomanager3/release-notes/security-advisories)

# Critical Cleanup Policy Bug Advisory

 

This bug is fixed in release 3.41.1, which is now available. Please [download](https://help.sonatype.com/repomanager3/product-information/download) and upgrade to this or a later release.



Sonatype has discovered a [critical bug](https://issues.sonatype.org/browse/NEXUS-34642?__hstc=31049440.8ede5f702ac4d1e3e4b219a2c6292ddd.1676943147945.1676943147945.1676947588654.2&__hssc=31049440.10.1676947588654&__hsfp=1931082791) that can cause [cleanup policies](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/cleanup-policies) to unintentionally delete binaries in Nexus Repository Pro deployments using H2 or PostgreSQL. 

## Who is Impacted?

Deployments meeting **all** of the following criteria are impacted by this bug:

- Your deployment must be using Nexus Repository **Pro** 3.31.0 to 3.41.0
- Your deployment must have been explicitly migrated to or originally deployed using an **H2** or **PostgreSQL** database
- Cleanup policies must be applied to one or more repositories
- Those cleanup policies must have *Component Age* or *Component Usage* criteria

## What Database am I Using?

If you are unsure what database you are using, take the following steps:

- Check your 

  ```
  $data-dir/etc/nexus.properties
  ```

   for 

  ```
  nexus.datastore.enabled=true
  ```

  - Property **does not exist** - you are using OrientDB and are not impacted by this bug

  - Property 

    does

     exist - you are using either H2 or PostgreSQL and are potentially impacted by this bug

    - If there is also a `$nexus-dir/etc/fabric/nexus-store.properties `file that contains a Postgres JDBC URL, then you are using PostgreSQL
    - If no Postgres JDBC URL exists, but you do have `nexus.datastore.enabled=true` in your `$data-dir/etc/nexus.properties`, then you are using H2

Or

- Log into your Nexus Repository instance as an administrator

- Navigate to 

  Admininistration

   → 

  Repository

   and see if there is a 

  Data Store

   menu item available

  - Menu item **does not exist** - you are using OrientDB and are not impacted by this bug
  - Menu item **does** exist - you are using either H2 or PostgreSQL and are potentially impacted by this bug

## What Should You Do if You are Currently Impacted?

If your deployment meets all of the above criteria, you should immediately take the following actions:

- Disable cleanup policy tasks 
  - Navigate to *Admin* → *System* → *Tasks* 
  - Locate the *Cleanup Service* task in the task list; select this task to open the detailed view
  - Uncheck the *Task enabled* box
- Disable the blob store compact task (if using File or Azure Blob Stores)
  - Navigate to *Admin* → *System* → *Tasks* 
  - Locate the [ *Admin - Compact blob store* ](https://help.sonatype.com/repomanager3/nexus-repository-administration/tasks#Tasks-Admin-Compactblobstore) task; select this task to open to detailed view
  - Uncheck the *Task enabled* box
- Disable S3 Lifecycle Rules (if using S3 Blob Stores, see https://aws.amazon.com/blogs/aws/amazon-s3-lifecycle-management-update/)
- Disable Azure Blob Storage Lifecycle Rules if configured on the Azure side (if using Azure Blob Stores, see https://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview?tabs=azure-portal)
- Preserve backup media that have already been made of your Nexus Repository deployment

## How Can You Prevent Impact if Not Currently Affected?

If your deployment does not meet all of the above criteria, you should take the following actions to prevent impact:

- If not currently using H2 or PostgreSQL, do not migrate at this time
- If you are already using H2 or PostgreSQL and have any cleanup policies,  disable the cleanup policy tasks and blob store cleanup task:
  - Navigate to *Admin* → *System* → *Tasks* 
  - Locate the *Cleanup Service* task and the [ *Admin - Compact blob store* ](https://help.sonatype.com/repomanager3/nexus-repository-administration/tasks#Tasks-Admin-Compactblobstore) task in the task list; select each task to open the detailed view
  - Uncheck the *Task enabled* box for each task
- If you are already using H2 or PostgreSQL, do not create new cleanup policies.

## What is Sonatype Doing to Address this Bug?

We are aggressively working to correct this bug ([NEXUS-34642](https://issues.sonatype.org/browse/NEXUS-34642?__hstc=31049440.8ede5f702ac4d1e3e4b219a2c6292ddd.1676943147945.1676943147945.1676947588654.2&__hssc=31049440.10.1676947588654&__hsfp=1931082791)) and will release a fix in a point release as soon as possible.

To receive an email when the issue is updated, take the following steps:

1. Log into [https://issues.sonatype.org](https://issues.sonatype.org/?__hstc=31049440.8ede5f702ac4d1e3e4b219a2c6292ddd.1676943147945.1676943147945.1676947588654.2&__hssc=31049440.10.1676947588654&__hsfp=1931082791) with your Sonatype support account credentials and navigate to the issue ([NEXUS-34642](https://issues.sonatype.org/browse/NEXUS-34642?__hstc=31049440.8ede5f702ac4d1e3e4b219a2c6292ddd.1676943147945.1676943147945.1676947588654.2&__hssc=31049440.10.1676947588654&__hsfp=1931082791)).
2. Under the *People* section, select the link next to Watchers that reads *Start watching this issue*.

​        

# Planning Your Implementation

Before installing Nexus Repository, you should take  some time to consider your specific needs so that you can plan a robust  implementation. Some specific things to consider include the following:

- Do you need a [resilient deployment](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability)? 
- Will you use an [external PostgreSQL database](https://help.sonatype.com/repomanager3/planning-your-implementation/database-options)?
- What [installation method](https://help.sonatype.com/repomanager3/installation-and-upgrades/installation-methods) will you use?
- Have you considered how to [secure your implementation](https://help.sonatype.com/repomanager3/planning-your-implementation/securing-nexus-repository-manager)?

------

Read about the above topics and much more in this section:

 

- Resiliency and High Availability
  - [Single-Node Cloud Resilient Deployment Example Using AWS](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-aws)
  - [Single-Node Cloud Resilient Deployment Example Using Azure](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-azure)
  - [Single Data Center On-Premises Deployment Example Using Kubernetes](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-data-center-on-premises-deployment-example-using-kubernetes)
  - High Availability Clustering (Legacy)
    - [Configuring Nodes](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-nodes)
    - [Configuring Hazelcast](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast)
    - [Designing your Cluster Backup/Restore Process](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process)
    - [Initial Setup - High Availability Clustering (Legacy)](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/initial-setup---high-availability-clustering-(legacy))
    - Operating your cluster
      - [JMX Lifecycle Operations](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/jmx-lifecycle-operations)
      - [JMX Maintenance Operations](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/jmx-maintenance-operations)
      - [Upgrading your cluster](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/upgrading-your-cluster)
    - [Load Balancing](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/load-balancing)
- [Scaling with Proxy Nodes](https://help.sonatype.com/repomanager3/planning-your-implementation/scaling-with-proxy-nodes)
- Database Options
  - [Feature Availability for PostgreSQL and H2 Databases](https://help.sonatype.com/repomanager3/planning-your-implementation/database-options/feature-availability-for-postgresql-and-h2-databases)
- Storage Guide
  - [Storage Planning](https://help.sonatype.com/repomanager3/planning-your-implementation/storage-guide/storage-planning)
  - [Using Replicated S3 Blob Stores for Recovery or Testing](https://help.sonatype.com/repomanager3/planning-your-implementation/storage-guide/using-replicated-s3-blob-stores-for-recovery-or-testing)
- [Run Behind a Reverse Proxy](https://help.sonatype.com/repomanager3/planning-your-implementation/run-behind-a-reverse-proxy)
- [Securing Nexus Repository Manager](https://help.sonatype.com/repomanager3/planning-your-implementation/securing-nexus-repository-manager)
- [Keeping Disk Usage Low](https://help.sonatype.com/repomanager3/planning-your-implementation/keeping-disk-usage-low)
- Backup and Restore
  - [Configure and Run the Backup Task](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/configure-and-run-the-backup-task)
  - [Prepare a Backup](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/prepare-a-backup)
  - [Restore Exported Databases](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/restore-exported-databases)
  - [Backup and Restore in Amazon Web Services](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/backup-and-restore-in-amazon-web-services)
- [Quick Start Guide - Proxying Maven and NPM](https://help.sonatype.com/repomanager3/planning-your-implementation/quick-start-guide---proxying-maven-and-npm)
- [Staging Concepts](https://help.sonatype.com/repomanager3/planning-your-implementation/staging-concepts)

# Resiliency and High Availability

 

 

 

Already have a Nexus Repository instance and want to migrate to a resilient architecture? See our [migration documentation](https://help.sonatype.com/repomanager3/installation-and-upgrades/migrating-an-existing-nexus-repository-instance-to-a-resiliency-architecture).

## What is Resiliency?

Choosing the appropriate resiliency options to meet your needs should be your primary goal when designing your Nexus Repository architecture. Resiliency refers to the ability to recover from disruptions to  critical processes and supporting technology systems. Disruptions may  include any of the following:

- failure of a single service (the repository node, the external relational database, or the artifact storage)
- a data center outage for the production environment
- an availability zone outage in the case of cloud services

The scope of interruption you are planning to mitigate will determine which architecture you will need to achieve the level of resiliency  required. 

### Backup and Restoration

As you review backup strategies, there are two important terms to remember:

- Recovery Point Objective - the amount of data loss that is acceptable if a restore becomes necessary
- Recovery Time Objective - the length of time required to restore the service 

Your backup plan will need to balance the cost of maintenance with the risk  of potential data loss and disruptions to the  service. Setting requirements for fast recovery with the least risk will increase infrastructure complexity and maintenance cost for achieving those results. You will also need to regularly test the recovery process to ensure that  the process is successful and to provide training for process owners.  Regardless of implementation size, make sure to document your plan and  to keep it up to date with any infrastructure changes.

You can configure your architecture to schedule database exports or use third-party tooling to transfer and back up files from one location to another. 

For OrientDB or H2, Nexus Repository provides [tasks](https://help.sonatype.com/repomanager3/nexus-repository-administration/tasks) to create database snapshots and relocate them to a target disk. Other directories in your local instance (or instances) should also be copied and rebuilt on a backup disk (see [Prepare a Backup](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/prepare-a-backup)).

You will need to back up blob storage outside of the repository service. 

See [Backup and Restore](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore) (for H2 and OrientDB) and [Backup and Restore in Amazon Web Services](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/backup-and-restore-in-amazon-web-services) for further information.

### Library of Patterns  

The matrix below lists various deployment patterns that you might use depending on the level of resiliency you wish to achieve.

 

Note that our content replication feature is not appropriate for disaster  recovery and is not included in our library of patterns. 

| Pattern Name                      | Description                                                  | Use Cases                                                  | Limitations                                                  | Examples*                                                    |
| --------------------------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Single Node with Backup           | Single active node with a cold backup that can be used to recover from a data loss. | Reduce data loss                                           | Manual backupMore downtime on recovery vs. other patternsLess availability than other patterns | [Backup and Restore](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore)[Backup and Restore in Amazon Web Services](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore/backup-and-restore-in-amazon-web-services) |
| Single Node with Dynamic Failover | Single active node in one availability zone. Should a node or availability  zone fail, Kubernetes automatically spins up a second node in either the same or a second availability zone. | Achieve automatic failover Reduce downtimeReduce data loss | More downtime on recovery vs. other patternsLess availability than other patterns | [Single-Node Cloud Deployment Example Using AWS](https://help.sonatype.com/repomanager3/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-aws)[Single-Node Cloud Deployment Example Using Azure](https://help.sonatype.com/repomanager3/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-azure)[Single Data Center On-Premises Deployment Example Using Kubernetes](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-data-center-on-premises-deployment-example-using-kubernetes) |

\* We will continue to update this section with more examples as we validate them.

## What is Legacy High Availability Clustering?

 

High Availability is not yet supported in the newer H2 or PostgreSQL  deployment options. We recommend using one of the resiliency patterns  listed in the [Library of Patterns](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability#ResiliencyandHighAvailability-Patterns) until High Availability for H2 and PostgreSQL is available.

[High Availability Clustering (HA-C)](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)) is a Nexus Repository feature implemented only in OrientDB that is meant to improve uptime by having a cluster of redundant Nexus Repository instances (i.e.,  nodes) within a single data center. This should allow you to maintain  availability to your Nexus Repository if one of the nodes becomes  unavailable. However, if you are using the newer H2 or PostgreSQL database options, you'll want  to take advantage of newer resiliency patterns as outlined in [Library of Patterns](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability#ResiliencyandHighAvailability-Patterns).

# Single-Node Cloud Resilient Deployment Example Using AWS

 

 

 

**Only available in Nexus Repository Pro. Interested in a free trial? [Start here](https://www.sonatype.com/products/repository-pro/trial).**

 

[Helm charts](https://github.com/sonatype/nxrm3-helm-repository/tree/main/nxrm-aws-resiliency) are available for this deployment example. Be sure to read the deployment instructions in the [associated README file](https://github.com/sonatype/nxrm3-helm-repository/blob/main/nxrm-aws-resiliency/README.md) before using these charts.

Already have a Nexus Repository instance and want to migrate to a resilient architecture? See our [migration documentation](https://help.sonatype.com/repomanager3/installation-and-upgrades/migrating-an-existing-nexus-repository-instance-to-a-resiliency-architecture).

We recognize that Nexus Repository is mission-critical to your business. With an Amazon Web Services  (AWS)-based Nexus Repository deployment, you can ensure that your Nexus  Repository instance is available even if disaster strikes. Whether a  single service or an entire data center goes down, you can ensure that  you still have access to Nexus Repository.

This section provides  instructions and explanations for setting up a resilient AWS-based Nexus Repository deployment like the one illustrated below.

 

Similar architecture could be used for other cloud or on-premise deployments with Kubernetes and  file-based or other supported blob storage. If you would like to manage  your own deployment, see [Single Data Center On-Premises Deployment Example Using Kubernetes](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-data-center-on-premises-deployment-example-using-kubernetes).



![Nexus Repository Resiliency AWS Reference Architecture. One active Nexus Repository instance with one failover instance in different availability zones all within one AWS region.](https://help.sonatype.com/repomanager3/files/82215162/87198593/1/1633375844938/Active+diagram_w_o_+EFS+%281%29.png)

## Use Cases

This reference architecture is designed to protect against the following scenarios:

- An AWS Availability Zone (AZ) outage within a single AWS region
- A node/server (i.e., EC2) failure
- A Nexus Repository service failure

You would use this architecture if you fit the following profiles:

- You are a Nexus Repository Pro user looking for a resilient Nexus Repository deployment option in AWS in order to reduce downtime
- You would like to achieve automatic failover and fault tolerance as part of your deployment goals
- You already have an Elastic Kubernetes Service (EKS) cluster set up as part of your deployment pipeline for your other in-house applications and  would like to leverage the same for your Nexus Repository deployment
- You have migrated or set up Nexus Repository with an external PostgreSQL  database and want to fully reap the benefits of an externalized database setup
- You do not need High Availability (HA) active-active mode

## Requirements

In order to set up an environment like the one illustrated above and described in this section, you will need the following:

- A Nexus Repository Pro license
- Nexus Repository 3.33.0 or later
- An AWS account with permissions for accessing the following AWS services:
  - Elastic Kubernetes Service (EKS)
  - Relational Database Service (RDS) for PostgreSQL
  - Application Load Balancer (ALB)
  - CloudWatch
  - Simple Storage Service (S3)
  - Secrets Manager

 

If you require your clients to access more than one Docker Repository, you must use one of the following:

- An external load balancer (e.g., NGINX) as a [reverse proxy](https://help.sonatype.com/nxrm3master/nexus-repository-administration/formats/docker-registry/docker-repository-reverse-proxy-strategies) instead of the provided[ ingress for Docker YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-ingress-for-docker-connector.yaml) 

or

- A [Docker Subdomain Connector](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry/docker-subdomain-connector) with [external DNS](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-aws#SingleNodeCloudResilientDeploymentExampleUsingAWS-dns) to automatically create 'A' records for each Docker subdomain

## Limitations

In this reference architecture, a maximum of one Nexus Repository instance is running at a time. Having more than one Nexus Repository instance replica will not work.

## Setting Up the Architecture

### Step 1 - AWS EKS Cluster

Nexus Repository runs on a single-node AWS EKS cluster spread across two AZs within a single AWS region. After you configure EKS to run only one instance (by setting the min, max, and desired nodes to one), EKS ensures that only one instance of Nexus Repository runs at any one time in the  entire cluster. If something causes the instance or the node to go down, another will be spun up. If an AZ becomes unavailable, AWS spins up a  new node in the secondary AZ with a new pod running Nexus Repository.

Begin by setting up the EKS cluster in the AWS web console. AWS provides instructions for managed nodes (i.e., EC2)[ in their documentation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html). 

Your EKS cluster should have a max node count of one spread across two AZs in an AWS region.

### Step 2 - AWS Aurora PostgreSQL Cluster

An Aurora PostgreSQL cluster containing three databases (one writer node  and two replicas) spread across three AZs in the region where you've  deployed your EKS provides an external database for Nexus Repository  configurations and component metadata.

AWS provides instructions on creating an Aurora database cluster[ in their documentation](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.CreateInstance.html).

### Step 3 - AWS Load Balancer Controller

The AWS Load Balancer Controller allows you to provision an AWS ALB via an  Ingress type specified in your Kubernetes deployment YAML file. This  load balancer, which is provisioned in one of the public subnets  specified when you create the cluster, allows you to reach the Nexus  Repository pod from outside the EKS cluster. This is necessary because  the nodes on which EKS runs the Nexus Repository pod are in private  subnets and are otherwise unreachable from outside the EKS cluster.

Follow the [AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html) to deploy the AWS LBC to your EKS cluster.

### Step 4 - Kubernetes Namespace

A namespace allows you to isolate groups of resources in a single  cluster. Resource names must be unique within a namespace, but not  across namespaces. See the [ Kubernetes documentation ](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)about namespaces for more information.

To create a namespace, use a command like the one below with the kubectl command-line tool:

```
kubectl create namespace <namespace>
```

### Step 5 - AWS Secrets Manager  

AWS Secrets Manager stores your Nexus Repository Pro license as well as the database username, password, and host address. In the event of a failover, Secrets Manager can retrieve the license  when the new Nexus Repository container starts. This way, your Nexus  Repository always starts in Pro mode.

Use the [AWS Secrets Store CSI drivers](https://github.com/aws/secrets-store-csi-driver-provider-aws/blob/main/README.md) to mount the license secret, which is stored in AWS Secrets Manager, as a volume in the pod running Nexus Repository.

Include the --syncSecret.enabled=true flag when running the helm command for installing the Secret Store CSI  Driver. This will ensure that secrets are automatically synced from AWS  Secrets Manager into the Kubernetes secrets specified in the [secrets YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-secrets.yaml). 

Note that only the AWS CLI can support storing a binary license file. AWS [provides documentation](https://docs.aws.amazon.com/cli/latest/reference/secretsmanager/put-secret-value.html) for using a `--secret-binary` argument in the CLI.

The command will look as follows:

```
aws secretsmanager create-secret --name supersecretlicense --secret-binary fileb://super-secret-license-file.lic --region <region>
```

This will return a response such as this:

```
{
    "VersionId": "4cd22597-f0a9-481c-8ccd-683a5210eb2b",
    "Name": "supersecretlicense",
    "ARN": "arn:aws:secretsmanager:<region>:<account id>:secret:supersecretlicense-abcdef"
}
```

You will put the `ARN` value in the [secrets YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-secrets.yaml).

 

If updating the license (e.g., when renewing your license and receiving a  new license binary), you'll need to restart the Nexus Repository pod  after uploading the license to the AWS Secrets Manager. The AWS CLI  command for updating a secret is `put-secret-value`.

### Step 6 - AWS CloudWatch (Optional, but Recommended)  

When running Nexus Repository on Kubernetes, it is possible for it to run on different nodes in different AZs over the course of the same day. In  order to be able to access Nexus Repository's logs from nodes in all  AZs, you must externalize your logs. We recommend that you externalize  your logs to CloudWatch for this architecture; however, if you choose  not to use CloudWatch, you must externalize your logs to another  external log aggregator to make sure that the Nexus Repository logs survive node crashes or when pods are  scheduled on different nodes as is possible when running Nexus on  Kubernetes. Follow [AWS documentation ](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html)to set up Fluentbit for gathering logs from your EKS cluster.

When first installed, Nexus Repository sends task logs to separate files.  Those files do not exist at startup and only exist as the tasks are  being run. In order to facilitate sending logs to CloudWatch, you need  the log file to exist when Nexus Repository starts up. The [nxrm-logback-tasklogfile-override.yaml](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-nxrm-logback-tasklogfile-override.yaml) available in our sample files GitHub repository sets this up.

Once Fluentbit is set up and running on your EKS cluster, apply the [fluent-bit.yaml](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-fluent-bit.yaml) (example available in our sample files GitHub) to configure it to  stream Nexus Repository's logs to CloudWatch. The specified Fluentbit  YAML sends the logs to CloudWatch log streams within a `nexus-logs` log group.

[AWS also provides documentation](https://docs.aws.amazon.com/cloudwatch/index.html) for setting up and using CloudWatch.

### Step 7 - External DNS (Optional)  

If you are using or wish to use our [Docker Subdomain Connector](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry/docker-subdomain-connector) feature, you will need to use [external-dns](https://github.com/kubernetes-sigs/external-dns) to create 'A' records in AWS Route 53.

You must meet all Docker Subdomain Connector feature requirements, and you must specify an HTTPS certificate ARN in [Ingress YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-ingress-for-docker-connector.yaml).

#### Permissions

You must first ensure you have appropriate permissions. To grant these  permissions, open a terminal that has connectivity to your EKS cluster  and run the following commands:

```
cat <<'EOF' >> external-dns-r53-policy.json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "route53:ChangeResourceRecordSets"
      ],
      "Resource": [
        "arn:aws:route53:::hostedzone/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "route53:ListHostedZones",
        "route53:ListResourceRecordSets"
      ],
      "Resource": [
        "*"
      ]
    }
  ]
}
EOF


aws iam create-policy --policy-name "AllowExternalDNSUpdates" --policy-document file://external-dns-r53-policy.json


POLICY_ARN=$(aws iam list-policies --query 'Policies[?PolicyName==`AllowExternalDNSUpdates`].Arn' --output text)


EKS_CLUSTER_NAME=<Your EKS Cluster Name>


aws eks describe-cluster --name $EKS_CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text


eksctl utils associate-iam-oidc-provider --cluster $EKS_CLUSTER_NAME --approve

ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
OIDC_PROVIDER=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text | sed -e 's|^https://||')
```

 

Note: The value you assign to the `EXTERNALDNS_NS` variable below should be the same as the one you specify in your values.yaml for `namespaces.externaldnsNs`

```
EXTERNALDNS_NS=nexus-externaldns

cat <<-EOF > externaldns-trust.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::$ACCOUNT_ID:oidc-provider/$OIDC_PROVIDER"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "$OIDC_PROVIDER:sub": "system:serviceaccount:${EXTERNALDNS_NS}:external-dns",
                    "$OIDC_PROVIDER:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}
EOF

IRSA_ROLE="nexusrepo-external-dns-irsa-role"
aws iam create-role --role-name $IRSA_ROLE --assume-role-policy-document file://externaldns-trust.json 
aws iam attach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN

ROLE_ARN=$(aws iam get-role --role-name $IRSA_ROLE --query Role.Arn --output text)
echo $ROLE_ARN
```

Take note of the `ROLE_ARN` output last above and specify it in your values.yaml for` serviceAccount.externaldns.role.`

#### External DNS YAML

After running the permissions above, run the [external-dns.yaml](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-external-dns-rbac.yml).

Then, in the [ingress.yaml](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-ingress-for-docker-connector.yaml), specify the Docker subdomains you want to use for your Docker repositories.

**Step 8 - Local Persistent Volume and Local Persistent Volume Claim**

Using a local persistent volume allows Elasticsearch indexes to survive pod  restarts on a particular node. When used in conjunction with the `NEXUS_SEARCH_INDEX_REBUILD_ON_STARTUP` flag, this ensures that the Elasticsearch index is only rebuilt if it is  empty when the Nexus Repository pod starts. This means that the only  time the Elasticsearch index is rebuilt is the first time that a Nexus  Repository pod starts on a node.

Storage space will be allocated to a local persistent volume from your root EBS volume (i.e., the EBS volume attached to the provisioned node).  Therefore, you must ensure that the size you specify for your EKS node's root EBS volume is sufficient for your usage. Ensure that the size of  the root EBS volume is bigger than that specified in the local  persistent volume claim so that there's some spare storage capacity on  the node. For example, for a local persistent volume claim size of 100  Gigabytes, you could make the actual size of the root EBS volume 120  Gigabytes.

See the sample [storage class YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-storage-class.yaml), [local persistent volume YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-local-persistent-volume.yaml), and [local persistent volume claim YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-local-persistent-volume-claim.yaml) examples in our sample files GitHub repository.

### Step 9 - AWS S3

Located in the same region as your EKS deployment, AWS S3 provides your object (blob) storage. AWS provides detailed documentation for S3[ on their website](https://docs.aws.amazon.com/s3/index.html).

## Sample Kubernetes YAML Files

 

The YAML files linked in this section are just examples and cannot be used  as-is. You must fill them out with the appropriate information for your  deployment to be able to use them.

You can use the[ sample AWS resiliency YAML files ](https://github.com/sonatype/nxrm-sample-files-repo/tree/main/aws-resiliency-yamls)from our sample files GitHub repository to help set up the YAMLs you will  need for a resilient deployment. Note that these are diferent from the [AWS resiliency Helm charts](https://github.com/sonatype/nxrm3-helm-repository/tree/main/nxrm-aws-resiliency).

Before creating and running the YAML files linked below, you must create a  namespace. To create a namespace, use a command like the one below:

```
kubectl create namespace <namespace>
```

Then, **you must run your YAML files in the order below**:

1. [Storage Class YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-storage-class.yaml)
2. [Secrets YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-secrets.yaml) as mentioned in [Secrets Manager setup](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-aws#SingleNodeCloudResilientDeploymentExampleUsingAWS-secretsmana)
3. [Fluent-bit Setup](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-fluent-bit.yaml) as mentioned in the [CloudWatch section](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-aws#SingleNodeCloudResilientDeploymentExampleUsingAWS-CloudWatch)
4. [nxrm-logback-tasklogfile-override YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-nxrm-logback-tasklogfile-override.yaml)
5. [External DNS YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-external-dns-rbac.yml)
6. [Local Persistent Volume YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-local-persistent-volume.yaml)
7. [Local Persistent Volume Claim YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-local-persistent-volume-claim.yaml)
8. [Deployment YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-nxrm-deployment.yaml)
9. Services YAML
   1. [Ingress for Docker (Optional)](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-ingress-for-docker-connector.yaml)
   2. [Nodeport for Docker (Optional)](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/aws-resiliency-yamls/aws-resiliency-nodeport-for-docker-connector.yaml)

 

The resources created by these YAMLs are not in the default namespace.

# Single-Node Cloud Resilient Deployment Example Using Azure

 

 

 

**Only available in Nexus Repository Pro. Interested in a free trial? [Start here](https://www.sonatype.com/products/repository-pro/trial).**

 

Already have a Nexus Repository instance and want to migrate to a resilient architecture? See our [migration documentation](https://help.sonatype.com/repomanager3/installation-and-upgrades/migrating-an-existing-nexus-repository-instance-to-a-resiliency-architecture).

You never know when disaster may strike. With a resilient deployment on  Azure like the one outlined below, you can ensure that you still have  access to Nexus Repository in the event of service or data center  outage. 

 

Similar architecture could be used for other cloud or on-premise deployments with Kubernetes and  file-based or other supported blob storage. If you would like to manage  your own deployment, see [Single Data Center On-Premises Deployment Example Using Kubernetes](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-data-center-on-premises-deployment-example-using-kubernetes). If you prefer to use AWS, see [Single-Node Cloud Resilient Deployment Using AWS](https://help.sonatype.com/repomanager3/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-aws).

![Nexus Repository Resiliency Azure Reference Architecture. One active Nexus Repository instance with one failover instance in different availability zones all within one Azure region.](https://help.sonatype.com/repomanager3/files/90833420/90833423/1/1634576291054/NXRM_Azure_Resiliency+%284%29.png)

## Use Cases

This reference architecture is designed to protect against the following scenarios:

- An Azure Availability Zone (AZ) outage within a single region
- A node/server failure
- A Nexus Repository service failure

You would use this architecture if you fit the following profiles:

- You are a Nexus Repository Pro user looking for a resilient Nexus Repository deployment option in Azure in order to reduce downtime
- You would like to achieve automatic failover and fault tolerance as part of your deployment goals
- You already have an Azure Kubernetes Service (AKS) cluster set up as part of your deployment pipeline for your other  in-house applications and would like to leverage the same for your Nexus Repository deployment
- You have migrated or set up Nexus  Repository with an external PostgreSQL database and want to fully reap  the benefits of an externalized database setup
- You do not need High Availability (HA) active-active mode

## Requirements

- A Nexus Repository Pro license

- Nexus Repository 3.33.0 or later

- kubectl command-line tool

- An Azure account with permissions for accessing the following 

  Azure

   services:

  - Azure Kubernetes Service (AKS)
  - Azure database for PostgreSQL
  - Azure Monitor
  - Azure KeyVault
  - Azure Command-Line Interface

- Kustomize

 

If you require your clients to access more than one Docker Repository, you must use an external load balancer (e.g., NGINX) as a [reverse proxy](https://help.sonatype.com/nxrm3master/nexus-repository-administration/formats/docker-registry/docker-repository-reverse-proxy-strategies) instead of the provided [ingress for Docker YAML](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-node-cloud-resilient-deployment-example-using-azure#SingleNodeCloudResilientDeploymentExampleUsingAzure-Ingress).

## Limitations

In this reference architecture, a maximum of one Nexus Repository instance is running at a time. Having more than one Nexus Repository failover instance will not work.

## Setting Up the Architecture

### Step 1 - Azure AKS Cluster

The first thing you must do is create a resource group. A resource group is a container that holds related resources for an Azure solution. In this case, everything we are about to set up will be contained within this  new resource group that you create. Follow [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal) to create a resource group from the Azure portal. Ensure you put the  resource group in the same region you intend to setup your all your  resources.

Then, you can follow [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal) for creating an AKS cluster.

 

Ensure you enable Azure Monitor when creating your AKS cluster.

### Step 2 - Azure PostgreSQL

We recommend Azure database for PostgreSQL server for storing Nexus Repository configurations and component metadata.

Follow [ Microsoft's documentation ](https://docs.microsoft.com/en-us/azure/postgresql/quickstart-create-server-database-portal)for creating an Azure database for PostgreSQL server.

- Use the single server option.
- When setting up the database, be sure to select the resource group you created when setting up your AKS cluster.
- Be sure to select the same region as the resource group you created.

Use the Azure CLI to create the `nexus` database using [Microsoft's documentation](https://docs.microsoft.com/en-us/cli/azure/postgres/db?view=azure-cli-latest#az_postgres_db_create) as a reference. (See [Microsoft's documentation](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) for installing the Azure CLI.)

### Step 3 - Ingress Controller

An ingress controller provides reverse proxy, configurable traffic  routing, and TLS termination for Kubernetes services. You can configure  the ingress controller to associate a static IP address with the Nexus  Repository pod. This way, if the Nexus Repository pod restarts, it can still be accessed through the same IP address. 

Follow [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/aks/ingress-basic) for setting up an ingress controller in AKS.

### Step 4 - Create Kubernetes Namespace

Follow the [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/) for creating a Kubernetes namespace with the kubectl command-line tool. You will use the following command to create the namespace:

```
kubectl create namespace <namespace>
```

### Step 5 - Licensing

#### Azure Key Vault

Azure Key Vault provides secrets, key, and certificate management. In the  event of a failover, Key Vault can retrieve the license when the new  Nexus Repository container starts. This way, your Nexus Repository always starts in Pro mode.

Follow [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/key-vault/general/quick-create-portal) for creating a key vault.

- Be sure to select the resource group you created when setting up your AKS cluster.
- Be sure to select the same region as the resource group you created.
- Restrict it to your virtual network.

You will then need to add your license file to the key vault using the  Azure CLI.  The command to add your license will look similar to the  following:

```
az keyvault secret set --name <name_for_secret> --vault-name <name_of_keyvault> --file <path_of_license_file.lic> --encoding base64
```

You will also need to add the secrets for your username and password for  your database; you can do this manually in the key vault you created via the portal. Follow [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-portal) for adding secrets via the portal.

The Kubernetes node on which Nexus Repository will run requires the CSI  Secrets Store driver to be able to retrieve the license and database  user/password information from the key vault. This is a  Kubernetes-specific plugin; directions for obtaining and installing this driver are available in [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver).

 

Note that the CSI Secrets Store Driver for AKS is still in preview. This is  why we suggest using Kustomize to import the license as a ConfigMap that is mounted as a volume into the Nexus Repository pod to make the  license available to Nexus Repository.

#### Kustomize

Your Nexus Repository license is passed into the Nexus Repository container  by using Kustomize to create a configuration map that is mounted as a  volume of the pod running Nexus Repository. See [Kubernetes documentation for using Kustomize ](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/)for more information.

### Step 6 - Azure Monitor 

You should enable Azure Monitor when creating your AKS cluster.

When running Nexus Repository on Kubernetes, it is possible for it to run on different nodes in different AZs over the course of the same day. In  order to be able to access Nexus Repository's logs from nodes in all  AZs, we recommend that you externalize your logs to Azure Monitor. 

When first installed, Nexus Repository sends task logs to separate files.  Those files do not exist at startup and only exist as the tasks are  being run. In order to facilitate sending logs to Azure Monitor, you  need the log file to exist when Nexus Repository starts up. This can be  accomplished using the YAML to configure logback shown in the [example YAML files](https://github.com/sonatype/nxrm-sample-files-repo/tree/main/azure-resiliency-yamls). 

 

Apply the YAML to configure logback to the AKS cluster before applying the deployment YAML.

Enabling Azure Monitor when creating the AKS cluster automatically pushes logs  to stdout from all containers in the Nexus Repository pod to Azure  Monitor.

Therefore, in addition to the main log file (i.e.,  nexus.log), Nexus Repository uses side car containers to log the  contents of the other log files (request, audit, and task logs) to  stdout so that they can be automatically pushed to Azure Monitor.

#### Example of Obtaining Log Messages

Below is an example of using a query to find out container IDs and use them to view log messages.

Open the Azure portal and navigate to the Log Analytics workspace that you  specified for Azure Monitor during AKS cluster creation. Select the *Logs* tab and open a query editor.

Run the following query for each of the containers in the Nexus Repository pod:

```
ContainerInventory
| where Name contains "<container_name>" and ContainerState == "Running"
| order by TimeGenerated desc
```



`container_name` could be one of the following:

- `nxrm-app_nxrm-deployment-nexus` 
- `tasks-log_nxrm-deployment-nexus`
- `request-log_nxrm-deployment-nexus`
- `audit-log_nxrm-deployment-nexus`



This will return a result such as the following:

```
![Example results from the last 24 hours. Columns include TimeGenerated, Computer, ContainerID, Name, and ContainerHostname](https://help.sonatype.com/repomanager3/files/90833420/90833422/1/1634576291030/Screenshot+2021-10-13+at+14.11.46.png)
```



You can then copy the `ContainerID` from this result and use it to access the logs in another query such as the following:

```
ContainerLog
| where TimeGenerated > ago (1h)
| where ContainerID in ('4f5165265cd79f4876b308fa95b89a0de08b93cf6abbdb47a9a2c25f7ef3736d')
| order by LogEntry desc
```



This would return a result such as the following with log messages shown in the *LogEntry* column:

![img](https://help.sonatype.com/repomanager3/files/90833420/90833421/1/1634576290932/Screenshot+2021-10-13+at+14.22.01.png)

### Step 7 - Nexus Work Directory YAMLs

Use the [Create Nexus Work Directory Config YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/create-nexus-workdir-config.yaml) and [Create Nexus Work Directory YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/create-nexus-work-dir.yaml) to create a directory to which the `nexus-data` volume will be mapped on the node/VM. Nexus Repository will store state information such as logs, elasticsearch indexes, and configuration data in that directory so that such information is persistent across Nexus  Repository pod restarts.

### Step 8 - Local Persistent Volume and Local Persistent Volume Claim

Using a local persistent volume allows Elasticsearch indexes to survive pod  restarts on a particular node. When used in conjunction with the `NEXUS_SEARCH_INDEX_REBUILD_ON_STARTUP` flag, this ensures that the Elasticsearch index is only rebuilt if it is  empty when the Nexus Repository pod starts. This means that the only  time the Elasticsearch index is rebuilt is the first time that a Nexus  Repository pod starts on a node.

Storage space will be allocated to a local persistent volume from your root  volume (i.e., the volume attached to the provisioned node). Therefore,  you must ensure that the size you specify for your node's root volume is sufficient for your usage. Ensure that the size of the root volume is  bigger than that specified in the local persistent volume claim so that  there's some spare storage capacity on the node. For example, for a  local persistent volume claim size of 100 Gigabytes, you could make the  actual size of the root volume 120 Gigabytes.

See the sample storage class YAML, local persistent volume YAML, and local persistent volume claim YAML in our [example YAML files](https://github.com/sonatype/nxrm-sample-files-repo/tree/main/azure-resiliency-yamls).

### Step 9 - Azure Blob

Azure Blob Storage provides your object (blob) storage.

Use [Microsoft's documentation](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal) to first set up a storage account and then begin working with blobs.

- When setting up your storage account, be sure to select the resource group you created when setting up your AKS cluster.

- Select the same region as the resource group you created.

- If available in your region, use the 

  premium performance

   option. Otherwise, use the standard option.

  - If using the premium performance option, select the block blobs premium account type.

- Select the zone-redundant storage option.

- Restrict it to your virtual network.

## Upgrading Nexus Repository when Deployed in Kubernetes

To upgrade Nexus Repository deployed in Kubernetes, you must complete the following steps:

1. Update the deployment YAML (as shown in [example YAML files](https://github.com/sonatype/nxrm-sample-files-repo/tree/main/azure-resiliency-yamls)) to the Nexus Repository Docker image version to which you want to upgrade.
2. Apply the updated deployment YAML to your cluster.

## Sample Kubernetes YAML Files

 

The YAML files linked in this section are just examples and cannot be used  as-is. You must fill them out with the appropriate information for your  deployment to be able to use them.

You can use the [sample Azure resiliency YAML files](https://github.com/sonatype/nxrm-sample-files-repo/tree/main/azure-resiliency-yamls) from our sample files GitHub repository to help set up the YAMLs you will need for a resilient deployment. Ensure you have filled out the YAML files with appropriate information for your deployment.

Before running the YAML files in this section, you must first create a namespace. To create a namespace, use a command like the one below with the kubectl command-line tool:

```
kubectl create namespace <namespace>
```

Then, **you must run your YAML files in the order below**:

1. [Storage Class YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-storage-class.yaml)
2. [Secrets YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-secrets.yaml) or [License Configuration Mapping YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-license-configuration-mapping.yaml)
3. [YAML to Configure Logback](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-nxrm-logback-tasklogfile-override.yaml)
4. [YAML to Create Nexus Work Directory Config](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/create-nexus-workdir-config.yaml)
5. [YAML to Create Nexus Work Directory](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/create-nexus-work-dir.yaml)
6. [Local Persistent Volume YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-local-persistent-volume.yaml)
7. [Local Persistent Volume Claim YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-local-persistent-volume-claim.yaml)
8. [Kustomize Deployment YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-kustomize-deployment.yaml) or [Secrets Store CSI Driver Deployment YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-secrets-store-CSI-deployment.yaml)
9. Services YAML
   1. [Ingress for Docker (Optional)](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-ingress-for-docker.yaml)
   2. [Nodeport for Docker (Optional)](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/azure-resiliency-yamls/azure-resiliency-nodeport-for-Docker.yaml)

 

The resources created by these YAMLs are not in the default namespace.

# Single Data Center On-Premises Deployment Example Using Kubernetes

 

 

 

**Only available in Nexus Repository Pro. Interested in a free trial? [Start here](https://www.sonatype.com/products/repository-pro/trial).**

 

Already have a Nexus Repository instance and want to migrate to a resilient architecture? See our [migration documentation](https://help.sonatype.com/repomanager3/installation-and-upgrades/migrating-an-existing-nexus-repository-instance-to-a-resiliency-architecture).

This example architecture illustrates how to use a Kubernetes cluster and  PostgreSQL database to create a resilient Nexus Repository deployment.  To learn more about resiliency and protecting your data in the event of  an outage or disaster, see [Resiliency and High Availability](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability).



![Nexus Repository Resiliency On Premises Reference Architecture. One active Nexus Repository instance with one failover instance on servers all within one data center.](https://help.sonatype.com/repomanager3/files/83495120/87198601/1/1633376354607/Nexus+Repository+Resiliency+-+On+-premises+deployment+with+a+Data+center++%283%29.png)

## Important Terms

Before proceeding, you should know these important terms:

- Node - A virtual or physical machine
- Pod - A group of one or more containers with shared storage and network resources and a specification for how to run containers
- Container - A package with the program to execute (Nexus Repository) and  everything required for it to run (e.g., code, runtime, system  libraries)
- Instance - An instance of Nexus Repository

## Use Cases

This reference architecture is designed to protect against the following scenarios:

- A node/server failure within a data center
- A Nexus Repository service failure

You would use this architecture if you fit the following profiles:

- You are a Nexus Repository Pro user looking for a resilient Nexus Repository deployment option on-premises in order to reduce downtime
- You would like to achieve automatic failover and fault tolerance as part of your deployment goals
- You already have a Kubernetes cluster set up as part of your deployment pipeline for your other in-house applications and would like to leverage the same for your Nexus Repository deployment
- You have migrated or set up Nexus Repository with an external PostgreSQL  database and want to fully reap the benefits of an externalized database setup
- You do not need High Availability (HA) active-active mode

## Requirements

- A Nexus Repository Pro license
- Nexus Repository 3.33.0 or later
- A tool such as [Kops](https://github.com/kubernetes/kops) for setting up Kubernetes clusters
- kubectl command-line tool
- [Kustomize](https://github.com/kubernetes-sigs/kustomize) to customize Kubernetes objects
- Bare metal servers/virtual machines for configuring master and worker nodes
- A PostgreSQL database 
- A load balancer
- File storage (Network File System)

 

If you require your clients to access more than one Docker Repository, you must use an external load balancer (e.g., NGINX) as a [reverse proxy](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry/docker-repository-reverse-proxy-strategies) instead of the provided [ingress for Docker YAML](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/single-data-center-on-premises-deployment-example-using-kubernetes#SingleDataCenterOnPremisesDeploymentExampleUsingKubernetes-Ingress).

## Limitations

In this reference architecture, a maximum of one Nexus Repository instance is running at a time. Having more than one Nexus Repository instance replica will not work.

## Setting Up the Architecture

### Step 1 - Kubernetes Cluster

Kubernetes works by placing containers into pods to run on nodes. You must set up a Kubernetes cluster comprising one master node and two worker nodes.  Nexus Repository will run on one of your worker nodes. In the event of a worker node failure, Kubernetes will spin up a new Nexus Repository instance on the other worker node.

See [Kops documentation](https://kubernetes.io/docs/setup/production-environment/tools/kops/) for an example of how to set up a Kubernetes cluster with one master and two worker nodes using Kops. 

### Step 2 - PostgreSQL Database

At any given time, only one instance will communicate with the database  and blob store. Set up a PostgreSQL database and ensure the worker nodes can communicate with this database. See [Configuring Nexus Repository Pro for External PostgreSQL](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-nexus-repository-pro-for-h2-or-postgresql#ConfiguringNexusRepositoryProforH2orPostgreSQL-ConfiguringforExternalPostgreSQL) for more information.

In order to avoid single points of failure, we recommend you set up a highly available PostgreSQL cluster.

### Step 3 - Creating a Namespace

Kubernetes namespaces allow you to organize Kubernetes clusters into virtual  sub-clusters. Before creating a namespace, you must have already  configured a Kubernetes cluster and you must configure the kubectl  command-line tool to communicate with your cluster.

See [Kubernetes documentation](https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/) on creating a namespace.

### Step 4 - Kustomize

Kustomize is bundled with kubectl, and you can apply it using the following command where `<kustomization_directory>` is a directory containing all of the YAML files, including the kustomize.yaml file: 

```
kubectl apply -k <kustomization_directory> 
```

Your Nexus Repository license is passed into the Nexus Repository container  by using Kustomize to create a configuration map that is mounted as a  volume of the pod running Nexus Repository. See [Kubernetes documentation for using Kustomize ](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/)for more information.

### Step 5 - File System

The Nexus Repository container will store logs and Elastic Search indexes in the `/nexus-data` directory on the node.

### Step 6 - Persistent Volume and Persistent Volume Claim

Using a persistent volume allows Elasticsearch indexes to survive pod  restarts on a particular node. When used in conjunction with the `NEXUS_SEARCH_INDEX_REBUILD_ON_STARTUP` flag, this ensures that the Elasticsearch index is only rebuilt if it is  empty when the Nexus Repository pod starts. This means that the only  time the Elasticsearch index is rebuilt is the first time that a Nexus  Repository pod starts on a node.

Storage space will be allocated to a persistent volume from your root volume  (i.e., the volume attached to the provisioned node). Therefore, you must ensure that the size you specify for your node's root volume is  sufficient for your usage. Ensure that the size of the root volume is  bigger than that specified in the persistent volume claim so that  there's some spare storage capacity on the node. For example, for a  persistent volume claim size of 100 Gigabytes, you could make the actual size of the root volume 120 Gigabytes.

## Sample Kubernetes YAML Files

 

The YAML files linked in this section are just examples and cannot be used  as-is. You must fill them out with the appropriate information for your  deployment to be able to use them.

You can use the [sample on-premises resiliency YAML files](https://github.com/sonatype/nxrm-sample-files-repo/tree/main/on-prem-resiliency-yamls) from our sample files GitHub repository to help set up the YAMLs you will need for a resilient deployment. Ensure you have filled out the YAML files with appropriate information for your deployment.

Before running the YAML files in this section, you must first create a namespace. To create a namespace, use a command like the one below with the kubectl command-line tool:

```
kubectl create namespace <namespace>
```

**You must then run your YAML files in the order below**:

1. [Persistent Volume YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/on-prem-resiliency-yamls/on-prem-resiliency-nfs-persistent-volume.yaml)
2. [Persistent Volume Claim YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/on-prem-resiliency-yamls/on-prem-resiliency-nfs-persistent-volume-claim.yaml)
3. [License Configuration Mapping YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/on-prem-resiliency-yamls/on-prem-resiliency-license-config-mapping.yaml)
4. [Deployment YAML](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/on-prem-resiliency-yamls/on-prem-resiliency-deployment.yaml)
5. Services YAML
   1. [Ingress for Docker (Optional)](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/on-prem-resiliency-yamls/on-prem-resiliency-ingress-for-docker-connector.yaml)
   2. [Nodeport for Docker (Optional)](https://github.com/sonatype/nxrm-sample-files-repo/blob/main/on-prem-resiliency-yamls/on-prem-resiliency-nodeport-for-docker-connector.yaml)

 

The resources created by these YAMLs are not in the default namespace.

# High Availability Clustering (Legacy)

 

 

 

High Availability Clustering is a legacy feature that only works on the  Orient database. All new deployments should use other resiliency options as described in [Resiliency and High Availability](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability).

 

**Only available in Nexus Repository Pro. Interested in a free trial? [Start here](https://www.sonatype.com/products/repository-pro/trial).**

## Overview

Legacy High Availability Clustering (HA-C) is a feature designed to improve uptime by having a cluster of redundant Nexus Repository "nodes" (instances) within a single data center. This feature allows you to maintain availability to your Nexus Repository in the event one of the nodes becomes unavailable. A typical Nexus Repository HA-C cluster is shown below:

![HA-C cluster comprising three nodes with inter-node replication.](https://help.sonatype.com/repomanager3/files/2630528/16351269/1/1527606289406/high-availability-arch-simple.png)

## Software and Hardware Requirements

Here’s a list of the most important things you will need:

- A **test environment** to evaluate your cluster configuration and processes
- Nexus Repository Manager **Professional license** and **the latest version**
- A **load balancer**, such as [NGINX](https://www.nginx.com/) or [Apache HTTP](https://httpd.apache.org/) or [AWS ELB](https://aws.amazon.com/elasticloadbalancing/)
- **3 separate instances** of Nexus Repository installed on **3 different systems** (e.g., 3 different EC2 instances) in a single datacenter
- **Network connectivity between the 3 different systems** so the Nexus Repository can communicate with each other on several ports
- **Separate \*local\* and \*shared\* storage** (the difference will be described below)
- Each node needs to be configured according to our [general requirements](https://help.sonatype.com/repomanager3/product-information/system-requirements).

In addition to these items, this guide will clarify the key concepts of **Node**, **Blob Store**, and **Hazelcast** and outline the operational considerations specific to HA-C.

## Known Issues and Limitations

- HA-C is not desgined for dynamic orchestration, as such deployment to  environments such as Kubernetes or OpenShift is not supported.
- The initial functionality intends to support a three-node cluster in a **single datacenter**. Cross-datacenter (WAN) replication is not supported at this time. For AWS, all nodes must be in the same availability zone.
- [Upgrading](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/upgrading-your-cluster) an already established cluster to a new version of Nexus Repository  requires shutting down all nodes. Support for rolling upgrades is not  yet available.

### Format Limitations

- HA-C supports the following formats: Bower, Docker, Git LFS, Maven, npm, NuGet, PyPI, Raw, Rubygems, and Yum.
- Apt, Cocoapods, Conan, Conda, Go, Helm, p2, and R formats are currently disabled.

## Topics in this Section

 

- [Configuring Nodes](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-nodes)
- [Configuring Hazelcast](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast)
- [Designing your Cluster Backup/Restore Process](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process)
- [Initial Setup - High Availability Clustering (Legacy)](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/initial-setup---high-availability-clustering-(legacy))
- Operating your cluster
  - [JMX Lifecycle Operations](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/jmx-lifecycle-operations)
  - [JMX Maintenance Operations](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/jmx-maintenance-operations)
  - [Upgrading your cluster](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/upgrading-your-cluster)
- [Load Balancing](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/load-balancing)

 Configuring Nodes

## What is a Node?

We refer to each separate instance of NXRM in a high availability cluster  as a node. Nodes can run on physical or virtual servers, containers, or cloud services like Amazon Web Services EC2 instances. While multiple nodes can exist on the same physical or virtual server, each bound to different ports for testing purposes, but such a setup is discouraged in production as it does not provide for a redundant clustered environment.

Each node needs its own local storage for the `$data-dir` folder. This folder must not be shared by any other node in the cluster. Within the local storage, a node will store:

- local instance configuration (network ports, nexus.properties, logging)
- local log files
- a complete copy of the internally managed [OrientDB](http://orientdb.com/orientdb/) databases NXRM uses to store configuration and asset metadata
- a complete copy of the [ElasticSearch](https://www.elastic.co/products/elasticsearch) indexes

In addition, all nodes need shared storage for [blob stores](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/configuring-blob-stores).

## How many Nodes will you need?

NXRM HA-C is intended for use with 3 nodes. It will not work properly with less than 3.

Any node can accept a write, but for the write to be committed it must be  replicated to a 'majority' of available running nodes. [OrientDB](https://orientdb.com/docs/2.2/Distributed-Configuration.html#default-configuration) calculates a majority using the following formula: `(number of expected running nodes / 2) + 1`

A write (e.g. publishing assets, changing system configuration) will fail if nodes in your cluster unexpectedly lose connectivity or die and fewer than the  calculated majority are running. However you can still write to your  NXRM HA-C environment with only 1 running node as long as the other  members of the cluster have either a) not started yet or b) were  shutdown cleanly. The NXRM HA-C feature includes [monitoring ](https://help.sonatype.com/nxrm3master/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster#Operatingyourcluster-MonitoringNodeHealth)for this condition and controls to help you get your cluster running again.

 

Adding more than 3 nodes will result decreased performance due to the  additional overhead incurred by database synchronization between the  nodes. We recommend using no more and no less than 3 nodes in an HA  cluster.

##  What data is replicated between Nodes?

NXRM HA-C uses [Hazelcast](https://hazelcast.org/) to keep the OrientDB databases completely in sync in an "Active-Active" configuration. 

The ElasticSearch indices on the running nodes operate independently and do not share  data; they do automatically index all content replicated in OrientDB.

Blobs, the assets (.jar, .xml, .tar.gz, docker images, etc.) in your repositories, are not replicated. They are stored in your blob stores.



 

Your next step is to plan your blob stores with [Configuring Blob Stores](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/configuring-blob-stores) before proceeding to [Initial Setup - High Availability Clustering (Legacy)](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/initial-setup---high-availability-clustering-(legacy)).

# Configuring Hazelcast

 

 

## Network Preparation

The nodes in a Nexus Repository Manager cluster need to be able to communicate  with each other over TCP/IP. For cluster communications, Nexus  Repository Manager will use a range of ports starting with 5701. Each  node in the Nexus Repository Manager cluster will require that an extra  port is available; the extra ports used by Nexus Repository Manager will be bound sequentially by default.

For example, in a three-node cluster, each of the nodes in the cluster will need to have ingress opened on ports  `    5701  `, `5702`, and `5703`. The nodes will use ephemeral ports, randomly selected by the operating system, for outbound (egress) communications. If outbound or inbound network communications need to  be customized and may be blocked by a firewall or other network  appliance, the ports used for cluster communications can be customized  in the Hazelcast configuration.

 

For more information on customizing Hazelcast and the ports it uses, please see the documentation for [Hazelcast cluster configuration](http://docs.hazelcast.org/docs/3.6/manual/html-single/index.html#setting-up-clusters).

The nodes in a Nexus Repository Manager cluster will also replicate  database transactions among the nodes in the cluster. The database  requires ports `2424` and `2434` be open for ingress and egress to each of the rest of the nodes in the cluster.

## Node Discovery

Hazelcast has multiple methods for discovering other nodes. In the default  configuration, Nexus Repository Manager uses multicast to discover other Nexus Repository Manager nodes. This is done to simplify cluster  configuration. However, multicast may not work reliably in all network  environments.

To customize discovery, copy `NEXUS_HOME/etc/fabric/hazelcast-network-default.xml` to `$data-dir/etc/fabric/hazelcast-network.xml` and adjust the settings enclosed in the `<join>` tag.

 

In NXRM 3.6.1 or earlier, these changes must be applied directly to `NEXUS_HOME/etc/fabric/hazelcast.xml, where NEXUS_HOME` is where Nexus Repository Manager is installed.



### Multicast Discovery

Multicast is the default discovery method and is recommended unless it is not  supported on your network. To test multicast connectivity between nodes, we recommend using [iPerf](https://iperf.fr/). iPerf is freely available for Windows, Linux and Mac OSX under the BSD licence.

To test multicast connectivity between two nodes, each node will need to  have iPerf installed (note: iPerf3 does not support multicast client  testing). During testing, one node will act as the "server" while the  other node acts as the "client." This is important during testing, and  further, we suggest that each node be tested as a server and as a client to ensure proper two-way multicast communication.

 **Testing multicast from the server side** 

```
iperf -s -B 224.2.2.3 -p 54327 -i 1
```

**Testing multicast from the client side**

```
iperf -c 224.2.2.3 -p 54327 -u -i 1
```

The address and port `224.2.2.3:54327` was chosen because this is the default port and address that will be used  by Nexus Repository Manager during the node discovery. When the client  is able to successfully connect and communicate with the server node,  the client will output a set of brief diagnostic messages indicating how much data was sent and received. The following is a sample of the  output from a client in a successful session:

**Sample Output**

```
Client connecting to 224.2.2.3, UDP port 54327
Sending 1470 byte datagrams
Setting multicast TTL to 1
UDP buffer size:  208 KByte (default)

[  3] local 10.10.0.102 port 33743 connected with 224.2.2.3 port 54327
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0- 1.0 sec   129 KBytes  1.06 Mbits/sec
[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  3.0- 4.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  4.0- 5.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  5.0- 6.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  6.0- 7.0 sec   129 KBytes  1.06 Mbits/sec
[  3]  7.0- 8.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  8.0- 9.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  9.0-10.0 sec   128 KBytes  1.05 Mbits/sec
[  3]  0.0-10.0 sec  1.25 MBytes  1.05 Mbits/sec
[  3] Sent 893 datagrams
```

The advantage of the using multicast for Nexus Repository Manager node  discovery is that nodes can be added and removed from the cluster  without cluster administrators needing to perform configuration or  configuration changes. However, routers may not be able to route  multicast requests properly between subnets, or multicast may be  disabled altogether. In these situations the cluster configuration can  be done manually. If multicast is not available, either [AWS Discovery](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast#ConfiguringHazelcast-AWSDiscovery) (if you are running NXRM inside AWS on EC2 instances) or [TCP/IP Discovery](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast#ConfiguringHazelcast-TCP/IPDiscovery) can be used.

For a production instance, we advise that the default address and port (224.2.2.3:54327) is not used, to avoid another cluster within the same network and using the same default configuration from unintentionally joining an existing cluster.

### AWS Discovery

Nexus Repository Manager can be deployed on cloud-computing services, such as Amazon Web Services (AWS), where multicast discovery is not supported.  Hazelcast AWS discovery is recommended.

#### AWS Environment

The NXRM servers need permissions to find other nodes, granted through [  AWS Identity and Access Management  ](https://aws.amazon.com/iam/)  (IAM). The following configuration settings should be used:

1. The EC2 instances running NXRM should be assigned an `InstanceProfile`
2. The `InstanceProfile` should have an `InstanceRole` with a policy granting the permission `ec2:DescribeInstances` on all resources.

In CloudFormation, the role would look similar to this:

```
"InstanceRole": {
  "Type": "AWS::IAM::Role",
  "Properties": {
    "AssumeRolePolicyDocument": {
      "Version": "2012-10-17",
      "Statement": [{
        "Effect": "Allow",
        "Principal": {
          "Service": "ec2.amazonaws.com"
        },
        "Action": [
          "sts:AssumeRole"
        ]
      }]
    },
    "Policies": [{
      "PolicyName": "hazelcastDiscovery",
      "PolicyDocument": {
        "Version": "2012-10-17",
        "Statement": [{
          "Effect": "Allow",
          "Action": [
            "ec2:DescribeInstances"
          ],
          "Resource": ["*"]
        }]
      }
    }]
  }
}
```

#### NXRM Configuration for AWS Discovery

To configure Hazelcast for automatic node discovery, you need the IAM role name, AWS region, and security group of the EC2 instances. Find the `<join>` tag in `$install-dir/etc/fabric/hazelcast-network.xml`. Then, edit the file for each node:

1. Change the value in `<multicast enabled="true">` to `"false"`.
2. Update the `<discovery-strategies>` section as explained below.
3. Save the file.
4. Reboot each node in the cluster.

The `$data-dir/etc/fabric/hazelcast-network.xml` file with the modified properties will look similar to this:

```
    <join>
      <!-- deactivating other discoveries -->
      <multicast enabled="false"/>
      <tcp-ip enabled="false"/>
      <aws enabled="false"/>

      <discovery-strategies>
        <discovery-strategy enabled="true" class="com.hazelcast.aws.AwsDiscoveryStrategy">
          <properties>
            <!--
			 | Required: use the command 'aws iam list-instance-profiles' on the EC2 instance to locate the name
			 | of the role used by the IAM Instance Profile you created previously
			 -->
            <property name="iam-role">EC2_IAM_ROLE_NAME</property>

			<!-- Required: set this to the region where your EC2 instances running Nexus Repository Manager are located -->
            <property name="region">us-west-1</property>

			<!--
			 | The next few options give you a choice to how the nodes are enumerated. You can specify:
			 | * Just a security group name, if the security group only contains Nexus Repository Manager hosts
			 | * or a tag-key/tag-value pair, if you have tagged the EC2 instances
			 | * or a combination of the two, if the security group has other hosts in it.
			 -->
            <property name="security-group-name">EC2_SECURITY_GROUP_NAME</property>

            <!-- example tag idea, you are free to use whatever tag naming convention you need-->
            <property name="tag-key">Purpose</property>
            <property name="tag-value">Nexus Repository Manager</property>
     
          </properties>
        </discovery-strategy>
      </discovery-strategies>
    </join>
```

 

For more information on configuring Hazelcast in an AWS environment, please see the documentation for [the AWS EC2 discovery plugin for hazelcast](https://github.com/hazelcast/hazelcast-aws).

### TCP/IP Discovery

When multicast discovery and AWS discovery are not available, TCP/IP  discovery can be configured. TCP/IP discovery requires the IP address  of each node to included in the configuration. Please see the  documentation for [Hazelcast cluster configuration](http://docs.hazelcast.org/docs/3.6/manual/html-single/index.html#setting-up-clusters). Find the `<join>` tag in `$install-dir/etc/fabric/hazelcast-network.xml`. Then, edit the file for each node:

1.  Add or set the following in `$data-dir/etc/nexus.properties: nexus.hazelcast.discovery.isEnabled=false`
2. Update the `<join>` section as explained below.
3. Save the file.
4. Reboot each node in the cluster.

In this case the `$data-dir/etc/fabric/hazelcast-network.xml` file with the modified properties will look similar to this:

```
<join>
    <multicast enabled="false"/>
    <tcp-ip enabled="true">
        <member-list>
          <member>10.0.1.10</member>
          <member>10.0.1.11</member>
          <member>10.0.1.12</member>
        </member-list>
    </tcp-ip>
    <aws enabled="false"/>
</join>
```

# Designing your Cluster Backup/Restore Process

  You must have a reliable, tested process for backing up your Nexus Repository Manager deployment   .

  This section will cover what you need to create a backup and restore process appropriate for your High Availability deployment   .

 

 

## High-level Overview

An appropriate backup process for Nexus Repository Manager needs to  capture state from two separate, but interdependent parts of your  deployment:

- the contents of each blob store
- a copy of the local storage for each node (i.e. the `      $data-dir    ` directory, including the OrientDB databases within)

In the worst case scenario, a Nexus Repository Manager High Availability environment can be recreated minimally from:

- the contents of each blob store
- a complete set of OrientDB database exports from one of the nodes participating in the cluster

## Backing up Shared File Systems   for your Blob Store(s)

 

 Nexus   Repository Manager requires that you choose and execute a separate backup process appropriate for the file systems under your blob store(s).

### Generic NFS

Tools that back up the file system under the blob store can be safely executed while Nexus Repository Manager is running.

### NFS over Amazon Elastic File System (EFS)

If you are using file backed blob stores on top of Amazon Elastic File System, you will need to review [Amazon's documentation specific to backing up EFS](http://docs.aws.amazon.com/efs/latest/ug/efs-backup.html).

### Amazon Simple Storage Service (S3)

The backup process for your S3 backed blob stores will involve creating a separate S3 "bucket" and periodically synchronizing the content from the source bucket under your blob store(s).

The [AWS Command Line Interface (CLI)](https://aws.amazon.com/cli/) provides an `s3 sync` command that you can invoke periodically to perform this:

- http://docs.aws.amazon.com/cli/latest/reference/s3/sync.html

There are also a number of third party tools that can perform this task.

## Backing up local storage for your Nodes

Each node in your Nexus Repository Manager High Availability cluster has its own separate `$data-dir` directory. The `$data-dir` `/db` directory within contains OrientDB databases and requires special treatment.

The `db` directory cannot be backed up by basic file system backup tools while Nexus Repository Manager is running. Instead create an *Admin - Export databases for backup* task as described in the [Backup and Restore](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore) section.

Please note:

- All nodes in your Nexus Repository Manager cluster will automatically enter  Read-Only mode when the backup task is running on any of the nodes
- It is recommended to schedule the backup task on at least 2 nodes. The  tasks should run at different times to avoid concurrency issues. The  backup from any node can be used to perform a restore, but running on  more than one node ensures that there isn't a single point of failure  for the backup.
- The folder specified in the backup task for a node must be available and writable by the user running Nexus Repository Manager on the nodes
- The backup task produces a set of files each time it is run that must be  protected. Be sure to use a file system backup tool to store these files separately.

For the rest of the content within the `$data-dir` directory, traditional file system backup tools will suffice. It is safe to skip backing up the following directories within `$data-dir`:

- `      backup    `
- `cache`
- `      db    ` (backed up via the *Admin - Export databases for backup* task)
- `elasticsearch` (generated via the *Repair - Rebuild respository search* task)
- `logs` (see the [Operating your cluster](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster) section for preserving logs)
- `tmp`

 

We recommend you execute   backup   of your   local storage   at the same starting time as your blob store file system backup.

## Restoring from backup

 

If you determine you need to restore your Nexus Repository Manager deployment from backup,  you must first focus only on starting one node and stabilizing it before you add new nodes to the High Availability cluster.

Start this process by ensuring all Nexus Repository Manager nodes in your cluster are offline.

Assuming all nodes in your cluster are offline:

1. Identify the host that will be the first node of your restored deployment. Locate the Orient DB exports from that node and identify the date and time  they were taken. Use this same (or as close to) date and time when  restoring the blob store file systems.

2. Restore the blob store file systems and host connectivity. Be sure to restore the blob store file system to the exact same absolute file system path used previously; don't try to restore to a different file system path.

3. Restore the local storage (`$data-dir`) for the node. Again, preserve the same absolute file system paths used previously.

4. Remove the following directories from 

   ```
   $data-dir
   ```

   ```
   /db
   ```

   - `component`
   - `config`
   - `security`

5. Copy the complete set of OrientDB exports from that node to `$data-dir/restore-from-back`up for restoration (**Note**: For version 3.10.0 or earlier use `$data-dir``/backup` as the restore location).

6. Start Nexus Repository Manager on only this node.

Confirm that your instance is in a stable state before proceeding:

- Inspect configuration and confirm it matches expectations
- Attempt to retrieve and/or publish new components

Once the first node is online and stable, it is safe to proceed in bringing  additional nodes back into your cluster. Focus on adding only one node  at a time to the cluster. On each of the additional nodes, delete the following folders from within `$data-dir`   `/db `prior to starting Nexus Repository Manager

- `component`
- `config`
- `security`

The OrientDB databases removed from the additional node will automatically be rebuilt from the instances already running in the High Availability cluster.

​        

# Initial Setup - High Availability Clustering (Legacy)

This section will cover the steps for enabling High Availability Clustering in your Nexus Repository Manager deployment.

 

This document assumes that you have completely read and prepared your choices for [Configuring Storage](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/initial-setup---high-availability-clustering-(legacy)#), [Configuring Hazelcast](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast), and [Designing your Cluster Backup/Restore Process](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process).

Do not proceed until you have a complete plan ready for all 3.

Once you begin, focus on stabilizing one single node at a time.

 

 

## Node Deployment Steps for High Availability Clustering

### First Node

1. Follow the usual [Installation Methods](https://help.sonatype.com/repomanager3/installation-and-upgrades/installation-methods). Create the file `sonatype-work/nexus3/etc/nexus.properties` with the following contents:

   ```
   # Jetty section
   application-port=8081
   application-host=0.0.0.0
   nexus-args=${jetty.etc}/jetty.xml,${jetty.etc}/jetty-http.xml,${jetty.etc}/jetty-requestlog.xml
   nexus-context-path=/
   
   # Nexus section
   nexus-edition=nexus-pro-edition
   nexus-features=\
    nexus-pro-feature
   
   nexus.clustered=true
   # nexus.licenseFile is only necessary for the first run
   # replace /path/to/your/sonatype-license.lic with the path to your license, and ensure the user running Nexus Repository manager can read it
   nexus.licenseFile=/path/to/your/sonatype-license.lic
   ```

2. Next, deploy your chosen [hazelcast configuration](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast) to the path `sonatype-work/nexus3/etc/fabric/hazelcast-network.xml`. For NXRM 3.6.1 or earlier, these changes must be made in `NEXUS_HOME/etc/fabric/hazelcast.xml`, where NEXUS_HOME is where Nexus Repository Manager is installed.

3. Start Nexus Repository Manager. Log in as an administrator, then visit the *System* → *Nodes* screen. You should see your first node, like this example:
   ![Nodes screen](https://help.sonatype.com/repomanager3/files/16351278/16351279/1/1527606883287/Screen+Shot+2017-10-23+at+10.30.07+AM.png)

You may want to assign a friendly *Node Name* to each node in your cluster to make it easier to identify. Without a  node name, screens that refer to individual nodes will display the  unique ID labeled *Node Identity* in the screenshot above. Simply click on the entry in the list and you will have the ability to set a node name.

When Nexus Repository Manager is started with `nexus.clustered=true` and a PRO license, it will not create the default blobstore or initial  example repositories. You are free to set these up now, or later as you  add nodes.

### Second Node and Beyond

  

 

Do not copy the `sonatype-work` directory to your additional nodes. Each node must be allowed to initialize its own private `sonatype-work` directory; data generated on first run is unique to each instance.

Starting a second node off of a copy of the `sonatype-work` directory will result in the inability to correctly form a cluster.

Again follow the usual [Installation Methods](https://help.sonatype.com/repomanager3/installation-and-upgrades/installation-methods). Before starting Nexus Repository Manager, repeat the steps you performed on the first node:

1. Create the file `sonatype-work/nexus3/etc/nexus.properties` with the same contents as the first node (note that you will have to  choose a different port number only if you're running two or more nodes  on a single host):

   ```
   # Jetty section
   application-port=8081
   application-host=0.0.0.0
   nexus-args=${jetty.etc}/jetty.xml,${jetty.etc}/jetty-http.xml,${jetty.etc}/jetty-requestlog.xml
   nexus-context-path=/
   
   # Nexus section
   nexus-edition=nexus-pro-edition
   nexus-features=\
    nexus-pro-feature
   
   nexus.clustered=true
   # nexus.licenseFile is only necessary for the first run
   # replace /path/to/your/sonatype-license.lic with the path to your license, and ensure the user running Nexus Repository manager can read it
   nexus.licenseFile=/path/to/your/sonatype-license.lic
   ```

2. Deploy your chosen hazelcast configuration to the path `sonatype-work/nexus3/etc/fabric/hazelcast-network.xml`

3. Start Nexus Repository Manager

After each node joins the cluster, confirm it is visible in the *System* → *Nodes* screen. Set a node name for each node as desired. Repeat this section for each node you wish to join your High Availability Cluster until all are running.

## Enabling High Availability on an existing Nexus Repository Manager Deployment 

If you already have a single node Nexus Repository Manager deployment, you will still need to read and prepare your choices for [Configuring Storage](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/initial-setup---high-availability-clustering-(legacy)#), [Configuring Hazelcast](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/configuring-hazelcast), and [Designing your Cluster Backup/Restore Process](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process). You will also need to upgrade your single node deployment to a version of Nexus Repository Manager that supports [High Availability Clustering (Legacy)](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)) before you attempt to establish a cluster. All nodes within a cluster  must be running the exact same version of Nexus Repository Manager.

You additionally may have to design a strategy for migrating the content on your existing blob  stores to a shared filesystem accessible to all nodes in the cluster.  Moving blobs from one blobstore to another can be achieved using Dynamic Storage.

### Blob Store Content Migration

All blob stores used by HA-C nodes must be shared. If you need to move  blobstore blobs to new blobstores in preparation for sharing between  nodes, first perform that procedure in a single node configuration using the [Dynamic Storage ](https://guides.sonatype.com/repo3/technical-guides/scaling-dynamic-storage/)operations.

### Configuration

Once you have all the pieces in place, enable high availability by  performing the steps listed in the Second Node and Beyond section above.

​        

# Operating your cluster

 

 



 

It is recommended that the nodes are started in the reverse order in which they were shutdown wherever possible (for example, if they were shutdown in the order A,  B, C, then C should be started first, followed by B and then A). The  last node shutdown will have the most current data and should be the  first one started up after a full cluster shutdown. Each node should be allowed to start completely before starting the next node.

## Startup and Confirming Node Connectivity

After enabling HA-C for your Nexus Repository Manager installation, check the console or log file after launching a clustered node to confirm that  all nodes have been detected.

When you start the nodes, you will see a message in the nexus.log confirming the connection of the cluster members, like the one below:

```
2017-11-06 11:04:12,045-0500 INFO [hz.nexus.generic-operation.thread-0] *SYSTEM com.hazelcast.internal.cluster.ClusterService - [172.19.3.78]:5703 [nexus] [3.7.8] 


Members [3] { 
        Member [172.19.3.78]:5701 - 6f4df1bc-606d-4821-8a3b-0980f093abd1 
        Member [172.19.3.78]:5702 - 436158e4-2865-40fe-bfaa-d350db8dedb1 
        Member [172.19.3.78]:5703 - 7ff93c39-23b3-4bf5-9b8d-51264ccecee2 this 
}
```

### Running in Docker

Running your Nexus Repository Manager cluster within a container network is  supported. However, there is an important consideration. When running  Nexus Repository Manager inside of a docker container, the default  behavior of the "docker stop" command is to first send a TERM signal,  but if the process has not quit within 10 seconds, a KILL signal is  sent. This has been known to cause Nexus Repository Manager clusters to  become corrupted.

WARNING

 

When running Nexus Repository Manager nodes inside docker containers, you will need to specify an extended timeout, such as

```
docker stop --time=120 <CONTAINER_NAME>
```

to allow nodes to exit the cluster cleanly.

## Shutdown

A node leaves the cluster through a clean shutdown of Nexus Repository Manager. A proper shutdown is performed by using the `nexus.exe stop` command on Windows-based systems and `nexus stop` on all others. When the node is leaving by a clean shutdown, the node  is unregistered from the cluster. If a node that is registered in the  cluster becomes abruptly unavailable (for example, the node's network  link is broken, or the node's operating system crashes), that node may  remain registered in the cluster despite no longer participating in  database replication.

 

The clean shutdown is an important consideration when designing scripts for stopping a Nexus Repository Manager instance.

Nexus Repository Manager will initiate a shutdown if it receives the TERM  signal, which is the default signal sent with the kill command on  unix-like operating systems. A clean shutdown is also important for  recovering from an outage event.

## Backup

Make sure to take some time to read the section on  [designing your backup and restore plan](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process). Running Nexus Repository Manager in a cluster does not reduce the need for a robust backup strategy. With a cluster, you will need to specify the repository manager instance  where the task will execute. This is necessary because the backup will  result in exported database backup files. These files will be placed in  the configured location, on the chosen node's file system.  However, since the task is relegated to a single node, this introduces  the possibility that the database backup may not be executed if the node is no longer participating in the cluster. In some environments it may  be preferable to configure two different nodes to execute backup tasks  on alternating schedules. While the [Backup and Restore](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore) task is running, the Nexus Repository Manager cluster will be in read-only mode. 

### Testing your Backup/Restore process

We encourage you to test your [restore process](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process#DesigningyourClusterBackup/RestoreProcess-Restoringfrombackup) for disaster recovery minimally every 6 months. We also strongly suggest  that you test the disaster recovery plan prior to upgrading Nexus  Repository Manager to a new release. 

 

Please keep in mind Nexus Repository Manager requires that you choose and execute a separate backup process of your blob store(s) appropriate for the file system.

## Node Names

When clustered, Nexus Repository Manager offers the ability to specify an  alias for each node. Behind the scenes, Nexus Repository Manager  generates a unique identifier for each node that joins a cluster. The  alias that is specified by the administrator allows a logical mapping  between the system-generated identifier and the specified name. The  alias for a node is stored in `sonatype-work/nexus3/etc/node-name.properties`. The name can be updated in the Nexus Repository Manager user interface  or directly in the properties file. If the name is updated directly in  the properties file, it is necessary to restart the renamed node for the change to take effect. When an alias has been provided for a node, that alias will be used in place of the identifier where appropriate. For  instance, the alias will be used in logging messages and in the user  interface. 

## Maintaining log files

Clustered Nexus Repository Manager nodes will create the same set of log files as unclustered installations. However, in a cluster, the name of the node  will be added to each log message. The name that is used in the log  messages will be the given alias or default to the node's system-generated identifier. The log files on each node will be rotated daily. The log files for the previous day will be renamed and compressed to save disk space.

The Nexus Repository Manager user interface contains a log viewer available to administrators. This log viewer is limited to only display log  messages that have been emitted on the local node. In an environment  where traffic is routed through a load balancer, this view may not be helpful. In that case, we suggest that a log  aggregation system be put in place. Having a node identifier as a part  of each log entry should enable better searching within such a system. 

## Verifying Synchronization

At run-time, the repository manager user interface allows you to view the status of the clustered nodes.

See [Nodes ](https://help.sonatype.com/repomanager3/nexus-repository-administration/nodes#Nodes-Nodes) for details on viewing active nodes in a cluster.

 

In the event a single node loses connection to the cluster, the remaining  nodes will continue to make decisions on which data changes are valid.  The disconnected node will reject further writes until it rejoins the  cluster.

### Database Quorum and Cluster Reset

To maintain data consistency, Nexus Repository Manager forces database  transactions to be accepted and written to multiple nodes in the  cluster. The goal is that all transactions are replicated within the  cluster before they are accepted. However, this introduces some  situations that need explanation. To allow for continued operation, even during some failures, the transactions are not expected to replicate to all nodes in a cluster before the transaction is accepted. The Nexus  Repository Manager cluster does expect that the transaction is  replicated to the majority of nodes within a cluster. This means that in a cluster with three nodes, no less than two nodes must accept a write  before the transaction is considered a success. To calculate the size of the majority, Nexus Repository Manager will use the following equation:

 

```
ROUNDED_DOWN( $N / 2 ) + 1
```

Where `$N` represents the number of nodes registered in the cluster and `ROUNDED_DOWN` finds the largest whole number less than or equal to the given parameter.

Nodes register with the cluster when they are launched. 

Although Nexus Repository Manager clustering replicates data across all nodes in the cluster, a cluster can be reduced to a single node. A single node  cluster will accept write transactions. You can still write to your  Nexus Repository Manager cluster environment with only one running node  as long as the other members of the cluster have either been shutdown  cleanly or they have not started yet. If nodes in your cluster  unexpectedly lose connectivity or die and fewer than the calculated  majority are running, then attempts to write (such as publishing assets, changing system configuration) will fail.

Because of the quorum calculation and cluster registration algorithms, it is  possible that Nexus Repository Manager's cluster table can enter a state where it does not accurately reflect the true state of your cluster.  For example, if a registered member is abruptly removed from the cluster and a fresh installation is added to replace the failed node, then  Repository Manager clustering may be calculating the write quorum for  database transactions still considering the removed node. In this  example, Nexus Repository Manager will continue to operate. However,  after each abrupt removal of a node, it becomes more likely that a  cluster could have difficulty achieving write quorum.

### Troubleshooting

If your cluster enters a state where write quorum cannot be achieved, then Nexus Repository Manager will present the option to reset your cluster. The write quorum not reached warning appears as a horizontal bar across the top of the Nexus Repository Manager user interface.

Before resetting your Nexus Repository Manager cluster, you should attempt to  manually resolve the cluster issues. There are a few ways you can  manually resolve the issues.

- Try to cleanly shutdown all clustered nodes, except one, and then try to bring your cluster back online by restarting the nodes one at a time waiting for each to finish starting before starting the next.
- You can also try to reconnect or restart nodes that may have been killed or disconnected. If the restarted nodes are not meant to be permanent  members of the cluster, then performing a clean shutdown after  restarting or reconnecting will properly remove the nodes from the  cluster.

As a last resort, you can click the *Troubleshoot* link in the write quorum warning. On the cluster reset page, you will see the *Reset Database Quorum* button. When you click on the button, all entries are removed from the cluster  table except for the node that processes the request to reset the  cluster. In practice, this means that all other nodes need to be  shutdown and removed from the load balancer rotation before resetting  the cluster.

WARNING

 

It is only safe to reset the cluster once all other nodes have been cleanly shutdown.

After the reset completes, the cluster will immediately begin accepting writes on the remaining  node. Nodes can then be added to the cluster and placed back into the  load balancer rotation. 

## Monitoring Node Health

Once your HA environment is set up, you should monitor the health of the  nodes in your cluster. The read-only status of the cluster is visible at the `http://<serveripaddress>:<port>/service/metrics/data` endpoint, as "readonly.enabled", under "gauges". See the [support article](https://support.sonatype.com/hc/en-us/articles/226254487) for the HTTP endpoints that HA-C exposes and/or the [support article](https://support.sonatype.com/hc/en-us/articles/218501277) for enabling JMX, for more information.

You can also use the load balancer friendly [status resource](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/load-balancing#LoadBalancing-MonitoringNodeHealth) to check the health of the nodes in an HA cluster.

## Task Management

In a Nexus Repository Manager cluster, it is necessary to consider which  node or nodes execute a task. Knowing how the tasks run in a cluster  will be an important element of monitoring cluster health. Nexus  Repository Manager now places task logging output in a separate file that corresponds to the execution of the task. Many tasks will be executed on a random single node in the cluster to balance the resource usage across the cluster. Other  tasks may execute simultaneously on all nodes in the cluster. Monitoring of the task must take into account the behavior of the execution of the task. The node requirements for common tasks are listed below.

### Admin - Execute script

In a Nexus Repository Manager cluster, when you create an *Admin - Execute script* task, you are able to specify whether the task runs on all nodes  simultaneously or whether the script is only executed on a single node.  When a task manipulates a resource that is shared within the cluster,  such as repository configuration or a blob store, the task should only  be executed on a single node. However, if the task is interacting with a resource that is local to the node, such as the search indices, then  the task needs to be configured with *Run task on all nodes in the cluster* selected. 

### Export Configuration & Metadata for Backup Task

Please see the section on [designing your backup plan](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process).

### Single-node Tasks

The following Single-node Tasks will run on a single, randomly-selected node in a Nexus Repository Manager cluster:

- *Admin - Cleanup tags*
- *Admin - Compact blob store*
- *Admin - Delete orphaned API keys*
- *Admin - Log database table record counts*
- *Admin - Remove a member from a blob store group*
- *Docker - Delete incomplete uploads*
- *Docker - Delete unused manifests and images*
- *Maven - Delete SNAPSHOT*
- *Maven - Delete unused SNAPSHOT*
- *Maven - Publish Maven Indexer files*
- *Maven - Unpublish Maven Indexer files*
- *Repair - Rebuild Maven repository metadata (maven-metadata.xml)*
- *Repair - Rebuild Yum repository metadata (repodata)*
- *Repair - Reconcile component database from blob store*
- *Repository - Delete unused components*

The following Single-node Tasks will run on a single selected node in a Nexus Repository Manager cluster:

- *Admin - Export database for backup*
- *Repair - Rebuild repository browse*
- *Repair - Reconcile date metadata for blob store*

### Multi-node Tasks

The following tasks may run on all nodes or on the current node (i.e. the node the UI is interacting with):

- *Admin - Execute script*
- *Repair - Rebuild repository search*
- *Repair - Reconcile npm /-/v1/search metadata*

# JMX Lifecycle Operations

 

 

 

These operations are intended to be used with the guidance of Sonatype  support.  Usage without supervision of Sonatype is not supported or  recommended.

## ManagedLifecycleBean

This JMX bean provides a set of attributes and operations to allow the user  to inspect and manipulate the current lifecycle phase of a Nexus Repo  Manager instance. The expectation is that these will be used for  recovery operations. A Phase is a step in the start up process used to  group similar components together and ensure that their dependencies are started before them. In order, the phases are: OFF, KERNEL, STORAGE,  RESTORE, UPGRADE, SCHEMAS, EVENTS, SECURITY, SERVICES, CAPABILITIES, and TASKS. 

1. OFF - NXRM is completely off. 
2. KERNEL - The most basic parts of the node are running. 
3. STORAGE - The databases and caches are setup. 
4. RESTORE - Any restoring from backups happens in this phase. 
5. UPGRADE - If any upgrades are needed, they will occur is this phase.
6. SCHEMAS - This creates any missing schemas in the database.
7. EVENTS - The framework that lets services communicate events is started.
8. SECURITY - The parts of NXRM responsible for security are enabled.
9. SERVICES - The bulk of an instances components are started here. 
10. CAPABILITIES - The capabilities are started, see [Nodes](https://help.sonatype.com/repomanager3/nexus-repository-administration/nodes#Nodes-AccessingandConfiguringCapabilities)
11. TASKS - The task scheduler is enabled, and tasks may run.

### Attributes

#### Phase

![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/30375993/30375994/1/1556037905369/phase.png)

Possible Value: Case sensitive string, one of OFF, KERNEL, STORAGE, RESTORE, UPGRADE, SCHEMAS, EVENTS, SECURITY, SERVICES, CAPABILITIES, or TASKS. 

This sets NXRM to run in a particular phase. By moving a Nexus Repo manager instance to a new phase, you can start or stop select components. For  example, if your instance is running in the TASKS phase and you set the  phase to CAPABILITIES, then that instance will not run any scheduled  tasks until it is moved back to the TASKS phase.

### Operations

#### Bounce

![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/30375993/30375995/1/1556037905404/bounce.png)

Possible Value for phase: Case sensitive string, one of OFF, KERNEL, STORAGE, RESTORE, UPGRADE, SCHEMAS, EVENTS, SECURITY, SERVICES, CAPABILITIES, or TASKS.

This operation takes the instance down to the supplied Lifecycle phase, and  then back to the phase in which it was originally running. This  effectively restarts the phases. 

# JMX Maintenance Operations

 

**Available in Nexus Repository Pro**

 

 

## DatabaseMaintenanceBean

![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/28345668/28345674/1/1555103888247/DatabaseMaintenanceBean.png)

This JMX bean provides a set of attributes and operations to allow the user  to inspect and manipulate the state of a node in Orient database  cluster. The expectation is that these will be used mainly for  troubleshooting and recovery operations.

### Attributes

#### DatabaseRole

 

Only available in HA-C

 

Applies to all databases on the current node



![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/28345668/28345673/1/1555103888227/DatabaseRole.png)

**Possible Values:** MASTER, REPLICA

This attribute represents the role of the node in an Orient cluster. The  role of MASTER represents a fully writable node in an Orient cluster  which participates in the `writeQuorum`. The role of REPLICA represents a node in read-only mode which is accepting only idempotent commands, e.g. read and queries.

When setting a node's role to REPLICA an override is added to Orient's configuration so that node will always be a REPLICA, regardless of the cluster's default role. When the node  is set back to MASTER that override is removed, so it will defer to the cluster's default role - which is MASTER unless it's been frozen.

#### References

- [Distributed-Architecture: Server Roles](http://orientdb.com/docs/2.2.x/Distributed-Architecture.html#server-roles)

#### DatabaseStatus

 

Only available in HA-C

 

Applies to all databases on the current node



![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/28345668/28345672/1/1555103888206/DatabaseStatus.png)

**Possible Values**: ONLINE, OFFLINE, NOT_AVAILABLE

This attribute represents the status of the node in an Orient cluster. The  status of ONLINE represents a node in a normal state which is fully  participating in the cluster. The status of OFFLINE represents a node  that is not currently participating in the cluster. The status of  NOT_AVAILABLE represents a node that is not currently available for some reason.

 

Setting the database status to NOT_AVAILABLE triggers an automatic delta-sync  with the rest of the cluster. If this is successful, it will move back  to ONLINE.

### Operations

#### checkDatabase

![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/28345668/28345670/1/1555103888162/checkDatabase.png)

**Possible Database Names:** accesslog, component, config, security

Checks database pages for corruption and checks that indices cover all records (i.e. no duplicates).

#### repairDatabase

![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/28345668/28345669/1/1555103888139/repairDatabase.png)

**Possible Database Names:** accesslog, component, config, security

Attempts to repair any corrupt database pages and rebuilds indices.

#### reinstallDatabase

 

Only available in HA-C

![Java Monitoring and Management Console](https://help.sonatype.com/repomanager3/files/28345668/28345671/1/1555103888186/reinstallDatabase.png)

**Possible Database Names:** accesslog, component, config, security

Attempts to reinstall the full database from the rest of the cluster.

 

The user can't force a reinstall from a specific node, but typically Orient will choose the node that most recently took a backup or the oldest  member (assuming that's not the node requesting the reinstall)

# Upgrading your cluster

 

 

## Upgrading an HA-C Environment from 3.x to 3.y

### Prepare for HA-C Upgrade

Do not upgrade unless you have established a reliable and working [HA-C backup and restore](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process#DesigningyourClusterBackup/RestoreProcess-Restoringfrombackup) process. This includes a backup of shared blob stores filesystems.

HA-C upgrade is not supported and will fail when:

- clustering is enabled ( `nexus.clustered=true` )
- databases are read-only
- restoring databases from a backup created with an earlier version

Download, unpack and [pre-configure new version distribution install files on each cluster node](https://support.sonatype.com/hc/en-us/articles/115000350007-Upgrading-Nexus-Repository-Manager-3-1-0-and-Newer) before performing the upgrade.

### Perform HA-C Upgrade

We will refer to your three nodes as **NODE-A**, **NODE-B** and **NODE-C**.

**NODE-A** will be upgraded to a new version in unclustered mode.

**NODE-B** and **NODE-C** will join and sync databases from **NODE-A** when they start for the first time with the new version.

For each individual node being upgraded, make sure you [adhere to the instructions for upgrading a single node](https://support.sonatype.com/hc/en-us/articles/115000350007-Upgrading-Nexus-Repository-Manager-3-1-0-and-Newer), especially the part of merging configurration file changes

1. Schedule an outage for your service; deny access to all nodes in the cluster using your load balancer.
2. Perform an HA-C backup. [Backup the databases](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/designing-your-cluster-backup-restore-process) of **NODE-A** and **at least one other node**.
3. Shut down **NODE-C** gracefully and wait for it to stop.
4. Shut down **NODE-B** gracefully and wait for it to stop.
5. Edit **NODE-A** `$data-dir/etc/nexus.properties` file and set `nexus.clustered=false` to disable clustering. Save the file.
6. Shut down **NODE-A** gracefully and wait for it to stop.
7. Start **NODE-A** using the new version. Wait for it to startup in un-clustered mode and perform automatic upgrade steps.
8. Verify **NODE-A** is functioning after the upgrade in un-clustered mode, e.g. use the UI and access repository content.
9. If **NODE-A** fails verification, follow our [HA-C upgrade rollback procedure](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)/operating-your-cluster/upgrading-your-cluster#Upgradingyourcluster-RollbackaFailedHA-CUpgrade).
10. If **NODE-A** passes verification, then delete the **NODE-B** and **NODE-C** `$data-dir/db` directory (you have already made database backups earlier).
11. Edit **NODE-A** `$data-dir/etc/nexus.properties` file and set `nexus.clustered=true` to enable clustering on the next startup. Save the file.
12. Restart **NODE-A**. Wait for it to startup in clustered mode.
13. Start **NODE-B** with the new version. Wait for it to startup in HA-C mode and sync the databases from **NODE-A**.
14. Start **NODE-C** with the new version. Wait for it to startup in HA-C mode and sync the databases from **NODE-A** or **NODE-B**.
15. Sign-in as an administrator and confirm that all nodes are visible in the *System* → *Nodes* administration page and that there are no status warnings about cluster health.
16. Verify that all nodes are functioning normally on the new version, e.g. use the UI and access repository content.
17. Restore access through your load balancer and resume operations.

## Rollback a Failed HA-C Upgrade

Do not attempt to start a older version of NXRM against an already upgraded or partially upgraded [data directory](https://help.sonatype.com/repomanager3/installation-and-upgrades/directories) or databases. This will fail.

The only supported way to rollback to an old version is to start NXRM  against a set of databases and data directory were created using an  identical NXRM version.

# Load Balancing

 

 

## Overview

Load balancing is required to achieve transparent redundancy in a High  Availability Cluster. The following outlines considerations relating specifically to Nexus Repository.

## Ports

Nexus Repository listens on port 8081 for HTTP by default. There is no  default port for HTTPS. These ports are also configurable (see [Changing the HTTP Port](https://help.sonatype.com/repomanager3/installation-and-upgrades/configuring-the-runtime-environment#ConfiguringtheRuntimeEnvironment-ChangingtheHTTPPort)).

### Docker Repositories

The docker client does not allow a context as part of the path to a registry, as the namespace and image name are embedded in the URLs it uses. Nexus  Repository allows custom ports to be exposed to create a listener on a  root context. See [SSL and Repository Connector Configuration](https://help.sonatype.com/repomanager3/nexus-repository-administration/formats/docker-registry/ssl-and-repository-connector-configuration) for more details. Alternatively you may choose to use a [URL rewriting scheme](https://support.sonatype.com/hc/en-us/articles/360000761828) to achieve the same results without needing to configure custom ports (not covered here). Using URL rewriting allows you to sidestep this limitation, otherwise exposing and using the configured port(s) are required.

## Sticky Sessions

The Nexus Repository UI uses a cookie named NXSESSIONID to maintain a users state. The load balancer should be configured to enforce sticky sessions for requests containing this cookie.

User UI state is unique to each node in the cluster. All UI requests for the same session should be directed to the same node.

## Monitoring Node Health

Nexus Repository provides two endpoints you can use to check the status of your node. The *Readable Health Check* endpoint verifies that a node can handle read requests. The *Writable Health Check* endpoint verifies that a node can handle read and write requests. Since a node that can  handle read requests but not write requests may still be considered  serviceable, it is left to the user to decide which status endpoint to use.

These endpoints can be used by a load balancer to determine if a node should  be considered to handle requests. For both status endpoints, Success is  represented as `HTTP 200 OK`. Failure is represented as `HTTP 503 SERVICE UNAVAILABLE`. The nexus.log file for the node should be inspected for further details.

The URLs for the Health Check endpoints are:

| Name                      | URL                                                          |
| ------------------------- | ------------------------------------------------------------ |
| **Readable Health Check** | **`http://<hostname>:<port>/service/rest/v1/status`**        |
| **Writable Health Check** | **`http://<hostname>:<port>/service/rest/v1/status/writable`** |

## SSL Termination

Nexus Repository can be configured to serve content using SSL (see [Configuring to Serve Content via HTTPS](https://help.sonatype.com/repomanager3/nexus-repository-administration/configuring-ssl#ConfiguringSSL-InboundSSL-ConfiguringtoServeContentviaHTTPS)). The recommended approach for HA-C is to use the load balancer for SSL  termination as it alleviates the need to configure certificates on each  node.

# Scaling with Proxy Nodes

 

 

Operating a mission-critical Nexus Repository deployment can sometimes mean  serving a significant read load. This topic covers a common scalability  pattern that relies on proxy repositories.

## Use Cases

This reference architecture is designed to do the following:

- Improve scalability when a large read load is too much for your primary Nexus Repository instance
- Improve availability in response to large spikes in read requests

## Requirements

To implement this reference architecture, you will need the following:

- The latest version of Nexus Repository OSS or Pro
- Load Balancer (e.g., nginx, AWS ELB, etc.)
- Separate storage for each node

## Setting up the Architecture

This architecture contains the following elements:

- A Nexus Repository Pro **primary node**: the system of record for components in hosted repositories
- One or more Nexus Repository **proxy nodes**
- A **load balancer** to distribute traffic among the proxy nodes

![Diagram of scaling with proxy nodes as described above.](https://help.sonatype.com/repomanager3/files/100139166/100139167/1/1639747635333/Scaling+with+Proxy+Repositories+%282%29.png)

### The Primary Node

This document assumes that the primary node is pre-existing and that only the layer of proxy nodes is new.

The primary node may itself be a more complex deployment with its own load balancer, such as one of the [resilient deployment models](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability), or a legacy [High-Availability Clustering (HA-C)](https://help.sonatype.com/repomanager3/planning-your-implementation/resiliency-and-high-availability/high-availability-clustering-(legacy)) cluster.

### Configuring the Proxy Nodes

#### Proxy Nodes Location

In this reference architecture, the proxy nodes are deployed in the same  data center or availability zone (AZ) as the primary node in order to  keep the request latency low.

In the case of cloud  deployments that have a resilient primary node (a deployment that spans  multiple cloud AZs), the proxy nodes should also be spread between  multiple AZs. This ensures that proxy nodes with warm caches are still  accessible should an AZ fail.

 

All repositories in the primary node must exist in the proxy nodes in both AZs.

#### Proxy Nodes are Independent

Other than proxy connections, each proxy node is independent from its peers  and from the primary node. They must not share blob storage, application folders, or external database instances (if applicable), nor should  they be part of an HA-C cluster.

#### Repositories on the Proxy Nodes

The proxy nodes should have a proxy repository configured for each end user-accessible repository on the primary node.

The set of repositories on the proxy nodes does not need to exactly match  the primary node. While the primary node will often have a number of  hosted and proxy repositories, these may not all be directly accessible  to downstream consumers. Instead, they should be aggregated behind group repositories for ease of use. When this is the case, the proxy node  only needs to define a proxy repository for each group on the primary.

![Diagram showing a primary node containing a group repository with proxy and hosted repositories. The proxy repository points downstream to public repositories. A proxy node points downstream to the group repository within the primary node.](https://help.sonatype.com/repomanager3/files/100139166/100139168/1/1639747635452/proxyimage3.png)

#### Proxy Cache Settings

Proxy repositories have negative cache settings that control how frequently they re-check for unavailable  components. You should leave these set to their default values on the  proxy nodes. 

If you frequently publish new versions  of components, reduce the setting for maximum metadata age from the  defaults or set it to zero to ensure that the proxy node regularly  checks the primary node.

#### Third-Party Components

For this reference architecture, we recommend that you proxy third-party  components through the primary node. This is key to ensure that the  primary node is a cache of record for third-party dependencies, which  allows repeatable builds.

This also centralizes any repository routing rules at the primary node, reducing the amount of configuration that you must maintain at each proxy node.

#### Cleanup Policies

Because the proxy nodes are not the system of record for any components, you  can configure aggressive cleanup policies for them. As a baseline, we  suggest removing all components that haven’t been downloaded in only a  few weeks.

### Load Balancer

This reference architecture is compatible with a broad range of load balancers such as the following:

- AWS ELB
- Nginx

One simple balancing strategy is "[round robin](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/configuring-blob-stores#ConfiguringBlobStores-WhatisaFillPolicy?)," and this is adequate for most deployments; for other options, see [Advanced Considerations](https://help.sonatype.com/repomanager3/planning-your-implementation/scaling-with-proxy-nodes#ScalingwithProxyNodes-AdvancedConsiderations) below.

Setting up a load balancer requires header changes similar to those required by a reverse proxy. See [Run Behind a Reverse Proxy](https://help.sonatype.com/repomanager3/planning-your-implementation/run-behind-a-reverse-proxy) for examples.

## Considerations and Limitations

### Choosing the Number of Proxy Nodes

The number of proxy nodes you will need depends on your use case. To  improve the scalability of a primary node under continuous read load, a  single proxy node may be sufficient.

For situations  with large request spikes, it may be necessary to use multiple proxy  nodes with pre-warmed caches in order to sufficiently protect the  primary.

### Authentication

If the read load is anonymous (unauthenticated), then no special handling  is required. However, if the read load is authenticated, then you should use an external identity provider ([LDAP](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/ldap), [SAML](https://help.sonatype.com/repomanager3/nexus-repository-administration/user-authentication/saml)) so that credentials are synchronized for each user.

For authenticated read loads, you will need to set up the appropriate  roles, privileges and content selectors on each proxy node.

### User Token Authentication

We do not recommend using this pattern with user tokens as they are not synchronized between the various instances.

### Maven Repositories and Search Results

Using this pattern may reduce search result completeness for Maven users as they must rely on Nexus Repository’s [built-in search features](https://help.sonatype.com/repomanager3/using-nexus-repository/searching-for-components) (the UI or REST endpoints).

Nexus Repository’s built-in search only shows results for components that  have been cached on that proxy node; it will not show components that  are available on the primary and not yet cached on a proxy node.

This limitation does not apply to the search commands of non-Maven clients  using Visual Studio’s NuGet functionality or the npm command-line  client. Nexus Repository passes these ecosystem-specific search commands upstream from proxy node to primary node and from primary to remote  repositories (if applicable).

### Changes to Primary Node Repositories

From time to time, you may add or remove repositories from your primary  node. This will require keeping the proxy nodes updated with appropriate configurations.

For proxy nodes that are long-lasting, the simplest approach is to use the [Repositories REST API](https://help.sonatype.com/repomanager3/integrations/rest-and-integration-api/repositories-api) to add, update, and remove repositories from the proxy nodes as necessary.

## Advanced Considerations  

### Node Sizing and Configuration

When considering this pattern, the primary node should already be an  enterprise-sized machine capable of coping with your regular loads. For  this pattern, the number of Jetty Threads can be increased from 400  (default) to 600. See [System Requirements](https://help.sonatype.com/repomanager3/product-information/system-requirements) for more information.

An example primary node configuration could appear as follows: 

- **CPU**: 16 Cores
- **RAM**: 64GB
- **Memory Configuration**: -Xms6G -Xmx6G -XX:MaxDirectMemorySize=15530M
- **Jetty Thread Configuration**: 600 


A proxy node machine could look like this:

- **CPU**: 16 Cores
- **RAM**: 32GB
- **Memory Configuration**: -Xms6G -Xmx6G -XX:MaxDirectMemorySize=15530M
- **Jetty Thread Configuration**: 400 
- **Proxy Repositories - Metadata Maximum Age**: 0
- **Cleanup Policy:** Aggressive, see Cleanup Policy section above.

 

Do not modify negative cache settings from their defaults for this use case.

### Proxy Node Storage Size

The optimal storage size for each proxy node will depend on the  characteristics of your load (e.g., if there is diversity or commonality in the components most requested by the readers) and the rate of new  component production.

Whatever size you choose, use a [cleanup policy](https://help.sonatype.com/repomanager3/nexus-repository-administration/repository-management/cleanup-policies) that will keep the overall storage level within the allocated storage.

### Multiple Proxy Node Sets

If the primary node has a large number of repositories, you may want to  split them among separate sets of proxy nodes in order to improve the  chance of a cache hit. Each pool only covers a subset of the  repositories in the primary node.

In this model, the load balancer must decode the HTTP request and route the traffic to the appropriate proxy node pool.

### Proxy Node Cloning

In the recommended model, any new proxy nodes added to the pool will start with empty caches; cache misses will be common until the proxy node has time to catch up. Initially, this may detract from overall performance  for downstream clients. If the primary node has a lot of components  (i.e., hundreds of thousands or millions), it may take some time before  the cache is sufficiently warm to improve performance overall.

One technique for reducing the cache warming time is to clone an existing proxy node. You can do this using a normal [backup and restore](https://help.sonatype.com/repomanager3/planning-your-implementation/backup-and-restore) process on a proxy node, or by creating a snapshot at the VM or storage level and using it to instantiate a new proxy node.

Cloned nodes will start with a warm cache. Depending on the nature of the  workload, even a clone from a days-old backup can be a significant  improvement over an empty cache.

If you are using a cloud  blob store (e.g., S3, Azure, etc.), the Nexus database includes a  reference to a specific storage bucket. If you clone the Nexus  Repository database/instance, this can cause two Nexus Repository  instances to attempt to use the same bucket for blob storage, which will cause issues. Instead, you will need to create a different bucket and  change the blob store configuration using the `NEXUS_BLOB_STORE_OVERRIDE` environment variable to use the new bucket as described in [Using Replicated S3 Blob Stores for Recovery or Testing](https://help.sonatype.com/repomanager3/planning-your-implementation/storage-guide/using-replicated-s3-blob-stores-for-recovery-or-testing ).

### Proxy Node Cache Metrics

Cache hit and miss rates can be calculated from the data available at the Nexus Repository metrics endpoint `./service/metrics/data`. Calculating the inbound vs. outbound requests through the proxy node.