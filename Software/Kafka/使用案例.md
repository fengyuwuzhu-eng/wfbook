# 使用案例

 Here is a description of a few of the popular use cases for Apache Kafka®. For an overview of a number of these areas in action, see [this blog post](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/). 
以下是 Apache Kafka® 的一些常见使用案例的描述。有关其中许多领域的实际应用概述，请参阅[此博客文章 ](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/)。

#### [Messaging 消息](https://kafka.apache.org/uses#uses_messaging)

Kafka works well as a replacement for a more traditional message broker. Message brokers are used for a variety of reasons (to decouple  processing from data producers, to buffer unprocessed messages, etc). In comparison to most messaging systems Kafka has better throughput,  built-in partitioning, replication, and fault-tolerance which makes it a good solution for large scale message processing applications. 
Kafka 可以很好地替代更传统的消息代理。 使用消息代理的原因有很多（将处理与数据创建者分离、缓冲未处理的消息等）。 与大多数消息传递系统相比，Kafka 具有更好的吞吐量、内置分区、复制和容错功能，这使其成为一个很好的 适用于大规模消息处理应用程序的解决方案。

In our experience messaging uses are often comparatively low-throughput, but may require low end-to-end latency and often depend on the strong durability guarantees Kafka provides. 
根据我们的经验，消息传递用途的吞吐量通常相对较低，但可能需要较低的端到端延迟，并且通常依赖于 Kafka 提供的强大持久性保证。

In this domain Kafka is comparable to traditional messaging systems such as [ActiveMQ](https://activemq.apache.org) or [RabbitMQ](https://www.rabbitmq.com). 
在这个领域，Kafka 与传统消息传递系统（如 [ActiveMQ](https://activemq.apache.org) 或 [RabbitMQ 的 RabbitMQ](https://www.rabbitmq.com) 中。

#### [Website Activity Tracking 网站活动跟踪](https://kafka.apache.org/uses#uses_website)

The original use case for Kafka was to be able to rebuild a user  activity tracking pipeline as a set of real-time publish-subscribe  feeds. This means site activity (page views, searches, or other actions users  may take) is published to central topics with one topic per activity  type. These feeds are available for subscription for a range of use cases  including real-time processing, real-time monitoring, and loading into  Hadoop or offline data warehousing systems for offline processing and reporting. 
Kafka 的原始用例是能够将用户活动跟踪管道重建为一组实时发布-订阅源。 这意味着网站活动（页面查看、搜索或用户可能执行的其他作）将发布到中心主题，每个活动类型一个主题。 这些订阅源可用于一系列使用案例，包括实时处理、实时监控和加载到 Hadoop 或 用于离线处理和报告的离线数据仓库系统。

Activity tracking is often very high volume as many activity messages are generated for each user page view. 
活动跟踪通常非常高，因为每个用户页面视图都会生成许多活动消息。

#### [Metrics 指标](https://kafka.apache.org/uses#uses_metrics)

Kafka is often used for operational monitoring data. This involves aggregating statistics from distributed applications to produce centralized feeds of operational data. 
Kafka 通常用于运营监控数据。 这涉及聚合来自分布式应用程序的统计数据，以生成运营数据的集中源。

#### [Log Aggregation 日志聚合](https://kafka.apache.org/uses#uses_logs)

Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and  puts them in a central place (a file server or HDFS perhaps) for  processing. Kafka abstracts away the details of files and gives a cleaner  abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers  equally good performance, stronger durability guarantees due to  replication, and much lower end-to-end latency. 
许多人使用 Kafka 作为日志聚合解决方案的替代品。 日志聚合通常从服务器收集物理日志文件，并将它们放在一个中心位置（可能是文件服务器或 HDFS）进行处理。 Kafka 抽象出文件的详细信息，并将日志或事件数据更清晰地抽象为消息流。 这允许更低的处理延迟，并更轻松地支持多个数据源和分布式数据使用。 与 Scribe 或 Flume 等以日志为中心的系统相比，Kafka 提供了同样好的性能、更强的持久性保证（由于复制）、 以及更低的端到端延迟。

#### [Stream Processing 流处理](https://kafka.apache.org/uses#uses_streamprocessing)

Many users of Kafka process data in processing pipelines consisting of  multiple stages, where raw input data is consumed from Kafka topics and  then aggregated, enriched, or otherwise transformed into new topics for  further consumption or follow-up processing. For example, a processing pipeline for recommending news articles might  crawl article content from RSS feeds and publish it to an "articles"  topic; further processing might normalize or deduplicate this content and  publish the cleansed article content to a new topic; a final processing stage might attempt to recommend this content to  users. Such processing pipelines create graphs of real-time data flows based on the individual topics. Starting in 0.10.0.0, a light-weight but powerful stream processing  library called [Kafka Streams](https://kafka.apache.org/documentation/streams) is available in Apache Kafka to perform such data processing as described above. Apart from Kafka Streams, alternative open source stream processing tools include [Apache Storm](https://storm.apache.org/) and [Apache Samza](https://samza.apache.org/). 
Kafka 的许多用户在由多个阶段组成的处理管道中处理数据，其中原始输入数据从 Kafka 主题中使用，然后 聚合、丰富或以其他方式转换为新主题以供进一步使用或后续处理。 例如，用于推荐新闻文章的处理管道可能会从 RSS 源中抓取文章内容并将其发布到“文章”主题; 进一步的处理可能会规范化或删除重复的内容，并将清理后的文章内容发布到新主题; 最终处理阶段可能会尝试向用户推荐此内容。 此类处理管道根据各个主题创建实时数据流图。 从 0.10.0.0 开始，Apache Kafka 中提供了一个名为 [Kafka Streams](https://kafka.apache.org/documentation/streams) 的轻量级但功能强大的流处理库，用于执行上述数据处理。除了 Kafka Streams 之外，其他开源流处理工具还包括 [Apache Storm](https://storm.apache.org/) 和 [阿帕奇萨姆扎 ](https://samza.apache.org/)。

#### [Event Sourcing 事件溯源](https://kafka.apache.org/uses#uses_eventsourcing)

[Event sourcing](https://martinfowler.com/eaaDev/EventSourcing.html) is a style of application design where state changes are logged as a time-ordered sequence of records. Kafka's support for very large stored  log data makes it an excellent backend for an application built in this  style. 
[事件溯源](https://martinfowler.com/eaaDev/EventSourcing.html)是一种应用程序设计风格，其中状态更改被记录为 按时间排序的记录序列。Kafka 对非常大的存储日志数据的支持使其成为以这种样式构建的应用程序的出色后端。

#### [Commit Log 提交日志](https://kafka.apache.org/uses#uses_commitlog)

Kafka can serve as a kind of external commit-log for a distributed  system. The log helps replicate data between nodes and acts as a  re-syncing mechanism for failed nodes to restore their data. The [log compaction](https://kafka.apache.org/documentation.html#compaction) feature in Kafka helps support this usage. In this usage Kafka is similar to [Apache BookKeeper](https://bookkeeper.apache.org/) project.  			
Kafka 可以用作分布式系统的一种外部提交日志。日志有助于在节点之间复制数据并充当重新同步 失败节点恢复其数据的机制。 Kafka 中的[日志压缩](https://kafka.apache.org/documentation.html#compaction)功能有助于支持这种用法。在这个用法中，Kafka 类似于 [Apache BookKeeper](https://bookkeeper.apache.org/) 项目。