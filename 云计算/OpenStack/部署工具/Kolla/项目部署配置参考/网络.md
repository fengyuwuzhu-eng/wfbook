# Networking 联网

​        version 版本              



Kolla deploys Neutron by default as OpenStack networking component. This section describes configuring and running Neutron extensions like Networking-SFC, QoS, and so on.
默认情况下，Kolla 将 Neutron 部署为 OpenStack 网络组件。本节介绍如何配置和运行 Neutron 扩展，如 Networking-SFC、QoS 等。

- [Designate - DNS service
  指定 - DNS 服务](https://docs.openstack.org/kolla-ansible/latest/reference/networking/designate-guide.html)
- [DPDK DPDK的](https://docs.openstack.org/kolla-ansible/latest/reference/networking/dpdk.html)
- [Neutron - Networking Service
  Neutron - 网络服务](https://docs.openstack.org/kolla-ansible/latest/reference/networking/neutron.html)
- [Neutron Extensions 中子扩展](https://docs.openstack.org/kolla-ansible/latest/reference/networking/neutron-extensions.html)
- [Octavia 奥克塔维娅](https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html)
- [SRIOV 斯里奥夫](https://docs.openstack.org/kolla-ansible/latest/reference/networking/sriov.html)

# Designate - DNS service 指定 - DNS 服务

​        version 版本              





## Overview 概述 ¶

Designate provides DNSaaS services for OpenStack:
Designate 为 OpenStack 提供 DNSaaS 服务：

- REST API for domain/record management
  用于域/记录管理的 REST API
- Multi-tenant 多租户
- Integrated with Keystone for authentication
  与 Keystone 集成以进行身份验证
- Framework in place to integrate with Nova and Neutron notifications (for auto-generated records)
  与 Nova 和 Neutron 通知集成的框架（用于自动生成的记录）
- Support for Bind9 and Infoblox out of the box
  开箱即用的 Bind9 和 Infoblox 支持

## Configuration on Kolla deployment Kolla 部署上的配置 ¶

Enable Designate service in `/etc/kolla/globals.yml`
启用指定 `/etc/kolla/globals.yml` 服务

```
enable_designate: "yes"
neutron_dns_domain: "example.org."
```



 

Important 重要



The `neutron_dns_domain` value has to be different to `openstacklocal` (its default value) and has to end with a period `.`.
该 `neutron_dns_domain` 值必须不同 `openstacklocal` 于 （其默认值），并且必须以句点 `.` 结尾。



 

Important 重要



`DNS Integration` is enabled by default and can be disabled by adding `neutron_dns_integration: no` to `/etc/kolla/globals.yml` and reconfiguring with `--tags` neutron.
 `DNS Integration` 默认情况下处于启用状态，可以通过添加 `neutron_dns_integration: no`  `/etc/kolla/globals.yml` 和重新配置 `--tags` Neutron 来禁用。

Configure Designate options in `/etc/kolla/globals.yml`
配置“ `/etc/kolla/globals.yml` 指定”选项



 

Important 重要



Designate MDNS node requires the `dns_interface` to be reachable from management network.
指定 MDNS 节点要求可从管理网络访问。 `dns_interface` 

```
dns_interface: "eth1"
designate_ns_record:
  - "ns1.sample.openstack.org"
```



 

Important 重要



If multiple nodes are assigned to be Designate workers, then you must enable a supported coordination backend, currently only `redis` is supported. The backend choice can be overridden via the `designate_coordination_backend` variable. It defaults to `redis` when `redis` is enabled (`enable_redis` is set to `yes`).
如果将多个节点分配给指定工作线程，则必须启用受支持的协调后端，目前仅 `redis` 支持。可以通过 `designate_coordination_backend` 变量覆盖后端选项。它默认为 `redis` when `redis` is enabled （ `enable_redis` 设置为 `yes` ）。

The following additional variables are required depending on which backend you intend to use:
根据您打算使用的后端，需要以下附加变量：

### Bind9 Backend Bind9 后端 ¶

Configure Designate options in `/etc/kolla/globals.yml`
配置“ `/etc/kolla/globals.yml` 指定”选项

```
designate_backend: "bind9"
```

### Infoblox Backend Infoblox 后端 ¶



 

Important 重要



When using Infoblox as the Designate backend the MDNS node requires the container to listen on port 53. As this is a privilaged port you will need to build your designate-mdns container to run as the user root rather than designate.
使用 Infoblox 作为 Designate 后端时，MDNS 节点要求容器侦听端口 53。由于这是一个特权端口，因此您需要构建 designate-mdns 容器以用户根而不是指定身份运行。

Configure Designate options in `/etc/kolla/globals.yml`
配置“ `/etc/kolla/globals.yml` 指定”选项

```
designate_backend: "infoblox"
designate_backend_infoblox_nameservers: "192.168.1.1,192.168.1.2"
designate_infoblox_host: "192.168.1.1"
designate_infoblox_wapi_url: "https://infoblox.example.com/wapi/v2.1/"
designate_infoblox_auth_username: "username"
designate_infoblox_ns_group: "INFOBLOX"
```

Configure Designate options in `/etc/kolla/passwords.yml`
配置“ `/etc/kolla/passwords.yml` 指定”选项

```
designate_infoblox_auth_password: "password"
```

For more information about how the Infoblox backend works, see [Infoblox backend](https://docs.openstack.org/designate/latest/admin/backends/infoblox.html).
有关 Infoblox 后端工作原理的更多信息，请参阅 Infoblox 后端。

## Neutron and Nova Integration Neutron 和 Nova 集成 ¶

The `designate-sink` is an optional service which listens for event notifications, such as compute.instance.create.end, handlers are available for Nova and Neutron. Notification events can then be used to trigger record creation & deletion.
这是一项 `designate-sink` 可选服务，用于侦听事件通知，例如 compute.instance.create.end，处理程序可用于 Nova 和 Neutron。然后，可以使用通知事件来触发记录的创建和删除。



 

Note 注意



Service `designate-sink` in kolla deployments is disabled by default and can be enabled by `designate_enable_notifications_sink: yes`.
默认情况下，kolla 部署中的服务 `designate-sink` 处于禁用状态 `designate_enable_notifications_sink: yes` ，可以通过 启用。

Create default Designate Zone for Neutron:
为 Neutron 创建默认指定区域：

```
openstack zone create --email admin@sample.openstack.org sample.openstack.org.
```

Create designate-sink custom configuration folder:
创建 designate-sink 自定义配置文件夹：

```
mkdir -p /etc/kolla/config/designate/
```

Append Designate Zone ID in `/etc/kolla/config/designate/designate-sink.conf`
追加 Designate Zone ID in `/etc/kolla/config/designate/designate-sink.conf` 

```
[handler:nova_fixed]
zone_id = <ZONE_ID>
[handler:neutron_floatingip]
zone_id = <ZONE_ID>
```

Reconfigure Designate: 重新配置指定：

```
kolla-ansible reconfigure -i <INVENTORY_FILE> --tags designate,neutron,nova
```

## Verify operation 验证操作 ¶

List available networks:
列出可用的网络：

```
openstack network list
```

Associate a domain to a network:
将域关联到网络：

```
openstack network set <NETWORK_ID> --dns-domain sample.openstack.org.
```

Start an instance: 启动实例：

```
openstack server create \
  --image cirros \
  --flavor m1.tiny \
  --key-name mykey \
  --nic net-id=${NETWORK_ID} \
  my-vm
```

Check DNS records in Designate:
检查“指定”中的 DNS 记录：

```
openstack recordset list sample.openstack.org.

+--------------------------------------+---------------------------------------+------+---------------------------------------------+--------+--------+
| id                                   | name                                  | type | records                                     | status | action |
+--------------------------------------+---------------------------------------+------+---------------------------------------------+--------+--------+
| 5aec6f5b-2121-4a2e-90d7-9e4509f79506 | sample.openstack.org.                 | SOA  | sample.openstack.org.                       | ACTIVE | NONE   |
|                                      |                                       |      | admin.sample.openstack.org. 1485266928 3514 |        |        |
|                                      |                                       |      | 600 86400 3600                              |        |        |
| 578dc94a-df74-4086-a352-a3b2db9233ae | sample.openstack.org.                 | NS   | sample.openstack.org.                       | ACTIVE | NONE   |
| de9ff01e-e9ef-4a0f-88ed-6ec5ecabd315 | 192-168-190-232.sample.openstack.org. | A    | 192.168.190.232                             | ACTIVE | NONE   |
| f67645ee-829c-4154-a988-75341050a8d6 | my-vm.None.sample.openstack.org.      | A    | 192.168.190.232                             | ACTIVE | NONE   |
| e5623d73-4f9f-4b54-9045-b148e0c3342d | my-vm.sample.openstack.org.           | A    | 192.168.190.232                             | ACTIVE | NONE   |
+--------------------------------------+---------------------------------------+------+---------------------------------------------+--------+--------+
```

Query instance DNS information to Designate `dns_interface` IP address:
查询实例DNS信息以指定 `dns_interface` IP地址：

```
dig +short -p 5354 @<DNS_INTERFACE_IP> my-vm.sample.openstack.org. A
192.168.190.232
```

For more information about how Designate works, see [Designate, a DNSaaS component for OpenStack](https://docs.openstack.org/designate/latest/).
有关 Designate 工作原理的更多信息，请参阅 Designate，OpenStack 的 DNSaaS 组件。

# DPDK DPDK的

​        version 版本              





## Introduction 简介 ¶

Open vSwitch (ovs) is an open source software virtual switch developed and distributed via openvswitch.org. The Data Plane Development Kit (dpdk) is a collection of userspace libraries and tools that facilitate the development of high-performance userspace networking applications.
Open vSwitch （ovs） 是通过 openvswitch.org 开发和分发的开源软件虚拟交换机。数据平面开发工具包 （dpdk） 是用户空间库和工具的集合，可促进高性能用户空间网络应用程序的开发。

As of the ovs 2.2 release, the ovs netdev datapath has supported integration with dpdk for accelerated userspace networking. As of the pike release of kolla support for deploying ovs with dpdk (ovs-dpdk) has been added to kolla ansible. The ovs-dpdk role introduced in the pike release has been tested on centos 7 and ubuntu 16.04 hosts, however, ubuntu is recommended due to conflicts with the cgroup configuration created by the default systemd version shipped with centos 7.
从 ovs 2.2 版本开始，ovs netdev 数据路径支持与 dpdk 集成，以加速用户空间网络。从 kolla 的 pike  版本开始，kolla ansible 中添加了对使用 dpdk （ovs-dpdk） 部署 ovs 的支持。派克发行版中引入的 ovs-dpdk 角色已在 centos 7 和 ubuntu 16.04 主机上测试过，然而，由于与 centos 7 附带的默认 systemd  版本所创建的 cgroup 配置冲突，因此建议使用 ubuntu。

## Prerequisites 先决条件 ¶

DPDK is a high-performance userspace networking library, as such it has several requirements to function correctly that are not required when deploying ovs without dpdk.
DPDK 是一个高性能的用户空间网络库，因此它有几个正确运行的要求，这些要求在部署没有 DPDK 的 ovs 时是不需要的。

To function efficiently one of the mechanisms dpdk uses to accelerate memory access is the utilisation of kernel hugepages. The use of hugepage memory minimises the chance of a translation lookaside buffer(TLB) miss when translating virtual to physical memory as it increases the total amount of addressable memory that can be cached via the TLB. Hugepage memory pages are unswappable contiguous blocks of memory of typically 2MiB or 1GiB in size, that can be used to facilitate efficient sharing of memory between guests and a vSwitch or DMA mapping between physical nics and the userspace ovs datapath.
为了有效地运行，dpdk 用来加速内存访问的机制之一是利用内核 hugepages。在将虚拟内存转换为物理内存时，使用大页内存可以最大限度地减少转换后备缓冲区  （TLB） 未命中的可能性，因为它增加了可通过 TLB 缓存的可寻址内存总量。Hugepage 内存页是大小通常为 2MiB 或 1GiB  的不可交换连续内存块，可用于促进客户机之间以及物理网卡和用户空间 ovs 数据路径之间的 vSwitch 或 DMA 映射之间的内存有效共享。

To deploy ovs-dpdk on a platform a proportion of system memory should be allocated hugepages. While it is possible to allocate hugepages at runtime it is advised to allocate them via the kernel command line instead to prevent memory fragmentation. This can be achieved by adding the following to the grub config and regenerating your grub file.
要在平台上部署 ovs-dpdk，应为系统内存分配一定比例的 hugepages。虽然可以在运行时分配大页面，但建议通过内核命令行分配它们，以防止内存碎片。这可以通过将以下内容添加到 grub 配置并重新生成 grub 文件来实现。

```
default_hugepagesz=2M hugepagesz=2M hugepages=25000
```

As dpdk is a userspace networking library it requires userspace compatible drivers to be able to control the physical interfaces on the platform. dpdk technically support 3 kernel drivers `igb_uio`, `uio_pci_generic` and `vfio_pci`. While it is technically possible to use all 3 only `uio_pci_generic` and `vfio_pci` are recommended for use with kolla. `igb_uio` is BSD licenced and distributed as part of the dpdk library. While it has some advantages over `uio_pci_generic` loading the `igb_uio` module will taint the kernel and possibly invalidate distro support. To successfully deploy `ovs-dpdk`, `vfio_pci` or `uio_pci_generic` kernel module must be present on the platform. Most distros include `vfio_pci` or `uio_pci_generic` as part of the default kernel though on some distros you may need to install `kernel-modules-extra` or the distro equivalent prior to running **kolla-ansible deploy**.
由于 dpdk 是一个用户空间网络库，因此它需要与用户空间兼容的驱动程序才能控制平台上的物理接口。DPDK 在技术上支持 3 个内核驱动程序 `igb_uio` 和 `uio_pci_generic` `vfio_pci` 。虽然从技术上讲，只能 `uio_pci_generic` 使用所有 3 种，但 `vfio_pci` 建议与 kolla 一起使用。 `igb_uio` BSD 已获得许可，并作为 dpdk 库的一部分进行分发。虽然它比 `uio_pci_generic` 加载该 `igb_uio` 模块有一些优势，但它会污染内核，并可能使发行版支持失效。要成功部署 `ovs-dpdk` ， `vfio_pci` 或 `uio_pci_generic` 内核模块必须存在于平台上。大多数发行版都包含 `vfio_pci` 默认内核或 `uio_pci_generic` 作为默认内核的一部分，但在某些发行版上，您可能需要在运行 kolla-ansible deploy 之前安装 `kernel-modules-extra` 或等效发行版。

## Installation 安装 ¶

To enable ovs-dpdk, add the following configuration to `/etc/kolla/globals.yml` file:
要启用 ovs-dpdk，请将以下配置添加到 `/etc/kolla/globals.yml` 文件中：

```
ovs_datapath: "netdev"
enable_ovs_dpdk: yes
enable_openvswitch: yes
tunnel_interface: "dpdk_bridge"
neutron_bridge_name: "dpdk_bridge"
```



 

Note 注意



Kolla doesn’t support ovs-dpdk for RHEL-based distros due to the lack of a suitable package.
由于缺乏合适的软件包，Kolla 不支持基于 RHEL 的发行版的 ovs-dpdk。

Unlike standard Open vSwitch deployments, the interface specified by neutron_external_interface should have an ip address assigned. The ip address assigned to neutron_external_interface will be moved to the “dpdk_bridge” as part of deploy action. When using ovs-dpdk the tunnel_interface must be an ovs bridge with a physical interfaces attached for tunnelled traffic to be accelerated by dpdk. Note that due to a limitation in ansible variable names which excluded the use of - in a variable name it is not possible to use the default br-ex name for the neutron_bridge_name or tunnel_interface.
与标准 Open vSwitch 部署不同，neutron_external_interface 指定的接口应分配 IP  地址。作为部署操作的一部分，分配给neutron_external_interface的 IP 地址将移动到“dpdk_bridge”。使用  ovs-dpdk 时，tunnel_interface必须是连接了物理接口的 ovs 网桥，以便 dpdk 加速隧道流量。请注意，由于  ansible 变量名称中的限制排除了在变量名称中使用 - 的限制，因此无法对 neutron_bridge_name 或  tunnel_interface 使用默认的 br-ex 名称。

At present, the tunnel interface ip is configured using network manager on on ubuntu and systemd on centos family operating systems. systemd is used to work around a limitation of the centos network manager implementation which does not consider the creation of an ovs bridge to be a hotplug event. In the future, a new config option will be introduced to allow systemd to be used on all host distros for those who do not wish to enable the network manager service on ubuntu.
目前，隧道接口 ip 在 ubuntu 上使用 network manager on 配置，在 centos 系列操作系统上使用 systemd  配置。systemd 是用来解决 CentOS Network Manager 实现的一个限制，该限制不将 OVS  网桥的创建视为热插拔事件。将来，将引入一个新的配置选项，以允许 systemd 在所有主机发行版上使用，供那些不希望在 ubuntu  上启用网络管理器服务的人使用。

## Limitations 限制 ¶

Reconfiguration from kernel ovs to ovs dpdk is currently not supported. Changing ovs datapaths on a deployed node requires neutron config changes and libvirt xml changes for all running instances including a hard reboot of the vm.
目前不支持从内核 ovs 到 ovs dpdk 的重新配置。更改已部署节点上的 ovs 数据路径需要对所有正在运行的实例进行 neutron 配置更改和 libvirt xml 更改，包括硬重启 vm。

When upgrading ovs-dpdk it should be noted that this will always involve a dataplane outage. Unlike kernel OVS the dataplane for ovs-dpdk executes in the ovs-vswitchd process. This means the lifetime of the dpdk dataplane is tied to the lifetime of the ovsdpdk_vswitchd container. As such it is recommended to always evacuate all vm workloads from a node running ovs-dpdk prior to upgrading.
升级 ovs-dpdk 时，应该注意的是，这将始终涉及数据平面中断。与内核 OVS 不同，ovs-dpdk 的数据平面在 ovs-vswitchd  进程中执行。这意味着 dpdk 数据平面的生存期与ovsdpdk_vswitchd容器的生存期相关联。因此，建议在升级之前始终从运行  ovs-dpdk 的节点中撤出所有 vm 工作负载。

On ubuntu network manager is required for tunnel networking. This requirement will be removed in the future.
在 ubuntu 上，隧道网络需要网络管理器。此要求将在将来删除。

# Neutron - Networking Service Neutron - 网络服务

​        version 版本              





## Preparation and deployment 准备和部署 ¶

Neutron is enabled by default in `/etc/kolla/globals.yml`:
默认情况下，Neutron 在以下情况下 `/etc/kolla/globals.yml` 启用：

```
#enable_neutron: "{{ enable_openstack_core | bool }}"
```

## Network interfaces 网络接口 ¶

Neutron external interface is used for communication with the external world, for example provider networks, routers and floating IPs. For setting up the neutron external interface modify `/etc/kolla/globals.yml` setting `neutron_external_interface` to the desired interface name. This interface is used by hosts in the `network` group. It is also used by hosts in the `compute` group if `enable_neutron_provider_networks` is set or DVR is enabled.
Neutron外部接口用于与外部世界的通信，例如提供商网络，路由器和浮动IP。要设置 neutron 外部接口，请将设置 `neutron_external_interface` 修改 `/etc/kolla/globals.yml` 为所需的接口名称。 `network` 此接口由组中的主机使用。如果 `enable_neutron_provider_networks` 设置了或启用了 DVR， `compute` 则组中的主机也会使用它。

The interface is plugged into a bridge (Open vSwitch or Linux Bridge, depending on the driver) defined by `neutron_bridge_name`, which defaults to `br-ex`. The default Neutron physical network is `physnet1`.
该接口插入到 的 `neutron_bridge_name` 网桥（Open vSwitch 或 Linux 网桥，具体取决于驱动程序）中，该网桥默认为 `br-ex` 。默认的 Neutron 物理网络是 `physnet1` 。

### Example: single interface 示例：单接口 ¶

In the case where we have only a single Neutron external interface, configuration is simple:
在我们只有一个 Neutron 外部接口的情况下，配置很简单：

```
neutron_external_interface: "eth1"
```

### Example: multiple interfaces 示例：多个接口 ¶

In some cases it may be necessary to have multiple external network interfaces. This may be achieved via comma-separated lists:
在某些情况下，可能需要具有多个外部网络接口。这可以通过逗号分隔的列表来实现：

```
neutron_external_interface: "eth1,eth2"
neutron_bridge_name: "br-ex1,br-ex2"
```

These two lists are “zipped” together, such that `eth1` is plugged into the `br-ex1` bridge, and `eth2` is plugged into the `br-ex2` bridge.  Kolla Ansible maps these interfaces to Neutron physical networks `physnet1` and `physnet2` respectively.
这两个列表被“压缩”在一起，这样 `eth1` 就插入 `br-ex1` 到桥接器中，并 `eth2` 插入 `br-ex2` 到桥接器中。Kolla Ansible 将这些接口分别映射到 Neutron 物理网络 `physnet1` 。 `physnet2` 

### Example: shared interface 示例：共享接口 ¶

Sometimes an interface used for Neutron external networking may also be used for other traffic. Plugging an interface directly into a bridge would prevent us from having a usable IP address on the interface. One solution to this issue is to use an intermediate Linux bridge and virtual Ethernet pair, then assign IP addresses on the Linux bridge. This setup is supported by [Kayobe](https://docs.openstack.org/kayobe/latest//). It is out of scope here, as it is non-trivial to set up in a persistent manner.
有时，用于 Neutron 外部网络的接口也可用于其他流量。将接口直接插入网桥会阻止我们在接口上拥有可用的 IP 地址。此问题的一种解决方案是使用中间  Linux 网桥和虚拟以太网对，然后在 Linux 网桥上分配 IP 地址。Kayobe  支持此设置。它超出了此处的范围，因为以持久的方式进行设置并非易事。

## Provider networks 提供商网络 ¶

Provider networks allow to connect compute instances directly to physical networks avoiding tunnels. This is necessary for example for some performance critical applications. Only administrators of OpenStack can create such networks.
提供商网络允许将计算实例直接连接到物理网络，从而避免隧道。例如，对于某些性能关键型应用程序，这是必需的。只有 OpenStack 的管理员才能创建这样的网络。

To use provider networks in instances you also need to set the following in `/etc/kolla/globals.yml`:
要在实例中使用提供商网络，您还需要在以下位置 `/etc/kolla/globals.yml` 设置以下内容：

```
enable_neutron_provider_networks: yes
```

For provider networks, compute hosts must have an external bridge created and configured by Ansible (this is also necessary when [Neutron Distributed Virtual Routing (DVR)](https://docs.openstack.org/neutron/latest/admin/deploy-ovs-ha-dvr.html) mode is enabled). In this case, ensure `neutron_external_interface` is configured correctly for hosts in the `compute` group.
对于提供商网络，计算主机必须具有由 Ansible 创建和配置的外部网桥（启用 Neutron 分布式虚拟路由 （DVR） 模式时，这也是必需的）。在这种情况下，请确保 `neutron_external_interface` 为 `compute` 组中的主机正确配置。

## Internal DNS resolution 内部 DNS 解析 ¶

The Networking service enables users to control the name assigned to ports using two attributes associated with ports, networks, and floating IPs. The following table shows the attributes available for each one of these resources:
网络服务使用户能够使用与端口、网络和浮动 IP 关联的两个属性来控制分配给端口的名称。下表显示了每个资源的可用属性：

| Resource             | dns_name | dns_domain |
| -------------------- | -------- | ---------- |
| Ports 港口           | Yes      | Yes        |
| Networks 网络        | No       | Yes        |
| Floating IPs 浮动 IP | Yes      | Yes        |

To enable this functionality, you need to set the following in `/etc/kolla/globals.yml`:
要启用此功能，您需要在以下位置设置 `/etc/kolla/globals.yml` 以下内容：

```
neutron_dns_integration: "yes"
neutron_dns_domain: "example.org."
```



 

Important 重要



The `neutron_dns_domain` value has to be different to `openstacklocal` (its default value) and has to end with a period `.`.
该 `neutron_dns_domain` 值必须不同 `openstacklocal` 于 （其默认值），并且必须以句点 `.` 结尾。



 

Note 注意



The integration of the Networking service with an external DNSaaS (DNS-as-a-Service) is described in [Designate - DNS service](https://docs.openstack.org/kolla-ansible/latest/reference/networking/designate-guide.html#designate-guide).
指定 - DNS 服务中介绍了网络服务与外部 DNSaaS（DNS 即服务）的集成。

## OpenvSwitch (ml2/ovs) OpenvSwitch （ml2/ovs） ¶

By default `kolla-ansible` uses `openvswitch` as its underlying network mechanism, you can change that using the `neutron_plugin_agent` variable in `/etc/kolla/globals.yml`:
默认 `kolla-ansible` 情况下，使用 `openvswitch` 作为其底层网络机制，您可以使用以下 `neutron_plugin_agent` 变量进行 `/etc/kolla/globals.yml` 更改：

```
neutron_plugin_agent: "openvswitch"
```

When using Open vSwitch on a compatible kernel (4.3+ upstream, consult the documentation of your distribution for support details), you can switch to using the native OVS firewall driver by employing a configuration override (see [OpenStack Service Configuration in Kolla](https://docs.openstack.org/kolla-ansible/latest/admin/advanced-configuration.html#service-config)). You can set it in `/etc/kolla/config/neutron/openvswitch_agent.ini`:
在兼容内核（4.3+ 上游，有关支持详细信息，请参阅发行版的文档）上使用 Open vSwitch 时，可以通过使用配置覆盖切换到使用本机 OVS 防火墙驱动程序（请参阅 Kolla 中的 OpenStack 服务配置）。您可以在以下位置 `/etc/kolla/config/neutron/openvswitch_agent.ini` 设置它：

```
[securitygroup]
firewall_driver = openvswitch
```

## L3 agent high availability L3 代理高可用性 ¶

L3 and DHCP agents can be created in a high availability (HA) state with:
可以使用以下命令在高可用性 （HA） 状态下创建 L3 和 DHCP 代理：

```
enable_neutron_agent_ha: "yes"
```

This allows networking to fail over across controllers if the active agent is stopped. If this option is enabled, it can be advantageous to also set:
这允许在活动代理停止时跨控制器进行网络故障转移。如果启用此选项，则还可以设置：

```
neutron_l3_agent_failover_delay:
```

Agents sometimes need to be restarted. This delay (in seconds) is invoked between the restart operations of each agent. When set properly, it will stop network outages caused by all agents restarting at the same time. The exact length of time it takes to restart is dependent on hardware and the number of routers present. A general rule of thumb is to set the value to `40 + 3n` where `n` is the number of routers. For example, with 5 routers, `40 + (3 * 5) = 55` so the value could be set to 55. A much better approach however would be to first time how long an outage lasts, then set the value accordingly.
代理有时需要重新启动。此延迟（以秒为单位）在每个代理的重新启动操作之间调用。如果设置正确，它将阻止所有代理同时重启导致的网络中断。重新启动所需的确切时间长度取决于硬件和存在的路由器数量。一般的经验法则是将值设置为 `40 + 3n` where `n` is the number of router。例如，有 5 个路由器， `40 + (3 * 5) = 55` 因此该值可以设置为 55。然而，一个更好的方法是第一次确定中断持续多长时间，然后相应地设置值。

The default value is 0. A nonzero starting value would only result in outages if the failover time was greater than the delay, which would be more difficult to diagnose than consistent behaviour.
默认值为 0。只有当故障转移时间大于延迟时，非零起始值才会导致中断，这比一致行为更难诊断。

## OVN (ml2/ovn) OVN （ml2/ovn） ¶

In order to use `OVN` as mechanism driver for `neutron`, you need to set the following:
为了用作 `OVN` `neutron` 的机制驱动程序，您需要设置以下内容：

```
neutron_plugin_agent: "ovn"
```

When using OVN - Kolla Ansible will not enable distributed floating ip functionality (not enable external bridges on computes) by default. To change this behaviour you need to set the following:
使用 OVN 时 - 默认情况下，Kolla Ansible 不会启用分布式浮动 IP 功能（不在计算上启用外部网桥）。若要更改此行为，需要设置以下内容：

```
neutron_ovn_distributed_fip: "yes"
```

Similarly - in order to have Neutron DHCP agents deployed in OVN networking scenario, use:
同样 - 为了在 OVN 网络场景中部署 Neutron DHCP 代理，请使用：

```
neutron_ovn_dhcp_agent: "yes"
```

This might be desired for example when Ironic bare metal nodes are used as a compute service. Currently OVN is not able to answer DHCP queries on port type external, this is where Neutron agent helps.
例如，当 Ironic 裸机节点用作计算服务时，这可能是需要的。目前，OVN 无法应答外部端口类型的 DHCP 查询，这就是 Neutron 代理的用武之地。

In order to deploy Neutron OVN Agent you need to set the following:
要部署 Neutron OVN Agent，您需要设置以下内容：

```
neutron_enable_ovn_agent: "yes"
```

Currently the agent is only needed for QoS for hardware offloaded ports.
目前，只有硬件卸载端口的 QoS 才需要代理。

## Mellanox Infiniband (ml2/mlnx) Mellanox Infiniband （ml2/mlnx） ¶

In order to add `mlnx_infiniband` to the list of mechanism driver for `neutron` to support Infiniband virtual funtions, you need to set the following (assuming neutron SR-IOV agent is also enabled using `enable_neutron_sriov` flag):
为了 `mlnx_infiniband` 添加到支持 Infiniband 虚拟功能的机制驱动程序 `neutron` 列表中，您需要设置以下内容（假设 neutron SR-IOV 代理也使用 `enable_neutron_sriov` flag 启用）：

```
enable_neutron_mlnx: "yes"
```

Additionally, you will also need to provide physnet:interface mappings via `neutron_mlnx_physnet_mappings` which is presented to `neutron_mlnx_agent` container via `mlnx_agent.ini` and `neutron_eswitchd` container via `eswitchd.conf`:
此外，您还需要提供 physnet：interface 映射，通过 `neutron_mlnx_physnet_mappings` 该映射呈现给 `neutron_mlnx_agent` 容器 via `mlnx_agent.ini` 和 `neutron_eswitchd` container via `eswitchd.conf` ：

```
neutron_mlnx_physnet_mappings:
  ibphysnet: "ib0"
```

## SSH authentication in external systems (switches) 外部系统（交换机）中的 SSH 身份验证 ¶

Kolla, by default, generates and copies an ssh key to the `neutron_server` container (under `/var/lib/neutron/.ssh/id_rsa`) which can be used for authentication in external systems (e.g. in `networking-generic-switch` or `networking-ansible` managed switches).
默认情况下，Kolla会生成一个ssh密钥并将其复制到 `neutron_server` 容器（下 `/var/lib/neutron/.ssh/id_rsa` ），该密钥可用于外部系统（例如在或 `networking-ansible` 托管交换机中 `networking-generic-switch` ）进行身份验证。

You can set `neutron_ssh_key` variable in `passwords.yml` to control the used key.
您可以设置 `neutron_ssh_key` 变量 in `passwords.yml` 来控制使用的密钥。

## Custom Kernel Module Configuration for Neutron Neutron 的自定义内核模块配置 ¶

Neutron may require specific kernel modules for certain functionalities. While there are predefined default modules in the Ansible role, users have the flexibility to add custom modules as needed.
Neutron 可能需要特定的内核模块来实现某些功能。虽然 Ansible 角色中有预定义的默认模块，但用户可以根据需要灵活地添加自定义模块。

To add custom kernel modules for Neutron, modify the configuration in `/etc/kolla/globals.yml`:
要为 Neutron 添加自定义内核模块，请修改 ： `/etc/kolla/globals.yml` 

```
neutron_modules_extra:
  - name: 'nf_conntrack_tftp'
    params: 'hashsize=4096'
```

In this example: 在此示例中：

- neutron_modules_extra: Allows users to specify additional modules and their associated parameters. The given configuration adjusts the hashsize parameter for the nf_conntrack_tftp module.
  neutron_modules_extra：允许用户指定其他模块及其关联参数。给定的配置调整nf_conntrack_tftp模块的哈希大小参数。

​                      

# Neutron Extensions 中子扩展

​        version 版本              





## Networking-SFC 网络-SFC ¶

### Preparation and deployment 准备和部署 ¶

Modify the `/etc/kolla/globals.yml` file as the following example shows:
修改文件， `/etc/kolla/globals.yml` 如以下示例所示：

```
enable_neutron_sfc: "yes"
```

### Verification 验证 ¶

For setting up a testbed environment and creating a port chain, please refer to [networking-sfc documentation](https://docs.openstack.org/networking-sfc/latest/contributor/system_design_and_workflow.html).
有关设置测试平台环境和创建端口链的信息，请参阅 networking-sfc 文档。

## Neutron VPNaaS (VPN-as-a-Service) Neutron VPNaaS（VPN即服务） ¶

### Preparation and deployment 准备和部署 ¶

Modify the `/etc/kolla/globals.yml` file as the following example shows:
修改文件， `/etc/kolla/globals.yml` 如以下示例所示：

```
enable_neutron_vpnaas: "yes"
```

### Verification 验证 ¶

VPNaaS is a complex subject, hence this document provides directions for a simple smoke test to verify the service is up and running.
VPNaaS 是一个复杂的主题，因此本文档提供了简单冒烟测试的说明，以验证服务是否已启动并运行。

On the network node(s), the `neutron_vpnaas_agent` should be up (image naming and versioning may differ depending on deploy configuration):
在网络节点上， `neutron_vpnaas_agent` 应启动（映像命名和版本控制可能因部署配置而异）：

```
docker ps --filter name=neutron_vpnaas_agent

CONTAINER ID   IMAGE                                                               COMMAND         CREATED          STATUS        PORTS  NAMES
97d25657d55e   operator:5000/kolla/centos-source-neutron-vpnaas-agent:4.0.0   "kolla_start"   44 minutes ago   Up 44 minutes        neutron_vpnaas_agent
```



 

Warning 警告



You are free to use the following `init-runonce` script for demo purposes but note it does **not** have to be run in order to use your cloud. Depending on your customisations, it may not work, or it may conflict with the resources you want to create. You have been warned.
您可以自由地将以下 `init-runonce` 脚本用于演示目的，但请注意，不必运行它即可使用您的云。根据您的自定义项，它可能不起作用，或者可能与要创建的资源冲突。你已经被警告了。

Similarly, the `init-vpn` script does **not** have to be run unless you want to follow this particular demo.
同样，除非您想遵循此特定演示，否则不必运行该 `init-vpn` 脚本。

Kolla Ansible includes a small script that can be used in tandem with `tools/init-runonce` to verify the VPN using two routers and two Nova VMs:
Kolla Ansible 包含一个小脚本，可以与使用两个路由器和两个 Nova VM 验证 VPN 一起使用 `tools/init-runonce` ：

```
tools/init-runonce
tools/init-vpn
```

Verify both VPN services are active:
验证两个 VPN 服务均处于活动状态：

```
neutron vpn-service-list

+--------------------------------------+----------+--------------------------------------+--------+
| id                                   | name     | router_id                            | status |
+--------------------------------------+----------+--------------------------------------+--------+
| ad941ec4-5f3d-4a30-aae2-1ab3f4347eb1 | vpn_west | 051f7ce3-4301-43cc-bfbd-7ffd59af539e | ACTIVE |
| edce15db-696f-46d8-9bad-03d087f1f682 | vpn_east | 058842e0-1d01-4230-af8d-0ba6d0da8b1f | ACTIVE |
+--------------------------------------+----------+--------------------------------------+--------+
```

Two VMs can now be booted, one on vpn_east, the other on vpn_west, and encrypted ping packets observed being sent from one to the other.
现在可以启动两个 VM，一个在 vpn_east 上，另一个在 vpn_west 上，并观察到加密的 ping 数据包从一个 VM 发送到另一个 VM。

For more information on this and VPNaaS in Neutron refer to the [Neutron VPNaaS Testing](https://docs.openstack.org/neutron-vpnaas/latest/contributor/index.html#testing) and the [OpenStack wiki](https://wiki.openstack.org/wiki/Neutron/VPNaaS).
有关此内容和 Neutron 中的 VPNaaS 的更多信息，请参阅 Neutron VPNaaS 测试和 OpenStack wiki。

### Trunking 中继 ¶

The network trunk service allows multiple networks to be connected to an instance using a single virtual NIC (vNIC). Multiple networks can be presented to an instance by connecting it to a single port.
网络中继服务允许使用单个虚拟网卡 （vNIC） 将多个网络连接到实例。通过将实例连接到单个端口，可以向实例呈现多个网络。

Modify the `/etc/kolla/globals.yml` file as the following example shows:
修改文件， `/etc/kolla/globals.yml` 如以下示例所示：

```
enable_neutron_trunk: "yes"
```

## Neutron Logging Framework Neutron 测井框架 ¶

### Preparation and deployment 准备和部署 ¶

Modify the `/etc/kolla/globals.yml` file as the following example shows:
修改文件， `/etc/kolla/globals.yml` 如以下示例所示：

```
enable_neutron_packet_logging: "yes"
```

For OVS deployment, you need to override the firewall driver in openvswitch_agent.ini to:
对于 OVS 部署，需要覆盖防火墙驱动程序，openvswitch_agent.ini以便：

```
[security_group]
firewall_driver = openvswitch
```

### Verification 验证 ¶

Verify that loggable resources are properly registered:
验证是否正确注册了可记录资源：

```
openstack network loggable resources list
+-----------------+
| Supported types |
+-----------------+
| security_group  |
+-----------------+
```

The output shows security groups logging is now enabled.
输出显示安全组日志记录现已启用。

You may now create a network logging rule to log all events based on a security group object:
您现在可以创建一个网络日志记录规则，以记录基于安全组对象的所有事件：

```
openstack network log create --resource-type security_group \
--description "Collecting all security events" \
--event ALL Log_Created
```

More examples and information can be found at: https://docs.openstack.org/neutron/latest/admin/config-logging.html
更多示例和信息可在以下位置找到：https://docs.openstack.org/neutron/latest/admin/config-logging.html

# Octavia 奥克塔维娅

​        version 版本              



Octavia provides load balancing as a service. This guide covers two providers:
Octavia 提供负载均衡即服务。本指南涵盖两个提供商：

- Amphora 双耳细颈椭圆土罐
- OVN

## Enabling Octavia 启用 Octavia ¶

Enable the octavia service in `globals.yml`:
在以下位置 `globals.yml` 启用 octavia 服务：

```
enable_octavia: "yes"
```

## Amphora provider 双耳瓶提供程序 ¶

This section covers configuration of Octavia for the Amphora driver. See the [Octavia documentation](https://docs.openstack.org/octavia/latest/) for full details. The [installation guide](https://docs.openstack.org/octavia/latest/install/install-ubuntu.html) is a useful reference.
本节介绍 Amphora 驱动程序的 Octavia 配置。有关完整的详细信息，请参阅 Octavia 文档。安装指南是一个有用的参考。

### Certificates 证书 ¶

Octavia requires various TLS certificates for operation. Since the Victoria release, Kolla Ansible supports generating these certificates automatically.
Octavia 需要各种 TLS 证书才能运行。自 Victoria 版本以来，Kolla Ansible 支持自动生成这些证书。

#### Option 1: Automatically generating Certificates 选项 1：自动生成证书 ¶

Kolla Ansible provides default values for the certificate issuer and owner fields. You can customize this via `globals.yml`, for example:
Kolla Ansible 为证书颁发者和所有者字段提供默认值。您可以通过以下方式 `globals.yml` 自定义它，例如：

```
octavia_certs_country: US
octavia_certs_state: Oregon
octavia_certs_organization: OpenStack
octavia_certs_organizational_unit: Octavia
```

Generate octavia certificates:
生成 octavia 证书：

```
kolla-ansible octavia-certificates
```

The certificates and keys will be generated under `/etc/kolla/config/octavia`.
证书和密钥将在 `/etc/kolla/config/octavia` .

#### Option 2: Manually generating certificates 选项 2：手动生成证书 ¶

Follow the [octavia documentation](https://docs.openstack.org/octavia/latest/admin/guides/certificates.html) to generate certificates for Amphorae. These should be copied to the Kolla Ansible configuration as follows:
按照 octavia 文档生成 Amphorae 的证书。这些应复制到 Kolla Ansible 配置中，如下所示：

```
cp client_ca/certs/ca.cert.pem /etc/kolla/config/octavia/client_ca.cert.pem
cp server_ca/certs/ca.cert.pem /etc/kolla/config/octavia/server_ca.cert.pem
cp server_ca/private/ca.key.pem /etc/kolla/config/octavia/server_ca.key.pem
cp client_ca/private/client.cert-and-key.pem /etc/kolla/config/octavia/client.cert-and-key.pem
```

The following option should be set in `passwords.yml`, matching the password used to encrypt the CA key:
应在 中 `passwords.yml` 设置以下选项，以匹配用于加密 CA 密钥的密码：

```
octavia_ca_password: <CA key password>
```



#### Monitoring certificate expiry 监控证书到期 ¶

You can use the following command to check if any of the certificates will expire within a given number of days:
您可以使用以下命令检查是否有任何证书将在给定的天数内过期：

```
kolla-ansible octavia-certificates --check-expiry <days>
```

### Networking 网络 ¶

Octavia worker and health manager nodes must have access to the Octavia management network for communication with Amphorae.
Octavia 工作线程和运行状况管理器节点必须能够访问 Octavia 管理网络才能与 Amphorae 进行通信。

If using a VLAN for the Octavia management network, enable Neutron provider networks:
如果将 VLAN 用于 Octavia 管理网络，请启用 Neutron 提供商网络：

```
enable_neutron_provider_networks: yes
```

Configure the name of the network interface on the controllers used to access the Octavia management network. If using a VLAN provider network, ensure that the traffic is also bridged to Open vSwitch on the controllers.
在用于访问 Octavia 管理网络的控制器上配置网络接口的名称。如果使用 VLAN 提供商网络，请确保流量也桥接到控制器上的 Open vSwitch。

```
octavia_network_interface: <network interface on controllers>
```

This interface should have an IP address on the Octavia management subnet.
此接口应在 Octavia 管理子网上具有 IP 地址。

### Registering OpenStack resources 注册 OpenStack 资源 ¶

Since the Victoria release, there are two ways to configure Octavia.
自 Victoria 发布以来，有两种方法可以配置 Octavia。

1. Kolla Ansible automatically registers resources for Octavia during deployment
   Kolla Ansible 在部署期间自动为 Octavia 注册资源
2. Operator registers resources for Octavia after it is deployed
   操作员在部署 Octavia 后为其注册资源

The first option is simpler, and is recommended for new users. The second option provides more flexibility, at the cost of complexity for the operator.
第一个选项更简单，建议新用户使用。第二种选择提供了更大的灵活性，但代价是操作员的复杂性。

### Option 1: Automatic resource registration (default, recommended) 选项 1：自动资源注册（默认，推荐） ¶

For automatic resource registration, Kolla Ansible will register the following resources:
对于自动资源注册，Kolla Ansible 将注册以下资源：

- Nova flavor 新星风味
- Nova SSH keypair Nova SSH 密钥对
- Neutron network and subnet
  中子网络和子网
- Neutron security groups Neutron 安全组

The configuration for these resources may be customised before deployment.
这些资源的配置可以在部署之前进行自定义。

Note that for this to work access to the Nova and Neutron APIs is required. This is true also for the `kolla-ansible genconfig` command and when using Ansible check mode.
请注意，要做到这一点，需要访问 Nova 和 Neutron API。 `kolla-ansible genconfig` 对于命令和使用 Ansible 检查模式时也是如此。

#### Customize Amphora flavor 自定义双耳瓶口味 ¶

The default amphora flavor is named `amphora` with 1 VCPUs, 1GB RAM and 5GB disk. you can customize this flavor by changing `octavia_amp_flavor` in `globals.yml`.
默认的双耳瓶风格以 1 个 VCPU、1GB RAM 和 5GB 磁盘命名 `amphora` 。您可以通过更改 `octavia_amp_flavor` 来 `globals.yml` 自定义此风格。

See the `os_nova_flavor` Ansible module for details. Supported parameters are:
有关详细信息， `os_nova_flavor` 请参阅 Ansible 模块。支持的参数包括：

- `disk`
- `ephemeral` (optional) `ephemeral` （可选）
- `extra_specs` (optional) `extra_specs` （可选）
- `flavorid` (optional) `flavorid` （可选）
- `is_public` (optional) `is_public` （可选）
- `name`
- `ram`
- `swap` (optional) `swap` （可选）
- `vcpus`

The following defaults are used:
使用以下默认值：

```
octavia_amp_flavor:
  name: "amphora"
  is_public: no
  vcpus: 1
  ram: 1024
  disk: 5
```

#### Customise network and subnet 自定义网络和子网 ¶

Configure Octavia management network and subnet with `octavia_amp_network` in `globals.yml`. This must be a network that is [accessible from the controllers](https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#octavia-network). Typically a VLAN provider network is used.
使用 `octavia_amp_network` in `globals.yml` 配置 Octavia 管理网络和子网。这必须是可从控制器访问的网络。通常使用 VLAN 提供商网络。

See the `os_network` and `os_subnet` Ansible modules for details. Supported parameters:
有关详细信息， `os_network` 请参阅 和 `os_subnet` Ansible 模块。支持的参数：

The network parameter has the following supported parameters:
network 参数具有以下支持的参数：

- `external` (optional) `external` （可选）
- `mtu` (optional) `mtu` （可选）
- `name`
- `provider_network_type` (optional) `provider_network_type` （可选）
- `provider_physical_network` (optional) `provider_physical_network` （可选）
- `provider_segmentation_id` (optional) `provider_segmentation_id` （可选）
- `shared` (optional) `shared` （可选）
- `subnet`

The subnet parameter has the following supported parameters:
subnet 参数具有以下支持的参数：

- `allocation_pool_start` (optional) `allocation_pool_start` （可选）
- `allocation_pool_end` (optional) `allocation_pool_end` （可选）
- `cidr`
- `enable_dhcp` (optional) `enable_dhcp` （可选）
- `gateway_ip` (optional) `gateway_ip` （可选）
- `name`
- `no_gateway_ip` (optional) `no_gateway_ip` （可选）
- `ip_version` (optional) `ip_version` （可选）
- `ipv6_address_mode` (optional) `ipv6_address_mode` （可选）
- `ipv6_ra_mode` (optional) `ipv6_ra_mode` （可选）

For example: 例如：

```
octavia_amp_network:
  name: lb-mgmt-net
  provider_network_type: vlan
  provider_segmentation_id: 1000
  provider_physical_network: physnet1
  external: false
  shared: false
  subnet:
    name: lb-mgmt-subnet
    cidr: "10.1.2.0/24"
    allocation_pool_start: "10.1.2.100"
    allocation_pool_end: "10.1.2.200"
    gateway_ip: "10.1.2.1"
    enable_dhcp: yes
```

Deploy Octavia with Kolla Ansible:
使用 Kolla Ansible 部署 Octavia：

```
kolla-ansible -i <inventory> deploy --tags common,horizon,octavia
```

Once the installation is completed, you need to [register an amphora image in glance](https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#octavia-amphora-image).
安装完成后，您需要在一目了然中注册一个双耳瓶图像。

### Option 2: Manual resource registration 选项 2：手动资源注册 ¶

In this case, Kolla Ansible will not register resources for Octavia. Set `octavia_auto_configure` to no in `globals.yml`:
在这种情况下，Kolla Ansible 不会为 Octavia 注册资源。在以下情况下 `globals.yml` 设置为 `octavia_auto_configure` 否：

```
octavia_auto_configure: no
```

All resources should be registered in the `service` project. This can be done as follows:
所有资源都应在 `service` 项目中注册。这可以按如下方式完成：

```
. /etc/kolla/octavia-openrc.sh
```



 

Note 注意



Ensure that you have executed `kolla-ansible post-deploy` and set `enable_octavia` to yes in `global.yml`
确保您已执行 `kolla-ansible post-deploy` 并设置为 `enable_octavia` `global.yml` yes



 

Note 注意



In Train and earlier releases, resources should be registered in the `admin` project. This is configured via `octavia_service_auth_project`, and may be set to `service` to avoid a breaking change when upgrading to Ussuri. Changing the project on an existing system requires at a minimum registering a new security group in the new project. Ideally the flavor and network should be recreated in the new project, although this will impact existing Amphorae.
在 Train 和早期版本中，应在 `admin` 项目中注册资源。这是通过 `octavia_service_auth_project` 配置的，可以设置为 `service` 在升级到乌苏里时避免中断性更改。在现有系统上更改项目至少需要在新项目中注册新的安全组。理想情况下，应该在新项目中重新创建风味和网络，尽管这会影响现有的双耳瓶。

#### Amphora flavor 双耳瓶风味 ¶

Register the flavor in Nova:
在 Nova 中注册风格：

```
openstack flavor create --vcpus 1 --ram 1024 --disk 2 "amphora" --private
```

Make a note of the ID of the flavor, or specify one via `--id`.
记下口味的 ID，或通过 `--id` 指定一个。

#### Keypair 密钥对 ¶

Register the keypair in Nova:
在 Nova 中注册密钥对：

```
openstack keypair create --public-key <path to octavia public key> octavia_ssh_key
```

#### Network and subnet 网络和子网 ¶

Register the management network and subnet in Neutron. This must be a network that is [accessible from the controllers](https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#octavia-network). Typically a VLAN provider network is used.
在 Neutron 中注册管理网络和子网。这必须是可从控制器访问的网络。通常使用 VLAN 提供商网络。

```
OCTAVIA_MGMT_SUBNET=192.168.43.0/24
OCTAVIA_MGMT_SUBNET_START=192.168.43.10
OCTAVIA_MGMT_SUBNET_END=192.168.43.254

openstack network create lb-mgmt-net --provider-network-type vlan --provider-segment 107  --provider-physical-network physnet1
openstack subnet create --subnet-range $OCTAVIA_MGMT_SUBNET --allocation-pool \
  start=$OCTAVIA_MGMT_SUBNET_START,end=$OCTAVIA_MGMT_SUBNET_END \
  --network lb-mgmt-net lb-mgmt-subnet
```

Make a note of the ID of the network.
记下网络的 ID。

#### Security group 安全组 ¶

Register the security group in Neutron.
在 Neutron 中注册安全组。

```
openstack security group create lb-mgmt-sec-grp
openstack security group rule create --protocol icmp lb-mgmt-sec-grp
openstack security group rule create --protocol tcp --dst-port 22 lb-mgmt-sec-grp
openstack security group rule create --protocol tcp --dst-port 9443 lb-mgmt-sec-grp
```

Make a note of the ID of the security group.
记下安全组的 ID。

#### Kolla Ansible configuration Kolla Ansible 配置 ¶

The following options should be added to `globals.yml`.
应将 `globals.yml` 以下选项添加到 中。

Set the IDs of the resources registered previously:
设置之前注册的资源的 ID：

```
octavia_amp_boot_network_list: <ID of lb-mgmt-net>
octavia_amp_secgroup_list: <ID of lb-mgmt-sec-grp>
octavia_amp_flavor_id: <ID of amphora flavor>
```

Now deploy Octavia: 现在部署 Octavia：

```
kolla-ansible -i <inventory> deploy --tags common,horizon,octavia
```



### Amphora image 双耳瓶图像 ¶

It is necessary to build an Amphora image. On CentOS / Rocky 9:
有必要构建一个 Amphora 映像。在 CentOS / Rocky 9 上：

```
sudo dnf -y install epel-release
sudo dnf install -y debootstrap qemu-img git e2fsprogs policycoreutils-python-utils
```

On Ubuntu: 在 Ubuntu 上：

```
sudo apt -y install debootstrap qemu-utils git kpartx
```

Acquire the Octavia source code:
获取 Octavia 源代码：

```
git clone https://opendev.org/openstack/octavia -b <branch>
```

Install `diskimage-builder`, ideally in a virtual environment:
安装 `diskimage-builder` ，理想情况下在虚拟环境中安装：

```
python3 -m venv dib-venv
source dib-venv/bin/activate
pip install diskimage-builder
```

Create the Amphora image:
创建 Amphora 映像：

```
cd octavia/diskimage-create
./diskimage-create.sh
```

Source octavia user openrc:
来源：octavia 用户 openrc：

```
. /etc/kolla/octavia-openrc.sh
```



 

Note 注意



Ensure that you have executed `kolla-ansible post-deploy`
确保您已执行 `kolla-ansible post-deploy` 

Register the image in Glance:
在 Glance 中注册图像：

```
openstack image create amphora-x64-haproxy.qcow2 --container-format bare --disk-format qcow2 --private --tag amphora --file amphora-x64-haproxy.qcow2 --property hw_architecture='x86_64' --property hw_rng_model=virtio
```



 

Note 注意



the tag should match the `octavia_amp_image_tag` in `/etc/kolla/globals.yml`, by default, the tag is “amphora”, octavia uses the tag to determine which image to use.
标签应与 `octavia_amp_image_tag` in `/etc/kolla/globals.yml` 匹配，默认情况下，标签为“amphora”，Octavia 使用标签来确定要使用的图像。

### Debug 调试 ¶

#### SSH to an amphora SSH 到双耳瓶 ¶

login into one of octavia-worker nodes, and ssh into amphora.
登录到 Octavia-worker 节点之一，然后通过 SSH 登录 Amphora。

```
ssh -i /etc/kolla/octavia-worker/octavia_ssh_key ubuntu@<amphora_ip>
```



 

Note 注意



amphora private key is located at `/etc/kolla/octavia-worker/octavia_ssh_key` on all octavia-worker nodes.
Amphora 私钥位于所有 Octavia-worker 节点 `/etc/kolla/octavia-worker/octavia_ssh_key` 上。

### Upgrade 升级 ¶

If you upgrade from the Ussuri release, you must disable `octavia_auto_configure` in `globals.yml` and keep your other octavia config as before.
如果您从乌苏里版本升级，则必须 `octavia_auto_configure` 禁用 `globals.yml` 并像以前一样保留其他 octavia 配置。

### Development or Testing 开发或测试 ¶

Kolla Ansible provides a simple way to setup Octavia networking for development or testing, when using the Neutron Open vSwitch ML2 mechanism driver. In this case, Kolla Ansible will create a tenant network and configure Octavia control services to access it. Please do not use this option in production, the network may not be reliable enough for production.
在使用 Neutron Open vSwitch ML2 机制驱动程序时，Kolla Ansible 提供了一种简单的方法来设置 Octavia  网络以进行开发或测试。在这种情况下，Kolla Ansible 将创建一个租户网络并配置 Octavia  控制服务以访问它。请不要在生产中使用此选项，网络可能不够可靠，无法进行生产。

Add `octavia_network_type` to `globals.yml` and set the value to `tenant`
添加到 `octavia_network_type`  `globals.yml` 并将值设置为 `tenant` 

```
octavia_network_type: "tenant"
```

Next，follow the deployment instructions as normal.
接下来，照常按照部署说明进行操作。

## OVN provider OVN 提供程序 ¶

This section covers configuration of Octavia for the OVN driver. See the [Octavia documentation](https://docs.openstack.org/octavia/latest/) and [OVN Octavia provider documentation](https://docs.openstack.org/ovn-octavia-provider/latest/) for full details.
本部分介绍 OVN 驱动程序的 Octavia 配置。有关完整详细信息，请参阅 Octavia 文档和 OVN Octavia 提供程序文档。

To enable the OVN provider, set the following options in `globals.yml`:
要启用 OVN 提供程序，请在 `globals.yml` ：

```
octavia_provider_drivers: "ovn:OVN provider"
octavia_provider_agents: "ovn"
```

​                      

# SRIOV 斯里奥夫

​        version 版本              





## Neutron SRIOV 中子SRIOV ¶

### Preparation and deployment 准备和部署 ¶

SRIOV requires specific NIC and BIOS configuration and is not supported on all platforms. Consult NIC and platform specific documentation for instructions on enablement.
SRIOV 需要特定的 NIC 和 BIOS 配置，并非所有平台都支持。有关启用的说明，请参阅 NIC 和特定于平台的文档。

Modify the `/etc/kolla/globals.yml` file as the following example shows which automatically appends `sriovnicswitch` to the `mechanism_drivers` inside `ml2_conf.ini`.
如以下示例所示修改 `/etc/kolla/globals.yml` 文件，该文件会自动追加 `sriovnicswitch` 到 `mechanism_drivers` 内部 `ml2_conf.ini` 。

```
enable_neutron_sriov: "yes"
```

It is also a requirement to define physnet:interface mappings for all SRIOV devices as shown in the following example where `sriovtenant1` is the physnet mapped to `ens785f0` interface:
还需要为所有 SRIOV 设备定义 physnet：interface 映射，如以下示例所示，其中 `sriovtenant1` physnet 映射到 `ens785f0` 接口：

```
neutron_sriov_physnet_mappings:
  sriovtenant1: ens785f0
```

However, the provider networks using SRIOV should be configured. Both flat and VLAN are configured with the same physical network name in this example:
但是，应配置使用 SRIOV 的提供商网络。在此示例中，平坦网络和 VLAN 都配置了相同的物理网络名称：

```
[ml2_type_vlan]
network_vlan_ranges = sriovtenant1:1000:1009

[ml2_type_flat]
flat_networks = sriovtenant1
```

Modify the `nova.conf` file and add `PciPassthroughFilter` to `enabled_filters`. This filter is required by the Nova Scheduler service on the controller node.
修改 `nova.conf` 文件并添加到 `PciPassthroughFilter` `enabled_filters` .控制器节点上的 Nova Scheduler 服务需要此过滤器。

```
[filter_scheduler]
enabled_filters = <existing filters>, PciPassthroughFilter
available_filters = nova.scheduler.filters.all_filters
```

PCI devices listed under `neutron_sriov_physnet_mappings` will be whitelisted on the Compute hosts inside `nova.conf`.
下面 `neutron_sriov_physnet_mappings` 列出的 PCI 设备将在内部 `nova.conf` 的计算主机上列入白名单。

Physical network to interface mappings in `neutron_sriov_physnet_mappings` will be automatically added to `sriov_agent.ini`. Specific VFs can be excluded via `excluded_devices`. However, leaving blank (default) leaves all VFs enabled:
物理 `neutron_sriov_physnet_mappings` 网络到接口的映射将自动添加到 `sriov_agent.ini` 中。可以通过 排除特定的 VF `excluded_devices` 。但是，留空（默认）会使所有 VF 处于启用状态：

```
[sriov_nic]
exclude_devices =
```

To use OpenvSwitch hardware offloading modify /etc/kolla/globals.yml`:
要使用 OpenvSwitch 硬件卸载，请修改 /etc/kolla/globals.yml'：

```
openvswitch_hw_offload: "yes"
```

Run deployment. 运行部署。

### Verification 验证 ¶

Check that VFs were created on the compute node(s). VFs will appear in the output of both `lspci` and `ip link show`.  For example:
检查是否在计算节点上创建了 VF。VF 将出现在 `lspci` 和 `ip link show` 的输出中。例如：

```
lspci | grep net
05:10.0 Ethernet controller: Intel Corporation 82599 Ethernet Controller Virtual Function (rev 01)
ip -d link show ens785f0
4: ens785f0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT qlen 1000
link/ether 90:e2:ba:ba:fb:20 brd ff:ff:ff:ff:ff:ff promiscuity 1
openvswitch_slave addrgenmode eui64
vf 0 MAC 52:54:00:36:57:e0, spoof checking on, link-state auto, trust off
vf 1 MAC 52:54:00:00:62:db, spoof checking on, link-state auto, trust off
vf 2 MAC fa:16:3e:92:cf:12, spoof checking on, link-state auto, trust off
vf 3 MAC fa:16:3e:00:a3:01, vlan 1000, spoof checking on, link-state auto, trust off
```

Verify the SRIOV Agent container is running on the compute node(s):
验证 SRIOV 代理容器是否在计算节点上运行：

```
docker ps --filter name=neutron_sriov_agent
CONTAINER ID   IMAGE                                                                COMMAND        CREATED         STATUS         PORTS  NAMES
b03a8f4c0b80   10.10.10.10:4000/registry/centos-source-neutron-sriov-agent:17.04.0  "kolla_start"  18 minutes ago  Up 18 minutes         neutron_sriov_agent
```

Verify the SRIOV Agent service is present and UP:
验证 SRIOV 代理服务是否存在且已启动：

```
openstack network agent list

+--------------------------------------+--------------------+-------------+-------------------+-------+-------+---------------------------+
| ID                                   | Agent Type         | Host        | Availability Zone | Alive | State | Binary                    |
+--------------------------------------+--------------------+-------------+-------------------+-------+-------+---------------------------+
| 7c06bda9-7b87-487e-a645-cc6c289d9082 | NIC Switch agent   | av09-18-wcp | None              | :-)   | UP    | neutron-sriov-nic-agent   |
+--------------------------------------+--------------------+-------------+-------------------+-------+-------+---------------------------+
```

Create a new provider network. Set `provider-physical-network` to the physical network name that was configured in `/etc/kolla/config/nova.conf`. Set `provider-network-type` to the desired type. If using VLAN, ensure `provider-segment` is set to the correct VLAN ID. This example uses `VLAN` network type:
创建新的提供商网络。设置为 `provider-physical-network` 在 中 `/etc/kolla/config/nova.conf` 配置的物理网络名称。设置为 `provider-network-type` 所需的类型。如果使用 VLAN，请确保 `provider-segment` 设置为正确的 VLAN ID。此示例使用 `VLAN` 网络类型：

```
openstack network create --project=admin \
  --provider-network-type=vlan \
  --provider-physical-network=sriovtenant1 \
  --provider-segment=1000 \
  sriovnet1
```

Create a subnet with a DHCP range for the provider network:
为提供商网络创建具有 DHCP 范围的子网：

```
openstack subnet create --network=sriovnet1 \
  --subnet-range=11.0.0.0/24 \
  --allocation-pool start=11.0.0.5,end=11.0.0.100 \
  sriovnet1_sub1
```

Create a port on the provider network with `vnic_type` set to `direct`:
在提供商网络上创建一个端口 `direct` ，设置为 `vnic_type` ：

```
openstack port create --network sriovnet1 --vnic-type=direct sriovnet1-port1
```

Start a new instance with the SRIOV port assigned:
启动分配了 SRIOV 端口的新实例：

```
openstack server create --flavor flavor1 \
  --image fc-26 \
  --nic port-id=`openstack port list | grep sriovnet1-port1 | awk '{print $2}'` \
  vm1
```

Verify the instance boots with the SRIOV port. Verify VF assignment by running dmesg on the compute node where the instance was placed.
验证实例是否使用 SRIOV 端口启动。通过在放置实例的计算节点上运行 dmesg 来验证 VF 分配。

```
dmesg
[ 2896.849970] ixgbe 0000:05:00.0: setting MAC fa:16:3e:00:a3:01 on VF 3
[ 2896.850028] ixgbe 0000:05:00.0: Setting VLAN 1000, QOS 0x0 on VF 3
[ 2897.403367] vfio-pci 0000:05:10.4: enabling device (0000 -> 0002)
```

For more information see [OpenStack SRIOV documentation](https://docs.openstack.org/neutron/latest/admin/config-sriov.html).
有关更多信息，请参阅 OpenStack SRIOV 文档。

## Nova SRIOV

### Preparation and deployment 准备和部署 ¶

Nova provides a separate mechanism to attach PCI devices to instances that is independent from Neutron.  Using the PCI alias configuration option in nova.conf, any PCI device (PF or VF) that supports passthrough can be attached to an instance.  One major drawback to be aware of when using this method is that the PCI alias option uses a device’s product id and vendor id only, so in environments that have NICs with multiple ports configured for SRIOV, it is impossible to specify a specific NIC port to pull VFs from.
Nova 提供了一种单独的机制，用于将 PCI 设备连接到独立于 Neutron 的实例。使用 nova.conf 中的 PCI  别名配置选项，任何支持直通的 PCI 设备（PF 或 VF）都可以连接到实例。使用此方法时需要注意的一个主要缺点是 PCI  别名选项仅使用设备的产品 ID 和供应商 ID，因此在具有为 SRIOV 配置了多个端口的 NIC 的环境中，无法指定要从中拉取 VF 的特定  NIC 端口。

Modify the file `/etc/kolla/config/nova.conf`.  The Nova Scheduler service on the control node requires the `PciPassthroughFilter` to be added to the list of filters and the Nova Compute service(s) on the compute node(s) need PCI device whitelisting.  The Nova API service on the control node and the Nova Compute service on the compute node also require the `alias` option under the `[pci]` section.  The alias can be configured as ‘type-VF’ to pass VFs or ‘type-PF’ to pass the PF. Type-VF is shown in this example:
修改文件 `/etc/kolla/config/nova.conf` 。控制节点上的 Nova Scheduler 服务需要 `PciPassthroughFilter` 添加到筛选器列表中，计算节点上的 Nova Compute 服务需要 PCI 设备白名单。控制节点上的 Nova API 服务和计算节点上的 Nova Compute 服务也需要该 `[pci]` 部分下的 `alias` 选项。别名可以配置为“type-VF”以传递 VF，或将“type-PF”配置为传递 PF。

```
[filter_scheduler]
enabled_filters = <existing filters>, PciPassthroughFilter
available_filters = nova.scheduler.filters.all_filters

[pci]
device_spec = [{"vendor_id": "8086", "product_id": "10fb"}]
alias = {"vendor_id":"8086", "product_id":"10ed", "device_type":"type-VF", "name":"vf1"}
```

Run deployment. 运行部署。

### Verification 验证 ¶

Create (or use an existing) flavor, and then configure it to request one PCI device from the PCI alias:
创建（或使用现有）变种，然后将其配置为从 PCI 别名请求一个 PCI 设备：

```
openstack flavor set sriov-flavor --property "pci_passthrough:alias"="vf1:1"
```

Start a new instance using the flavor:
使用以下风格启动新实例：

```
openstack server create --flavor sriov-flavor --image fc-26 vm2
```

Verify VF devices were created and the instance starts successfully as in the Neutron SRIOV case.
验证是否已创建 VF 设备，并且实例是否成功启动，就像 Neutron SRIOV 案例一样。

For more information see [OpenStack PCI passthrough documentation](https://docs.openstack.org/nova/latest/admin/pci-passthrough.html).
有关更多信息，请参阅 OpenStack PCI 直通文档。