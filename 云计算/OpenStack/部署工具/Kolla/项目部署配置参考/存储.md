# Storage 存储

​        version 版本              



This section describes configuration of the different storage backends supported by kolla.
本节介绍 kolla 支持的不同存储后端的配置。

- [External Ceph 外部 Ceph](https://docs.openstack.org/kolla-ansible/latest/reference/storage/external-ceph-guide.html)
- [Cinder - Block storage
  Cinder - 块存储](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide.html)
- [Hitachi NAS Platform iSCSI and NFS drives for OpenStack
  适用于 OpenStack 的 Hitachi NAS Platform iSCSI 和 NFS 硬盘](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide-hnas.html)
- [Quobyte Storage for OpenStack
  OpenStack 的 Quobyte 存储](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide-quobyte.html)
- [Pure Storage FlashArray for OpenStack
  用于 OpenStack 的纯存储 FlashArray](https://docs.openstack.org/kolla-ansible/latest/reference/storage/cinder-guide-pure.html)
- [Manila - Shared filesystems service
  马尼拉 - 共享文件系统服务](https://docs.openstack.org/kolla-ansible/latest/reference/storage/manila-guide.html)
- [Hitachi NAS Platform File Services Driver for OpenStack
  适用于 OpenStack 的 Hitachi NAS 平台文件服务驱动程序](https://docs.openstack.org/kolla-ansible/latest/reference/storage/manila-hnas-guide.html)
- [Swift - Object storage service
  Swift - 对象存储服务](https://docs.openstack.org/kolla-ansible/latest/reference/storage/swift-guide.html)

# External Ceph 外部 Ceph

​        version 版本              





Kolla Ansible does not provide support for provisioning and configuring a Ceph cluster directly. Instead, administrators should use a tool dedicated to this purpose, such as:
Kolla Ansible 不支持直接预置和配置 Ceph 集群。相反，管理员应使用专用于此目的的工具，例如：

- [ceph-ansible](https://docs.ceph.com/projects/ceph-ansible/en/latest/)
- [cephadm CEPHADM公司](https://docs.ceph.com/en/latest/cephadm/install/)

The desired pool(s) and keyrings should then be created via the Ceph CLI or similar.
然后，应通过 Ceph CLI 或类似工具创建所需的池和密钥环。

## Requirements 要求 ¶

- An existing installation of Ceph
  Ceph 的现有安装
- Existing Ceph storage pools
  现有 Ceph 存储池
- Existing credentials in Ceph for OpenStack services to connect to Ceph (Glance, Cinder, Nova, Gnocchi, Manila)
  Ceph 中用于连接到 Ceph 的 OpenStack 服务的现有凭证（Glance、Cinder、Nova、Gnocchi、Manila）

Refer to https://docs.ceph.com/en/latest/rbd/rbd-openstack/ for details on creating the pool and keyrings with appropriate permissions for each service.
有关为每个服务创建具有适当权限的池和密钥环的详细信息，请参阅 https://docs.ceph.com/en/latest/rbd/rbd-openstack/。

## Configuring External Ceph 配置外部 Ceph ¶

Ceph integration is configured for different OpenStack services independently.
Ceph 集成是针对不同的 OpenStack 服务独立配置的。



 

Note 注意



Commands like `ceph config generate-minimal-conf` generate configuration files that have leading tabs. These tabs break Kolla Ansible’s ini parser. Be sure to remove the leading tabs from your `ceph.conf` files when copying them in the following sections.
生成具有前导选项卡的配置文件等 `ceph config generate-minimal-conf` 命令。这些选项卡破坏了 Kolla Ansible 的 ini 解析器。在以下各节中复制 `ceph.conf` 文件时，请务必从文件中删除前导选项卡。

### Glance 一瞥 ¶

Ceph RBD can be used as a storage backend for Glance images. Configuring Glance for Ceph includes the following steps:
Ceph RBD 可用作 Glance 映像的存储后端。为 Ceph 配置 Glance 包括以下步骤：

- Enable Glance Ceph backend in `globals.yml`:
  在以下位置 `globals.yml` 启用 Glance Ceph 后端：

  ```
  glance_backend_ceph: "yes"
  ```

- Configure Ceph authentication details in `/etc/kolla/globals.yml`:
  在以下位置 `/etc/kolla/globals.yml` 配置 Ceph 身份验证详细信息：

  - `ceph_glance_keyring` (default: `client.glance.keyring`)
     `ceph_glance_keyring` （默认值： `client.glance.keyring` ）
  - `ceph_glance_user` (default: `glance`)
     `ceph_glance_user` （默认值： `glance` ）
  - `ceph_glance_pool_name` (default: `images`)
     `ceph_glance_pool_name` （默认值： `images` ）

- Copy Ceph configuration file to `/etc/kolla/config/glance/ceph.conf`
  将 Ceph 配置文件复制到 `/etc/kolla/config/glance/ceph.conf` 

  ```
  [global]
  fsid = 1d89fec3-325a-4963-a950-c4afedd37fe3
  keyring = /etc/ceph/ceph.client.glance.keyring
  mon_initial_members = ceph-0
  mon_host = 192.168.0.56
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

- Copy Ceph keyring to `/etc/kolla/config/glance/ceph.<ceph_glance_keyring>`
  将 Ceph 密钥环复制到 `/etc/kolla/config/glance/ceph.<ceph_glance_keyring>` 

To configure multiple Ceph backends with Glance, which is useful for multistore:
要使用 Glance 配置多个 Ceph 后端，这对多存储很有用：

- Copy the Ceph configuration files into `/etc/kolla/config/glance/` using different names for each
  将 Ceph 配置文件复制为 `/etc/kolla/config/glance/` 对每个配置文件使用不同的名称

  `/etc/kolla/config/glance/ceph.conf`

  ```
  [global]
  fsid = 1d89fec3-325a-4963-a950-c4afedd37fe3
  keyring = /etc/ceph/ceph.client.glance.keyring
  mon_initial_members = ceph-0
  mon_host = 192.168.0.56
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

  `/etc/kolla/config/glance/rbd1.conf`

  ```
  [global]
  fsid = dbfea068-89ca-4d04-bba0-1b8a56c3abc8
  keyring = /etc/ceph/rbd1.client.glance.keyring
  mon_initial_members = ceph-0
  mon_host = 192.10.0.100
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

- Declare Ceph backends in `globals.yml`
  在 `globals.yml` 

  ```
  glance_ceph_backends:
    - name: "rbd"
      type: "rbd"
      cluster: "ceph"
      enabled: "{{ glance_backend_ceph | bool }}"
    - name: "another-rbd"
      type: "rbd"
      cluster: "rbd1"
      enabled: "{{ glance_backend_ceph | bool }}"
  ```

- Copy Ceph keyring to `/etc/kolla/config/glance/ceph.<ceph_glance_keyring>` and analogously to `/etc/kolla/config/glance/rbd1.<ceph_glance_keyring>`
  将 Ceph 密钥环复制到 `/etc/kolla/config/glance/ceph.<ceph_glance_keyring>` 并类似地复制到 `/etc/kolla/config/glance/rbd1.<ceph_glance_keyring>` 

- For copy-on-write set following in `/etc/kolla/config/glance.conf`:
  对于写入时复制集，请在以下位置设置 `/etc/kolla/config/glance.conf` ：

  ```
  [DEFAULT]
  show_image_direct_url = True
  ```



 

Warning 警告



`show_image_direct_url` can present a security risk if using more than just Ceph as Glance backend(s). Please see [Glance show_image_direct_url](https://docs.openstack.org/glance/latest/configuration/glance_api.html#DEFAULT.show_image_direct_url)
 `show_image_direct_url` 如果使用多个 Ceph 作为 Glance 后端，则可能会带来安全风险。请参阅 Glance show_image_direct_url

### Cinder 煤渣 ¶

Ceph RBD can be used as a storage backend for Cinder volumes. Configuring Cinder for Ceph includes following steps:
Ceph RBD 可用作 Cinder 卷的存储后端。为 Ceph 配置 Cinder 包括以下步骤：

- When using external Ceph, there may be no nodes defined in the storage group.  This will cause Cinder and related services relying on this group to fail.  In this case, operator should add some nodes to the storage group, all the nodes where `cinder-volume` and `cinder-backup` will run:
  使用外部 Ceph 时，存储组中可能没有定义任何节点。这将导致依赖于此组的 Cinder 和相关服务失败。在这种情况下，操作员应该将一些节点添加到存储组中，所有节点的位置 `cinder-volume` 和 `cinder-backup` 将要运行：

  ```
  [storage]
  control01
  ```

- Enable Cinder Ceph backend in `globals.yml`:
  在以下位置 `globals.yml` 启用 Cinder Ceph 后端：

  ```
  cinder_backend_ceph: "yes"
  ```

- Configure Ceph authentication details in `/etc/kolla/globals.yml`:
  在以下位置 `/etc/kolla/globals.yml` 配置 Ceph 身份验证详细信息：

  - `ceph_cinder_keyring` (default: `client.cinder.keyring`)
     `ceph_cinder_keyring` （默认值： `client.cinder.keyring` ）
  - `ceph_cinder_user` (default: `cinder`)
     `ceph_cinder_user` （默认值： `cinder` ）
  - `ceph_cinder_pool_name` (default: `volumes`)
     `ceph_cinder_pool_name` （默认值： `volumes` ）
  - `ceph_cinder_backup_keyring` (default: `client.cinder-backup.keyring`)
     `ceph_cinder_backup_keyring` （默认值： `client.cinder-backup.keyring` ）
  - `ceph_cinder_backup_user` (default: `cinder-backup`)
     `ceph_cinder_backup_user` （默认值： `cinder-backup` ）
  - `ceph_cinder_backup_pool_name` (default: `backups`)
     `ceph_cinder_backup_pool_name` （默认值： `backups` ）

- Copy Ceph configuration file to `/etc/kolla/config/cinder/ceph.conf`
  将 Ceph 配置文件复制到 `/etc/kolla/config/cinder/ceph.conf` 

  Separate configuration options can be configured for cinder-volume and cinder-backup by adding ceph.conf files to `/etc/kolla/config/cinder/cinder-volume` and `/etc/kolla/config/cinder/cinder-backup` respectively. They will be merged with `/etc/kolla/config/cinder/ceph.conf`.
  通过分别添加 ceph.conf 文件， `/etc/kolla/config/cinder/cinder-volume`  `/etc/kolla/config/cinder/cinder-backup` 可以为 cinder-volume 和 cinder-backup 配置单独的配置选项。它们将与 `/etc/kolla/config/cinder/ceph.conf` .

- Copy Ceph keyring files to:
  将 Ceph 密钥环文件复制到：

  - `/etc/kolla/config/cinder/cinder-volume/ceph.<ceph_cinder_keyring>`
  - `/etc/kolla/config/cinder/cinder-backup/ceph.<ceph_cinder_keyring>`
  - `/etc/kolla/config/cinder/cinder-backup/ceph. <ceph_cinder_backup_keyring>`



 

Note 注意



`cinder-backup` requires two keyrings for accessing volumes and backup pool.
 `cinder-backup` 需要两个密钥环来访问卷和备份池。

To configure `multiple Ceph backends` with Cinder, which is useful for the use with availability zones:
要使用 Cinder 进行配置 `multiple Ceph backends` ，这对于与可用性区域一起使用很有用：

- Copy their Ceph configuration files into `/etc/kolla/config/cinder/` using different names for each
  复制他们的 Ceph 配置文件， `/etc/kolla/config/cinder/` 为每个文件使用不同的名称

  `/etc/kolla/config/cinder/ceph.conf`

  ```
  [global]
  fsid = 1d89fec3-325a-4963-a950-c4afedd37fe3
  mon_initial_members = ceph-0
  mon_host = 192.168.0.56
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

  `/etc/kolla/config/cinder/rbd2.conf`

  ```
  [global]
  fsid = dbfea068-89ca-4d04-bba0-1b8a56c3abc8
  mon_initial_members = ceph-0
  mon_host = 192.10.0.100
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

- Declare Ceph backends in `globals.yml`
  在 `globals.yml` 

  ```
  cinder_ceph_backends:
    - name: "rbd-1"
      cluster: "ceph"
      enabled: "{{ cinder_backend_ceph | bool }}"
    - name: "rbd-2"
      cluster: "rbd2"
      availability_zone: "az2"
      enabled: "{{ cinder_backend_ceph | bool }}"
  ```

- Copy Ceph keyring files for all Ceph backends:
  复制所有 Ceph 后端的 Ceph 密钥环文件：

  - `/etc/kolla/config/cinder/cinder-volume/ceph.<ceph_cinder_keyring>`
  - `/etc/kolla/config/cinder/cinder-backup/ceph.<ceph_cinder_keyring>`
  - `/etc/kolla/config/cinder/cinder-backup/ceph. <ceph_cinder_backup_keyring>`
  - `/etc/kolla/config/cinder/cinder-volume/rbd2.<ceph_cinder_keyring>`
  - `/etc/kolla/config/cinder/cinder-backup/rbd2.<ceph_cinder_keyring>`
  - `/etc/kolla/config/cinder/cinder-backup/rbd2. <ceph_cinder_backup_keyring>`



 

Note 注意



`cinder-backup` requires two keyrings for accessing volumes and backup pool.
 `cinder-backup` 需要两个密钥环来访问卷和备份池。

Nova must also be configured to allow access to Cinder volumes:
还必须将 Nova 配置为允许访问 Cinder 卷：

- Configure Ceph authentication details in `/etc/kolla/globals.yml`:
  在以下位置 `/etc/kolla/globals.yml` 配置 Ceph 身份验证详细信息：
  - `ceph_cinder_keyring` (default: `client.cinder.keyring`)
     `ceph_cinder_keyring` （默认值： `client.cinder.keyring` ）
- Copy Ceph keyring file(s) to:
  将 Ceph keyring 文件复制到：
  - `/etc/kolla/config/nova/ceph.<ceph_cinder_keyring>`

To configure `different Ceph backend` for nova-compute host, which is useful for the use with availability zones:
 `different Ceph backend` 要配置 nova-compute 主机，这对于与可用性区域一起使用很有用，请执行以下操作：

- Copy Ceph keyring file to:
  将 Ceph 密钥环文件复制到：
  - `/etc/kolla/config/nova/<hostname>/ceph.<ceph_cinder_keyring>`

If `zun` is enabled, and you wish to use cinder volumes with zun, it must also be configured to allow access to Cinder volumes:
如果 `zun` 启用，并且您希望将 cinder 卷与 zun 一起使用，则还必须将其配置为允许访问 Cinder 卷：

- Enable Cinder Ceph backend for Zun in `globals.yml`:
  在以下情况下 `globals.yml` 为 Zun 启用 Cinder Ceph 后端：

  ```
  zun_configure_for_cinder_ceph: "yes"
  ```

- Copy Ceph configuration file to:
  将 Ceph 配置文件复制到：

  - `/etc/kolla/config/zun/zun-compute/ceph.conf`

- Copy Ceph keyring file(s) to:
  将 Ceph keyring 文件复制到：

  - `/etc/kolla/config/zun/zun-compute/ceph.<ceph_cinder_keyring>`

### Nova 新星 ¶

Ceph RBD can be used as a storage backend for Nova instance ephemeral disks. This avoids the requirement for local storage for instances on compute nodes. It improves the performance of migration, since instances’ ephemeral disks do not need to be copied between hypervisors.
Ceph RBD 可以用作 Nova 实例临时磁盘的存储后端。这避免了计算节点上的实例对本地存储的需求。它提高了迁移的性能，因为不需要在虚拟机管理程序之间复制实例的临时磁盘。

Configuring Nova for Ceph includes following steps:
为 Ceph 配置 Nova 包括以下步骤：

- Enable Nova Ceph backend in `globals.yml`:
  在以下位置 `globals.yml` 启用 Nova Ceph 后端：

  ```
  nova_backend_ceph: "yes"
  ```

- Configure Ceph authentication details in `/etc/kolla/globals.yml`:
  在以下位置 `/etc/kolla/globals.yml` 配置 Ceph 身份验证详细信息：

  - `ceph_nova_keyring` (by default it’s the same as `ceph_cinder_keyring`)
     `ceph_nova_keyring` （默认情况下与 `ceph_cinder_keyring` ）
  - `ceph_nova_user` (by default it’s the same as `ceph_cinder_user`)
     `ceph_nova_user` （默认情况下与 `ceph_cinder_user` ）
  - `ceph_nova_pool_name` (default: `vms`)
     `ceph_nova_pool_name` （默认值： `vms` ）

- Copy Ceph configuration file to `/etc/kolla/config/nova/ceph.conf`
  将 Ceph 配置文件复制到 `/etc/kolla/config/nova/ceph.conf` 

- Copy Ceph keyring file(s) to:
  将 Ceph keyring 文件复制到：

  - `/etc/kolla/config/nova/ceph.<ceph_nova_keyring>`

  

   

  Note 注意

  

  If you are using a Ceph deployment tool that generates separate Ceph keys for Cinder and Nova, you will need to override `ceph_nova_keyring` and `ceph_nova_user` to match.
  如果您使用的是为 Cinder 和 Nova 生成单独 Ceph 密钥的 Ceph 部署工具，则需要覆盖 `ceph_nova_keyring` 并 `ceph_nova_user` 匹配。

To configure `different Ceph backend` for nova-compute host, which is useful for the use with availability zones:
 `different Ceph backend` 要配置 nova-compute 主机，这对于与可用性区域一起使用很有用，请执行以下操作：

- Copy Ceph configuration file to `/etc/kolla/config/nova/ <hostname>/ceph.conf`
  将 Ceph 配置文件复制到 `/etc/kolla/config/nova/ <hostname>/ceph.conf` 
- Copy Ceph keyring file(s) to:
  将 Ceph keyring 文件复制到：
  - `/etc/kolla/config/nova/<hostname>/ceph.<ceph_nova_keyring>`

### Gnocchi

Ceph object storage can be used as a storage backend for Gnocchi metrics. Configuring Gnocchi for Ceph includes following steps:
Ceph 对象存储可以用作 Gnocchi 指标的存储后端。为 Ceph 配置 Gnocchi 包括以下步骤：

- Enable Gnocchi Ceph backend in `globals.yml`:
  在以下位置 `globals.yml` 启用 Gnocchi Ceph 后端：

  ```
  gnocchi_backend_storage: "ceph"
  ```

- Configure Ceph authentication details in `/etc/kolla/globals.yml`:
  在以下位置 `/etc/kolla/globals.yml` 配置 Ceph 身份验证详细信息：

  - `ceph_gnocchi_keyring` (default: `client.gnocchi.keyring`)
     `ceph_gnocchi_keyring` （默认值： `client.gnocchi.keyring` ）
  - `ceph_gnocchi_user` (default: `gnocchi`)
     `ceph_gnocchi_user` （默认值： `gnocchi` ）
  - `ceph_gnocchi_pool_name` (default: `gnocchi`)
     `ceph_gnocchi_pool_name` （默认值： `gnocchi` ）
  - `ceph_gnocchi_conf` (default: `ceph.conf`)
     `ceph_gnocchi_conf` （默认值： `ceph.conf` ）

- Copy Ceph configuration file to `/etc/kolla/config/gnocchi/<ceph_gnocchi_conf>`
  将 Ceph 配置文件复制到 `/etc/kolla/config/gnocchi/<ceph_gnocchi_conf>` 

- Copy Ceph keyring to `/etc/kolla/config/gnocchi/ceph.<ceph_gnocchi_keyring>`
  将 Ceph 密钥环复制到 `/etc/kolla/config/gnocchi/ceph.<ceph_gnocchi_keyring>` 

### Manila 马尼拉 ¶

CephFS can be used as a storage backend for Manila shares. Configuring Manila for Ceph includes following steps:
CephFS 可以用作 Manila 共享的存储后端。为 Ceph 配置 Manila 包括以下步骤：

- Enable Manila Ceph backend in `globals.yml`:
  在以下位置 `globals.yml` 启用 Manila Ceph 后端：

  ```
  enable_manila_backend_cephfs_native: "yes"
  ```

- Configure Ceph authentication details in `/etc/kolla/globals.yml`:
  在以下位置 `/etc/kolla/globals.yml` 配置 Ceph 身份验证详细信息：

  - `ceph_manila_keyring` (default: `client.manila.keyring`)
     `ceph_manila_keyring` （默认值： `client.manila.keyring` ）
  - `ceph_manila_user` (default: `manila`)
     `ceph_manila_user` （默认值： `manila` ）

  

   

  Note 注意

  

  Required Ceph identity caps for manila user are documented in [CephFS Native driver](https://docs.openstack.org/manila/latest/admin/cephfs_driver.html#authorizing-the-driver-to-communicate-with-ceph).
  马尼拉用户所需的 Ceph 身份上限记录在 CephFS Native 驱动程序中。

- Copy Ceph configuration file to `/etc/kolla/config/manila/ceph.conf`
  将 Ceph 配置文件复制到 `/etc/kolla/config/manila/ceph.conf` 

- Copy Ceph keyring to `/etc/kolla/config/manila/ceph.<ceph_manila_keyring>`
  将 Ceph 密钥环复制到 `/etc/kolla/config/manila/ceph.<ceph_manila_keyring>` 

To configure `multiple Ceph backends` with Manila, which is useful for the use with availability zones:
要使用 Manila 进行配置 `multiple Ceph backends` ，这对于与可用性区域一起使用很有用：

- Copy their Ceph configuration files into `/etc/kolla/config/manila/` using different names for each
  复制他们的 Ceph 配置文件， `/etc/kolla/config/manila/` 为每个文件使用不同的名称

  `/etc/kolla/config/manila/ceph.conf`

  ```
  [global]
  fsid = 1d89fec3-325a-4963-a950-c4afedd37fe3
  mon_initial_members = ceph-0
  mon_host = 192.168.0.56
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

  `/etc/kolla/config/manila/rbd2.conf`

  ```
  [global]
  fsid = dbfea068-89ca-4d04-bba0-1b8a56c3abc8
  mon_initial_members = ceph-0
  mon_host = 192.10.0.100
  auth_cluster_required = cephx
  auth_service_required = cephx
  auth_client_required = cephx
  ```

- Declare Ceph backends in `globals.yml`
  在 `globals.yml` 

  ```
  manila_ceph_backends:
    - name: "cephfsnative1"
      share_name: "CEPHFS1"
      driver: "cephfsnative"
      cluster: "ceph"
      enabled: "{{ enable_manila_backend_cephfs_native | bool }}"
      protocols:
        - "CEPHFS"
    - name: "cephfsnative2"
      share_name: "CEPHFS2"
      driver: "cephfsnative"
      cluster: "rbd2"
      enabled: "{{ enable_manila_backend_cephfs_native | bool }}"
      protocols:
        - "CEPHFS"
    - name: "cephfsnfs1"
      share_name: "CEPHFSNFS1"
      driver: "cephfsnfs"
      cluster: "ceph1"
      enabled: "{{ enable_manila_backend_cephfs_nfs | bool }}"
      protocols:
        - "NFS"
        - "CIFS"
    - name: "cephfsnfs2"
      share_name: "CEPHFSNFS2"
      driver: "cephfsnfs"
      cluster: "rbd2"
      enabled: "{{ enable_manila_backend_cephfs_nfs | bool }}"
      protocols:
        - "NFS"
        - "CIFS"
  ```

- Copy Ceph keyring files for all Ceph backends:
  复制所有 Ceph 后端的 Ceph 密钥环文件：

  - `/etc/kolla/config/manila/manila-share/ceph.<ceph_manila_keyring>`
  - `/etc/kolla/config/manila/manila-share/rbd2.<ceph_manila_keyring>`

- If using multiple filesystems (Ceph Pacific+), set `manila_cephfs_filesystem_name` in `/etc/kolla/globals.yml` to the name of the Ceph filesystem Manila should use. By default, Manila will use the first filesystem returned by the `ceph fs volume ls` command.
  如果使用多个文件系统 （Ceph Pacific+），请设置为 `manila_cephfs_filesystem_name` `/etc/kolla/globals.yml` Manila 应使用的 Ceph 文件系统的名称。默认情况下，Manila 将使用该 `ceph fs volume ls` 命令返回的第一个文件系统。

- Setup Manila in the usual way
  以通常的方式设置马尼拉

For more details on the rest of the Manila setup, such as creating the share type `default_share_type`, please see [Manila in Kolla](https://docs.openstack.org/kolla-ansible/latest/reference/storage/manila-guide.html).
有关 Manila 设置的其余部分的更多详细信息，例如创建共享类型 `default_share_type` ，请参阅 Kolla 中的 Manila。

For more details on the CephFS Native driver, please see [CephFS Native driver](https://docs.openstack.org/manila/latest/admin/cephfs_driver.html).
有关 CephFS Native 驱动程序的更多详细信息，请参阅 CephFS Native 驱动程序。

### RadosGW

As of the Xena 13.0.0 release, Kolla Ansible supports integration with Ceph RadosGW. This includes:
从 Xena 13.0.0 版本开始，Kolla Ansible 支持与 Ceph RadosGW 集成。这包括：

- Registration of Swift-compatible endpoints in Keystone
  在 Keystone 中注册与 Swift 兼容的端点
- Load balancing across RadosGW API servers using HAProxy
  使用 HAProxy 在 RadosGW API 服务器之间实现负载平衡

See the [Ceph documentation](https://docs.ceph.com/en/latest/radosgw/keystone/) for further information, including changes that must be applied to the Ceph cluster configuration.
有关更多信息，请参阅 Ceph 文档，包括必须应用于 Ceph 集群配置的更改。

Enable Ceph RadosGW integration:
启用 Ceph RadosGW 集成：

```
enable_ceph_rgw: true
```

#### Keystone integration Keystone 集成 ¶

A Keystone user and endpoints are registered by default, however this may be avoided by setting `enable_ceph_rgw_keystone` to `false`. If registration is enabled, the username is defined via `ceph_rgw_keystone_user`, and this defaults to `ceph_rgw`. The hostnames used by the endpoints default to `ceph_rgw_external_fqdn` and `ceph_rgw_internal_fqdn` for the public and internal endpoints respectively. These default to `kolla_external_fqdn` and `kolla_internal_fqdn` respectively. The port used by the endpoints is defined via `ceph_rgw_port`, and defaults to 6780.
默认情况下，Keystone 用户和端点是注册的，但是可以通过设置为 `enable_ceph_rgw_keystone` `false` 来避免这种情况。如果启用了注册，则用户名将通过 定义 `ceph_rgw_keystone_user` ，默认为 `ceph_rgw` 。端点使用的主机名分别默认为 `ceph_rgw_external_fqdn`  `ceph_rgw_internal_fqdn` 公共端点和内部端点。它们分别默认为 `kolla_external_fqdn` 和 `kolla_internal_fqdn` 。端点使用的端口通过 `ceph_rgw_port` 定义，默认为 6780。

By default RadosGW supports both Swift and S3 API, and it is not completely compatible with Swift API. The option `ceph_rgw_swift_compatibility` can enable/disable complete RadosGW compatibility with Swift API.  This should match the configuration used by Ceph RadosGW. After changing the value, run the `kolla-ansible deploy` command to enable.
默认情况下，RadosGW 同时支持 Swift 和 S3 API，并且与 Swift API 并不完全兼容。该选项 `ceph_rgw_swift_compatibility` 可以启用/禁用 RadosGW 与 Swift API 的完全兼容性。这应该与Ceph RadosGW使用的配置相匹配。更改值后，运行 `kolla-ansible deploy` 命令以启用。

By default, the RadosGW endpoint URL does not include the project (account) ID. This prevents cross-project and public object access. This can be resolved by setting `ceph_rgw_swift_account_in_url` to `true`. This should match the `rgw_swift_account_in_url` configuration option in Ceph RadosGW.
默认情况下，RadosGW 端点 URL 不包含项目（帐户）ID。这样可以防止跨项目和公共对象访问。这可以通过设置为 `ceph_rgw_swift_account_in_url` `true` 来解决。这应该与 `rgw_swift_account_in_url` Ceph RadosGW 中的配置选项匹配。

#### Load balancing 负载均衡 ¶



 

Warning 警告



Users of Ceph RadosGW can generate very high volumes of traffic. It is advisable to use a separate load balancer for RadosGW for anything other than small or lightly utilised RadosGW deployments, however this is currently out of scope for Kolla Ansible.
Ceph RadosGW的用户可以产生非常高的流量。建议对 RadosGW 使用单独的负载均衡器，用于小型或轻度利用的 RadosGW 部署以外的任何部署，但这目前超出了 Kolla Ansible 的范围。

Load balancing is enabled by default, however this may be avoided by setting `enable_ceph_rgw_loadbalancer` to `false`. If using load balancing, the RadosGW hosts and ports must be configured. Each item should contain `host` and `port` keys. The `ip` and `port` keys are optional. If `ip` is not specified, the `host` values should be resolvable from the host running HAProxy. If the `port` is not specified, the default HTTP (80) or HTTPS (443) port will be used. For example:
默认情况下，负载平衡处于启用状态，但可以通过设置为 `enable_ceph_rgw_loadbalancer` `false` 来避免这种情况。如果使用负载平衡，则必须配置 RadosGW 主机和端口。每个项目都应包含 `host` 和 `port` 键。 `ip` 和 `port` 键是可选的。如果 `ip` 未指定，则这些 `host` 值应可从运行 HAProxy 的主机解析。如果未指定， `port` 则将使用默认的 HTTP （80） 或 HTTPS （443） 端口。例如：

```
ceph_rgw_hosts:
  - host: rgw-host-1
  - host: rgw-host-2
    ip: 10.0.0.42
    port: 8080
```

The HAProxy frontend port is defined via `ceph_rgw_port`, and defaults to 6780.
HAProxy 前端端口通过 `ceph_rgw_port` 定义，默认为 6780。

#### Cephadm and Ceph Client Version Cephadm 和 Ceph 客户端版本 ¶

When configuring Zun with Cinder volumes, kolla-ansible installs some Ceph client packages on zun-compute hosts. You can set the version of the Ceph packages installed by,
使用 Cinder 卷配置 Zun 时，kolla-ansible 会在 zun-compute 主机上安装一些 Ceph 客户端软件包。您可以设置由以下机构安装的 Ceph 软件包的版本：

- Configuring Ceph version details in `/etc/kolla/globals.yml`:
  配置 `/etc/kolla/globals.yml` Ceph 版本详细信息：
  - `ceph_version` (default: `pacific`)
     `ceph_version` （默认值： `pacific` ）

​                      

# Cinder - Block storage Cinder - 块存储

​        version 版本              





## Overview 概述 ¶

Cinder can be deployed using Kolla and supports the following storage backends:
Cinder 可以使用 Kolla 进行部署，并支持以下存储后端：

- ceph 西弗
- hnas_nfs
- iscsi
- lvm
- nfs

## LVM

When using the `lvm` backend, a volume group should be created on each storage node. This can either be a real physical volume or a loopback mounted file for development.  Use `pvcreate` and `vgcreate` to create the volume group.  For example with the devices `/dev/sdb` and `/dev/sdc`:
使用 `lvm` 后端时，应在每个存储节点上创建一个卷组。这可以是真正的物理卷，也可以是用于开发的环回挂载文件。使用 `pvcreate` 和 `vgcreate` 创建卷组。例如，设备 `/dev/sdb` 和 `/dev/sdc` ：

```
<WARNING ALL DATA ON /dev/sdb and /dev/sdc will be LOST!>

pvcreate /dev/sdb /dev/sdc
vgcreate cinder-volumes /dev/sdb /dev/sdc
```

During development, it may be desirable to use file backed block storage. It is possible to use a file and mount it as a block device via the loopback system.
在开发过程中，可能需要使用文件支持的块存储。可以使用文件并通过环回系统将其挂载为块设备。

```
free_device=$(losetup -f)
fallocate -l 20G /var/lib/cinder_data.img
losetup $free_device /var/lib/cinder_data.img
pvcreate $free_device
vgcreate cinder-volumes $free_device
```

Enable the `lvm` backend in `/etc/kolla/globals.yml`:
在以下位置 `/etc/kolla/globals.yml` 启用 `lvm` 后端：

```
enable_cinder_backend_lvm: "yes"
```



 

Note 注意



There are currently issues using the LVM backend in a multi-controller setup, see [_bug 1571211](https://launchpad.net/bugs/1571211) for more info.
目前在多控制器设置中使用 LVM 后端存在问题，有关详细信息，请参阅_bug 1571211。

## NFS

To use the `nfs` backend, configure `/etc/exports` to contain the mount where the volumes are to be stored:
要使用 `nfs` 后端，请配置 `/etc/exports` 为包含要存储卷的装载：

```
/kolla_nfs 192.168.5.0/24(rw,sync,no_root_squash)
```

In this example, `/kolla_nfs` is the directory on the storage node which will be `nfs` mounted, `192.168.5.0/24` is the storage network, and `rw,sync,no_root_squash` means make the share read-write, synchronous, and prevent remote root users from having access to all files.
在此示例中， `/kolla_nfs` 是要 `nfs` 挂载的存储节点上的目录， `192.168.5.0/24` 是存储网络，并表示 `rw,sync,no_root_squash` 使共享读写、同步，并防止远程 root 用户访问所有文件。

Then start `nfsd`: 然后开始 `nfsd` ：

```
systemctl start nfs
```

On the deploy node, create `/etc/kolla/config/nfs_shares` with an entry for each storage node:
在部署节点上，使用每个存储节点的条目进行创建 `/etc/kolla/config/nfs_shares` ：

```
storage01:/kolla_nfs
storage02:/kolla_nfs
```

Finally, enable the `nfs` backend in `/etc/kolla/globals.yml`:
最后，在以下位置 `/etc/kolla/globals.yml` 启用 `nfs` 后端：

```
enable_cinder_backend_nfs: "yes"
```

## Validation 验证 ¶

Create a volume as follows:
按如下方式创建卷：

```
openstack volume create --size 1 steak_volume
<bunch of stuff printed>
```

Verify it is available. If it says “error”, then something went wrong during LVM creation of the volume.
验证它是否可用。如果它显示“错误”，则表示在 LVM 创建卷期间出了问题。

```
openstack volume list

+--------------------------------------+--------------+-----------+------+-------------+
| ID                                   | Display Name | Status    | Size | Attached to |
+--------------------------------------+--------------+-----------+------+-------------+
| 0069c17e-8a60-445a-b7f0-383a8b89f87e | steak_volume | available |    1 |             |
+--------------------------------------+--------------+-----------+------+-------------+
```

Attach the volume to a server using:
使用以下命令将卷连接到服务器：

```
openstack server add volume steak_server 0069c17e-8a60-445a-b7f0-383a8b89f87e
```

Check the console log to verify the disk addition:
检查控制台日志以验证磁盘添加：

```
openstack console log show steak_server
```

A `/dev/vdb` should appear in the console log, at least when booting cirros. If the disk stays in the available state, something went wrong during the iSCSI mounting of the volume to the guest VM.
A `/dev/vdb` 应该出现在控制台日志中，至少在引导 cirros 时是这样。如果磁盘保持可用状态，则表示在将卷装载到客户机虚拟机的 iSCSI 期间出现问题。

## Cinder LVM2 backend with iSCSI 带有 iSCSI 的 Cinder LVM2 后端 ¶

As of Newton-1 milestone, Kolla supports LVM2 as cinder backend. It is accomplished by introducing two new containers `tgtd` and `iscsid`. `tgtd` container serves as a bridge between cinder-volume process and a server hosting Logical Volume Groups (LVG). `iscsid` container serves as a bridge between nova-compute process and the server hosting LVG.
从 Newton-1 里程碑开始，Kolla 支持 LVM2 作为 cinder 后端。它是通过引入两个新容器 `tgtd` 和 `iscsid` . `tgtd` 容器充当 cinder-volume 进程和托管逻辑卷组 （LVG） 的服务器之间的桥梁。 `iscsid` 容器充当 nova-compute 进程和托管 LVG 的服务器之间的桥梁。

In order to use Cinder’s LVM backend, a LVG named `cinder-volumes` should exist on the server and following parameter must be specified in `globals.yml`:
为了使用 Cinder 的 LVM 后端，服务器上应该存在一个名为 `cinder-volumes` LVG，并且必须在 ： `globals.yml` 

```
enable_cinder_backend_lvm: "yes"
```

### For Ubuntu and LVM2/iSCSI 对于 Ubuntu 和 LVM2/iSCSI ¶

`iscsd` process uses configfs which is normally mounted at `/sys/kernel/config` to store discovered targets information, on centos/rhel type of systems this special file system gets mounted automatically, which is not the case on debian/ubuntu. Since `iscsid` container runs on every nova compute node, the following steps must be completed on every Ubuntu server targeted for nova compute role.
 `iscsd` 进程使用通常挂载的 `/sys/kernel/config` configfs 来存储发现的目标信息，在 centos/rhel 类型的系统上，这个特殊的文件系统会自动挂载，而在 debian/ubuntu 上则不是这样。由于 `iscsid` 容器在每个 nova 计算节点上运行，因此必须在每个针对 nova 计算角色的 Ubuntu 服务器上完成以下步骤。

- Add configfs module to `/etc/modules`
  将 configfs 模块添加到 `/etc/modules` 

- Rebuild initramfs using: `update-initramfs -u` command
  使用 command `update-initramfs -u` 重建 initramfs

- Stop `open-iscsi` system service due to its conflicts with iscsid container.
  由于系统服务与 iscsid 容器冲突，因此停止 `open-iscsi` 系统服务。

  Ubuntu 16.04 (systemd): `systemctl stop open-iscsi; systemctl stop iscsid`
  Ubuntu 16.04 （systemd）： `systemctl stop open-iscsi; systemctl stop iscsid` 

- Make sure configfs gets mounted during a server boot up process. There are multiple ways to accomplish it, one example:
  确保在服务器启动过程中挂载 configfs。有多种方法可以实现它，一个示例：

  ```
  mount -t configfs /etc/rc.local /sys/kernel/config
  ```

  

   

  Note 注意

  

  There is currently an issue with the folder /sys/kernel/config as it is either empty or does not exist in several operating systems, see [_bug 1631072](https://bugs.launchpad.net/kolla/+bug/1631072) for more info
  文件夹 /sys/kernel/config 当前存在问题，因为它在多个操作系统中为空或不存在，有关详细信息，请参阅_bug 1631072

## Cinder backend with external iSCSI storage 带有外部 iSCSI 存储的 Cinder 后端 ¶

In order to use external storage system (like the ones from EMC or NetApp) the following parameter must be specified in `globals.yml`:
要使用外部存储系统（如 EMC 或 NetApp 的存储系统），必须在 `globals.yml` ：

```
enable_cinder_backend_iscsi: "yes"
```

Also `enable_cinder_backend_lvm` should be set to `no` in this case.
在这种情况下，也 `enable_cinder_backend_lvm` 应设置为 `no` 。

## Skip Cinder prechecks for Custom backends 跳过 Cinder 对自定义后端的预检查 ¶

In order to use custom storage backends which currently not yet implemented in Kolla, the following parameter must be specified in `globals.yml`:
为了使用当前尚未在 Kolla 中实现的自定义存储后端，必须在 `globals.yml` 中指定以下参数：

```
skip_cinder_backend_check: True
```

All configuration for custom NFS backend should be performed via `cinder.conf` in config overrides directory.
自定义 NFS 后端的所有配置都应通过 `cinder.conf` config overrides 目录执行。

## Cinder-Backup with S3 Backend 使用 S3 后端的 Cinder-Backup ¶

Configuring Cinder-Backup for S3 includes the following steps:
为 S3 配置 Cinder-Backup 包括以下步骤：

1. Enable Cinder-Backup S3 backend in `globals.yml`:
   在以下位置 `globals.yml` 启用 Cinder-Backup S3 后端：

```
cinder_backup_driver: "s3"
```

1. Configure S3 connection details in `/etc/kolla/globals.yml`:
   在以下位置 `/etc/kolla/globals.yml` 配置 S3 连接详细信息：
   - `cinder_backup_s3_url` (example: `http://127.0.0.1:9000`)
      `cinder_backup_s3_url` （示例： `http://127.0.0.1:9000` ）
   - `cinder_backup_s3_access_key` (example: `minio`)
      `cinder_backup_s3_access_key` （示例： `minio` ）
   - `cinder_backup_s3_bucket` (example: `cinder`)
      `cinder_backup_s3_bucket` （示例： `cinder` ）
   - `cinder_backup_s3_secret_key` (example: `admin`)
      `cinder_backup_s3_secret_key` （示例： `admin` ）

\#. If you wish to use a single S3 backend for all supported services, use the following variables:
\#.如果您希望对所有受支持的服务使用单个 S3 后端，请使用以下变量：

> - `s3_url`
> - `s3_access_key`
> - `s3_glance_bucket`
> - `s3_secret_key`
>
> All Cinder-Backup S3 configurations use these options as default values.
> 所有 Cinder-Backup S3 配置都使用这些选项作为默认值。

### Customizing backend names in cinder.conf 在 cinder.conf 中自定义后端名称 ¶



 

Note 注意



This is an advanced configuration option. You cannot change these variables if you already have volumes that use the old name without additional steps. Sensible defaults exist out of the box.
这是一个高级配置选项。如果已有使用旧名称的卷，则无法更改这些变量，而无需执行其他步骤。合理的默认值开箱即用。

The following variables are available to customise the default backend name that appears in cinder.conf:
以下变量可用于自定义 cinder.conf 中显示的默认后端名称：

| Driver                                                       | Variable                                   | Default value                               |
| ------------------------------------------------------------ | ------------------------------------------ | ------------------------------------------- |
| Ceph 塞夫                                                    | cinder_backend_ceph_name                   | rbd-1 RBD-1型                               |
| Logical Volume Manager (LVM) 逻辑卷管理器 （LVM）            | cinder_backend_lvm_name                    | lvm-1 LVM-1型                               |
| Network File System (NFS) 网络文件系统 （NFS）               | cinder_backend_nfs_name                    | nfs-1 NFS-1型                               |
| Hitachi NAS Platform NFS Hitachi NAS 平台 NFS                | cinder_backend_hnas_nfs_name               | hnas-nfs HNAS-NFS型                         |
| VMware Virtual Machine Disk File VMware 虚拟机磁盘文件       | cinder_backend_vmwarevc_vmdk_name          | vmwarevc-vmdk                               |
| VMware VStorage (Object Storage) VMware VStorage（对象存储） | cinder_backend_vmware_vstorage_object_name | vmware-vstorage-object vmware-vstorage-对象 |
| Quobyte Storage for OpenStack OpenStack 的 Quobyte 存储      | cinder_backend_quobyte_name                | QuobyteHD QuobyteHD的                       |
| Pure Storage FlashArray for OpenStack (iSCSI) 用于 OpenStack 的纯存储 FlashArray （iSCSI） | cinder_backend_pure_iscsi_name             | Pure-FlashArray-iscsi                       |
| Pure Storage FlashArray for OpenStack 用于 OpenStack 的纯存储 FlashArray | cinder_backend_pure_fc_name                | Pure-FlashArray-fc 纯 FlashArray-fc         |
| Pure Storage FlashArray for OpenStack 用于 OpenStack 的纯存储 FlashArray | cinder_backend_pure_roce_name              | Pure-FlashArray-roce                        |

These are the names you use when [configuring](https://docs.openstack.org/cinder/latest/admin/multi-backend.html#volume-type) `volume_backend_name` on cinder volume types. It can sometimes be useful to provide a more descriptive name.
这些是您在 cinder 卷类型上配置 `volume_backend_name` 时使用的名称。有时，提供更具描述性的名称会很有用。

# Hitachi NAS Platform iSCSI and NFS drives for OpenStack 适用于 OpenStack 的 Hitachi NAS Platform iSCSI 和 NFS 硬盘

​        version 版本              





## Overview 概述 ¶

The Block Storage service provides persistent block storage resources that Compute instances can consume. This includes secondary attached storage similar to the Amazon Elastic Block Storage (EBS) offering. In addition, you can write images to a Block Storage device for Compute to use as a bootable persistent instance.
块存储服务提供计算实例可以使用的持久性块存储资源。这包括类似于 Amazon Elastic Block Storage （EBS） 产品的辅助附加存储。此外，您可以将映像写入块存储设备，以便 Compute 用作可启动的持久实例。

### Requirements 要求 ¶

- Hitachi NAS Platform Models 3080, 3090, 4040, 4060, 4080, and 4100.
  Hitachi NAS 平台型号 3080、3090、4040、4060、4080 和 4100。
- HNAS/SMU software version is 12.2 or higher.
  HNAS/SMU 软件版本为 12.2 或更高版本。
- HNAS configuration and management utilities to create a storage pool (span) and an EVS.
  HNAS配置和管理实用程序，用于创建存储池（span）和EVS。
  - GUI (SMU). 图形用户界面 （SMU）。
  - SSC CLI. SSC 命令行界面。
- You must set an iSCSI domain to EVS
  您必须将 iSCSI 域设置为 EVS

### Supported shared file systems and operations 支持的共享文件系统和操作 ¶

The NFS and iSCSI drivers support these operations:
NFS 和 iSCSI 驱动程序支持以下操作：

- Create, delete, attach, and detach volumes.
  创建、删除、附加和分离卷。
- Create, list, and delete volume snapshots.
  创建、列出和删除卷快照。
- Create a volume from a snapshot.
  从快照创建卷。
- Copy an image to a volume.
  将图像复制到卷。
- Copy a volume to an image.
  将卷复制到映像。
- Clone a volume. 克隆卷。
- Extend a volume. 扩展卷。
- Get volume statistics. 获取交易量统计信息。
- Manage and unmanage a volume.
  管理和取消管理卷。
- Manage and unmanage snapshots (HNAS NFS only).
  管理和取消管理快照（仅限 HNAS NFS）。

## Configuration example for Hitachi NAS Platform NFS Hitachi NAS Platform NFS 配置示例 ¶

### NFS backend NFS 后端 ¶

Enable cinder hnas backend nfs in `/etc/kolla/globals.yml`
启用 `/etc/kolla/globals.yml` cinder hnas 后端 nfs

```
enable_cinder_backend_hnas_nfs: "yes"
```

Create or modify the file `/etc/kolla/config/cinder.conf` and add the contents:
创建或修改文件 `/etc/kolla/config/cinder.conf` 并添加内容：

```
[DEFAULT]
enabled_backends = hnas-nfs

[hnas-nfs]
volume_driver = cinder.volume.drivers.hitachi.hnas_nfs.HNASNFSDriver
volume_nfs_backend = hnas_nfs_backend
hnas_nfs_username = supervisor
hnas_nfs_mgmt_ip0 = <hnas_ip>
hnas_chap_enabled = True

hnas_nfs_svc0_volume_type = nfs_gold
hnas_nfs_svc0_hdp = <svc0_ip>/<export_name>
```

Then set password for the backend in `/etc/kolla/passwords.yml`:
然后在以下位置 `/etc/kolla/passwords.yml` 设置后端密码：

```
hnas_nfs_password: supervisor
```

### Configuration on Kolla deployment Kolla 部署上的配置 ¶

Enable Shared File Systems service and HNAS driver in `/etc/kolla/globals.yml`
在 `/etc/kolla/globals.yml` 

```
enable_cinder: "yes"
```

### Configuration on HNAS HNAS上的配置 ¶

Create the data HNAS network in Kolla OpenStack:
在 Kolla OpenStack 中创建数据 HNAS 网络：

List the available tenants:
列出可用的租户：

```
openstack project list
```

Create a network to the given tenant (service), providing the tenant ID, a name for the network, the name of the physical network over which the virtual network is implemented, and the type of the physical mechanism by which the virtual network is implemented:
创建给定租户（服务）的网络，提供租户 ID、网络名称、实现虚拟网络的物理网络的名称以及实现虚拟网络的物理机制的类型：

```
neutron net-create --tenant-id <SERVICE_ID> hnas_network \
--provider:physical_network=physnet2 --provider:network_type=flat
```

Create a subnet to the same tenant (service), the gateway IP of this subnet, a name for the subnet, the network ID created before, and the CIDR of subnet:
为同一租户（服务）创建子网、该子网的网关 IP、子网的名称、之前创建的网络 ID 以及子网的 CIDR：

```
neutron subnet-create --tenant-id <SERVICE_ID> --gateway <GATEWAY> \
--name hnas_subnet <NETWORK_ID> <SUBNET_CIDR>
```

Add the subnet interface to a router, providing the router ID and subnet ID created before:
将子网接口添加到路由器，提供之前创建的路由器 ID 和子网 ID：

```
neutron router-interface-add <ROUTER_ID> <SUBNET_ID>
```

## Create volume 创建卷 ¶

Create a non-bootable volume.
创建不可引导的卷。

```
openstack volume create --size 1 my-volume
```

Verify Operation. 验证操作。

```
cinder show my-volume

+--------------------------------+--------------------------------------+
| Property                       | Value                                |
+--------------------------------+--------------------------------------+
| attachments                    | []                                   |
| availability_zone              | nova                                 |
| bootable                       | false                                |
| consistencygroup_id            | None                                 |
| created_at                     | 2017-01-17T19:02:45.000000           |
| description                    | None                                 |
| encrypted                      | False                                |
| id                             | 4f5b8ae8-9781-411e-8ced-de616ae64cfd |
| metadata                       | {}                                   |
| migration_status               | None                                 |
| multiattach                    | False                                |
| name                           | my-volume                            |
| os-vol-host-attr:host          | compute@hnas-iscsi#iscsi_gold        |
| os-vol-mig-status-attr:migstat | None                                 |
| os-vol-mig-status-attr:name_id | None                                 |
| os-vol-tenant-attr:tenant_id   | 16def9176bc64bd283d419ac2651e299     |
| replication_status             | disabled                             |
| size                           | 1                                    |
| snapshot_id                    | None                                 |
| source_volid                   | None                                 |
| status                         | available                            |
| updated_at                     | 2017-01-17T19:02:46.000000           |
| user_id                        | fb318b96929c41c6949360c4ccdbf8c0     |
| volume_type                    | None                                 |
+--------------------------------+--------------------------------------+

nova volume-attach INSTANCE_ID VOLUME_ID auto

+----------+--------------------------------------+
| Property | Value                                |
+----------+--------------------------------------+
| device   | /dev/vdc                             |
| id       | 4f5b8ae8-9781-411e-8ced-de616ae64cfd |
| serverId | 3bf5e176-be05-4634-8cbd-e5fe491f5f9c |
| volumeId | 4f5b8ae8-9781-411e-8ced-de616ae64cfd |
+----------+--------------------------------------+

openstack volume list

+--------------------------------------+---------------+----------------+------+-------------------------------------------+
| ID                                   | Display Name  | Status         | Size | Attached to                               |
+--------------------------------------+---------------+----------------+------+-------------------------------------------+
| 4f5b8ae8-9781-411e-8ced-de616ae64cfd | my-volume     | in-use         |    1 | Attached to private-instance on /dev/vdb  |
+--------------------------------------+---------------+----------------+------+-------------------------------------------+
```

For more information about how to manage volumes, see the [Manage volumes](https://docs.openstack.org/cinder/latest/cli/cli-manage-volumes.html).
有关如何管理卷的详细信息，请参阅管理卷。

For more information about how HNAS driver works, see [Hitachi NAS Platform iSCSI and NFS drives for OpenStack](https://docs.openstack.org/newton/config-reference/block-storage/drivers/hds-hnas-driver.html).
有关 HNAS 驱动程序工作原理的更多信息，请参见适用于 OpenStack 的 Hitachi NAS Platform iSCSI 和 NFS 驱动器。

​                      

# Quobyte Storage for OpenStack OpenStack 的 Quobyte 存储

​        version 版本              





## Quobyte Cinder Driver Quobyte Cinder 驱动程序 ¶

To use the `Quobyte` Cinder backend, enable and configure the `Quobyte` Cinder driver in `/etc/kolla/globals.yml`.
要使用 `Quobyte` Cinder 后端，请在 中 `/etc/kolla/globals.yml` 启用并配置 `Quobyte` Cinder 驱动程序。

```
enable_cinder_backend_quobyte: "yes"
```

Also set values for `quobyte_storage_host` and `quobyte_storage_volume` in `/etc/kolla/globals.yml` to the hostname or IP address of the Quobyte registry and the Quobyte volume respectively.
此外，将 for `quobyte_storage_host` 和 `quobyte_storage_volume` in `/etc/kolla/globals.yml` 的值分别设置为 Quobyte 注册表和 Quobyte 卷的主机名或 IP 地址。

Since `Quobyte` is proprietary software that requires a license, the use of this backend requires the `Quobyte` Client software package to be installed in the `cinder-volume` and `nova-compute` containers. To do this follow the steps outlined in the [Building Container Images](https://docs.openstack.org/kolla/latest/admin/image-building.html), particularly the `Package Customisation` and `Custom Repos` sections. The repository information is available in the `Quobyte` customer portal.
由于 `Quobyte` 是需要许可证的专有软件，因此使用此后端需要 `Quobyte` 将客户端软件包安装 `cinder-volume` 在容器 `nova-compute` 中。为此，请按照构建容器映像中概述的步骤操作，尤其是 `Package Customisation` 和 `Custom Repos` 部分。存储库信息可在客户门户中找到 `Quobyte` 。

# Pure Storage FlashArray for OpenStack 用于 OpenStack 的纯存储 FlashArray

​        version 版本              



## Pure Storage FlashArray Cinder Driver 纯存储 FlashArray Cinder 驱动程序 ¶

To use the `Pure Storage FlashArray iSCSI` Cinder backend, enable and configure the `FlashArray iSCSI` Cinder driver in `/etc/kolla/globals.yml`.
要使用 `Pure Storage FlashArray iSCSI` Cinder 后端，请在 中 `/etc/kolla/globals.yml` 启用并配置 `FlashArray iSCSI` Cinder 驱动程序。

```
enable_cinder_backend_pure_iscsi: "yes"
```

To use the `Pure Storage FlashArray FC` Cinder backend, enable and configure the `FlashArray FC` Cinder driver in `/etc/kolla/globals.yml`.
要使用 `Pure Storage FlashArray FC` Cinder 后端，请在 中 `/etc/kolla/globals.yml` 启用并配置 `FlashArray FC` Cinder 驱动程序。

```
enable_cinder_backend_pure_fc: "yes"
```

To use the `Pure Storage FlashArray NVMe-RoCE` Cinder backend, enable and configure the `FlashArray NVMe-RoCE` Cinder driver in `/etc/kolla/globals.yml`.
要使用 `Pure Storage FlashArray NVMe-RoCE` Cinder 后端，请在 中 `/etc/kolla/globals.yml` 启用并配置 `FlashArray NVMe-RoCE` Cinder 驱动程序。

```
enable_cinder_backend_pure_roce: "yes"
```



 

Note 注意



The NVMe-RoCE driver is only supported from OpenStack Zed and later.
NVMe-RoCE 驱动程序仅受 OpenStack Zed 及更高版本支持。

It is important to note that you cannot mix iSCSI and FC Pure Storage FlashArray drivers in the same OpenStack cluster.
需要注意的是，不能在同一 OpenStack 集群中混合使用 iSCSI 和 FC Pure Storage FlashArray 驱动程序。

Also set the values for the following parameters in `/etc/kolla/globals.yml`:
此外，在 `/etc/kolla/globals.yml` 中设置以下参数的值：

- `pure_api_token`
- `pure_san_ip`

For details on how to use these parameters, refer to the [Pure Storage Cinder Reference Guide](https://docs.openstack.org/cinder/latest/configuration/block-storage/drivers/pure-storage-driver.html).
有关如何使用这些参数的详细信息，请参阅 Pure Storage Cinder 参考指南。

There are numerous other parameters that can be set for this driver and these are detailed in the above link.
可以为此驱动程序设置许多其他参数，这些参数在上面的链接中进行了详细说明。

If you wish to use any of these parameters then refer to the [Service Configuration](https://docs.openstack.org/kolla-ansible/latest/admin/advanced-configuration.html#openstack-service-configuration-in-kolla) documentation for instructions using the INI update strategy.
如果要使用这些参数中的任何一个，请参阅服务配置文档，了解使用 INI 更新策略的说明。

The use of this backend requires that the `purestorage` SDK package is installed in the `cinder-volume` container. To do this follow the steps outlined in the [kolla image building guide](https://docs.openstack.org/kolla/latest/admin/image-building.html) particularly the `Package Customisation` and `Custom Repos` sections.
使用此后端需要在 `cinder-volume` 容器中安装 `purestorage` SDK 包。为此，请按照 kolla 图像构建指南中概述的步骤操作，尤其是 `Package Customisation` 和 `Custom Repos` 部分。

# Manila - Shared filesystems service 马尼拉 - 共享文件系统服务

​        version 版本              





## Overview 概述 ¶

Currently, Kolla can deploy following manila services:
目前，Kolla 可以部署以下马尼拉服务：

- manila-api 马尼拉-API
- manila-data 马尼拉数据
- manila-scheduler 马尼拉调度程序
- manila-share 马尼拉分享

The OpenStack Shared File Systems service (Manila) provides file storage to a virtual machine. The Shared File Systems service provides an infrastructure for managing and provisioning of file shares. The service also enables management of share types as well as share snapshots if a driver supports them.
OpenStack 共享文件系统服务（马尼拉）为虚拟机提供文件存储。共享文件系统服务提供用于管理和设置文件共享的基础结构。如果驱动程序支持共享类型和共享快照，则该服务还支持管理共享类型和共享快照。

## Important 重要 ¶

For simplicity, this guide describes configuring the Shared File Systems service to use the `generic` back end with the driver handles share server mode (DHSS) enabled that uses Compute (nova), Networking (neutron) and Block storage (cinder) services. Networking service configuration requires the capability of networks being attached to a public router in order to create shared networks.
为简单起见，本指南介绍如何将共享文件系统服务配置为使用启用了驱动程序句柄共享服务器模式 （DHSS） 的 `generic` 后端，该模式使用计算 （nova）、网络 （neutron） 和块存储 （cinder） 服务。网络服务配置需要将网络连接到公共路由器的功能，以便创建共享网络。

Before you proceed, ensure that Compute, Networking and Block storage services are properly working.
在继续操作之前，请确保计算、网络和块存储服务正常工作。

## Preparation and Deployment 准备和部署 ¶

Cinder is required, enable it in `/etc/kolla/globals.yml`:
Cinder 是必需的，请在以下位置 `/etc/kolla/globals.yml` 启用它：

```
enable_cinder: "yes"
```

Enable Manila and generic back end in `/etc/kolla/globals.yml`:
在以下位置 `/etc/kolla/globals.yml` 启用 Manila 和通用后端：

```
enable_manila: "yes"
enable_manila_backend_generic: "yes"
```

By default Manila uses instance flavor id 100 for its file systems. For Manila to work, either create a new nova flavor with id 100 (use *nova flavor-create*) or change *service_instance_flavor_id* to use one of the default nova flavor ids. Ex: *service_instance_flavor_id = 2* to use nova default flavor `m1.small`.
默认情况下，Manila 对其文件系统使用实例风格 ID 100。要使 Manila 正常工作，请创建 id 为 100 的新 nova 风格（使用 nova  flavor-create），或更改service_instance_flavor_id以使用默认的 nova 风格 id  之一。例如：service_instance_flavor_id = 2 使用 nova 默认风格 `m1.small` 。

Create or modify the file `/etc/kolla/config/manila-share.conf` and add the contents:
创建或修改文件 `/etc/kolla/config/manila-share.conf` 并添加内容：

```
[generic]
service_instance_flavor_id = 2
```

## Verify Operation 验证操作 ¶

Verify operation of the Shared File Systems service. List service components to verify successful launch of each process:
验证共享文件系统服务的操作。列出服务组件以验证每个进程是否成功启动：

```
manila service-list

+------------------+----------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |    Host        | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+----------------+------+---------+-------+----------------------------+-----------------+
| manila-scheduler | controller     | nova | enabled |   up  | 2014-10-18T01:30:54.000000 |       None      |
| manila-share     | share1@generic | nova | enabled |   up  | 2014-10-18T01:30:57.000000 |       None      |
+------------------+----------------+------+---------+-------+----------------------------+-----------------+
```

## Launch an Instance 启动实例 ¶

Before being able to create a share, the manila with the generic driver and the DHSS mode enabled requires the definition of at least an image, a network and a share-network for being used to create a share server. For that back end configuration, the share server is an instance where NFS/CIFS shares are served.
在能够创建共享之前，启用了通用驱动程序和 DHSS 模式的 manila 需要至少定义一个映像、一个网络和一个共享网络，以用于创建共享服务器。对于该后端配置，共享服务器是提供 NFS/CIFS 共享的实例。

## Determine the configuration of the share server 确定共享服务器的配置 ¶

Create a default share type before running manila-share service:
在运行 manila-share 服务之前创建默认共享类型：

```
manila type-create default_share_type True

+--------------------------------------+--------------------+------------+------------+-------------------------------------+-------------------------+
| ID                                   | Name               | Visibility | is_default | required_extra_specs                | optional_extra_specs    |
+--------------------------------------+--------------------+------------+------------+-------------------------------------+-------------------------+
| 8a35da28-0f74-490d-afff-23664ecd4f01 | default_share_type | public     | -          | driver_handles_share_servers : True | snapshot_support : True |
+--------------------------------------+--------------------+------------+------------+-------------------------------------+-------------------------+
```

Create a manila share server image to the Image service:
将 manila 共享服务器映像创建到映像服务：

```
wget https://tarballs.opendev.org/openstack/manila-image-elements/images/manila-service-image-master.qcow2
glance image-create --name "manila-service-image" \
  --file manila-service-image-master.qcow2 \
  --disk-format qcow2 --container-format bare \
  --visibility public --progress

[=============================>] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 48a08e746cf0986e2bc32040a9183445     |
| container_format | bare                                 |
| created_at       | 2016-01-26T19:52:24Z                 |
| disk_format      | qcow2                                |
| id               | 1fc7f29e-8fe6-44ef-9c3c-15217e83997c |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | manila-service-image                 |
| owner            | e2c965830ecc4162a002bf16ddc91ab7     |
| protected        | False                                |
| size             | 306577408                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2016-01-26T19:52:28Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+
```

List available networks to get id and subnets of the private network:
列出可用网络以获取专用网络的 ID 和子网：

```
+--------------------------------------+---------+----------------------------------------------------+
| id                                   | name    | subnets                                            |
+--------------------------------------+---------+----------------------------------------------------+
| 0e62efcd-8cee-46c7-b163-d8df05c3c5ad | public  | 5cc70da8-4ee7-4565-be53-b9c011fca011 10.3.31.0/24  |
| 7c6f9b37-76b4-463e-98d8-27e5686ed083 | private | 3482f524-8bff-4871-80d4-5774c2730728 172.16.1.0/24 |
+--------------------------------------+---------+----------------------------------------------------+
```

Create a shared network
创建共享网络

```
manila share-network-create --name demo-share-network1 \
  --neutron-net-id PRIVATE_NETWORK_ID \
  --neutron-subnet-id PRIVATE_NETWORK_SUBNET_ID

+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| name              | demo-share-network1                  |
| segmentation_id   | None                                 |
| created_at        | 2016-01-26T20:03:41.877838           |
| neutron_subnet_id | 3482f524-8bff-4871-80d4-5774c2730728 |
| updated_at        | None                                 |
| network_type      | None                                 |
| neutron_net_id    | 7c6f9b37-76b4-463e-98d8-27e5686ed083 |
| ip_version        | None                                 |
| nova_net_id       | None                                 |
| cidr              | None                                 |
| project_id        | e2c965830ecc4162a002bf16ddc91ab7     |
| id                | 58b2f0e6-5509-4830-af9c-97f525a31b14 |
| description       | None                                 |
+-------------------+--------------------------------------+
```

Create a flavor (**Required** if you not defined *manila_instance_flavor_id* in `/etc/kolla/config/manila-share.conf` file)
创建变种（如果未在 `/etc/kolla/config/manila-share.conf` 文件中定义manila_instance_flavor_id则为必填项）

```
nova flavor-create manila-service-flavor 100 128 0 1
```

## Create a share 创建共享 ¶

Create a NFS share using the share network:
使用共享网络创建 NFS 共享：

```
manila create NFS 1 --name demo-share1 --share-network demo-share-network1

+-----------------------------+--------------------------------------+
| Property                    | Value                                |
+-----------------------------+--------------------------------------+
| status                      | None                                 |
| share_type_name             | None                                 |
| description                 | None                                 |
| availability_zone           | None                                 |
| share_network_id            | None                                 |
| export_locations            | []                                   |
| host                        | None                                 |
| snapshot_id                 | None                                 |
| is_public                   | False                                |
| task_state                  | None                                 |
| snapshot_support            | True                                 |
| id                          | 016ca18f-bdd5-48e1-88c0-782e4c1aa28c |
| size                        | 1                                    |
| name                        | demo-share1                          |
| share_type                  | None                                 |
| created_at                  | 2016-01-26T20:08:50.502877           |
| export_location             | None                                 |
| share_proto                 | NFS                                  |
| consistency_group_id        | None                                 |
| source_cgsnapshot_member_id | None                                 |
| project_id                  | 48e8c35b2ac6495d86d4be61658975e7     |
| metadata                    | {}                                   |
+-----------------------------+--------------------------------------+
```

After some time, the share status should change from `creating` to `available`:
一段时间后，共享状态应从以下更改为 `creating` `available` ：

```
manila list

+--------------------------------------+-------------+------+-------------+-----------+-----------+--------------------------------------+-----------------------------+-------------------+
| ID                                   | Name        | Size | Share Proto | Status    | Is Public | Share Type Name                      | Host                        | Availability Zone |
+--------------------------------------+-------------+------+-------------+-----------+-----------+--------------------------------------+-----------------------------+-------------------+
| e1e06b14-ba17-48d4-9e0b-ca4d59823166 | demo-share1 | 1    | NFS         | available | False     | default_share_type                   | share1@generic#GENERIC      | nova              |
+--------------------------------------+-------------+------+-------------+-----------+-----------+--------------------------------------+-----------------------------+-------------------+
```

Configure user access to the new share before attempting to mount it via the network:
在尝试通过网络装载新共享之前，配置用户对新共享的访问：

```
manila access-allow demo-share1 ip INSTANCE_PRIVATE_NETWORK_IP
```

## Mount the share from an instance 从实例挂载共享 ¶

Get export location from share
从共享获取导出位置

```
manila show demo-share1

+-----------------------------+----------------------------------------------------------------------+
| Property                    | Value                                                                |
+-----------------------------+----------------------------------------------------------------------+
| status                      | available                                                            |
| share_type_name             | default_share_type                                                   |
| description                 | None                                                                 |
| availability_zone           | nova                                                                 |
| share_network_id            | fa07a8c3-598d-47b5-8ae2-120248ec837f                                 |
| export_locations            |                                                                      |
|                             | path = 10.254.0.3:/shares/share-422dc546-8f37-472b-ac3c-d23fe410d1b6 |
|                             | preferred = False                                                    |
|                             | is_admin_only = False                                                |
|                             | id = 5894734d-8d9a-49e4-b53e-7154c9ce0882                            |
|                             | share_instance_id = 422dc546-8f37-472b-ac3c-d23fe410d1b6             |
| share_server_id             | 4782feef-61c8-4ffb-8d95-69fbcc380a52                                 |
| host                        | share1@generic#GENERIC                                               |
| access_rules_status         | active                                                               |
| snapshot_id                 | None                                                                 |
| is_public                   | False                                                                |
| task_state                  | None                                                                 |
| snapshot_support            | True                                                                 |
| id                          | e1e06b14-ba17-48d4-9e0b-ca4d59823166                                 |
| size                        | 1                                                                    |
| name                        | demo-share1                                                          |
| share_type                  | 6e1e803f-1c37-4660-a65a-c1f2b54b6e17                                 |
| has_replicas                | False                                                                |
| replication_type            | None                                                                 |
| created_at                  | 2016-03-15T18:59:12.000000                                           |
| share_proto                 | NFS                                                                  |
| consistency_group_id        | None                                                                 |
| source_cgsnapshot_member_id | None                                                                 |
| project_id                  | 9dc02df0f2494286ba0252b3c81c01d0                                     |
| metadata                    | {}                                                                   |
+-----------------------------+----------------------------------------------------------------------+
```

Create a folder where the mount will be placed:
创建一个将放置挂载的文件夹：

```
mkdir ~/test_folder
```

Mount the NFS share in the instance using the export location of the share:
使用共享的导出位置在实例中挂载 NFS 共享：

```
mount -v 10.254.0.3:/shares/share-422dc546-8f37-472b-ac3c-d23fe410d1b6 ~/test_folder
```

## Share Migration 共享迁移 ¶

As administrator, you can migrate a share with its data from one location to another in a manner that is transparent to users and workloads. You can use manila client commands to complete a share migration.
作为管理员，您可以以对用户和工作负载透明的方式将包含其数据的共享从一个位置迁移到另一个位置。您可以使用 manila 客户端命令完成共享迁移。

For share migration, is needed modify `manila.conf` and set a ip in the same provider network for `data_node_access_ip`.
对于共享迁移，需要在同一提供商网络中修改 `manila.conf` 和设置 ip `data_node_access_ip` 。

Modify the file `/etc/kolla/config/manila.conf` and add the contents:
修改文件 `/etc/kolla/config/manila.conf` 并添加内容：

```
[DEFAULT]
data_node_access_ip = 10.10.10.199
```



 

Note 注意



Share migration requires have more than one back end configured. For details, see [Configure multiple back ends](https://docs.openstack.org/kolla-ansible/latest/reference/storage/manila-hnas-guide.html#hnas-configure-multiple-back-ends).
共享迁移需要配置多个后端。有关详细信息，请参阅配置多个后端。

Use the manila migration command, as shown in the following example:
使用 manila migration 命令，如以下示例所示：

```
manila migration-start --preserve-metadata True|False \
  --writable True|False --force_host_assisted_migration True|False \
  --new_share_type share_type --new_share_network share_network \
  shareID destinationHost
```

- `--force-host-copy`: Forces the generic host-based migration mechanism and bypasses any driver optimizations.
   `--force-host-copy` ：强制使用通用的基于主机的迁移机制，并绕过任何驱动程序优化。
- `destinationHost`: Is in this format `host#pool` which includes destination host and pool.
   `destinationHost` ：采用此格式 `host#pool` ，包括目标主机和池。
- `--writable` and `--preserve-metadata`: Are only for driver assisted.
   `--writable` 和 `--preserve-metadata` ： 仅适用于驾驶员协助。
- `--new_share_network`: Only if driver supports shared network.
   `--new_share_network` ：仅当驱动程序支持共享网络时。
- `--new_share_type`: Choose share type compatible with destinationHost.
   `--new_share_type` ：选择与 destinationHost 兼容的共享类型。

### Checking share migration progress 检查共享迁移进度 ¶

Use the **manila migration-get-progress shareID** command to check progress.
使用 manila migration-get-progress shareID 命令检查进度。

```
manila migration-get-progress demo-share1

+----------------+-----------------------+
| Property       | Value                 |
+----------------+-----------------------+
| task_state     | data_copying_starting |
| total_progress | 0                     |
+----------------+-----------------------+
manila migration-get-progress demo-share1
+----------------+-------------------------+
| Property       | Value                   |
+----------------+-------------------------+
| task_state     | data_copying_completing |
| total_progress | 100                     |
+----------------+-------------------------+
```

Use the **manila migration-complete shareID** command to complete share migration process.
使用 manila migration-complete shareID 命令完成共享迁移过程。

For more information about how to manage shares, see the [Manage shares](https://docs.openstack.org/manila/latest/user/create-and-manage-shares.html).
有关如何管理共享的详细信息，请参阅管理共享。

## GlusterFS

We have support for enabling Manila to provide users access to volumes from an external GlusterFS. For more details on the GlusterfsShareDriver, please see: https://docs.openstack.org/manila/latest/admin/glusterfs_driver.html
我们支持使 Manila 能够为用户提供从外部 GlusterFS 访问卷的权限。有关 GlusterfsShareDriver  的更多详细信息，请参阅：https://docs.openstack.org/manila/latest/admin/glusterfs_driver.html

Kolla-ansible supports using the GlusterFS shares with NFS. To enable this backend, add the following to `/etc/kolla/globals.yml`:
Kolla-ansible 支持将 GlusterFS 共享与 NFS 结合使用。若要启用此后端，请将以下内容添加到 `/etc/kolla/globals.yml` ：

```
enable_manila_backend_glusterfs_nfs: "yes"
```

### Layouts 布局 ¶

A layout is a strategy of allocating storage from GlusterFS backends for shares. Currently there are two layouts implemented:
布局是从 GlusterFS 后端为共享分配存储的策略。目前有两种布局实现：

### volume mapped layout 卷映射布局 ¶

You will also need to add the following configuration options to ensure the driver can connect to GlusterFS and exposes the correct subset of existing volumes in the system by adding the following in `/etc/kolla/globals.yml`:
您还需要添加以下配置选项，以确保驱动程序可以连接到 GlusterFS，并通过在以下内容中 `/etc/kolla/globals.yml` 添加以下内容来公开系统中现有卷的正确子集：

```
manila_glusterfs_servers:
  - glusterfs1.example.com
  - glusterfs2.example.com
manila_glusterfs_ssh_user: "root"
manila_glusterfs_ssh_password: "<glusterfs ssh password>"
manila_glusterfs_volume_pattern: "manila-share-volume-\\d+$"
```

The `manila_glusterfs_ssh_password` and `manila_glusterfs_ssh_user` configuration options are only required when the GlusterFS server runs remotely rather than on the system running the Manila share service.
仅当 GlusterFS 服务器远程运行时，而不是在运行 Manila 共享服务的系统上运行时， `manila_glusterfs_ssh_password` 才需要 and `manila_glusterfs_ssh_user` 配置选项。

### directory mapped layout 目录映射布局 ¶

You will also need to add the following configuration options to ensure the driver can connect to GlusterFS and exposes the correct subset of existing volumes in the system by adding the following in `/etc/kolla/globals.yml`:
您还需要添加以下配置选项，以确保驱动程序可以连接到 GlusterFS，并通过在以下内容中 `/etc/kolla/globals.yml` 添加以下内容来公开系统中现有卷的正确子集：

```
manila_glusterfs_share_layout: "layout_directory.GlusterfsDirectoryMappedLayout"
manila_glusterfs_target: "root@10.0.0.1:/volume"
manila_glusterfs_ssh_password: "<glusterfs ssh password>"
manila_glusterfs_mount_point_base: "$state_path/mnt"
```

- `manila_glusterfs_target`: If it’s of the format <username>@<glustervolserver>:/<glustervolid>, then we ssh to <username>@<glustervolserver> to execute gluster (<username> is supposed to have administrative privileges on <glustervolserver>).
   `manila_glusterfs_target` ：如果它的格式是 @：/，那么我们用 ssh 到 @ 来执行 gluster（ 应该对 具有管理权限）。
- `manila_glusterfs_ssh_password`: configuration options are only required when the GlusterFS server runs remotely rather than on the system running the Manila share service.
   `manila_glusterfs_ssh_password` ：仅当 GlusterFS 服务器远程运行时才需要配置选项，而不是在运行 Manila 共享服务的系统上运行。

# Hitachi NAS Platform File Services Driver for OpenStack 适用于 OpenStack 的 Hitachi NAS 平台文件服务驱动程序

​        version 版本              





## Overview 概述 ¶

The Hitachi NAS Platform File Services Driver for OpenStack provides NFS Shared File Systems to OpenStack.
适用于 OpenStack 的 Hitachi NAS 平台文件服务驱动程序为 OpenStack 提供 NFS 共享文件系统。

### Requirements 要求 ¶

- Hitachi NAS Platform Models 3080, 3090, 4040, 4060, 4080, and 4100.
  Hitachi NAS 平台型号 3080、3090、4040、4060、4080 和 4100。
- HNAS/SMU software version is 12.2 or higher.
  HNAS/SMU 软件版本为 12.2 或更高版本。
- HNAS configuration and management utilities to create a storage pool (span) and an EVS.
  HNAS配置和管理实用程序，用于创建存储池（span）和EVS。
  - GUI (SMU). 图形用户界面 （SMU）。
  - SSC CLI. SSC 命令行界面。

### Supported shared file systems and operations 支持的共享文件系统和操作 ¶

The driver supports CIFS and NFS shares.
驱动程序支持 CIFS 和 NFS 共享。

The following operations are supported:
支持以下操作：

- Create a share. 创建共享。
- Delete a share. 删除共享。
- Allow share access. 允许共享访问。
- Deny share access. 拒绝共享访问。
- Create a snapshot. 创建快照。
- Delete a snapshot. 删除快照。
- Create a share from a snapshot.
  从快照创建共享。
- Extend a share. 扩展共享。
- Shrink a share. 缩减份额。
- Manage a share. 管理共享。
- Unmanage a share. 取消管理共享。

## Preparation and Deployment 准备和部署 ¶



 

Note 注意



The manila-share node only requires the HNAS EVS data interface if you plan to use share migration.
如果您计划使用共享迁移，则 manila-share 节点仅需要 HNAS EVS 数据接口。



 

Important 重要



It is mandatory that HNAS management interface is reachable from the Shared File System node through the admin network, while the selected EVS data interface is reachable from OpenStack Cloud, such as through Neutron flat networking.
HNAS管理接口必须能够通过管理网络从共享文件系统节点访问，而所选的EVS数据接口可以从OpenStack云访问，例如通过Neutron扁平网络。

### Configuration on Kolla deployment Kolla 部署上的配置 ¶

Enable Shared File Systems service and HNAS driver in `/etc/kolla/globals.yml`
在 `/etc/kolla/globals.yml` 

```
enable_manila: "yes"
enable_manila_backend_hnas: "yes"
```

Configure the OpenStack networking so it can reach HNAS Management interface and HNAS EVS Data interface.
配置OpenStack网络，使其可以访问HNAS管理接口和HNAS云硬盘数据接口。

To configure two physical networks, physnet1 and physnet2, with ports eth1 and eth2 associated respectively:
要配置两个物理网络 physnet1 和 physnet2，并分别关联端口 eth1 和 eth2：

In `/etc/kolla/globals.yml` set: 在集合中 `/etc/kolla/globals.yml` ：

```
neutron_bridge_name: "br-ex,br-ex2"
neutron_external_interface: "eth1,eth2"
```



 

Note 注意



`eth1` is used to Neutron external interface and `eth2` is used to HNAS EVS data interface.
 `eth1` 用于Neutron外部接口， `eth2` 用于HNAS云端数据接口。

### HNAS back end configuration HNAS后端配置¶

In `/etc/kolla/globals.yml` uncomment and set:
在 `/etc/kolla/globals.yml` 取消注释并设置：

```
hnas_ip: "172.24.44.15"
hnas_user: "supervisor"
hnas_password: "supervisor"
hnas_evs_id: "1"
hnas_evs_ip: "10.0.1.20"
hnas_file_system_name: "FS-Manila"
```

### Configuration on HNAS HNAS上的配置 ¶

Create the data HNAS network in Kolla OpenStack:
在 Kolla OpenStack 中创建数据 HNAS 网络：

List the available tenants:
列出可用的租户：

```
openstack project list
```

Create a network to the given tenant (service), providing the tenant ID, a name for the network, the name of the physical network over which the virtual network is implemented, and the type of the physical mechanism by which the virtual network is implemented:
创建给定租户（服务）的网络，提供租户 ID、网络名称、实现虚拟网络的物理网络的名称以及实现虚拟网络的物理机制的类型：

```
neutron net-create --tenant-id <SERVICE_ID> hnas_network \
  --provider:physical_network=physnet2 --provider:network_type=flat
```

*Optional* - List available networks:
可选 - 列出可用网络：

```
neutron net-list
```

Create a subnet to the same tenant (service), the gateway IP of this subnet, a name for the subnet, the network ID created before, and the CIDR of subnet:
为同一租户（服务）创建子网、该子网的网关 IP、子网的名称、之前创建的网络 ID 以及子网的 CIDR：

```
neutron subnet-create --tenant-id <SERVICE_ID> --gateway <GATEWAY> \
  --name hnas_subnet <NETWORK_ID> <SUBNET_CIDR>
```

*Optional* - List available subnets:
可选 - 列出可用子网：

```
neutron subnet-list
```

Add the subnet interface to a router, providing the router ID and subnet ID created before:
将子网接口添加到路由器，提供之前创建的路由器 ID 和子网 ID：

```
neutron router-interface-add <ROUTER_ID> <SUBNET_ID>
```

Create a file system on HNAS. See the [Hitachi HNAS reference](http://www.hds.com/assets/pdf/hus-file-module-file-services-administration-guide.pdf).
在HNAS上创建文件系统。请参阅 Hitachi HNAS 参考。



 

Important 重要



Make sure that the filesystem is not created as a replication target. Refer official HNAS administration guide.
确保文件系统不是作为复制目标创建的。请参阅官方HNAS管理指南。

Prepare the HNAS EVS network.
准备HNAS云硬盘网络。

Create a route in HNAS to the tenant network:
在HNAS中创建到租户网络的路由：

```
console-context --evs <EVS_ID_IN_USE> route-net-add --gateway <FLAT_NETWORK_GATEWAY> \
  <TENANT_PRIVATE_NETWORK>
```



 

Important 重要



Make sure multi-tenancy is enabled and routes are configured per EVS.
确保已启用多租户，并按 EVS 配置路由。

```
console-context --evs 3 route-net-add --gateway 192.168.1.1 \
  10.0.0.0/24
```

## Create a share 创建共享 ¶

Create a default share type before running manila-share service:
在运行 manila-share 服务之前创建默认共享类型：

```
manila type-create default_share_hitachi False

+--------------------------------------+-----------------------+------------+------------+--------------------------------------+-------------------------+
| ID                                   | Name                  | visibility | is_default | required_extra_specs                 | optional_extra_specs    |
+--------------------------------------+-----------------------+------------+------------+--------------------------------------+-------------------------+
| 3e54c8a2-1e50-455e-89a0-96bb52876c35 | default_share_hitachi | public     | -          | driver_handles_share_servers : False | snapshot_support : True |
+--------------------------------------+-----------------------+------------+------------+--------------------------------------+-------------------------+
```

Create a NFS share using the HNAS back end:
使用 HNAS 后端创建 NFS 共享：

```
manila create NFS 1 \
  --name mysharehnas \
  --description "My Manila share" \
  --share-type default_share_hitachi
```

Verify Operation: 验证操作：

```
manila list

+--------------------------------------+----------------+------+-------------+-----------+-----------+-----------------------+-------------------------+-------------------+
| ID                                   | Name           | Size | Share Proto | Status    | Is Public | Share Type Name       | Host                    | Availability Zone |
+--------------------------------------+----------------+------+-------------+-----------+-----------+-----------------------+-------------------------+-------------------+
| 721c0a6d-eea6-41af-8c10-72cd98985203 | mysharehnas    | 1    | NFS         | available | False     | default_share_hitachi | control@hnas1#HNAS1     | nova              |
+--------------------------------------+----------------+------+-------------+-----------+-----------+-----------------------+-------------------------+-------------------+
manila show mysharehnas

+-----------------------------+-----------------------------------------------------------------+
| Property                    | Value                                                           |
+-----------------------------+-----------------------------------------------------------------+
| status                      | available                                                       |
| share_type_name             | default_share_hitachi                                           |
| description                 | My Manila share                                                 |
| availability_zone           | nova                                                            |
| share_network_id            | None                                                            |
| export_locations            |                                                                 |
|                             | path = 172.24.53.1:/shares/45ed6670-688b-4cf0-bfe7-34956648fb84 |
|                             | preferred = False                                               |
|                             | is_admin_only = False                                           |
|                             | id = e81e716f-f1bd-47b2-8a56-2c2f9e33a98e                       |
|                             | share_instance_id = 45ed6670-688b-4cf0-bfe7-34956648fb84        |
| share_server_id             | None                                                            |
| host                        | control@hnas1#HNAS1                                             |
| access_rules_status         | active                                                          |
| snapshot_id                 | None                                                            |
| is_public                   | False                                                           |
| task_state                  | None                                                            |
| snapshot_support            | True                                                            |
| id                          | 721c0a6d-eea6-41af-8c10-72cd98985203                            |
| size                        | 1                                                               |
| user_id                     | ba7f6d543713488786b4b8cb093e7873                                |
| name                        | mysharehnas                                                     |
| share_type                  | 3e54c8a2-1e50-455e-89a0-96bb52876c35                            |
| has_replicas                | False                                                           |
| replication_type            | None                                                            |
| created_at                  | 2016-10-14T14:50:47.000000                                      |
| share_proto                 | NFS                                                             |
| consistency_group_id        | None                                                            |
| source_cgsnapshot_member_id | None                                                            |
| project_id                  | c3810d8bcc3346d0bdc8100b09abbbf1                                |
| metadata                    | {}                                                              |
+-----------------------------+-----------------------------------------------------------------+
```



## Configure multiple back ends 配置多个后端 ¶

An administrator can configure an instance of Manila to provision shares from one or more back ends. Each back end leverages an instance of a vendor-specific implementation of the Manila driver API.
管理员可以将 Manila 实例配置为从一个或多个后端预配共享。每个后端都利用特定于供应商的 Manila 驱动程序 API 实现的实例。

The name of the back end is declared as a configuration option share_backend_name within a particular configuration stanza that contains the related configuration options for that back end.
后端的名称声明为配置选项，share_backend_name包含在包含该后端相关配置选项的特定配置节中。

So, in the case of an multiple back ends deployment, it is necessary to change the default share backends before deployment.
因此，在部署多个后端的情况下，有必要在部署之前更改默认共享后端。

Modify the file `/etc/kolla/config/manila.conf` and add the contents:
修改文件 `/etc/kolla/config/manila.conf` 并添加内容：

```
[DEFAULT]
enabled_share_backends = generic,hnas1,hnas2
```

Modify the file `/etc/kolla/config/manila-share.conf` and add the contents:
修改文件 `/etc/kolla/config/manila-share.conf` 并添加内容：

```
[generic]
share_driver = manila.share.drivers.generic.GenericShareDriver
interface_driver = manila.network.linux.interface.OVSInterfaceDriver
driver_handles_share_servers = True
service_instance_password = manila
service_instance_user = manila
service_image_name = manila-service-image
share_backend_name = GENERIC

[hnas1]
share_backend_name = HNAS1
share_driver = manila.share.drivers.hitachi.hnas.driver.HitachiHNASDriver
driver_handles_share_servers = False
hitachi_hnas_ip = <hnas_ip>
hitachi_hnas_user = <user>
hitachi_hnas_password = <password>
hitachi_hnas_evs_id = <evs_id>
hitachi_hnas_evs_ip = <evs_ip>
hitachi_hnas_file_system_name = FS-Manila1

[hnas2]
share_backend_name = HNAS2
share_driver = manila.share.drivers.hitachi.hnas.driver.HitachiHNASDriver
driver_handles_share_servers = False
hitachi_hnas_ip = <hnas_ip>
hitachi_hnas_user = <user>
hitachi_hnas_password = <password>
hitachi_hnas_evs_id = <evs_id>
hitachi_hnas_evs_ip = <evs_ip>
hitachi_hnas_file_system_name = FS-Manila2
```

For more information about how to manage shares, see the [Manage shares](https://docs.openstack.org/manila/latest/user/create-and-manage-shares.html).
有关如何管理共享的详细信息，请参阅管理共享。

For more information about how HNAS driver works, see [Hitachi NAS Platform File Services Driver for OpenStack](https://docs.openstack.org/manila/latest/admin/hitachi_hnas_driver.html).
有关 HNAS 驱动程序工作原理的更多信息，请参见适用于 OpenStack 的 Hitachi NAS 平台文件服务驱动程序。

# Swift - Object storage service Swift - 对象存储服务

​        version 版本              





## Overview 概述 ¶

Kolla can deploy a full working Swift setup in either a **all-in-one** or **multinode** setup.
Kolla 可以在一体式或多节点设置中部署完整的 Swift 设置。

## Networking 网络 ¶

The following interfaces are used by Swift:
Swift 使用以下接口：

- External API interface (`kolla_external_vip_interface`) 外部 API 接口 （ `kolla_external_vip_interface` ）

  This interface is used by users to access the Swift public API. 用户使用此接口访问 Swift 公共 API。

- Internal API interface (`api_interface`) 内部 API 接口 （ `api_interface` ）

  This interface is used by users to access the Swift internal API. It is also used by HAProxy to access the Swift proxy servers. 用户使用此接口来访问 Swift 内部 API。HAProxy 也使用它来访问 Swift 代理服务器。

- Swift Storage interface (`swift_storage_interface`) Swift 存储接口 （ `swift_storage_interface` ）

  This interface is used by the Swift proxy server to access the account, container and object servers. Swift 代理服务器使用此接口来访问帐户、容器和对象服务器。

- Swift replication interface (`swift_replication_interface`) Swift 复制接口 （ `swift_replication_interface` ）

  This interface is used for Swift storage replication traffic. This is optional as the default configuration uses the `swift_storage_interface` for replication traffic. 此接口用于 Swift 存储复制流量。这是可选的，因为默认配置使用 `swift_storage_interface` for 复制流量。

## Disks with a partition table (recommended) 带分区表的磁盘（推荐） ¶

Swift requires block devices to be available for storage. To prepare a disk for use as a Swift storage device, a special partition name and filesystem label need to be added.
Swift 要求块设备可用于存储。要准备用作 Swift 存储设备的磁盘，需要添加特殊的分区名称和文件系统标签。

The following should be done on each storage node, the example is shown for three disks:
应在每个存储节点上执行以下操作，该示例显示了三个磁盘：



 

Warning 警告



ALL DATA ON DISK will be LOST!
磁盘上的所有数据都将丢失！

```
index=0
for d in sdc sdd sde; do
    parted /dev/${d} -s -- mklabel gpt mkpart KOLLA_SWIFT_DATA 1 -1
    sudo mkfs.xfs -f -L d${index} /dev/${d}1
    (( index++ ))
done
```

For evaluation, loopback devices can be used in lieu of real disks:
对于评估，可以使用环回设备代替真实磁盘：

```
index=0
for d in sdc sdd sde; do
    free_device=$(losetup -f)
    fallocate -l 1G /tmp/$d
    losetup $free_device /tmp/$d
    parted $free_device -s -- mklabel gpt mkpart KOLLA_SWIFT_DATA 1 -1
    sudo mkfs.xfs -f -L d${index} ${free_device}p1
    (( index++ ))
done
```

## Disks without a partition table 没有分区表的磁盘 ¶

Kolla also supports unpartitioned disk (filesystem on `/dev/sdc` instead of `/dev/sdc1`) detection purely based on filesystem label. This is generally not a recommended practice but can be helpful for Kolla to take over Swift deployment already using disk like this.
Kolla 还支持完全基于文件系统标签的未分区磁盘（文件系统打开 `/dev/sdc` 而不是 `/dev/sdc1` ）检测。这通常不是推荐的做法，但对于 Kolla 接管已经使用此类磁盘的 Swift 部署很有帮助。

Given hard disks with labels swd1, swd2, swd3, use the following settings in `ansible/roles/swift/defaults/main.yml`.
给定标签为 swd1、swd2、swd3 的硬盘，请使用 . `ansible/roles/swift/defaults/main.yml` 

```
swift_devices_match_mode: "prefix"
swift_devices_name: "swd"
```

## Rings 戒指 ¶

Before running Swift we need to generate **rings**, which are binary compressed files that at a high level let the various Swift services know where data is in the cluster. We hope to automate this process in a future release.
在运行 Swift 之前，我们需要生成环，这些环是二进制压缩文件，在高层次上让各种 Swift 服务知道数据在集群中的位置。我们希望在将来的版本中自动执行此过程。

The following example commands should be run from the `operator` node to generate rings for a demo setup. The commands work with **disks with partition table** example listed above. Please modify accordingly if your setup is different.
应从 `operator` 节点运行以下示例命令，以生成演示设置的环。这些命令适用于上面列出的具有分区表示例的磁盘。如果您的设置不同，请进行相应的修改。

If using a separate replication network it is necessary to add the replication network IP addresses to the rings. See the [Swift documentation](https://docs.openstack.org/swift/latest/replication_network.html#dedicated-replication-network) for details on how to do that.
如果使用单独的复制网络，则需要将复制网络 IP 地址添加到环中。请参阅 Swift 文档，了解如何执行此操作的详细信息。

### Prepare for Rings generating 准备 Rings 生成 ¶

To prepare for Swift Rings generating, run the following commands to initialize the environment variable and create `/etc/kolla/config/swift` directory:
要准备生成 Swift Rings，请执行以下命令初始化环境变量并创建 `/etc/kolla/config/swift` 目录：

```
STORAGE_NODES=(192.168.0.2 192.168.0.3 192.168.0.4)
KOLLA_SWIFT_BASE_IMAGE="kolla/centos-source-swift-base:4.0.0"
mkdir -p /etc/kolla/config/swift
```

### Generate Object Ring 生成对象环 ¶

To generate Swift object ring, run the following commands:
要生成 Swift 对象环，请运行以下命令：

```
docker run \
  --rm \
  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
  swift-ring-builder \
    /etc/kolla/config/swift/object.builder create 10 3 1

for node in ${STORAGE_NODES[@]}; do
    for i in {0..2}; do
      docker run \
        --rm \
        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
        swift-ring-builder \
          /etc/kolla/config/swift/object.builder add r1z1-${node}:6000/d${i} 1;
    done
done
```

### Generate Account Ring 生成账号环 ¶

To generate Swift account ring, run the following commands:
要生成 Swift 帐户环，请运行以下命令：

```
docker run \
  --rm \
  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
  swift-ring-builder \
    /etc/kolla/config/swift/account.builder create 10 3 1

for node in ${STORAGE_NODES[@]}; do
    for i in {0..2}; do
      docker run \
        --rm \
        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
        swift-ring-builder \
          /etc/kolla/config/swift/account.builder add r1z1-${node}:6001/d${i} 1;
    done
done
```

### Generate Container Ring 生成容器环 ¶

To generate Swift container ring, run the following commands:
若要生成 Swift 容器环，请运行以下命令：

```
docker run \
  --rm \
  -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
  swift-ring-builder \
    /etc/kolla/config/swift/container.builder create 10 3 1

for node in ${STORAGE_NODES[@]}; do
    for i in {0..2}; do
      docker run \
        --rm \
        -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
        swift-ring-builder \
          /etc/kolla/config/swift/container.builder add r1z1-${node}:6002/d${i} 1;
    done
done
```

### Rebalance 再平衡 ¶

To rebalance the ring files:
要重新平衡环形文件：

```
for ring in object account container; do
  docker run \
    --rm \
    -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
KOLLA_SWIFT_BASE_IMAGE \
    swift-ring-builder \
      /etc/kolla/config/swift/${ring}.builder rebalance;
done
```

For more information, see [the Swift documentation](https://docs.openstack.org/swift/latest/install/initial-rings.html).
有关更多信息，请参阅 Swift 文档。

## Deploying 部署 ¶

Enable Swift in `/etc/kolla/globals.yml`:
在以下位置 `/etc/kolla/globals.yml` 启用 Swift：

```
enable_swift : "yes"
```

If you are to deploy multiple policies, override the variable `swift_extra_ring_files` with the list of your custom ring files, .builder and .ring.gz all together. This will append them to the list of default rings.
如果要部署多个策略，请 `swift_extra_ring_files` 使用自定义环文件、.builder 和 .ring.gz 的列表覆盖该变量。这会将它们附加到默认环列表中。

```
swift_extra_ring_files:
   - object-1.builder
   - object-1.ring.gz
```

Once the rings are in place, deploying Swift is the same as any other Kolla Ansible service:
一旦环就位，部署 Swift 与任何其他 Kolla Ansible 服务相同：

```
kolla-ansible deploy -i <path/to/inventory-file>
```

## Verification 验证 ¶

A very basic smoke test:
一个非常基本的烟雾测试：

```
openstack container create mycontainer

+---------------------------------------+--------------+------------------------------------+
| account                               | container    | x-trans-id                         |
+---------------------------------------+--------------+------------------------------------+
| AUTH_7b938156dba44de7891f311c751f91d8 | mycontainer  | txb7f05fa81f244117ac1b7-005a0e7803 |
+---------------------------------------+--------------+------------------------------------+
openstack object create mycontainer README.rst

+---------------+--------------+----------------------------------+
| object        | container    | etag                             |
+---------------+--------------+----------------------------------+
| README.rst    | mycontainer  | 2634ecee0b9a52ba403a503cc7d8e988 |
+---------------+--------------+----------------------------------+
openstack container show mycontainer

+--------------+---------------------------------------+
| Field        | Value                                 |
+--------------+---------------------------------------+
| account      | AUTH_7b938156dba44de7891f311c751f91d8 |
| bytes_used   | 6684                                  |
| container    | mycontainer                           |
| object_count | 1                                     |
+--------------+---------------------------------------+
openstack object store account show

+------------+---------------------------------------+
| Field      | Value                                 |
+------------+---------------------------------------+
| Account    | AUTH_7b938156dba44de7891f311c751f91d8 |
| Bytes      | 6684                                  |
| Containers | 1                                     |
| Objects    | 1                                     |
+------------+---------------------------------------+
```

## S3 API

The Swift S3 API can be enabled by setting `enable_swift_s3api` to `true` in `globals.yml`. It is disabled by default. In order to use this API it is necessary to obtain EC2 credentials from Keystone. See the [the Swift documentation](https://docs.openstack.org/swift/latest/admin/middleware.html#module-swift.common.middleware.s3api.s3api) for details.
可以通过 `true` 将 设置为 来 `globals.yml` 启用 `enable_swift_s3api` Swift S3 API。默认情况下，它处于禁用状态。要使用此 API，必须从 Keystone 获取 EC2 凭证。有关详细信息，请参阅 Swift 文档。

## Swift Recon 迅捷侦察 ¶

Enable Swift Recon in `/etc/kolla/globals.yml`:
在以下位置 `/etc/kolla/globals.yml` 启用 Swift Recon：

```
enable_swift_recon : "yes"
```

The Swift role in Kolla Ansible is still using the old role format. Unlike many other Kolla Ansible roles, it won’t automatically add the new volume to the containers in existing deployments when running kolla-ansible reconfigure. Instead we must use the kolla-ansible upgrade command, which will remove the existing containers and then put them back again.
Kolla Ansible 中的 Swift 角色仍在使用旧的角色格式。与许多其他 Kolla Ansible 角色不同，在运行  kolla-ansible reconfigure 时，它不会自动将新卷添加到现有部署中的容器中。相反，我们必须使用 kolla-ansible upgrade 命令，该命令将删除现有容器，然后将它们放回原处。

Example usage: 用法示例：

```
sudo docker exec swift_object_server swift-recon --all`
```

For more information, see [the Swift documentation](https://docs.openstack.org/swift/latest/admin/objectstorage-monitoring.html).
有关更多信息，请参阅 Swift 文档。