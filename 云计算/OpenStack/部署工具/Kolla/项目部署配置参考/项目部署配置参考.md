# 项目部署配置参考

## Compute 计算

​        version 版本              



This section describes configuring nova hypervisors and compute services.
本节介绍如何配置 nova 虚拟机管理程序和计算服务。

- [Libvirt - Nova Virtualisation Driver
  Libvirt - Nova 虚拟化驱动程序](https://docs.openstack.org/kolla-ansible/latest/reference/compute/libvirt-guide.html)
- [Masakari - Virtual Machines High Availability
  Masakari - 虚拟机高可用性](https://docs.openstack.org/kolla-ansible/latest/reference/compute/masakari-guide.html)
- [Nova Cells Nova 细胞](https://docs.openstack.org/kolla-ansible/latest/reference/compute/nova-cells-guide.html)
- [Nova Fake Driver 新星假司机](https://docs.openstack.org/kolla-ansible/latest/reference/compute/nova-fake-driver.html)
- [Nova - Compute Service
  Nova - 计算服务](https://docs.openstack.org/kolla-ansible/latest/reference/compute/nova-guide.html)
- [VMware - Nova Virtualisation Driver
  VMware - Nova 虚拟化驱动程序](https://docs.openstack.org/kolla-ansible/latest/reference/compute/vmware-guide.html)
- [Zun - Container service
  Zun - 容器服务](https://docs.openstack.org/kolla-ansible/latest/reference/compute/zun-guide.html)

# Libvirt - Nova Virtualisation Driver Libvirt - Nova 虚拟化驱动程序

​        version 版本              



## Overview 概述 ¶

Libvirt is the most commonly used virtualisation driver in OpenStack. It uses libvirt, backed by QEMU and when available, KVM. Libvirt is executed in the `nova_libvirt` container, or as a daemon running on the host.
Libvirt 是 OpenStack 中最常用的虚拟化驱动程序。它使用 libvirt，由 QEMU 和 KVM 提供支持（如果可用）。Libvirt 在 `nova_libvirt` 容器中执行，或作为在主机上运行的守护程序执行。

## Hardware Virtualisation 硬件虚拟化 ¶

Two values are supported for `nova_compute_virt_type` with libvirt - `kvm` and `qemu`, with `kvm` being the default.
libvirt 支持两个值 - `kvm` 和 `qemu` ， `kvm` 作为默认值 `nova_compute_virt_type` 。

For optimal performance, `kvm` is preferable, since many aspects of virtualisation can be offloaded to hardware.  If it is not possible to enable hardware virtualisation (e.g. Virtualisation Technology (VT) BIOS configuration on Intel systems), `qemu` may be used to provide less performant software-emulated virtualisation.
为了获得最佳性能， `kvm` 这是可取的，因为虚拟化的许多方面可以卸载到硬件上。如果无法启用硬件虚拟化（例如，英特尔系统上的虚拟化技术 （VT） BIOS 配置）， `qemu` 则可用于提供性能较低的软件模拟虚拟化。

## SASL Authentication SASL 身份验证 ¶

The default configuration of Kolla Ansible is to run libvirt over TCP, authenticated with SASL. This should not be considered as providing a secure, encrypted channel, since the username/password SASL mechanisms available for TCP are no longer considered cryptographically secure. However, it does at least provide some authentication for the libvirt API. For a more secure encrypted channel, use [libvirt TLS](https://docs.openstack.org/kolla-ansible/latest/reference/compute/libvirt-guide.html#libvirt-tls).
Kolla Ansible 的默认配置是通过 TCP 运行 libvirt，并通过 SASL 进行身份验证。这不应被视为提供安全的加密通道，因为可用于  TCP 的用户名/密码 SASL 机制不再被视为加密安全。但是，它至少为 libvirt API  提供了一些身份验证。对于更安全的加密通道，请使用 libvirt TLS。

SASL is enabled according to the `libvirt_enable_sasl` flag, which defaults to `true`.
根据 `libvirt_enable_sasl` 标志启用 SASL，该标志默认为 `true` 。

The username is configured via `libvirt_sasl_authname`, and defaults to `nova`. The password is configured via `libvirt_sasl_password`, and is generated with other passwords using `kolla-mergepwd` and `kolla-genpwd` and stored in `passwords.yml`.
用户名通过 `libvirt_sasl_authname` 配置，默认为 `nova` 。密码通过 `libvirt_sasl_password` 配置，并使用 `kolla-mergepwd` 和 `kolla-genpwd` 与其他密码一起生成，并存储在 `passwords.yml` 中。

The list of enabled authentication mechanisms is configured via `libvirt_sasl_mech_list`, and defaults to `["SCRAM-SHA-256"]` if libvirt TLS is enabled, or `["DIGEST-MD5"]` otherwise.
已启用的身份验证机制列表通过 `libvirt_sasl_mech_list` 进行配置， `["SCRAM-SHA-256"]` 默认为是否启用了 libvirt TLS，否则 `["DIGEST-MD5"]` 。

## Host vs containerised libvirt 主机与容器化 libvirt ¶

By default, Kolla Ansible deploys libvirt in a `nova_libvirt` container. In some cases it may be preferable to run libvirt as a daemon on the compute hosts instead.
默认情况下，Kolla Ansible 在 `nova_libvirt` 容器中部署 libvirt。在某些情况下，最好在计算主机上将 libvirt 作为守护程序运行。

Kolla Ansible does not currently support deploying and configuring libvirt as a host daemon. However, since the Yoga release, if a libvirt daemon has already been set up, then Kolla Ansible may be configured to use it. This may be achieved by setting `enable_nova_libvirt_container` to `false`.
Kolla Ansible 目前不支持将 libvirt 部署和配置为主机守护程序。但是，自 Yoga 版本以来，如果已经设置了 libvirt 守护程序，则 Kolla Ansible 可能会配置为使用它。这可以通过设置为 `enable_nova_libvirt_container` `false` 来实现。

When the firewall driver is set to `openvswitch`, libvirt will plug VMs directly into the integration bridge, `br-int`. To do this it uses the `ovs-vsctl` utility. The search path for this binary is controlled by the `$PATH` environment variable (as seen by the libvirt process). There are a few options to ensure that this binary can be found:
当防火墙驱动程序设置为 `openvswitch` 时，libvirt 会将 VM 直接插入集成网桥 `br-int` 。为此，它使用实用 `ovs-vsctl` 程序。此二进制文件的搜索路径由 `$PATH` 环境变量控制（如 libvirt 进程所示）。有几个选项可以确保可以找到此二进制文件：

- Set `openvswitch_ovs_vsctl_wrapper_enabled` to `True`. This will install a wrapper script to the path: `/usr/bin/ovs-vsctl` that will execute `ovs-vsctl` in the context of the `openvswitch_vswitchd` container. This option is useful if you do not have openvswitch installed on the host. It also has the advantage that the `ovs-vsctl` utility will match the version of the server.
  设置为 `openvswitch_ovs_vsctl_wrapper_enabled` `True` 。这会将包装脚本安装到路径： `/usr/bin/ovs-vsctl` ，该脚本将在 `openvswitch_vswitchd` 容器的上下文中执行 `ovs-vsctl` 。如果主机上未安装 openvswitch，则此选项非常有用。它还具有实用 `ovs-vsctl` 程序将与服务器版本匹配的优点。
- Install openvswitch on the hypervisor. Kolla mounts `/run/openvswitch` from the host into the `openvswitch_vswitchd` container. This means that socket is in the location `ovs-vsctl` expects with its default options.
  在虚拟机管理程序上安装 openvswitch。Kolla `/run/openvswitch` 从主机挂载到容器中 `openvswitch_vswitchd` 。这意味着套接字位于其默认选项所需的位置 `ovs-vsctl` 。

### Migration from container to host 从容器迁移到主机 ¶

The `kolla-ansible nova-libvirt-cleanup` command may be used to clean up the `nova_libvirt` container and related items on hosts, once it has been disabled. This should be run after the compute service has been disabled, and all active VMs have been migrated away from the host.
禁用该 `kolla-ansible nova-libvirt-cleanup` 命令后，该命令可用于清理主机上的 `nova_libvirt` 容器和相关项。这应该在禁用计算服务并且所有活动 VM 都已从主机迁移出去后运行。

By default, the command will fail if there are any VMs running on the host. If you are sure that it is safe to clean up the `nova_libvirt` container with running VMs, setting `nova_libvirt_cleanup_running_vms_fatal` to `false` will allow the command to proceed.
默认情况下，如果主机上运行任何 VM，则该命令将失败。如果确定使用正在运行的 VM 清理 `nova_libvirt` 容器是安全的，则设置为 `nova_libvirt_cleanup_running_vms_fatal`  `false` 将允许命令继续。

The `nova_libvirt` container has several associated Docker volumes: `libvirtd`, `nova_libvirt_qemu` and `nova_libvirt_secrets`. By default, these volumes are not cleaned up. If you are sure that the data in these volumes can be safely removed, setting `nova_libvirt_cleanup_remove_volumes` to `true` will cause the Docker volumes to be removed.
容器 `nova_libvirt` 具有多个关联的 Docker 卷： `libvirtd` 和 `nova_libvirt_qemu` `nova_libvirt_secrets` 。默认情况下，不会清理这些卷。如果确定可以安全地删除这些卷中的数据，则设置为 `nova_libvirt_cleanup_remove_volumes`  `true` 将导致删除 Docker 卷。

A future extension could support migration of existing VMs, but this is currently out of scope.
未来的扩展可以支持现有 VM 的迁移，但这目前超出了范围。



## Libvirt TLS

The default configuration of Kolla Ansible is to run libvirt over TCP, with SASL authentication. As long as one takes steps to protect who can access the network this works well. However, in a less trusted environment one may want to use encryption when accessing the libvirt API. To do this we can enable TLS for libvirt and make nova use it. Mutual TLS is configured, providing authentication of clients via certificates. SASL authentication provides a further level of security.
Kolla Ansible 的默认配置是通过 TCP 运行 libvirt，并进行 SASL  身份验证。只要采取措施保护谁可以访问网络，这就可以很好地工作。但是，在不太受信任的环境中，人们可能希望在访问 libvirt API  时使用加密。为此，我们可以为 libvirt 启用 TLS 并让 nova 使用它。配置了双向 TLS，通过证书提供客户端身份验证。SASL  身份验证提供了更高的安全性。

### Using libvirt TLS 使用 libvirt TLS ¶

Libvirt TLS can be enabled in Kolla Ansible by setting the following option in `/etc/kolla/globals.yml`:
Libvirt TLS 可以通过在 Kolla Ansible 中设置以下选项来 `/etc/kolla/globals.yml` 启用：

```
libvirt_tls: "yes"
```

Creation of production-ready TLS certificates is currently out-of-scope for Kolla Ansible.  You will need to either use an existing Internal CA or you will need to generate your own offline CA. For the TLS communication to work correctly you will have to supply Kolla Ansible the following pieces of information:
创建生产就绪的 TLS 证书目前超出了 Kolla Ansible 的范围。您需要使用现有的内部 CA，或者需要生成自己的脱机 CA。要使 TLS 通信正常工作，您必须向 Kolla Ansible 提供以下信息：

- cacert.pem cacert.pem（英语：cacert.pem）
  - This is the CA’s public certificate that all of the client and server certificates are signed with. Libvirt and nova-compute will need this so they can verify that all the certificates being used were signed by the CA and should be trusted.
    这是 CA 的公共证书，所有客户端和服务器证书都使用该证书进行签名。Libvirt 和 nova-compute 将需要它，以便他们可以验证正在使用的所有证书是否都由 CA 签名并且应该受信任。
- serverkey.pem (not used when using a host libvirt daemon)
  serverkey.pem（使用主机 libvirt 守护程序时不使用）
  - This is the private key for the server, and is no different than the private key of a TLS certificate. It should be carefully protected, just like the private key of a TLS certificate.
    这是服务器的私钥，与 TLS 证书的私钥没有什么不同。它应该受到仔细保护，就像TLS证书的私钥一样。
- servercert.pem (not used when using a host libvirt daemon)
  servercert.pem（使用主机 libvirt 守护程序时不使用）
  - This is the public certificate for the server. Libvirt will present this certificate to any connection made to the TLS port. This is no different than the public certificate part of a standard TLS certificate/key bundle.
    这是服务器的公共证书。Libvirt 会向与 TLS 端口建立的任何连接提供此证书。这与标准 TLS 证书/密钥捆绑包的公共证书部分没有什么不同。
- clientkey.pem 客户端密钥.pem
  - This is the client private key, which nova-compute/libvirt will use when it is connecting to libvirt. Think of this as an SSH private key and protect it in a similar manner.
    这是客户端私钥，nova-compute/libvirt 在连接到 libvirt 时将使用它。将其视为 SSH 私钥，并以类似的方式保护它。
- clientcert.pem 客户端证书.pem
  - This is the client certificate that nova-compute/libvirt will present when it is connecting to libvirt. Think of this as the public side of an SSH key.
    这是 nova-compute/libvirt 在连接到 libvirt 时将显示的客户端证书。将其视为 SSH 密钥的公共端。

Kolla Ansible will search for these files for each compute node in the following locations and order on the host where Kolla Ansible is executed:
Kolla Ansible 将在以下位置为每个计算节点搜索这些文件，并在执行 Kolla Ansible 的主机上按顺序排列：

- `/etc/kolla/config/nova/nova-libvirt/<hostname>/`
- `/etc/kolla/config/nova/nova-libvirt/`

In most cases you will want to have a unique set of server and client certificates and keys per hypervisor and with a common CA certificate. In this case you would place each of the server/client certificate and key PEM files under `/etc/kolla/config/nova/nova-libvirt/<hostname>/` and the CA certificate under `/etc/kolla/config/nova/nova-libvirt/`.
在大多数情况下，您需要为每个虚拟机管理程序提供一组唯一的服务器和客户端证书和密钥，并具有通用的 CA 证书。在这种情况下，您需要将每个服务器/客户端证书和密钥 PEM 文件放在 下 `/etc/kolla/config/nova/nova-libvirt/<hostname>/` ，将 CA 证书放在 `/etc/kolla/config/nova/nova-libvirt/` 下。

However, it is possible to make use of wildcard server certificate and a single client certificate that is shared by all servers. This will allow you to generate a single client certificate and a single server certificate that is shared across every hypervisor. In this case you would store everything under `/etc/kolla/config/nova/nova-libvirt/`.
但是，可以使用通配符服务器证书和由所有服务器共享的单个客户端证书。这将允许您生成单个客户端证书和单个服务器证书，这些证书在每个虚拟机管理程序之间共享。在这种情况下，您将所有内容存储在 `/etc/kolla/config/nova/nova-libvirt/` .

### Externally managed certificates 外部管理的证书 ¶

One more option for deployers who already have automation to get TLS certs onto servers is to disable certificate management under `/etc/kolla/globals.yaml`:
对于已经具有自动化功能的部署人员来说，将 TLS 证书获取到服务器上的另一种选择是在以下情况下禁用证书管理 `/etc/kolla/globals.yaml` ：

```
libvirt_tls_manage_certs: "no"
```

With this option disabled Kolla Ansible will simply assume that certificates and keys are already installed in their correct locations. Deployers will be responsible for making sure that the TLS certificates/keys get placed in to the correct container configuration directories on the servers so that they can get copied into the nova-compute and nova-libvirt containers. With this option disabled you will also be responsible for restarting the nova-compute and nova-libvirt containers when the certs are updated, as kolla-ansible will not be able to tell when the files have changed.
禁用此选项后，Kolla Ansible 将仅假定证书和密钥已安装在其正确的位置。部署人员将负责确保将 TLS  证书/密钥放置在服务器上正确的容器配置目录中，以便将它们复制到 nova-compute 和 nova-libvirt  容器中。禁用此选项后，您还将负责在证书更新时重新启动 nova-compute 和 nova-libvirt 容器，因为  kolla-ansible 将无法判断文件何时更改。

### Generating certificates for test and development 生成用于测试和开发的证书 ¶

Since the Yoga release, the `kolla-ansible certificates` command generates certificates for libvirt TLS. A single key and certificate is used for all hosts, with a Subject Alternative Name (SAN) entry for each compute host hostname.
自 Yoga 版本以来，该 `kolla-ansible certificates` 命令为 libvirt TLS 生成证书。所有主机都使用单个密钥和证书，每个计算主机主机名都有一个使用者备用名称 （SAN） 条目。

# Masakari - Virtual Machines High Availability Masakari - 虚拟机高可用性

​        version 版本              





## Overview 概述 ¶

Masakari provides Instances High Availability Service for OpenStack clouds by automatically recovering failed Instances. Currently, Masakari can recover KVM-based Virtual Machine(VM)s from failure events such as VM process down, provisioning process down, and nova-compute host failure. Masakari also provides an API service to manage and control the automated rescue mechanism.
Masakari 通过自动恢复故障实例来为 OpenStack 云提供实例高可用性服务。目前，Masakari 可以从 VM 进程关闭、配置进程关闭和  nova-compute 主机故障等故障事件中恢复基于 KVM 的虚拟机 （VM）。Masakari 还提供 API  服务来管理和控制自动救援机制。

Kolla deploys Masakari API, Masakari Engine and Masakari Monitor containers which are the main Masakari components only if `enable_masakari` is set in `/etc/kolla/globals.yml`. By default, both the Masakari Host Monitor and Masakari Instance Monitor containers are enabled. The deployment of each type of monitors can be controlled individually via `enable_masakari_instancemonitor` and `enable_masakari_hostmonitor`.
Kolla 部署 Masakari API、Masakari Engine 和 Masakari Monitor 容器，这些容器是 Masakari 的主要组件，前提是 `enable_masakari` 在 `/etc/kolla/globals.yml` 中设置。默认情况下，Masakari Host Monitor 和 Masakari Instance Monitor 容器都处于启用状态。每种类型的监视器的部署都可以通过 `enable_masakari_instancemonitor` 和 `enable_masakari_hostmonitor` 单独控制。



 

Note 注意



Support for deploying Masakari has been deprecated in the 2023.2 (Bobcat) release due to failures in the CI and lack of contributors working on fixing this.
在 2023.2 （Bobcat） 版本中，由于 CI 中的故障以及缺乏致力于修复此问题的贡献者，已弃用对部署 Masakari 的支持。

# Nova Cells 新星细胞

​        version 版本              



## Overview 概述 ¶

Nova cells V2 is a feature that allows Nova deployments to be scaled out to a larger size than would otherwise be possible. This is achieved through sharding of the compute nodes into pools known as *cells*, with each cell having a separate message queue and database.
Nova cells V2 是一项功能，允许将 Nova 部署扩展到比其他方式更大的规模。这是通过将计算节点分片到称为单元的池中来实现的，每个单元都有一个单独的消息队列和数据库。

Further information on cells can be found in the Nova documentation [here](https://docs.openstack.org/nova/latest/user/cells.html) and [here](https://docs.openstack.org/nova/latest/user/cellsv2-layout.html). This document assumes the reader is familiar with the concepts of cells.
有关细胞的更多信息，请参见 Nova 文档 此处 和 此处。本文档假定读者熟悉单元格的概念。

## Cells: deployment perspective 单元格：部署视角 ¶

From a deployment perspective, nova cell support involves separating the Nova services into two sets - global services and per-cell services.
从部署的角度来看，nova cell 支持涉及将 Nova 服务分为两组 - 全局服务和每小区服务。

Global services: 全球服务：

- `nova-api`
- `nova-scheduler`
- `nova-super-conductor` (in multi-cell mode)
   `nova-super-conductor` （在多单元格模式下）

Per-cell control services:
每单元控制服务：

- `nova-compute-ironic` (for Ironic cells)
   `nova-compute-ironic` （用于讽刺细胞）
- `nova-conductor`
- `nova-novncproxy`
- `nova-serialproxy`
- `nova-spicehtml5proxy`

Per-cell compute services:
每单元计算服务：

- `nova-compute`
- `nova-libvirt`
- `nova-ssh`

Another consideration is the database and message queue clusters that the cells depend on. This will be discussed later.
另一个考虑因素是单元所依赖的数据库和消息队列集群。这将在后面讨论。

### Service placement 服务放置 ¶

There are a number of ways to place services in a multi-cell environment.
有多种方法可以将服务放置在多单元环境中。

#### Single cell topology 单单元拓扑 ¶

The single cell topology is used by default, and is limited to a single cell:
默认情况下使用单单元拓扑，并且仅限于单个单元：

```
        +----------------+
        |                ++
        |                |-+
        |   controllers  |-|
        |                |-|
        |                |-|
        +------------------|
         +-----------------|
          +----------------+

+--------------+     +--------------+
|              |     |              |
|   cell 1     |     |   cell 1     |
|   compute 1  |     |   compute 2  |
|              |     |              |
+--------------+     +--------------+
```

All control services run on the controllers, and there is no superconductor.
所有控制服务都在控制器上运行，并且没有超导体。

#### Dedicated cell controller topology 专用单元控制器拓扑 ¶

In this topology, each cell has a dedicated group of controllers to run cell control services. The following diagram shows the topology for a cloud with two cells:
在此拓扑中，每个单元都有一组专用的控制器来运行单元控制服务。下图显示了具有两个单元格的云的拓扑：

```
                                +----------------+
                                |                ++
                                |                |-+
                                |   controllers  |-|
                                |                |-|
                                |                |-|
                                +------------------|
                                 +-----------------|
                                  +----------------+

                   +----------------+        +----------------+
                   |                ++       |                ++
                   |   cell 1       |-+      |   cell 2       |-+
                   |   controllers  |-|      |   controllers  |-|
                   |                |-|      |                |-|
                   +------------------|      +------------------|
                    +-----------------|       +-----------------|
                     +----------------+        +----------------+

+--------------+     +--------------+        +--------------+     +--------------+
|              |     |              |        |              |     |              |
|   cell 1     |     |   cell 1     |        |   cell 2     |     |   cell 2     |
|   compute 1  |     |   compute 2  |        |   compute 1  |     |   compute 2  |
|              |     |              |        |              |     |              |
+--------------+     +--------------+        +--------------+     +--------------+
```

#### Shared cell controller topology 共享单元控制器拓扑 ¶



 

Note 注意



This topology is not yet supported by Kolla Ansible.
Kolla Ansible 尚不支持此拓扑。

An alternative configuration is to place the cell control services for multiple cells on a single shared group of cell controllers. This might allow for more efficient use of hardware where the control services for a single cell do not fully consume the resources of a set of cell controllers:
另一种配置是将多个单元的单元控制服务放在单个共享单元控制器组上。这样可以更有效地使用硬件，其中单个单元的控制服务不会完全消耗一组单元控制器的资源：

```
                                +----------------+
                                |                ++
                                |                |-+
                                |   controllers  |-|
                                |                |-|
                                |                |-|
                                +------------------|
                                 +-----------------|
                                  +----------------+

                                +----------------+
                                |                ++
                                |   shared cell  |-+
                                |   controllers  |-|
                                |                |-|
                                +------------------|
                                 +-----------------|
                                  +----------------+

+--------------+     +--------------+        +--------------+     +--------------+
|              |     |              |        |              |     |              |
|   cell 1     |     |   cell 1     |        |   cell 2     |     |   cell 2     |
|   compute 1  |     |   compute 2  |        |   compute 1  |     |   compute 2  |
|              |     |              |        |              |     |              |
+--------------+     +--------------+        +--------------+     +--------------+
```

### Databases & message queues 数据库和消息队列 ¶

The global services require access to a database for the API and cell0 databases, in addition to a message queue. Each cell requires its own database and message queue instance. These could be separate database and message queue clusters, or shared database and message queue clusters partitioned via database names and virtual hosts. Currently Kolla Ansible supports deployment of shared database cluster and message queue clusters.
除了消息队列之外，全局服务还需要访问 API 和 cell0  数据库的数据库。每个单元都需要自己的数据库和消息队列实例。这些集群可以是单独的数据库和消息队列集群，也可以是通过数据库名称和虚拟主机进行分区的共享数据库和消息队列集群。目前，Kolla Ansible 支持部署共享数据库集群和消息队列集群。

## Configuration 配置 ¶



 

See also 另请参阅



Configuring Kolla Ansible for deployment of multiple cells typically requires use of [inventory host and group variables](https://docs.openstack.org/kolla-ansible/latest/user/multinode.html#multinode-host-and-group-variables).
配置 Kolla Ansible 以部署多个单元通常需要使用清单主机和组变量。

### Enabling multi-cell support 启用多单元支持 ¶

Support for deployment of multiple cells is disabled by default - nova is deployed in single conductor mode.
默认情况下，对部署多个单元的支持处于禁用状态 - nova 以单导体模式部署。

Deployment of multiple cells may be enabled by setting `enable_cells` to `yes` in `globals.yml`. This deploys nova in superconductor mode, with separate conductors for each cell.
可以通过将 `enable_cells` 设置为 `yes` 来 `globals.yml` 启用多个单元的部署。这将 nova 部署在超导体模式下，每个电池都有单独的导体。

### Naming cells 命名单元格 ¶

By default, all cell services are deployed in a single unnamed cell. This behaviour is backwards compatible with previous releases of Kolla Ansible.
缺省情况下，所有单元服务都部署在单个未命名单元中。此行为向后兼容 Kolla Ansible 的先前版本。

To deploy hosts in a different cell, set the `nova_cell_name` variable for the hosts in the cell. This can be done either using host variables or group variables.
要在其他单元中部署主机，请为单元中的主机设置 `nova_cell_name` 变量。这可以使用主变量或组变量来完成。

### Groups 组 ¶

In a single cell deployment, the following Ansible groups are used to determine the placement of services:
在单单元部署中，以下 Ansible 组用于确定服务的位置：

- `compute`: `nova-compute`, `nova-libvirt`, `nova-ssh`
- `nova-compute-ironic`: `nova-compute-ironic`
- `nova-conductor`: `nova-conductor`
- `nova-novncproxy`: `nova-novncproxy`
- `nova-serialproxy`: `nova-serialproxy`
- `nova-spicehtml5proxy`: `nova-spicehtml5proxy`

In a multi-cell deployment, this is still necessary - compute hosts must be in the `compute` group. However, to provide further control over where cell services are placed, the following variables are used:
在多单元部署中，这仍然是必需的 - 计算主机必须在 `compute` 组中。但是，为了进一步控制单元服务的放置位置，使用了以下变量：

- `nova_cell_compute_group`
- `nova_cell_compute_ironic_group`
- `nova_cell_conductor_group`
- `nova_cell_novncproxy_group`
- `nova_cell_serialproxy_group`
- `nova_cell_spicehtml5proxy_group`

For backwards compatibility, these are set by default to the original group names.  For a multi-cell deployment, they should be set to the name of a group containing only the compute hosts in that cell.
为了向后兼容，默认情况下，这些名称设置为原始组名称。对于多单元部署，应将其设置为仅包含该单元中计算主机的组的名称。

#### Example 示例 ¶

In the following example we have two cells, `cell1` and `cell2`. Each cell has two compute nodes and a cell controller.
在下面的示例中，我们有两个单元格和 `cell1` `cell2` 。每个单元有两个计算节点和一个单元控制器。

Inventory: 库存：

```
[compute:children]
compute-cell1
compute-cell2

[nova-conductor:children]
cell-control-cell1
cell-control-cell2

[nova-novncproxy:children]
cell-control-cell1
cell-control-cell2

[nova-spicehtml5proxy:children]
cell-control-cell1
cell-control-cell2

[nova-serialproxy:children]
cell-control-cell1
cell-control-cell2

[cell1:children]
compute-cell1
cell-control-cell1

[cell2:children]
compute-cell2
cell-control-cell2

[compute-cell1]
compute01
compute02

[compute-cell2]
compute03
compute04

[cell-control-cell1]
cell-control01

[cell-control-cell2]
cell-control02
```

Cell1 group variables (`group_vars/cell1`):
Cell1 组变量 （ `group_vars/cell1` ）：

```
nova_cell_name: cell1
nova_cell_compute_group: compute-cell1
nova_cell_conductor_group: cell-control-cell1
nova_cell_novncproxy_group: cell-control-cell1
nova_cell_serialproxy_group: cell-control-cell1
nova_cell_spicehtml5proxy_group: cell-control-cell1
```

Cell2 group variables (`group_vars/cell2`):
Cell2 组变量 （ `group_vars/cell2` ）：

```
nova_cell_name: cell2
nova_cell_compute_group: compute-cell2
nova_cell_conductor_group: cell-control-cell2
nova_cell_novncproxy_group: cell-control-cell2
nova_cell_serialproxy_group: cell-control-cell2
nova_cell_spicehtml5proxy_group: cell-control-cell2
```

Note that these example cell group variables specify groups for all console proxy services for completeness. You will need to ensure that there are no port collisions. For example, if in both cell1 and cell2, you use the default `novncproxy` console proxy, you could add `nova_novncproxy_port: 6082` to the cell2 group variables to prevent a collision with cell1.
请注意，为了完整起见，这些示例单元组变量为所有控制台代理服务指定了组。您需要确保没有端口冲突。例如，如果在 cell1 和 cell2 中都使用默认 `novncproxy` 控制台代理，则可以 `nova_novncproxy_port: 6082` 向 cell2 添加组变量以防止与 cell1 发生冲突。

### Databases 数据库 ¶

The database connection for each cell is configured via the following variables:
每个单元的数据库连接通过以下变量进行配置：

- `nova_cell_database_name`
- `nova_cell_database_user`
- `nova_cell_database_password`
- `nova_cell_database_address`
- `nova_cell_database_port`

By default the MariaDB cluster deployed by Kolla Ansible is used.  For an unnamed cell, the `nova` database is used for backwards compatibility.  For a named cell, the database is named `nova_<cell name>`.
默认情况下，使用 Kolla Ansible 部署的 MariaDB 集群。对于未命名的单元格， `nova` 数据库用于向后兼容。对于命名单元，数据库名为 `nova_<cell name>` 。

### Message queues 消息队列 ¶

The RPC message queue for each cell is configured via the following variables:
每个单元的 RPC 消息队列通过以下变量进行配置：

- `nova_cell_rpc_user`
- `nova_cell_rpc_password`
- `nova_cell_rpc_port`
- `nova_cell_rpc_group_name`
- `nova_cell_rpc_transport`
- `nova_cell_rpc_vhost`

And for notifications: 对于通知：

- `nova_cell_notify_user`
- `nova_cell_notify_password`
- `nova_cell_notify_port`
- `nova_cell_notify_group_name`
- `nova_cell_notify_transport`
- `nova_cell_notify_vhost`

By default the message queue cluster deployed by Kolla Ansible is used. For an unnamed cell, the `/` virtual host used by all OpenStack services is used for backwards compatibility.  For a named cell, a virtual host named `nova_<cell name>` is used.
默认情况下，使用 Kolla Ansible 部署的消息队列集群。对于未命名的单元，所有 OpenStack 服务使用的 `/` 虚拟主机都用于向后兼容。对于命名单元，使用名为 `nova_<cell name>` 的虚拟主机。

### Conductor & API database 导体 & API 数据库 ¶

By default the cell conductors are configured with access to the API database. This is currently necessary for [some operations](https://docs.openstack.org/nova/latest/user/cellsv2-layout.html#operations-requiring-upcalls) in Nova which require an *upcall*.
默认情况下，单元导体配置为对 API 数据库的访问权限。目前，对于 Nova 中需要上调的某些操作，这是必需的。

If those operations are not required, it is possible to prevent cell conductors from accessing the API database by setting `nova_cell_conductor_has_api_database` to `no`.
如果不需要这些操作，则可以通过设置为 `nova_cell_conductor_has_api_database` `no` 来阻止电池导体访问 API 数据库。

### Console proxies 控制台代理 ¶

General information on configuring console access in Nova is available [here](https://docs.openstack.org/kolla-ansible/latest/reference/compute/nova-guide.html#nova-consoles). For deployments with multiple cells, the console proxies for each cell must be accessible by a unique endpoint. We achieve this by adding an HAProxy frontend for each cell that forwards to the console proxies for that cell. Each frontend must use a different port. The port may be configured via the following variables:
有关在 Nova  中配置控制台访问的一般信息，请参阅此处。对于具有多个单元的部署，每个单元的控制台代理必须可通过唯一的终端节点访问。我们通过为每个单元添加一个  HAProxy 前端来实现这一点，该前端转发到该单元的控制台代理。每个前端必须使用不同的端口。可以通过以下变量配置端口：

- `nova_novncproxy_port`
- `nova_spicehtml5proxy_port`
- `nova_serialproxy_port`

### Ironic 具有讽刺意味的 ¶

Currently all Ironic-based instances are deployed in a single cell. The name of that cell is configured via `nova_cell_ironic_cell_name`, and defaults to the unnamed cell. `nova_cell_compute_ironic_group` can be used to set the group that the `nova-compute-ironic` services are deployed to.
目前，所有基于 Ironic 的实例都部署在单个单元中。该单元的名称通过 `nova_cell_ironic_cell_name` 配置，默认为未命名的单元。 `nova_cell_compute_ironic_group` 可用于设置 `nova-compute-ironic` 服务部署到的组。

## Deployment 部署 ¶

Deployment in a multi-cell environment does not need to be done differently than in a single-cell environment - use the `kolla-ansible deploy` command.
在多单元环境中的部署不需要与在单单元环境中进行不同的部署 - 使用命令 `kolla-ansible deploy` 。

### Scaling out 横向扩展 ¶

A common operational task in large scale environments is to add new compute resources to an existing deployment. In a multi-cell environment it is likely that these will all be added to one or more new or existing cells. Ideally we would not risk affecting other cells, or even the control hosts, when deploying these new resources.
在大规模环境中，常见的操作任务是将新的计算资源添加到现有部署中。在多单元环境中，这些单元很可能都会被添加到一个或多个新的或现有的单元中。理想情况下，在部署这些新资源时，我们不会冒着影响其他单元甚至控制主机的风险。

The Nova cells support in Kolla Ansible has been built such that it is possible to add new cells or extend existing ones without affecting the rest of the cloud. This is achieved via the `--limit` argument to `kolla-ansible`. For example, if we are adding a new cell `cell03` to an existing cloud, and all hosts for that cell (control and compute) are in a `cell03` group, we could use this as our limit:
Kolla Ansible 中的 Nova 单元支持已经构建，因此可以在不影响云的其余部分的情况下添加新单元或扩展现有单元。这是通过 `--limit` 参数 来实现 `kolla-ansible` 的。例如，如果我们要向现有云添加一个新单元 `cell03` ，并且该单元的所有主机（控制和计算）都在一个 `cell03` 组中，我们可以将其用作限制：

```
kolla-ansible deploy --limit cell03
```

When adding a new cell, we also need to ensure that HAProxy is configured for the console proxies in that cell:
添加新单元时，我们还需要确保为该单元中的控制台代理配置了 HAProxy：

```
kolla-ansible deploy --tags haproxy
```

Another benefit of this approach is that it should be faster to complete, as the number of hosts Ansible manages is reduced.
这种方法的另一个好处是，随着 Ansible 管理的主机数量的减少，它应该更快地完成。



## Upgrades 升级 ¶

Similar to deploys, upgrades in a multi-cell environment can be performed in the same way as single-cell environments, via `kolla-ansible upgrade`.
与部署类似，多单元环境中的升级可以采用与单单元环境相同的方式执行，即通过 `kolla-ansible upgrade` .

### Staged upgrades 分阶段升级 ¶



 

Note 注意



Staged upgrades are not applicable when `nova_safety_upgrade` is `yes`.
分阶段升级在以下 `yes` 情况下 `nova_safety_upgrade` 不适用。

In large environments the risk involved with upgrading an entire site can be significant, and the ability to upgrade one cell at a time is crucial. This is very much an advanced procedure, and operators attempting this should be familiar with the [Nova upgrade documentation](https://docs.openstack.org/nova/latest/user/upgrade).
在大型环境中，升级整个站点所涉及的风险可能很大，并且一次升级一个单元的能力至关重要。这是一个非常高级的过程，尝试这样做的操作员应该熟悉 Nova 升级文档。

Here we use Ansible tags and limits to control the upgrade process. We will only consider the Nova upgrade here. It is assumed that all dependent services have been upgraded (see `ansible/site.yml` for correct ordering).
在这里，我们使用 Ansible 标签和限制来控制升级过程。我们在这里只考虑 Nova 升级。假设所有从属服务都已升级（请参阅 `ansible/site.yml` 正确的排序）。

The first step, which may be performed in advance of the upgrade, is to perform the database schema migrations.
第一步（可在升级之前执行）是执行数据库架构迁移。

```
kolla-ansible upgrade --tags nova-bootstrap
```

Next, we upgrade the global services.
接下来，我们升级全局服务。

```
kolla-ansible upgrade --tags nova-api-upgrade
```

Now the cell services can be upgraded. This can be performed in batches of one or more cells at a time, using `--limit`. For example, to upgrade services in `cell03`:
现在，小区服务可以升级。这可以一次分批执行一个或多个单元，使用 `--limit` .例如，要升级以下服务 `cell03` ：

```
kolla-ansible upgrade --tags nova-cell-upgrade --limit cell03
```

At this stage, we might wish to perform testing of the new services, to check that they are functioning correctly before proceeding to other cells.
在这个阶段，我们可能希望对新服务进行测试，以检查它们是否正常运行，然后再继续其他单元。

Once all cells have been upgraded, we can reload the services to remove RPC version pinning, and perform online data migrations.
升级所有单元后，我们可以重新加载服务以删除 RPC 版本固定，并执行在线数据迁移。

```
kolla-ansible upgrade --tags nova-reload,nova-online-data-migrations
```

The nova upgrade is now complete, and upgrading of other services may continue.
nova 升级现已完成，其他服务的升级可能会继续。

# Nova Fake Driver 新星假司机

​        version 版本              



One common question from OpenStack operators is that “how does the control plane (for example, database, messaging queue, nova-scheduler ) scales?”. To answer this question, operators setup Rally to drive workload to the OpenStack cloud. However, without a large number of nova-compute nodes, it becomes difficult to exercise the control performance.
OpenStack 运营商的一个常见问题是“控制平面（例如，数据库、消息队列、nova-scheduler）如何扩展？为了回答这个问题，运营商设置了 Rally  来将工作负载驱动到 OpenStack 云。但是，如果没有大量的 nova 计算节点，则很难执行控制性能。

Given the built-in feature of Docker container, Kolla enables standing up many of Compute nodes with nova fake driver on a single host. For example, we can create 100 nova-compute containers on a real host to simulate the 100-hypervisor workload to the `nova-conductor` and the messaging queue.
鉴于 Docker 容器的内置功能，Kolla 可以在单个主机上使用 nova 假驱动程序建立许多计算节点。例如，我们可以在真实主机上创建 100 个 nova-compute 容器，以模拟 100 个虚拟机管理程序的工作负载到 `nova-conductor` 消息传递队列。

## Use nova-fake driver 使用 nova-fake 驱动程序 ¶

Nova fake driver can not work with all-in-one deployment. This is because the fake `neutron-openvswitch-agent` for the fake `nova-compute` container conflicts with `neutron-openvswitch-agent` on the Compute nodes. Therefore, in the inventory the network node must be different than the Compute node.
Nova 假驱动程序无法与多合一部署一起使用。这是因为假 `nova-compute` 容器的假与计算节点 `neutron-openvswitch-agent` 上的假 `neutron-openvswitch-agent` 冲突。因此，在清单中，网络节点必须与计算节点不同。

By default, Kolla uses libvirt driver on the Compute node. To use nova-fake driver, edit the following parameters in `/etc/kolla/globals.yml` or in the command line options.
默认情况下，Kolla 在 Compute 节点上使用 libvirt 驱动程序。要使用 nova-fake 驱动程序，请在命令行选项中 `/etc/kolla/globals.yml` 或命令行选项中编辑以下参数。

```
enable_nova_fake: "yes"
num_nova_fake_per_node: 5
```

Each Compute node will run 5 `nova-compute` containers and 5 `neutron-plugin-agent` containers. When booting instance, there will be no real instances created. But **nova list** shows the fake instances.
每个计算节点将运行 5 `nova-compute` 个容器和 5 `neutron-plugin-agent` 个容器。启动实例时，不会创建实际实例。但是 nova 列表显示了假实例。

# Nova - Compute Service Nova - 计算服务

​        version 版本              



Nova is a core service in OpenStack, and provides compute services. Typically this is via Virtual Machines (VMs), but may also be via bare metal servers if Nova is coupled with Ironic.
Nova 是 OpenStack 中的核心服务，提供计算服务。通常，这是通过虚拟机 （VM） 进行的，但如果 Nova 与 Ironic 结合使用，也可能通过裸机服务器进行。

Nova is enabled by default, but may be disabled by setting `enable_nova` to `no` in `globals.yml`.
默认情况下，Nova 处于启用状态，但可以通过设置为 `enable_nova` `no` in `globals.yml` 来禁用。

## Virtualisation Drivers 虚拟化驱动程序 ¶

The virtualisation driver may be selected via `nova_compute_virt_type` in `globals.yml`. Supported options are `qemu`, `kvm`, and `vmware`. The default is `kvm`.
可以通过 `nova_compute_virt_type` 中 `globals.yml` 选择虚拟化驱动程序。支持的选项包括 `qemu` 、 `kvm` 和 `vmware` 。默认值为 `kvm` 。

### Libvirt

Information on the libvirt-based drivers `kvm` and `qemu` can be found in [Libvirt - Nova Virtualisation Driver](https://docs.openstack.org/kolla-ansible/latest/reference/compute/libvirt-guide.html).
有关基于 libvirt 的驱动程序 `kvm` 的信息， `qemu` 可以在 Libvirt - Nova Virtualisation Driver 中找到。

### VMware VMware的 ¶

Information on the VMware-based driver `vmware` can be found in [VMware - Nova Virtualisation Driver](https://docs.openstack.org/kolla-ansible/latest/reference/compute/vmware-guide.html).
有关基于 VMware 的驱动程序 `vmware` 的信息，请参阅 VMware - Nova Virtualisation Driver。

### Bare Metal 裸机 ¶

Information on using Nova with Ironic to deploy compute instances to bare metal can be found in [Ironic - Bare Metal provisioning](https://docs.openstack.org/kolla-ansible/latest/reference/bare-metal/ironic-guide.html).
有关将 Nova 与 Ironic 结合使用以将计算实例部署到裸机的信息，请参阅 Ironic - 裸机预配。

### Fake Driver 假驱动程序 ¶

The fake driver can be used for testing Nova’s scaling properties without requiring access to a large amount of hardware resources. It is covered in [Nova Fake Driver](https://docs.openstack.org/kolla-ansible/latest/reference/compute/nova-fake-driver.html).
假驱动程序可用于测试 Nova 的扩展属性，而无需访问大量硬件资源。它被 Nova Fake Driver 所涵盖。



## Consoles 控制台 ¶

The console driver may be selected via `nova_console` in `globals.yml`. Valid options are `none`, `novnc` and `spice`. Additionally, serial console support can be enabled by setting `enable_nova_serialconsole_proxy` to `yes`.
可以通过 `nova_console` 中 `globals.yml` 选择控制台驱动程序。有效选项为 `none` 和 `novnc` `spice` 。此外，还可以通过设置为 `enable_nova_serialconsole_proxy` `yes` 来启用串行控制台支持。

## Cells 单元格 ¶

Information on using Nova Cells V2 to scale out can be found in [Nova Cells](https://docs.openstack.org/kolla-ansible/latest/reference/compute/nova-cells-guide.html).
有关使用 Nova Cells V2 进行横向扩展的信息，请参阅 Nova Cells。

## Vendordata 供应商数据 ¶

Nova supports passing deployer provided data to instances using a concept known as Vendordata. If a Vendordata file is located in the following path within the Kolla configuration, Kolla will automatically use it when the Nova service is deployed or reconfigured: `/etc/kolla/config/nova/vendordata.json`.
Nova 支持使用称为 Vendordata 的概念将部署者提供的数据传递给实例。如果 Vendordata 文件位于 Kolla 配置中的以下路径中，则 Kolla 将在部署或重新配置 Nova 服务时自动使用它： `/etc/kolla/config/nova/vendordata.json` 。

## Failure handling 故障处理 ¶

### Compute service registration 计算服务注册 ¶

During deployment, Kolla Ansible waits for Nova compute services to register themselves. By default, if a compute service does not register itself before the timeout, that host will be marked as failed in the Ansible run. This behaviour is useful at scale, where failures are more frequent.
在部署期间，Kolla Ansible 会等待 Nova 计算服务自行注册。默认情况下，如果计算服务在超时之前未自行注册，则该主机将在 Ansible 运行中标记为失败。此行为在故障更频繁的大规模中很有用。

Alternatively, to fail all hosts in a cell when any compute service fails to register, set `nova_compute_registration_fatal` to `true`.
或者，如果任何计算服务注册失败，则要使单元中的所有主机失败，请设置为 `nova_compute_registration_fatal` `true` 。

## Managing resource providers via config files 通过配置文件管理资源提供程序 ¶

In the Victoria cycle Nova merged support for managing resource providers via [configuration files](https://docs.openstack.org/nova/latest/admin/managing-resource-providers).
在维多利亚周期中，Nova 合并了对通过配置文件管理资源提供程序的支持。

Kolla Ansible limits the use of this feature to a single config file per Nova Compute service, which is defined via Ansible inventory group/host vars. The reason for doing this is to encourage users to configure each compute service individually, so that when further resources are added, existing compute services do not need to be restarted.
Kolla Ansible 将此功能的使用限制为每个 Nova Compute 服务的单个配置文件，该文件通过 Ansible 清单组/主机变量定义。这样做的原因是鼓励用户单独配置每个计算服务，以便在添加更多资源时，无需重新启动现有计算服务。

For example, a user wanting to configure a compute resource with GPUs for a specific host may add the following file to host_vars:
例如，想要为特定主机配置具有 GPU 的计算资源的用户可以将以下文件添加到host_vars：

```
cat gpu_compute_0001
nova_cell_compute_provider_config:
  meta:
    schema_version: '1.0'
  providers:
    - identification:
        name: $COMPUTE_NODE
      inventories:
        additional:
          - CUSTOM_GPU:
              total: 8
              reserved: 0
              min_unit: 1
              max_unit: 1
              step_size: 1
              allocation_ratio: 1.0
```

A similar approach can be used with group vars to cover more than one machine.
类似的方法可以用于组变量，以覆盖多台计算机。

Since a badly formatted file will prevent the Nova Compute service from starting, it should first be validated as described in the [documentation](https://docs.openstack.org/nova/latest/admin/managing-resource-providers). The Nova Compute service can then be reconfigured to apply the change.
由于格式不正确的文件会阻止 Nova Compute 服务启动，因此应首先按照文档中所述对其进行验证。然后，可以重新配置 Nova Compute 服务以应用更改。

To remove the resource provider configuration, it is simplest to leave the group/host vars in place without specifying any inventory or traits. This will effectively remove the configuration when the Nova Compute service is restarted. If you choose to undefine nova_cell_compute_provider_config on a host, you must manually remove the generated config from inside the container, or recreate the container.
要删除资源提供程序配置，最简单的方法是将组/主机变量保留在原位，而不指定任何清单或特征。这将在重新启动 Nova Compute  服务时有效地删除配置。如果选择在主机上取消定义nova_cell_compute_provider_config，则必须从容器内部手动删除生成的配置，或重新创建容器。

# VMware - Nova Virtualisation Driver VMware - Nova 虚拟化驱动程序

​        version 版本              





## Overview 概述 ¶

Kolla can deploy the Nova and Neutron Service(s) for VMware vSphere. Depending on the network architecture (NsxT, NsxV or DVS) you choose, Kolla deploys the following OpenStack services for VMware vSphere:
Kolla 可以为 VMware vSphere 部署 Nova 和 Neutron 服务。根据您选择的网络架构（NsxT、NsxV 或 DVS），Kolla 为 VMware vSphere 部署以下 OpenStack 服务：

For VMware NsxT: 对于 VMware NsxT：

- nova-compute
- neutron-server neutron-服务器

For VMware NsxV: 对于 VMware NsxV：

- nova-compute
- neutron-server neutron-服务器

For VMware DVS: 对于 VMware DVS：

- nova-compute
- neutron-server neutron-服务器
- neutron-dhcp-agent 中子-DHCP-代理
- neutron-metadata-agent 中子元数据代理

Kolla can deploy the Glance and Cinder services using VMware datastore as their backend. You can create Cinder volumes as VMDKs or as First Class Disks (FCDs).
Kolla 可以使用 VMware 数据存储作为后端来部署 Glance 和 Cinder 服务。您可以将 Cinder 卷创建为 VMDK 或头等类磁盘 （FCD）。

An FCD, also known as an Improved Virtual Disk (IVD) or Managed Virtual Disk, is a named virtual disk independent of a virtual machine. Using FCDs for Cinder volumes eliminates the need for shadow virtual machines.
FCD（也称为改进型虚拟磁盘 （IVD） 或托管虚拟磁盘）是独立于虚拟机的命名虚拟磁盘。将 FCD 用于 Cinder 卷后，无需使用卷影虚拟机。

The FCD backend is offered in addition to the default VMDK backend. If you use FCD as the backend driver for Cinder, you can use both FCD and VMDK volumes in the same deployment.
除了默认的 VMDK 后端之外，还提供 FCD 后端。如果使用 FCD 作为 Cinder 的后端驱动程序，则可以在同一部署中同时使用 FCD 和 VMDK 卷。

Ceilometer metering for vSphere is also supported.
还支持 vSphere 的云高仪计量。

Because the [vmware-nsx](https://github.com/openstack/vmware-nsx) drivers for neutron use completely different architecture than other types of virtualization, vmware-nsx drivers cannot coexist with other type of virtualization in one region. In neutron vmware-nsx drivers, neutron-server acts like an agent to translate OpenStack actions into what vSphere/NSX Manager API can understand. Neutron does not directly takes control of the Open vSwitch inside the VMware environment but through the API exposed by vSphere/NSX Manager.
由于适用于 Neutron 的 vmware-nsx 驱动程序使用的体系结构与其他类型的虚拟化完全不同，因此 vmware-nsx  驱动程序无法与同一区域中的其他类型的虚拟化共存。在 neutron vmware-nsx 驱动程序中，neutron-server  的作用类似于代理，用于将 OpenStack 操作转换为 vSphere/NSX Manager API 可以理解的内容。Neutron  不会直接控制 VMware 环境中的 Open vSwitch，而是通过 vSphere/NSX Manager 公开的 API 进行控制。



 

Note 注意



VMware NSX plugin is not in the kolla image by default. VMware NSX plugin has to be added in the neutron image and if you are using vmware_dvs also in neutron-dhcp-agent image.
默认情况下，VMware NSX 插件不在 kolla 映像中。VMware NSX 插件必须添加到 neutron 映像中，如果您正在使用 vmware_dvs，也必须在 neutron-dhcp-agent 映像中添加。

For VMware DVS, the Neutron DHCP agent does not attaches to Open vSwitch inside VMware environment, but attach to the Open vSwitch bridge called `br-dvs` on the OpenStack side and replies to/receives DHCP packets through VLAN. Similar to what the DHCP agent does, Neutron metadata agent attaches to `br-dvs` bridge and works through VLAN.
对于 VMware DVS，Neutron DHCP 代理不会连接到 VMware 环境中的 Open vSwitch，而是连接到 OpenStack 端调用 `br-dvs` 的 Open vSwitch 网桥，并通过 VLAN 回复/接收 DHCP 数据包。与 DHCP 代理的功能类似，Neutron 元数据代理连接到 `br-dvs` 网桥并通过 VLAN 工作。



 

Note 注意



VMware NSX-DVS plugin does not support tenant networks, so all VMs should attach to Provider VLAN/Flat networks.
VMware NSX-DVS 插件不支持租户网络，因此所有虚拟机都应连接到提供商 VLAN/扁平网络。

## VMware NSX-T

### Preparation 准备工作 ¶

You should have a working NSX-T environment, this part is out of scope of Kolla. For more information, please see [VMware NSX-T documentation](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html/). The NSX Manager provides a web-based user interface where you can manage your NSX-T environment. It also hosts the API server that processes API calls. The NSX Manager interface provides two modes for configuring resources:
您应该有一个正常工作的 NSX-T 环境，这部分超出了 Kolla 的范围。有关详细信息，请参见 VMware NSX-T 文档。NSX Manager  提供了一个基于 Web 的用户界面，您可以在其中管理 NSX-T 环境。它还托管处理 API 调用的 API 服务器。NSX Manager  界面提供两种用于配置资源的模式：

- Policy mode 策略模式
- Manager mode 管理器模式

In Kolla you will have the choice between both with neutron plugin vmware_nsxv3 for Manager mode and vmware_nsxp for Policy Mode. For more information, please see [documentation](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/installation/GUID-BB26CDC8-2A90-4C7E-9331-643D13FEEC4A.html/).
在 Kolla 中，您可以选择用于管理器模式的 neutron 插件vmware_nsxv3和用于策略模式的 vmware_nsxp。有关详细信息，请参阅文档。

------

In addition, it is important to modify the firewall rule of vSphere to make sure that VNC is accessible from outside VMware environment.
此外，请务必修改 vSphere 的防火墙规则，以确保可从 VMware 环境外部访问 VNC。

On every VMware host, edit `/etc/vmware/firewall/vnc.xml` as below:
在每个 VMware 主机上，按如下方式进行编辑 `/etc/vmware/firewall/vnc.xml` ：

```
<!-- FirewallRule for VNC Console -->
<ConfigRoot>
<service>
<id>VNC</id>
<rule id = '0000'>
<direction>inbound</direction>
<protocol>tcp</protocol>
<porttype>dst</porttype>
<port>
<begin>5900</begin>
<end>5999</end>
</port>
</rule>
<rule id = '0001'>
<direction>outbound</direction>
<protocol>tcp</protocol>
<porttype>dst</porttype>
<port>
<begin>0</begin>
<end>65535</end>
</port>
</rule>
<enabled>true</enabled>
<required>false</required>
</service>
</ConfigRoot>
```

Then refresh the firewall config by:
然后通过以下方式刷新防火墙配置：

```
esxcli network firewall refresh
```

Verify that the firewall config is applied:
验证是否应用了防火墙配置：

```
esxcli network firewall ruleset list
```

------

### Deployment 部署 ¶

The deployment below covers the Policy mode (vmware_nsxp)
下面的部署涵盖策略模式 （vmware_nsxp）

Enable VMware nova-compute plugin and NSX-T neutron-server plugin in `/etc/kolla/globals.yml`:
在以下位置 `/etc/kolla/globals.yml` 启用 VMware nova-compute 插件和 NSX-T neutron-server 插件：

```
enable_openvswitch: no
nova_compute_virt_type: "vmware"
neutron_plugin_agent: "vmware_nsxp"
```

If you want to set VMware datastore as cinder backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware 数据存储设置为 cinder 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
enable_cinder: "yes"
cinder_backend_vmwarevc_vmdk: "yes"
vmware_datastore_name: "TestDatastore"
```

If you want to set VMware First Class Disk (FCD) datastore as VMware vStorage Object backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware First Class Disk （FCD） 数据存储设置为 VMware vStorage Object 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
enable_cinder: "yes"
cinder_backend_vmware_vstorage_object: "yes"
vmware_datastore_name: "TestDatastore"
```

If you want to set VMware datastore as glance backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware 数据存储设置为 glance 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
glance_backend_vmware: "yes"
vmware_vcenter_name: "TestDatacenter"
vmware_datastore_name: "TestDatastore"
```

VMware options are required in `/etc/kolla/globals.yml`, these options should be configured correctly according to your NSX-T environment.
中 `/etc/kolla/globals.yml` 需要 VMware 选项，应根据 NSX-T 环境正确配置这些选项。

Options for `nova-compute` and `ceilometer`:
选项 `nova-compute` 和 `ceilometer` ：

```
vmware_vcenter_host_ip: "127.0.0.1"
vmware_vcenter_host_username: "admin"
vmware_vcenter_cluster_name: "cluster-1"
vmware_vcenter_insecure: "True"
vmware_vcenter_datastore_regex: ".*"
```



 

Note 注意



The VMware vCenter password has to be set in `/etc/kolla/passwords.yml`.
必须在 中 `/etc/kolla/passwords.yml` 设置 VMware vCenter 密码。

```
vmware_vcenter_host_password: "admin"
```

Options for Neutron NSX-T support:
Neutron NSX-T 支持选项：

```
vmware_nsxp_api_user: "admin"
vmware_nsxp_insecure: true
vmware_nsxp_default_tier0_router: "T0-Example"
vmware_nsxp_dhcp_profile: "dhcp-profile-example"
vmware_nsxp_metadata_proxy: "metadata_proxy-example"
vmware_nsxp_api_managers: "nsx-manager.local"
vmware_nsxp_default_vlan_tz: "vlan-tz-example"
vmware_nsxp_default_overlay_tz: "overlay-tz-example"
```



 

Note 注意



If you want to set secure connections to VMware, set `vmware_vcenter_insecure` to false. Secure connections to vCenter requires a CA file, copy the vCenter CA file to `/etc/kolla/config/vmware_ca`.
如果要设置与 VMware 的安全连接，请设置为 `vmware_vcenter_insecure` false。与 vCenter 的安全连接需要 CA 文件，请将 vCenter CA 文件复制到 `/etc/kolla/config/vmware_ca` 。



 

Note 注意



The VMware NSX-T password has to be set in `/etc/kolla/passwords.yml`.
必须在 中 `/etc/kolla/passwords.yml` 设置 VMware NSX-T 密码。

```
vmware_nsxp_api_password: "xxxxx"
vmware_nsxp_metadata_proxy_shared_secret: "xxxxx"
```

Then you should start **kolla-ansible** deployment normally as KVM/QEMU deployment.
然后，您应该以 KVM/QEMU 部署的形式正常启动 kolla-ansible 部署。

## VMware NSX-V

### Preparation 准备工作 ¶

You should have a working NSX-V environment, this part is out of scope of Kolla. For more information, please see [VMware NSX-V documentation](https://docs.vmware.com/en/VMware-NSX-for-vSphere/).
您应该有一个正常工作的 NSX-V 环境，这部分超出了 Kolla 的范围。有关详细信息，请参见 VMware NSX-V 文档。

------

In addition, it is important to modify the firewall rule of vSphere to make sure that VNC is accessible from outside VMware environment.
此外，请务必修改 vSphere 的防火墙规则，以确保可从 VMware 环境外部访问 VNC。

On every VMware host, edit `/etc/vmware/firewall/vnc.xml` as below:
在每个 VMware 主机上，按如下方式进行编辑 `/etc/vmware/firewall/vnc.xml` ：

```
<!-- FirewallRule for VNC Console -->
<ConfigRoot>
<service>
<id>VNC</id>
<rule id = '0000'>
<direction>inbound</direction>
<protocol>tcp</protocol>
<porttype>dst</porttype>
<port>
<begin>5900</begin>
<end>5999</end>
</port>
</rule>
<rule id = '0001'>
<direction>outbound</direction>
<protocol>tcp</protocol>
<porttype>dst</porttype>
<port>
<begin>0</begin>
<end>65535</end>
</port>
</rule>
<enabled>true</enabled>   <required>false</required>
</service>
</ConfigRoot>
```

Then refresh the firewall config by:
然后通过以下方式刷新防火墙配置：

```
esxcli network firewall refresh
```

Verify that the firewall config is applied:
验证是否应用了防火墙配置：

```
esxcli network firewall ruleset list
```

------

### Deployment 部署 ¶

Enable VMware nova-compute plugin and NSX-V neutron-server plugin in `/etc/kolla/globals.yml`:
在以下情况下 `/etc/kolla/globals.yml` 启用 VMware nova-compute 插件和 NSX-V neutron-server 插件：

```
nova_compute_virt_type: "vmware"
neutron_plugin_agent: "vmware_nsxv"
```



 

Note 注意



VMware NSX-V also supports Neutron FWaaS and VPNaaS services, you can enable them by setting these options in `globals.yml`:
VMware NSX-V 还支持 Neutron FWaaS 和 VPNaaS 服务，您可以通过在以下位置设置 `globals.yml` 以下选项来启用它们：

- enable_neutron_vpnaas: “yes”
  enable_neutron_vpnaas：“是”
- enable_neutron_fwaas: “yes”
  enable_neutron_fwaas：“是”

If you want to set VMware VMDK datastore as cinder backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware VMDK 数据存储设置为 cinder 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
enable_cinder: "yes"
cinder_backend_vmwarevc_vmdk: "yes"
vmware_datastore_name: "TestDatastore"
```

If you want to set VMware First Class Disk (FCD) datastore as VMware vStorage Object backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware First Class Disk （FCD） 数据存储设置为 VMware vStorage Object 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
enable_cinder: "yes"
cinder_backend_vmware_vstorage_object: "yes"
vmware_datastore_name: "TestDatastore"
```

If you want to set VMware datastore as glance backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware 数据存储设置为 glance 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
glance_backend_vmware: "yes"
vmware_vcenter_name: "TestDatacenter"
vmware_datastore_name: "TestDatastore"
```

VMware options are required in `/etc/kolla/globals.yml`, these options should be configured correctly according to your NSX-V environment.
中 `/etc/kolla/globals.yml` 需要 VMware 选项，应根据 NSX-V 环境正确配置这些选项。

Options for `nova-compute` and `ceilometer`:
选项 `nova-compute` 和 `ceilometer` ：

```
vmware_vcenter_host_ip: "127.0.0.1"
vmware_vcenter_host_username: "admin"
vmware_vcenter_cluster_name: "cluster-1"
vmware_vcenter_insecure: "True"
vmware_vcenter_datastore_regex: ".*"
```



 

Note 注意



The VMware vCenter password has to be set in `/etc/kolla/passwords.yml`.
必须在 中 `/etc/kolla/passwords.yml` 设置 VMware vCenter 密码。

```
vmware_vcenter_host_password: "admin"
```

Options for Neutron NSX-V support:
Neutron NSX-V 支持选项：

```
vmware_nsxv_user: "nsx_manager_user"
vmware_nsxv_manager_uri: "https://127.0.0.1"
vmware_nsxv_cluster_moid: "TestCluster"
vmware_nsxv_datacenter_moid: "TestDataCeter"
vmware_nsxv_resource_pool_id: "TestRSGroup"
vmware_nsxv_datastore_id: "TestDataStore"
vmware_nsxv_external_network: "TestDVSPort-Ext"
vmware_nsxv_vdn_scope_id: "TestVDNScope"
vmware_nsxv_dvs_id: "TestDVS"
vmware_nsxv_backup_edge_pool: "service:compact:1:2"
vmware_nsxv_spoofguard_enabled: "false"
vmware_nsxv_metadata_initializer: "false"
vmware_nsxv_edge_ha: "false"
```



 

Note 注意



If you want to set secure connections to VMware, set `vmware_vcenter_insecure` to false. Secure connections to vCenter requires a CA file, copy the vCenter CA file to `/etc/kolla/config/vmware_ca`.
如果要设置与 VMware 的安全连接，请设置为 `vmware_vcenter_insecure` false。与 vCenter 的安全连接需要 CA 文件，请将 vCenter CA 文件复制到 `/etc/kolla/config/vmware_ca` 。



 

Note 注意



The VMware NSX-V password has to be set in `/etc/kolla/passwords.yml`.
必须在 中设置 `/etc/kolla/passwords.yml` VMware NSX-V 密码。

```
vmware_nsxv_password: "nsx_manager_password"
```

Then you should start **kolla-ansible** deployment normally as KVM/QEMU deployment.
然后，您应该以 KVM/QEMU 部署的形式正常启动 kolla-ansible 部署。

## VMware NSX-DVS

### Preparation 准备工作 ¶

Before deployment, you should have a working VMware vSphere environment. Create a cluster and a vSphere Distributed Switch with all the host in the cluster attached to it.
在部署之前，您应该有一个有效的 VMware vSphere 环境。创建一个群集和一个 vSphere Distributed Switch，并将群集中的所有主机都连接到该群集。

For more information, please see [Setting Up Networking with vSphere Distributed Switches](http://pubs.vmware.com/vsphere-51/index.jsp#com.vmware.vsphere.networking.doc/GUID-375B45C7-684C-4C51-BA3C-70E48DFABF04.html).
有关详细信息，请参见使用 vSphere Distributed Switch 设置网络连接。

### Deployment 部署 ¶

Enable VMware nova-compute plugin and NSX-V neutron-server plugin in `/etc/kolla/globals.yml`:
在以下情况下 `/etc/kolla/globals.yml` 启用 VMware nova-compute 插件和 NSX-V neutron-server 插件：

```
nova_compute_virt_type: "vmware"
neutron_plugin_agent: "vmware_dvs"
```

If you want to set VMware VMDK datastore as cinder backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware VMDK 数据存储设置为 cinder 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
enable_cinder: "yes"
cinder_backend_vmwarevc_vmdk: "yes"
vmware_datastore_name: "TestDatastore"
```

If you want to set VMware First Class Disk (FCD) datastore as VMware vStorage Object backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware First Class Disk （FCD） 数据存储设置为 VMware vStorage Object 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
enable_cinder: "yes"
cinder_backend_vmware_vstorage_object: "yes"
vmware_datastore_name: "TestDatastore"
```

If you want to set VMware datastore as Glance backend, enable it in `/etc/kolla/globals.yml`:
如果要将 VMware 数据存储设置为 Glance 后端，请在以下位置启用 `/etc/kolla/globals.yml` 它：

```
glance_backend_vmware: "yes"
vmware_vcenter_name: "TestDatacenter"
vmware_datastore_name: "TestDatastore"
```

VMware options are required in `/etc/kolla/globals.yml`, these options should be configured correctly according to the vSphere environment you installed before. All option for nova, cinder, glance are the same as VMware-NSX, except the following options.
VMware 选项是必需的 `/etc/kolla/globals.yml` ，应根据之前安装的 vSphere 环境正确配置这些选项。nova、cinder、glance 的所有选项都与 VMware-NSX 相同，但以下选项除外。

Options for Neutron NSX-DVS support:
Neutron NSX-DVS 支持选项：

```
vmware_dvs_host_ip: "192.168.1.1"
vmware_dvs_host_port: "443"
vmware_dvs_host_username: "admin"
vmware_dvs_dvs_name: "VDS-1"
vmware_dvs_dhcp_override_mac: ""
```



 

Note 注意



The VMware NSX-DVS password has to be set in `/etc/kolla/passwords.yml`.
必须在 中设置 `/etc/kolla/passwords.yml` VMware NSX-DVS 密码。

```
vmware_dvs_host_password: "password"
```

Then you should start **kolla-ansible** deployment normally as KVM/QEMU deployment.
然后，您应该以 KVM/QEMU 部署的形式正常启动 kolla-ansible 部署。

For more information on OpenStack vSphere, see [VMware vSphere](https://docs.openstack.org/nova/latest/admin/configuration/hypervisor-vmware.html), [VMware-NSX package](https://github.com/openstack/vmware-nsx).
有关 OpenStack vSphere 的详细信息，请参见VMware vSphere VMware-NSX 软件包。

# Zun - Container service Zun - 容器服务

​        version 版本              



“Zun is an OpenStack Container service. It aims to provide an OpenStack API for provisioning and managing containerized workload on OpenStack.” For more details about Zun, see [OpenStack Zun Documentation](https://docs.openstack.org/zun/latest/).
“Zun 是一个 OpenStack 容器服务。它旨在提供一个OpenStack API，用于在OpenStack上配置和管理容器化工作负载。有关 Zun 的更多详细信息，请参阅 OpenStack Zun 文档。

## Preparation and Deployment 准备和部署 ¶

By default Zun and its dependencies are disabled. In order to enable Zun, you need to edit globals.yml and set the following variables:
默认情况下，Zun 及其依赖项处于禁用状态。为了启用 Zun，您需要编辑globals.yml并设置以下变量：

```
enable_zun: "yes"
enable_kuryr: "yes"
enable_etcd: "yes"
docker_configure_for_zun: "yes"
containerd_configure_for_zun: "yes"
```

Docker reconfiguration requires rebootstrapping before deploy. Make sure you understand the consequences of restarting Docker. Please see [Subsequent bootstrap considerations](https://docs.openstack.org/kolla-ansible/latest/reference/deployment-and-bootstrapping/bootstrap-servers.html#rebootstrapping) for details. If it’s initial deploy, then there is nothing to worry about because it’s initial bootstrapping as well and there are no running services to affect.
Docker 重新配置需要在部署之前重新引导。确保您了解重新启动 Docker 的后果。有关详细信息，请参阅后续引导注意事项。如果是初始部署，则无需担心，因为它也是初始引导，并且没有正在运行的服务会影响。

```
kolla-ansible bootstrap-servers
```

Finally deploy: 最后部署：

```
kolla-ansible deploy
```

## Verification 验证 ¶

1. Generate the credentials file:
   生成凭据文件：

```
kolla-ansible post-deploy
```

Source credentials file:
源凭据文件：

```
. /etc/kolla/admin-openrc.sh
```

Download and create a glance container image:
下载并创建一目了然的容器映像：

```
docker pull cirros
docker save cirros | openstack image create cirros --public \
  --container-format docker --disk-format raw
```

Create zun container: 创建 zun 容器：

```
zun create --name test --net network=demo-net cirros ping -c4 8.8.8.8
```



 

Note 注意



Kuryr does not support networks with DHCP enabled, disable DHCP in the subnet used for zun containers.
Kuryr 不支持启用了 DHCP 的网络，请在用于 zun 容器的子网中禁用 DHCP。

```
openstack subnet set --no-dhcp <subnet>
```

Verify container is created:
验证容器是否已创建：

```
zun list

+--------------------------------------+------+---------------+---------+------------+------------+-------+
| uuid                                 | name | image         | status  | task_state | addresses  | ports |
+--------------------------------------+------+---------------+---------+------------+------------+-------+
| 3719a73e-5f86-47e1-bc5f-f4074fc749f2 | test | cirros        | Created | None       | 172.17.0.3 | []    |
+--------------------------------------+------+---------------+---------+------------+------------+-------+
```

Start container: 启动容器：

```
zun start test
Request to start container test has been accepted.
```

Verify container: 验证容器：

1. ```
   zun logs test
   PING 8.8.8.8 (8.8.8.8): 56 data bytes
   64 bytes from 8.8.8.8: seq=0 ttl=45 time=96.396 ms
   64 bytes from 8.8.8.8: seq=1 ttl=45 time=96.504 ms
   64 bytes from 8.8.8.8: seq=2 ttl=45 time=96.721 ms
   64 bytes from 8.8.8.8: seq=3 ttl=45 time=95.884 ms
   
   --- 8.8.8.8 ping statistics ---
   4 packets transmitted, 4 packets received, 0% packet loss
   round-trip min/avg/max = 95.884/96.376/96.721 ms
   ```

For more information about how zun works, see [zun, OpenStack Container service](https://docs.openstack.org/zun/latest/).
有关 zun 工作原理的更多信息，请参阅 zun，OpenStack 容器服务。

## Bare Metal 裸机

​        version 版本              



This section describes configuring bare metal provisioning such as Ironic.
本部分介绍如何配置裸机预配，例如 Ironic。

- [Ironic - Bare Metal provisioning
  具有讽刺意味的是 - 裸机配置](https://docs.openstack.org/kolla-ansible/latest/reference/bare-metal/ironic-guide.html)

# Ironic - Bare Metal provisioning 具有讽刺意味的是 - 裸机配置

​        version 版本              





## Overview 概述 ¶

Ironic is the OpenStack service for handling bare metal, i.e., the physical machines. It can work standalone as well as with other OpenStack services (notably, Neutron and Nova).
具有讽刺意味的是，OpenStack服务用于处理裸机，即物理机器。它可以独立工作，也可以与其他OpenStack服务（特别是Neutron和Nova）一起工作。

## Pre-deployment Configuration 部署前配置 ¶

Enable Ironic in `/etc/kolla/globals.yml`:
启用 `/etc/kolla/globals.yml` 具有讽刺意味：

```
enable_ironic: "yes"
```

In the same file, define a network interface as the default NIC for dnsmasq and define a network to be used for the Ironic cleaning network:
在同一文件中，将网络接口定义为 dnsmasq 的默认 NIC，并定义用于 Ironic 清理网络的网络：

```
ironic_dnsmasq_interface: "eth1"
ironic_cleaning_network: "public1"
```

Finally, define at least one DHCP range for Ironic inspector:
最后，为 Ironic 检查器定义至少一个 DHCP 范围：

```
ironic_dnsmasq_dhcp_ranges:
  - range: "192.168.5.100,192.168.5.110"
```

Another example of a single range with a router (multiple routers are possible by separating addresses with commas):
使用路由器的单个范围的另一个示例（通过用逗号分隔地址，可以使用多个路由器）：

```
ironic_dnsmasq_dhcp_ranges:
  - range: "192.168.5.100,192.168.5.110"
    routers: "192.168.5.1"
```

To support DHCP relay, it is also possible to define a netmask in the range. It is advisable to also provide a router to allow the traffic to reach the Ironic server.
为了支持DHCP中继，还可以在范围内定义网络掩码。建议还提供路由器以允许流量到达 Ironic 服务器。

```
ironic_dnsmasq_dhcp_ranges:
  - range: "192.168.5.100,192.168.5.110,255.255.255.0"
    routers: "192.168.5.1"
```

Multiple ranges are possible, they can be either for directly-connected interfaces or relays (if with netmask):
可以有多个范围，它们可以用于直接连接的接口或继电器（如果带有网络掩码）：

```
ironic_dnsmasq_dhcp_ranges:
  - range: "192.168.5.100,192.168.5.110"
  - range: "192.168.6.100,192.168.6.110,255.255.255.0"
    routers: "192.168.6.1"
```

The default lease time for each range can be configured globally via `ironic_dnsmasq_dhcp_default_lease_time` variable or per range via `lease_time` parameter.
每个范围的默认租用时间可以通过 `ironic_dnsmasq_dhcp_default_lease_time` 变量全局配置，也可以通过 `lease_time` 参数按范围进行配置。

In the same file, specify the PXE bootloader file for Ironic Inspector. The file is relative to the `/var/lib/ironic/tftpboot` directory. The default is `pxelinux.0`, and should be correct for x86 systems. Other platforms may require a different value, for example aarch64 on Debian requires `debian-installer/arm64/bootnetaa64.efi`.
在同一文件中，指定 Ironic Inspector 的 PXE 引导加载程序文件。该文件是相对于目录的 `/var/lib/ironic/tftpboot` 。默认值为 `pxelinux.0` ，对于 x86 系统应正确。其他平台可能需要不同的值，例如 Debian 上的 aarch64 需要 `debian-installer/arm64/bootnetaa64.efi` .

```
ironic_dnsmasq_boot_file: pxelinux.0
```

Ironic inspector also requires a deploy kernel and ramdisk to be placed in `/etc/kolla/config/ironic/`. The following example uses coreos which is commonly used in Ironic deployments, though any compatible kernel/ramdisk may be used:
具有讽刺意味的是，检查器还需要将部署内核和 ramdisk 放置在 `/etc/kolla/config/ironic/` .以下示例使用 Ironic 部署中常用的 coreos，尽管可以使用任何兼容的内核/ramdisk：

```
curl https://tarballs.opendev.org/openstack/ironic-python-agent/dib/files/ipa-centos9-master.kernel \
  -o /etc/kolla/config/ironic/ironic-agent.kernel
curl https://tarballs.opendev.org/openstack/ironic-python-agent/dib/files/ipa-centos9-master.initramfs \
  -o /etc/kolla/config/ironic/ironic-agent.initramfs
```

You may optionally pass extra kernel parameters to the inspection kernel using:
您可以选择使用以下命令将额外的内核参数传递给检查内核：

```
ironic_inspector_kernel_cmdline_extras: ['ipa-lldp-timeout=90.0', 'ipa-collect-lldp=1']
```

in `/etc/kolla/globals.yml`. 在 `/etc/kolla/globals.yml` .

## Configure conductor’s HTTP server port (optional) 配置 conductor 的 HTTP 服务器端口（可选） ¶

The port used for conductor’s HTTP server is controlled via `ironic_http_port` in `/etc/kolla/globals.yml`:
用于导体的HTTP服务器的端口通过 `ironic_http_port` 以下方式 `/etc/kolla/globals.yml` 进行控制：

```
ironic_http_port: "8089"
```

## Revert to plain PXE (not recommended) 恢复为普通 PXE（不推荐） ¶

Starting with Yoga, Ironic has changed the default PXE from plain PXE to iPXE. Kolla Ansible follows this upstream decision by choosing iPXE as the default for Ironic Inspector but allows users to revert to the previous default of plain PXE by setting the following in `/etc/kolla/globals.yml`:
从 Yoga 开始，Ironic 已将默认 PXE 从普通 PXE 更改为 iPXE。Kolla Ansible 遵循这一上游决策，选择 iPXE 作为 Ironic Inspector 的默认值，但允许用户通过在以下 `/etc/kolla/globals.yml` 设置中恢复为以前的默认值 plain PXE：

```
ironic_dnsmasq_serve_ipxe: "no"
```

To revert Ironic to previous default as well, set `pxe` as `default_boot_interface` in `/etc/kolla/config/ironic.conf`:
要将 Ironic 也恢复到以前的默认值，请按以下 `/etc/kolla/config/ironic.conf` 方式 `default_boot_interface` 设置 `pxe` ：

```
[DEFAULT]
default_boot_interface = pxe
```

## Attach ironic to external keystone (optional) 将讽刺附加到外部梯形失真（可选） ¶

In [multi-regional](https://docs.openstack.org/kolla-ansible/latest/user/multi-regions.html) deployment keystone could be installed in one region (let’s say region 1) and ironic - in another region (let’s say region 2). In this case we don’t install keystone together with ironic in region 2, but have to configure ironic to connect to existing keystone in region 1. To deploy ironic in this way we have to set variable `enable_keystone` to `"no"`.
在多区域部署中，keystone 可以安装在一个区域（比如说区域 1）中，具有讽刺意味的是，可以安装在另一个区域（比如说区域 2）中。在本例中，我们不会在区域 2 中将  keystone 与 ironic 一起安装，但必须配置 ironic 以连接到区域 1 中的现有  keystone。要以这种方式部署具有讽刺意味的，我们必须将变量 `enable_keystone` 设置为 `"no"` 。

```
enable_keystone: "no"
```

It will prevent keystone from being installed in region 2.
它将阻止在区域 2 中安装梯形失真。

To add keystone-related sections in ironic.conf, it is also needed to set variable `ironic_enable_keystone_integration` to `"yes"`
要在 ironic.conf 中添加与 keystone 相关的部分，还需要将变量 `ironic_enable_keystone_integration` 设置为 `"yes"` 

```
ironic_enable_keystone_integration: "yes"
```

## Deployment 部署 ¶

Run the deploy as usual:
像往常一样运行部署：

```
kolla-ansible deploy
```

## Post-deployment configuration 部署后配置 ¶

The [Ironic documentation](https://docs.openstack.org/ironic/latest/install/configure-glance-images) describes how to create the deploy kernel and ramdisk and register them with Glance. In this example we’re reusing the same images that were fetched for the Inspector:
Ironic文档介绍了如何创建部署内核和虚拟硬盘，并在Glance中注册。在此示例中，我们将重用为 Inspector 获取的相同图像：

```
openstack image create --disk-format aki --container-format aki --public \
  --file /etc/kolla/config/ironic/ironic-agent.kernel deploy-vmlinuz

openstack image create --disk-format ari --container-format ari --public \
  --file /etc/kolla/config/ironic/ironic-agent.initramfs deploy-initrd
```

The [Ironic documentation](https://docs.openstack.org/ironic/latest/install/configure-nova-flavors) describes how to create Nova flavors for bare metal.  For example:
Ironic 文档描述了如何为裸机创建 Nova 风味。例如：

```
openstack flavor create my-baremetal-flavor \
  --ram 512 --disk 1 --vcpus 1 \
  --property resources:CUSTOM_BAREMETAL_RESOURCE_CLASS=1 \
  --property resources:VCPU=0 \
  --property resources:MEMORY_MB=0 \
  --property resources:DISK_GB=0
```

The [Ironic documentation](https://docs.openstack.org/ironic/latest/install/enrollment) describes how to enroll baremetal nodes and ports.  In the following example ensure to substitute correct values for the kernel, ramdisk, and MAC address for your baremetal node.
Ironic 文档介绍了如何注册裸机节点和端口。在以下示例中，请确保将裸机节点的内核、虚拟磁盘和 MAC 地址替换为正确的值。

```
openstack baremetal node create --driver ipmi --name baremetal-node \
  --driver-info ipmi_port=6230 --driver-info ipmi_username=admin \
  --driver-info ipmi_password=password \
  --driver-info ipmi_address=192.168.5.1 \
  --resource-class baremetal-resource-class --property cpus=1 \
  --property memory_mb=512 --property local_gb=1 \
  --property cpu_arch=x86_64 \
  --driver-info deploy_kernel=15f3c95f-d778-43ad-8e3e-9357be09ca3d \
  --driver-info deploy_ramdisk=9b1e1ced-d84d-440a-b681-39c216f24121

openstack baremetal port create 52:54:00:ff:15:55 \
  --node 57aa574a-5fea-4468-afcf-e2551d464412 \
  --physical-network physnet1
```

Make the baremetal node available to nova:
使裸机节点可供 nova 使用：

```
openstack baremetal node manage 57aa574a-5fea-4468-afcf-e2551d464412
openstack baremetal node provide 57aa574a-5fea-4468-afcf-e2551d464412
```

It may take some time for the node to become available for scheduling in nova. Use the following commands to wait for the resources to become available:
节点可能需要一些时间才能在 nova 中可用于调度。使用以下命令等待资源变为可用：

```
openstack hypervisor stats show
openstack hypervisor show 57aa574a-5fea-4468-afcf-e2551d464412
```

## Booting the baremetal 引导裸机 ¶

Assuming you have followed the examples above and created the demo resources as shown in the [Quick Start for deployment/evaluation](https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html), you can now use the following example command to boot the baremetal instance:
假设您已经按照上面的示例创建了演示资源，如部署/评估快速入门中所示，您现在可以使用以下示例命令来启动裸机实例：

```
openstack server create --image cirros --flavor my-baremetal-flavor \
  --key-name mykey --network public1 demo1
```

In other cases you will need to adapt the command to match your environment.
在其他情况下，您需要调整命令以匹配您的环境。

## Notes 注意事项 ¶

### Debugging DHCP 调试 DHCP ¶

The following tcpdump command can be useful when debugging why dhcp requests may not be hitting various pieces of the process:
以下 tcpdump 命令在调试 dhcp 请求可能未命中进程的各个部分的原因时非常有用：

```
tcpdump -i <interface> port 67 or port 68 or port 69 -e -n
```

### Configuring the Web Console 配置 Web 控制台 ¶

Configuration based off upstream [Node web console](https://docs.openstack.org/ironic/latest/admin/console.html#node-web-console).
基于上游节点 Web 控制台的配置。

Serial speed must be the same as the serial configuration in the BIOS settings. Default value: 115200bps, 8bit, non-parity.If you have different serial speed.
串行速度必须与 BIOS 设置中的串行配置相同。默认值：115200bps，8位，非奇偶校验。如果您的串行速度不同。

Set ironic_console_serial_speed in `/etc/kolla/globals.yml`:
将ironic_console_serial_speed设置在 `/etc/kolla/globals.yml` ：

```
ironic_console_serial_speed: 9600n8
```

### Deploying using virtual baremetal (vbmc + libvirt) 使用虚拟裸机（vbmc + libvirt）进行部署 ¶

See https://brk3.github.io/post/kolla-ironic-libvirt/
请参阅 https://brk3.github.io/post/kolla-ironic-libvirt/

​                      