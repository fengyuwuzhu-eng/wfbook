# Production architecture guide 生产体系结构指南

​        version 版本              



This guide will help with configuring Kolla to suit production needs. It is meant to answer some questions regarding basic configuration options that Kolla requires. This document also contains other useful pointers.
本指南将帮助配置 Kolla 以满足生产需求。它旨在回答有关 Kolla 所需的基本配置选项的一些问题。本文档还包含其他有用的指针。

## Node types and services running on them 节点类型和运行在节点上的服务 ¶

A basic Kolla inventory consists of several types of nodes, known in Ansible as `groups`.
基本的 Kolla 清单由几种类型的节点组成，在 Ansible 中称为 `groups` .

- Control - Cloud controller nodes which host control services like APIs and databases. This group should have odd number of nodes for quorum.
  控制 - 托管控制 API 和数据库等服务的云控制器节点。此组应具有奇数个节点作为仲裁。
- Network - Network nodes host Neutron agents along with haproxy / keepalived. These nodes will have a floating ip defined in `kolla_internal_vip_address`.
  网络 - 网络节点承载 Neutron 代理以及 haproxy / keepalived。这些节点将具有在 中定义的 `kolla_internal_vip_address` 浮动 IP。
- Compute - Compute nodes for compute services. This is where guest VMs live.
  计算 - 用于计算服务的计算节点。这是来宾 VM 所在的位置。
- Storage - Storage nodes for cinder-volume, LVM or Swift.
  存储 - cinder-volume、LVM 或 Swift 的存储节点。
- Monitoring - Monitor nodes which host monitoring services.
  监视 - 监视托管监视服务的节点。

## Network configuration 网络配置 ¶



### Interface configuration 接口配置 ¶

In Kolla operators should configure following network interfaces:
在 Kolla 中，操作员应配置以下网络接口：

- `network_interface` - While it is not used on its own, this provides the required default for other interfaces below.
   `network_interface` - 虽然它不能单独使用，但这为下面的其他接口提供了所需的默认值。
- `api_interface` - This interface is used for the management network. The management network is the network OpenStack services uses to communicate to each other and the databases. There are known security risks here, so it’s recommended to make this network internal, not accessible from outside. Defaults to `network_interface`.
   `api_interface` - 此接口用于管理网络。管理网络是 OpenStack 服务用来相互通信以及与数据库通信的网络。这里存在已知的安全风险，因此建议将此网络设为内部网络，无法从外部访问。默认值为 `network_interface` 。
- `kolla_external_vip_interface` - This interface is public-facing one. It’s used when you want HAProxy public endpoints to be exposed in different network than internal ones. It is mandatory to set this option when `kolla_enable_tls_external` is set to yes. Defaults to `network_interface`.
   `kolla_external_vip_interface` - 此界面是面向公众的界面。当您希望 HAProxy 公共终结点在与内部终结点不同的网络中公开时，会使用它。当设置为“是”时 `kolla_enable_tls_external` ，必须设置此选项。默认值为 `network_interface` 。
- `swift_storage_interface` - This interface is used by Swift for storage access traffic.  This can be heavily utilized so it’s recommended to use a high speed network fabric. Defaults to `network_interface`.
   `swift_storage_interface` - Swift 使用此接口处理存储访问流量。这可以被大量使用，因此建议使用高速网络结构。默认值为 `network_interface` 。
- `swift_replication_interface` - This interface is used by Swift for storage replication traffic.  This can be heavily utilized so it’s recommended to use a high speed network fabric. Defaults to `swift_storage_interface`.
   `swift_replication_interface` - Swift 使用此接口处理存储复制流量。这可以被大量使用，因此建议使用高速网络结构。默认值为 `swift_storage_interface` 。
- `tunnel_interface` - This interface is used by Neutron for vm-to-vm traffic over tunneled networks (like VxLan). Defaults to `network_interface`.
   `tunnel_interface` - Neutron 使用此接口通过隧道网络（如 VxLan）处理虚拟机到虚拟机的流量。默认值为 `network_interface` 。
- `neutron_external_interface` - This interface is required by Neutron. Neutron will put br-ex on it. It will be used for flat networking as well as tagged vlan networks. Has to be set separately.
   `neutron_external_interface` - 此接口是 Neutron 所必需的。中子会把br-ex放在上面。它将用于扁平网络以及标记的 VLAN 网络。必须单独设置。
- `dns_interface` - This interface is required by Designate and Bind9. Is used by public facing DNS requests and queries to bind9 and designate mDNS services. Defaults to `network_interface`.
   `dns_interface` - 此接口是 Designate 和 Bind9 所必需的。由面向公众的 DNS 请求和查询用于绑定 9 和指定 mDNS 服务。默认值为 `network_interface` 。
- `bifrost_network_interface` - This interface is required by Bifrost. Is used to provision bare metal cloud hosts, require L2 connectivity with the bare metal cloud hosts in order to provide DHCP leases with PXE boot options. Defaults to `network_interface`.
   `bifrost_network_interface` - Bifrost 需要此接口。用于置备裸机云主机，需要与裸机云主机建立 L2 连接，以便为 DHCP 租约提供 PXE 引导选项。默认值为 `network_interface` 。



 

Warning 警告



Ansible facts does not recognize interface names containing dashes, in example `br-ex` or `bond-0` cannot be used because ansible will read them as `br_ex` and `bond_0` respectively.
例如 `br-ex` ，Ansible 事实无法识别包含破折号的接口名称，或者 `bond-0` 不能使用，因为 ansible 会 `bond_0` 分别读取 `br_ex` 它们。



### Address family configuration (IPv4/IPv6) 地址系列配置 （IPv4/IPv6） ¶

Starting with the Train release, Kolla Ansible allows operators to deploy the control plane using IPv6 instead of IPv4. Each Kolla Ansible network (as represented by interfaces) provides a choice of two address families. Both internal and external VIP addresses can be configured using an IPv6 address as well. IPv6 is tested on all supported platforms.
从 Train 版本开始，Kolla Ansible 允许运营商使用 IPv6 而不是 IPv4 部署控制平面。每个 Kolla Ansible  网络（由接口表示）都提供两个地址系列的选择。内部和外部 VIP 地址也可以使用 IPv6 地址进行配置。IPv6  在所有受支持的平台上进行了测试。



 

Warning 警告



While Kolla Ansible Train requires Ansible 2.6 or later, IPv6 support requires Ansible 2.8 or later due to a bug: https://github.com/ansible/ansible/issues/63227
虽然 Kolla Ansible Train 需要 Ansible 2.6 或更高版本，但 IPv6 支持需要 Ansible 2.8  或更高版本，因为存在以下错误： https://github.com/ansible/ansible/issues/63227



 

Note 注意



Currently there is no dual stack support. IPv4 can be mixed with IPv6 only when on different networks. This constraint arises from services requiring common single address family addressing.
目前没有双栈支持。IPv4 只能在不同的网络上与 IPv6 混合使用。此约束源于需要通用单地址系列寻址的服务。

For example, `network_address_family` accepts either `ipv4` or `ipv6` as its value and defines the default address family for all networks just like `network_interface` defines the default interface. Analogically, `api_address_family` changes the address family for the API network. Current listing of networks is available in `globals.yml` file.
例如，接受 or 作为其值， `network_address_family` 并定义所有网络的默认地址族，就像定义默认接口一样 `network_interface` 。 `ipv4` `ipv6` 类似地， `api_address_family` 更改 API 网络的地址系列。当前网络列表可在 `globals.yml` 文件中找到。



 

Note 注意



While IPv6 support introduced in Train is broad, some services are known not to work yet with IPv6 or have some known quirks:
虽然 Train 中引入的 IPv6 支持范围很广，但已知某些服务尚不适用于 IPv6，或者有一些已知的怪癖：

- Bifrost does not support IPv6: https://storyboard.openstack.org/#!/story/2006689
  Bifrost 不支持 IPv6：https://storyboard.openstack.org/#!/story/2006689
- Docker does not allow IPv6 registry address: https://github.com/moby/moby/issues/39033 - the workaround is to use the hostname
  Docker 不允许 IPv6 注册表地址：https://github.com/moby/moby/issues/39033 - 解决方法是使用主机名
- Ironic DHCP server, dnsmasq, is not currently automatically configured to offer DHCPv6: https://bugs.launchpad.net/kolla-ansible/+bug/1848454
  具有讽刺意味的是，DHCP 服务器 dnsmasq 当前未自动配置为提供 DHCPv6：https://bugs.launchpad.net/kolla-ansible/+bug/1848454

## Docker configuration Docker 配置 ¶

Because Docker is core dependency of Kolla, proper configuration of Docker can change the experience of Kolla significantly. Following section will highlight several Docker configuration details relevant to Kolla operators.
由于 Docker 是 Kolla 的核心依赖项，因此正确配置 Docker 可以显著改变 Kolla 的体验。以下部分将重点介绍与 Kolla 运算符相关的几个 Docker 配置详细信息。

### Storage driver 存储驱动 ¶

While the default storage driver should be fine for most users, Docker offers more options to consider. For details please refer to [Docker documentation](https://docs.docker.com/engine/userguide/storagedriver/selectadriver/).
虽然默认存储驱动程序对于大多数用户来说应该没问题，但 Docker 提供了更多选项供考虑。有关详细信息，请参阅 Docker 文档。

### Volumes 卷 ¶

Kolla puts nearly all of persistent data in Docker volumes. These volumes are created in Docker working directory, which defaults to `/var/lib/docker` directory.
Kolla 将几乎所有的持久性数据都放在 Docker 卷中。这些卷是在 Docker 工作目录中创建的，该目录默认为 `/var/lib/docker` directory。

We recommend to ensure that this directory has enough space and is placed on fast disk as it will affect performance of builds, deploys as well as database commits and rabbitmq.
我们建议确保此目录具有足够的空间并放置在快速磁盘上，因为它会影响构建、部署以及数据库提交和 rabbitmq 的性能。

This becomes especially relevant when `enable_central_logging` and `openstack_logging_debug` are both set to true, as fully loaded 130 node cluster produced 30-50GB of logs daily.
当两者都 `openstack_logging_debug` 设置为 true 时 `enable_central_logging` ，这一点尤其重要，因为满载的 130 节点集群每天产生 30-50GB 的日志。

## High Availability (HA) and scalability 高可用性 （HA） 和可扩展性 ¶

HA is an important topic in production systems. HA concerns itself with redundant instances of services so that the overall service can be provided with close-to-zero interruption in case of failure. Scalability often works hand-in-hand with HA to provide load sharing by the use of load balancers.
HA 是生产系统中的一个重要主题。HA 关注的是冗余的服务实例，以便在发生故障时可以提供接近于零中断的整体服务。可伸缩性通常与 HA 携手合作，通过使用负载均衡器提供负载共享。

### OpenStack services OpenStack 服务 ¶

Multinode Kolla Ansible deployments provide HA and scalability for services. OpenStack API endpoints are a prime example here: redundant `haproxy` instances provide HA with `keepalived` while the backends are also deployed redundantly to enable both HA and load balancing.
多节点 Kolla Ansible 部署为服务提供 HA 和可扩展性。OpenStack API 端点就是一个很好的例子：冗余 `haproxy` 实例提供 HA， `keepalived` 而后端也以冗余方式部署，以实现 HA 和负载均衡。

### Other core services 其他核心服务 ¶

The core non-OpenStack components required by most deployments: the SQL database provided by `mariadb` and message queue provided by `rabbitmq` are also deployed in a HA way. Care has to be taken, however, as unlike previously described services, these have more complex HA mechanisms. The reason for that is that they provide the central, persistent storage of information about the cloud that each other service assumes to have a consistent state (aka integrity). This assumption leads to the requirement of quorum establishment (look up the CAP theorem for greater insight).
大多数部署所需的非 OpenStack 核心组件：SQL `mariadb` 数据库提供的 SQL 数据库和消息 `rabbitmq` 队列提供的 SQL 数据库也是以 HA 方式部署的。然而，必须小心，因为与前面描述的服务不同，这些服务具有更复杂的 HA  机制。这样做的原因是，它们提供了有关云的集中、持久的信息存储，每个服务都假定这些信息具有一致的状态（又称完整性）。这个假设导致了建立法定人数的要求（查找 CAP 定理以获得更深入的见解）。

Quorum needs a majority vote and hence deploying 2 instances of these do not provide (by default) any HA as a failure of one causes a failure of the other one. Hence the recommended number of instances is `3`, where 1 node failure is acceptable. For scaling purposes and better resilience it is possible to use `5` nodes and have 2 failures acceptable. Note, however, that higher numbers usually provide no benefits due to amount of communication between quorum members themselves and the non-zero probability of the communication medium failure happening instead.
仲裁需要多数票，因此部署其中的 2 个实例不会提供（默认情况下）任何 HA，因为一个实例的失败会导致另一个实例的失败。因此，建议的实例数为 `3` ，其中 1 个节点故障是可以接受的。为了扩展目的和更好的弹性，可以使用 `5` 节点并接受 2 个故障。但是请注意，由于仲裁成员本身之间的通信量以及通信介质故障发生的概率不为零，较高的数字通常不会带来任何好处。