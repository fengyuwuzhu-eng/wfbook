# 安装

vLLM 支持以下硬件平台：

GPU

- NVIDIA CUDA
- AMD ROCm
- Intel XPU

CPU

- Intel / AMD x86
- ARM AArch64
- Apple silicon

其他 AI 加速器

- Google TPU
- Intel Gaudi
- AWS Neuron
- OpenVINO

## GPU

vLLM 是一个支持如下 GPU 类型的 Python 库，根据您的 GPU 型号查看相应的说明。

* NVIDIA CUDA

  vLLM 包含预编译的 C++ 和 CUDA (12.1) 二进制库。

  * GPU：算力 7.0 及以上（如 V100、T4、RTX20xx、A100、L4、H100 等）

* AMD ROCm

  vLLM 支持拥有 ROCm 6.3 的 AMD GPU。

  **注意：** 该设备没有预构建的轮子，所以您必须使用预构建的 Docker 镜像或者从源代码构建 vLLM。

  - GPU: MI200s (gfx90a)、MI300 (gfx942)、Radeon RX 7900 系列 (gfx1100)
  - ROCm 6.3

* Inter XPU

  vLLM 最初在 Intel GPU 平台上支持基础的模型推理和服务。

  **注意：** 该设备没有预构建的轮子或映像，所以必须从源代码构建 vLLM。

  - 支持硬件：Intel Data Center GPU、Intel ARC GPU
  - OneAPI 2024.2

## 需求

- 系统：Linux
- Python: 3.9 – 3.12

## 使用 Python 安装

### 创建 1 个新 Python 环境

可以使用 conda 创建 1 个新的 Python 环境：

```bash
conda create -n vllm python=3.12 -y
conda activate vllm
```

> **注意：** 
>
> [PyTorch 已](https://github.com/pytorch/pytorch/issues/138506)[弃用](https://github.com/pytorch/pytorch/issues/138506)[该 conda ](https://github.com/pytorch/pytorch/issues/138506)[发布](https://github.com/pytorch/pytorch/issues/138506)[频道](https://github.com/pytorch/pytorch/issues/138506)。如果使用 conda，请仅使用它创建 Python 环境，而不要用它安装包。

或者可使用 [uv](https://docs.astral.sh/uv/) 创建 Python 环境，uv 是一个非常快速的 Python 环境管理器。请依照[该文档](https://docs.astral.sh/uv/#getting-started)安装 uv。安装 uv 以后，可以使用以下命令创建新的 Python 环境：

```bash
# (推荐) 创建一个新的 uv 环境。使用 `--seed` 在环境中安装 `pip` 和 `setuptools`。
uv venv vllm --python 3.12 --seed
source vllm/bin/activate
```

#### NVIDIA CUDA

> **注意：** 通过 conda 安装的 PyTorch 会静态链接 NCCL 库，这会导致当 vLLM 尝试使用 NCCL 时出错。

为了实现高性能，vLLM 需要编译多个 cuda 内核。然而，这一编译过程会导致与其他 CUDA 版本和 PyTorch 版本的二进制不兼容问题。即便是在相同版本的 PyTorch 中，不同的构建配置也可能引发此类不兼容性。

因此，建议使用**全新的** conda 环境安装 vLLM。如果有不同的 CUDA 版本，或者想要使用现有的 PyTorch 安装，则需要从源代码构建 vLLM 。

#### AMD ROCm

对于该设备没有关于创建新 Python 环境的额外信息。

#### Inter XPU

对于该设备没有关于创建新 Python 环境的额外信息。

### 预构建安装包

#### NVIDIA CUDA

可以使用 pip 或 uv pip 安装 vLLM。

```bash
# 安装使用 CUDA 12.1 的 vLLM
pip install vllm
uv pip install vllm
```

目前，vLLM 的二进制文件默认使用 CUDA 12.1 和公开发布的 PyTorch 版本进行编译。还提供使用 CUDA 12.1、11.8 和公开发布的 PyTorch 版本编译的 vLLM 二进制文件：

```bash
# 安装使用 CUDA 11.8 的vLLM
export VLLM_VERSION=0.6.1.post1
export PYTHON_VERSION=310
pip install https://github.com/vllm-project/vllm/releases/download/v${VLLM_VERSION}/vllm-${VLLM_VERSION}+cu118-cp${PYTHON_VERSION}-cp${PYTHON_VERSION}-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118
```

##### 安装最新代码

LLM 推理是一个快速发展的领域，最新代码可能包含错误修复、性能改进和尚未发布的新功能。为了让用户在下一个版本发布前就能体验到最新的代码，vLLM  为运行在 x86 架构上的 Linux 系统提供了 CUDA 12 的预编译包（wheel），这些预编译包覆盖了从 v0.5.3  版本开始的每一次代码提交。

##### 使用 pip 安装最新代码

```bash
pip install vllm --pre --extra-index-url https://wheels.vllm.ai/nightly
```

在使用 pip 安装时，必须加上 --pre 参数，这样 pip 才会考虑安装预发布版本。

如果你想获取先前提交的安装包（例如，用于分析行为变化或性能回退），由于 pip 的限制，你需要通过在 URL 中嵌入提交哈希值来指定轮子文件的完整 URL：

```bash
export VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch
pip install https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
```

请注意，这些安装包是使用 Python 3.8 ABI 构建的（有关 ABI 的更多详情，请参阅 [PEP 425](https://peps.python.org/pep-0425/)），因此它们兼容 Python 3.8 及更高版本。安装包文件名中的版本号 (1.0.0.dev) 只是一个占位符，用于提供统一的  URL，实际的安装包版本信息包含在安装包的元数据中（在额外索引 URL 中列出的安装包具有正确的版本号）。尽管我们不再支持 Python  3.8（因为 PyTorch 2.5 已经停止对 Python 3.8 的支持），但这些安装包仍然使用 Python 3.8 ABI  构建，以保持与之前相同的安装包名称。

##### 使用 uv 安装最新代码

另一种安装最新代码的方法是使用 uv：

```bash
uv pip install vllm --extra-index-url https://wheels.vllm.ai/nightly
```

如果你想获取先前提交的 wheels 安装包（例如，用于分析行为变化或性能回退），可以在 URL 中指定提交哈希值：

```bash
export VLLM_COMMIT=72d9c316d3f6ede485146fe5aabd4e61dbc59069 # use full commit hash from the main branch
uv pip install vllm --extra-index-url https://wheels.vllm.ai/${VLLM_COMMIT}
```

uv 方法适用于 vLLM v0.6.6 及更高版本，并提供了一条易于记忆的命令。uv 的一个独特特性是，来自 --extra-index-url 的软件包[优先级高于默认索引](https://docs.astral.sh/uv/pip/compatibility/#packages-that-exist-on-multiple-indexes)中的软件包。 例如，如果最新的公开发布版本是 v0.6.6.post1，uv 允许通过指定 --extra-index-url 安装 v0.6.6.post1 之前的某个提交版本。

相比之下，pip 会合并 --extra-index-url 和默认索引中的软件包，并仅选择最新版本，这使得安装早于已发布版本的开发版本变得困难。

#### AMD ROCm

目前没有预构建的 ROCm wheels 安装包。

#### Inter XPU

目前没有预构建的 XPU wheels 安装包。

### 从源码构建安装包 (wheel)

#### NVIDIA CUDA

##### 使用仅限 Python 的构建方式（无需编译）

如果您只需要修改 Python 代码，则可以在不进行编译的情况下构建并安装 vLLM。使用 `pip` 的 `--editable`[ 标志](https://pip.pypa.io/en/stable/topics/local-project-installs/#editable-installs)，您对代码的更改将在运行 vLLM 时生效：

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
VLLM_USE_PRECOMPILED=1 pip install --editable .
```

该命令将执行以下操作：

1. 在您的 vLLM 克隆中查找当前分支。
2. 确定主分支中对应的基础提交。
3. 下载该基础提交的预构建 wheel 文件。
4. 在安装过程中使用其已编译的库文件。

> **注意** 如果您修改了 C++ 或内核代码，则无法使用仅限 Python 的构建方式，否则会出现「找不到库 (library not found)」或「未定义符号 (undefined symbol)」的导入错误。 如果您对开发分支进行了 rebase，建议卸载 vLLM 并重新运行上述命令，以确保您的库文件是最新的。

如果运行上述命令时出现「找不到 wheel (the wheel not found)」错误，可能是因为您基于的主分支提交刚刚合并，而 wheel 文件仍在构建中。在这种情况下，您可以等待大约一小时后再尝试，或者手动指定先前的提交哈希值，并使用 `VLLM_PRECOMPILED_WHEEL_LOCATION` 环境变量进行安装。

```bash
export VLLM_COMMIT=72d9c316d3f6ede485146fe5aabd4e61dbc59069 # 使用主分支中的完整提交哈希值
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
pip install --editable .
```

您可以在[安装最新代码](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html?device=cuda#install-the-latest-code)中找到更多关于 vLLM wheel 文件的信息。

> **注意** 您的源代码提交 ID 可能与最新的 vLLM wheel 文件不同，这可能会导致未知错误。建议您使用与安装的 vLLM wheel 相同的提交 ID 进行构建。请参考「安装最新代码」部分了解如何安装指定的 wheel 文件。

##### 完整构建（包含编译）

如果您需要修改 C++ 或 CUDA 代码，则需要从源代码构建 vLLM。此过程可能需要几分钟时间：

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

> **提示**
>
> 从源代码构建需要大量编译过程。如果您需要多次从源代码构建，建议缓存编译结果，以提高效率。
>
> 例如，您可以使用 `conda install ccache` 或 `apt install ccache` 安装 [ccache](https://github.com/ccache/ccache)。只要 `which ccache` 命令可以找到 `ccache` 可执行文件，构建系统就会自动使用它。首次构建完成后，后续构建将会快得多。
>
> [sccache](https://github.com/mozilla/sccache) 的工作方式与 `ccache` 类似，但可以在远程存储环境中进行缓存。您可以设置以下环境变量来配置 vLLM 的 `sccache` 远程缓存:`SCCACHE_BUCKET=vllm-build-sccache SCCACHE_REGION=us-west-2 SCCACHE_S3_NO_CREDENTIALS=1`。我们还建议设置 `SCCACHE_IDLE_TIMEOUT=0`。

##### 使用现有 PyTorch 安装

某些情况下，PyTorch 依赖无法通过 pip 安装，例如：

- 使用 PyTorch nightly 版本或自定义 PyTorch 构建版本来编译 vLLM
- 在 aarch64 架构且支持 CUDA（GH200）的环境下编译 vLLM（PyPI 未提供对应 PyTorch 预编译包）。目前仅 PyTorch nightly 版本提供 aarch64 架构的 CUDA 预编译包。可通过运行  `pip3 install --pre torch torchvision torchaudio --index-url  [https://download.pytorch.org/whl/nightly/cu124](https://download.pytorch.org/whl/nightly/cu124)` 安装 PyTorch nightly 版本，然后在其基础上编译 vLLM

使用现有 PyTorch 安装编译 vLLM 的步骤：

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
python use_existing_torch.py
pip install -r requirements-build.txt
pip install -e . --no-build-isolation
```

##### 使用本地 cutlass 编译

当前 vLLM 默认从 GitHub 获取 cutlass 代码进行编译。若需使用本地 cutlass 版本，可通过设置环境变量  `VLLM_CUTLASS_SRC_DIR` 指定本地 cutlass 目录：

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
VLLM_CUTLASS_SRC_DIR=/path/to/cutlass pip install -e .
```

##### 故障排除

为避免系统负载过高，可通过  `MAX_JOBS` 环境变量限制并行编译任务数。例如：

```bash
export MAX_JOBS=6
pip install -e .
```

该设置对低性能机器尤为重要。例如在 WSL 环境下（[默认仅分配 50% 总内存](https://learn.microsoft.com/en-us/windows/wsl/wsl-config#main-wsl-settings)），使用  `export MAX_JOBS=1` 可避免因并行编译导致内存不足。副作用是编译时间显著延长。

此外，如果你在构建 vLLM 时遇到问题，建议使用 NVIDIA PyTorch Docker 镜像解决编译问题：

```bash
# Use `--ipc=host` to make sure the shared memory is large enough.
# 使用 --ipc=host 确保共享内存容量充足
docker run --gpus all -it --rm --ipc=host nvcr.io/nvidia/pytorch:23.10-py3
```

如果你不想使用 Docker，建议完整安装 CUDA Toolkit。你可以从[官方网站](https://developer.nvidia.com/cuda-toolkit-archive)下载并安装。安装后，需设置 `CUDA_HOME` 环境变量为 CUDA Toolkit 的安装路径，并确保 `nvcc` 编译器在 `PATH` 中，例如：

```bash
export CUDA_HOME=/usr/local/cuda
export PATH="${CUDA_HOME}/bin:$PATH"
```

可以使用以下命令检查 CUDA Toolkit 是否正确安装：

```bash
nvcc --version # verify that nvcc is in your PATH # 检查 nvcc 是否在 PATH 中
${CUDA_HOME}/bin/nvcc --version # verify that nvcc is in your CUDA_HOME
```

##### 不支持的操作系统构建

vLLM 仅能在 Linux 系统上完整运行，但对于开发目的，你仍然可以在其他操作系统（例如 macOS）上构建 vLLM，使其能够被导入，从而提供更便捷的开发环境。但请注意，vLLM 在非 Linux 系统上不会编译二进制文件，因此无法运行推理任务。

在安装前，可以禁用 `VLLM_TARGET_DEVICE` 变量，如下所示：

```bash
export VLLM_TARGET_DEVICE=empty
pip install -e .
```

#### AMD ROCm

1. 安装依赖（如果你已经处于一个已安装以下内容的环境中或 Docker 容器中，则可以跳过此步骤）:

- [ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/index.html)
- [PyTorch](https://pytorch.org/)

对于安装 PyTorch，您可以从 1 个新的 docker 镜像开始，例如 `rocm/pytorch:rocm6.3_ubuntu24.04_py3.12_pytorch_release_2.4.0`、`rocm/pytorch-nightly`。如果你正在使用 docker 镜像，跳到步骤 3。

或者，你可以使用 PyTorch wheels 安装 PyTorch，在 PyTorch [入门指南](https://pytorch.org/get-started/locally/) 中的 PyTorch 安装指南可以查看相关信息。

```plain
pip uninstall torch -y
pip install --no-cache-dir --pre torch --index-url https://download.pytorch.org/whl/rocm6.3
```

1. 安装 [Triton flash attention for ROCm](https://github.com/ROCm/triton)

按照 [ROCm/triton](https://github.com/ROCm/triton/blob/triton-mlir/README.md) 的说明安装 ROCm 的 Triton flash attention（默认 triton-mlir 分支）

```plain
python3 -m pip install ninja cmake wheel pybind11
pip uninstall -y triton
git clone https://github.com/OpenAI/triton.git
cd triton
git checkout e5be006
cd python
pip3 install .
cd ../..
```

> **注意** 如果在构建 triton 期间你遇见了有关下载包的 HTTP 问题，请再次尝试，因为 HTTP 问题是间歇性的。

1. 或者，如果您选择使用 CK flash Attention，您可以安装 [flash Attention for ROCm](https://github.com/ROCm/flash-attention/tree/ck_tile)。

按照 [ROCm/flash-attention](https://github.com/ROCm/flash-attention/tree/ck_tile#amd-gpurocm-support) 的说明安装 ROCm's Flash Attention (v2.7.2)。或者也可以在发布页面中找到专为 vLLM 使用准备的轮子 (wheel)。

例如，对于 ROCm 6.3，假定你的 gfx 架构是 `gfx90a`，想获取您的 gfx 架构，请运行 `rocminfo |grep gfx`。

```plain
git clone https://github.com/ROCm/flash-attention.git
cd flash-attention
git checkout b7d29fb
git submodule update --init
GPU_ARCHS="gfx90a" python3 setup.py install
cd ..
```

> **注意** 您可能需要将 "ninja" 版本降级到 1.10，编译 flash-attention-2 时不会使用它（例如 `pip install ninja==1.10.2.4`）

1. 构建 vLLM。例如，基于 ROCm 6.3 的 vLLM 可以通过以下步骤构建：

```plain
pip install --upgrade pip


# Build & install AMD SMI
# 构建并安装 AMD SMI
pip install /opt/rocm/share/amd_smi


# Install dependencies
# 安装依赖
pip install --upgrade numba scipy huggingface-hub[cli,hf_transfer] setuptools_scm
pip install "numpy<2"
pip install -r requirements-rocm.txt


# Build vLLM for MI210/MI250/MI300.
# 为 MI210/MI250/MI300 构建 vLLM
export PYTORCH_ROCM_ARCH="gfx90a;gfx942"
python3 setup.py develop
```

该步可能花费 5-10 分钟。目前 `pip install` 不适用于 ROCm 的安装。

> **提示**
>
> - 默认使用 Triton flash attention。为了进行基准测试，建议在收集性能数据之前先运行一个预热步骤。
> - Triton flash attention 目前不支持滑动窗口注意力 (sliding window attention)。如果使用半精度 (half precision)，请使用 CK flash-attention 以支持滑动窗口。
>
> 如果要使用 CK flash-attention 或 PyTorch naive attention，请使用以下命令关闭 Triton flash attention: `export VLLM_USE_TRITON_FLASH_ATTN=0`。
>
> 理想情况下，PyTorch 的 ROCm 版本最好与 ROCm 驱动程序版本匹配。

> **提示** 对于 MI300x (gfx942) 用户，为了获得最佳性能，请参考 [MI300x 调优指南](https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/index.html)，以获取系统和工作流级别的性能优化和调优建议。对于 vLLM，请参考 [vLLM 性能优化指南](https://rocm.docs.amd.com/en/latest/how-to/tuning-guides/mi300x/workload.html#vllm-performance-optimization)。

#### Inter XPU

- 首先，安装所需的驱动程序和 intel OneAPI 2024.2 (或更高版本)。
- 其次，安装用于 vLLM XPU 后端构建的 Python 包：

```plain
source /opt/intel/oneapi/setvars.sh
pip install --upgrade pip
pip install -v -r requirements-xpu.txt
```

- 最后，构建并安装 vLLM XPU 后端：

```plain
VLLM_TARGET_DEVICE=xpu python setup.py install
```

> **注意\****：** FP16 是当前 XPU 后端的默认数据类型。BF16 数据类型在英特尔数据中心 GPU 上受支持，但在英特尔 Arc GPU 上尚不支持。

## 使用 Docker 进行设置

### 预构建镜像

#### NVIDIA CUDA

查阅[使用 ](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image)[v](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image)[LLM 官方 Docker 镜像](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image)获得使用官方 Docker 镜像的教程。

另一种获取最新代码的方法是使用 Docker 镜像：

```plain
export VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch 使用主分支上的完整提交哈希哈值。
docker pull public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:${VLLM_COMMIT}
```

这些 docker 镜像仅用于 CI 和测试，不作为生产使用，将会在数日后过期。 最新代码可能包含 bug 且不稳定，请谨慎使用。

#### AMD ROCm

[AMD Infinity hub for vLLM](https://hub.docker.com/r/rocm/vllm/tags) 提供了预构建、优化过的 docker 镜像，旨在验证 AMD Instinct™ MI300X 加速器上的推理性能。

> **提示：** 请检查在 [AMD Instinct MI300X 上](https://rocm.docs.amd.com/en/latest/how-to/performance-validation/mi300x/vllm-benchmark.html)[对](https://rocm.docs.amd.com/en/latest/how-to/performance-validation/mi300x/vllm-benchmark.html)[ LLM 推理性能验证](https://rocm.docs.amd.com/en/latest/how-to/performance-validation/mi300x/vllm-benchmark.html)，并查看如何使用该预构建 docker 镜像的说明。

#### Inter XPU

目前没有预构建的 XPU 镜像。

### 从源代码构建镜像

#### NVIDIA CUDA

有关构建 Docker 映像的说明，请参阅[从源代码构建 vLLM Docker 镜像](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-build-image-from-source)。

#### AMD ROCm

将 vLLM 与 ROCm 结合使用的推荐方式是从源码构建 Docker 镜像。

##### （可选）构建包含 ROCm 软件栈的镜像

vLLM 需要从 [Dockerfile.rocm_base](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm_base) 构建一个包含 ROCm 软件栈的 Docker 镜像。**此步骤是可选的，因为\****此****rocm_base 镜像为了加快用户体验，通常已预构建并存储在**[Docker Hub](https://hub.docker.com/r/rocm/vllm-dev)**上，标签为**`rocm/vllm-dev:base`**。**如果您选择自己构建此 rocm_base 镜像，步骤如下。

注意，用户需要使用 BuildKit 启动 Docker 构建。用户可以通过在调用 `docker build` 命令时设置环境变量 `DOCKER_BUILDKIT=1`，或者在 Docker 守护进程配置文件 /etc/docker/daemon.jso` 中按照以下方式启用 BuildKit 并重启守护进程：

```json
{
  "features": {
    "buildkit": true
  }
}
```

如需为 MI200 和 MI300 系列构建基于 ROCm 6.3 的 vLLM 镜像，可以使用默认命令：

```bash
DOCKER_BUILDKIT=1 docker build -f Dockerfile.rocm_base -t rocm/vllm-dev:base .
```

##### 构建包含 vLLM 的镜像

首先，从 [Dockerfile.rocm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm) 构建 Docker 镜像，并从该镜像启动容器。注意，用户需要使用 BuildKit 启动 Docker 构建。可以通过在调用 `docker build` 命令时设置环境变量 `DOCKER_BUILDKIT=1`，或者在 Docker 守护进程配置文件 /etc/docker/daemon.jso` 中按照以下方式启用 BuildKit 并重启守护进程：

```json
{
  "features": {
    "buildkit": true
  }
}
```

[Dockerfile.rocm](https://github.com/vllm-project/vllm/blob/main/Dockerfile.rocm) 默认使用 ROCm 6.3，但也支持 ROCm 5.7、6.0、6.1 和 6.2（在较旧的 vLLM 分支中）。它提供了以下构建参数以灵活构建自定义 Docker 镜像：

- `BASE_IMAGE`：指定构建 Docker 镜像时使用的基础镜像。默认值 `rocm/vllm-dev:base` 是由 AMD 发布和维护的镜像，它是使用 `Dockerfile.rocm_base` 构建的。
- `USE_CYTHON`：在 Docker 构建时对部分 Python 文件子集运行 Cython 编译的选项。
- `BUILD_RPD`：在镜像中包含 RocmProfileData 性能分析工具。
- `ARG_PYTORCH_ROCM_ARCH`：允许覆盖基础镜像中的 gfx 架构值。

这些参数可以通过 `--build-arg` 选项传递给 `docker build` 命令。

如需为 MI200 和 MI300 系列构建基于 ROCm 6.3 的 vllm 镜像，可以使用默认命令：

```bash
DOCKER_BUILDKIT=1 docker build -f Dockerfile.rocm -t vllm-rocm .
```

如需为 Radeon RX7900 系列 (gfx1100) 构建基于 ROCm 6.3 的 vllm 镜像，应选择替代的基础镜像：

```bash
DOCKER_BUILDKIT=1 docker build --build-arg BASE_IMAGE="rocm/vllm-dev:navi_base" -f Dockerfile.rocm -t vllm-rocm .
```

请使用以下命令运行上述 Docker 镜像 `vllm-rocm`：

```bash
docker run -it \
   --network=host \
   --group-add=video \
   --ipc=host \
   --device /dev/kfd \
   --device /dev/dri \
   -v <path/to/model>:/app/model \
   vllm-rocm \
   bash
```

其中 `<path/to/model>` 是模型存储的路径，例如 llama2 或 llama3 模型的权重文件路径。

#### Inter XPU

```plain
docker build -f Dockerfile.xpu -t vllm-xpu-env --shm-size=4g .
docker run -it \
             --rm \
             --network=host \
             --device /dev/dri \
             -v /dev/dri/by-path:/dev/dri/by-path \
             vllm-xpu-env
```

## 支持的功能

### NVIDIA CUDA

在 [Feature x Hardware](https://docs.vllm.ai/en/latest/features/compatibility_matrix.html#feature-x-hardware) 兼容矩阵中可查看功能支持信息。

### AMD ROCm

在 [Feature x Hardware](https://docs.vllm.ai/en/latest/features/compatibility_matrix.html#feature-x-hardware) 兼容矩阵中可查看功能支持信息。

### Inter XPU

XPU 平台支持张量并行推理/服务，并且还支持在线服务中的流水线并行（作为测试版功能）。我们要求使用 Ray 作为分布式运行时后端。例如，参考的执行命令如下：

```plain
python -m vllm.entrypoints.openai.api_server \
     --model=facebook/opt-13b \
     --dtype=bfloat16 \
     --device=xpu \
     --max_model_len=1024 \
     --distributed-executor-backend=ray \
     --pipeline-parallel-size=2 \
     -tp=8
```

默认情况下，如果系统中未检测到现有的 Ray 实例，则会自动启动一个 Ray 实例，其中 `num-gpus` 等于 `parallel_config.world_size`。我们建议在执行前正确启动一个 Ray 集群，可以参考 [examples/online_serving/run_cluster.sh](https://github.com/vllm-project/vllm/blob/main/examples/online_serving/run_cluster.sh) 辅助脚本。

### CPU

### 其他 AI 加速器

# CPU

vLLM 是一个支持以下 CPU 变体的 Python 库。根据您的 CPU 类型查看厂商特定的说明：

#### Intel/AMD x86

vLLM 初步支持在 x86 CPU 平台进行基础模型推理和服务，支持 FP32、FP16 和 BF16 数据类型。

> **注意** 此设备没有预编译的 wheel 包或镜像，您必须从源码构建 vLLM。

#### ARM AArch64

vLLM 已适配支持具备 NEON 指令集的 ARM64 CPU，基于最初为 x86 平台开发的 CPU 后端实现。

ARM CPU 后端当前支持 Float32、FP16 和 BFloat16 数据类型。

> **注意** 此设备没有预编译的 wheel 包或镜像，您必须从源码构建 vLLM。

#### Apple silicon

vLLM 对 macOS 上的 Apple 芯片提供实验性支持。目前用户需从源码构建 vLLM 以在 macOS 上原生运行。

macOS 的 CPU 实现当前支持 FP32 和 FP16 数据类型。

> **注意** 此设备没有预编译的 wheel 包或镜像，您必须从源码构建 vLLM。

#### IBM Z (S390X)

vLLM 对 IBM Z 平台上的 s390x 架构提供实验性支持。目前用户需从源码构建 vLLM 以在 IBM Z 平台上原生运行。

s390x 架构的 CPU 实现当前仅支持 FP32 数据类型。

> **注意** 此设备没有预编译的 wheel 包或镜像，您必须从源码构建 vLLM。

## 系统要求

- Python: 3.9 – 3.12

#### Intel/AMD x86

- 操作系统：Linux
- 编译器：`gcc/g++ >= 12.3.0`（可选，推荐）
- 指令集架构 (ISA)：AVX512（可选，推荐）

> **提示** >[Intel Extension for PyTorch (IPEX)](https://github.com/intel/intel-extension-for-pytorch) 通过最新特性优化扩展 PyTorch，可在 Intel 硬件上获得额外性能提升。

#### ARM AArch64

- 操作系统：Linux
- 编译器：`gcc/g++ >= 12.3.0`（可选，推荐）
- 指令集架构 (ISA)：需要 NEON 支持

#### Apple silicon

- 操作系统：`macOS Sonoma` 或更高版本
- SDK：`XCode 15.4` 或更高版本（含命令行工具）
- 编译器：`Apple Clang >= 15.0.0`

#### IBM Z (S390X)

- 操作系统: `Linux`
- SDK：`gcc/g++ >= 12.3.0` 或更高版本（含命令行工具）
- 指令集架构 (ISA)：需要 VXE 支持（适用于 Z14 及以上机型）
- 需手动构建的 Python 包：`pyarrow`、`torch` 和  `torchvision`

## 使用 Python 安装

### 创建一个新的 Python 虚拟环境

您可以使用  `conda` 创建新环境：

```plain
# (Recommended) Create a new conda environment.
# （推荐）创建新的 conda 环境
conda create -n vllm python=3.12 -y
conda activate vllm
```

> **注意** >[PyTorch 已弃用 conda 发布渠道](https://github.com/pytorch/pytorch/issues/138506)。若使用  `conda`，建议仅用于创建环境而非安装软件包。 或者可以使用超快的 Python 环境管理工具  [uv](https://docs.astral.sh/uv/) 创建环境。安装  `uv` 后执行以下命令创建新的 Python 环境：

```plain
# (Recommended) Create a new uv environment. Use `--seed` to install `pip` and `setuptools` in the environment.
# （推荐）创建新的 uv 环境（使用 `--seed` 安装 `pip` 和 `setuptools`）
uv venv vllm --python 3.12 --seed
source vllm/bin/activate
```

### 预编译包

当前无预编译的 CPU 版本 wheel 包。

### 从源码构建 wheel

#### Intel/AMD x86

第一步，安装推荐编译器，我们建议使用 `gcc/g++ >= 12.3.0` 作为默认编译器，避免潜在问题。例如在 Ubuntu 22.4 上你可以运行：

```go
sudo apt-get update  -y
sudo apt-get install -y gcc-12 g++-12 libnuma-dev
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12
```

第二步，克隆 vLLM 仓库：

```go
git clone https://github.com/vllm-project/vllm.git vllm_source
cd vllm_source
```

第三步，安装 vLLM CPU 后端构建所需 Python 包：

```go
pip install --upgrade pip
pip install "cmake>=3.26" wheel packaging ninja "setuptools-scm>=8" numpy
pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
```

最后，构建并安装 vLLM CPU 后端：

```go
VLLM_TARGET_DEVICE=cpu python setup.py install
```

> **注意** >`AVX512_BF16` 指令集提供原生 BF16 数据类型转换和向量计算指令，性能优于纯 AVX512。构建脚本会自动检测 CPU 是否支持。 若需强制启用 AVX512_BF16（如交叉编译），可在构建前设置环境变量  `VLLM_CPU_AVX512BF16=1`。

#### ARM AArch64

第一步，安装推荐编译器，我们建议使用 `gcc/g++ >= 12.3.0` 作为默认编译器，避免潜在问题。例如在 Ubuntu 22.4 上你可以运行：

```go
sudo apt-get update  -y
sudo apt-get install -y gcc-12 g++-12 libnuma-dev
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12
```

第二步，克隆 vLLM 仓库：

```go
git clone https://github.com/vllm-project/vllm.git vllm_source
cd vllm_source
```

第三步，安装 vLLM CPU 后端构建所需 Python 包：

```go
pip install --upgrade pip
pip install "cmake>=3.26" wheel packaging ninja "setuptools-scm>=8" numpy
pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu
```

最后，构建并安装 vLLM CPU 后端：

```go
VLLM_TARGET_DEVICE=cpu python setup.py install
```

已在 AWS Graviton3 实例验证兼容性。

#### Apple silicon

在安装 XCode 和命令行工具（包括 Apple Clan）后，执行以下命令行，从源代码构建并安装 vLLM：

```go
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -r requirements/cpu.txt
pip install -e .
```

> **注意** macOS 会自动设置  `VLLM_TARGET_DEVICE=cpu`，此为当前唯一支持的设备。

#### 故障排查

若出现 C++ 头文件缺失错误（如  `'map' file not found`），尝试重新安装  [Xcode 命令行工具](https://developer.apple.com/download/all/)。

```plain
[...] fatal error: 'map' file not found
          1 | #include <map>
            |          ^~~~~
      1 error generated.
      [2/8] Building CXX object CMakeFiles/_C.dir/csrc/cpu/pos_encoding.cpp.o


[...] fatal error: 'cstddef' file not found
         10 | #include <cstddef>
            |          ^~~~~~~~~
      1 error generated.
```

#### IBM Z (S390X)

在构建 vLLM 前从包管理器中安装以下包，例如在 RHEL 9.4 中：

```go
dnf install -y \
    which procps findutils tar vim git gcc g++ make patch make cython zlib-devel \
    libjpeg-turbo-devel libtiff-devel libpng-devel libwebp-devel freetype-devel harfbuzz-devel \
    openssl-devel openblas openblas-devel wget autoconf automake libtool cmake numactl-devel
```

安装 Rust ≥1.80 `outlines-core` 和 `uvloop` python 包的安装需要它：

```go
curl https://sh.rustup.rs -sSf | sh -s -- -y && \
    . "$HOME/.cargo/env"
```

执行以下命令，从源代码构建并安装 vLLM。

> **提示** 在构建 vLLM 之前，请从源代码构建下列依赖：`torchvision`, `pyarrow`。

```go
    sed -i '/^torch/d' requirements-build.txt    # remove torch from requirements-build.txt since we use nightly builds
    pip install -v \
        --extra-index-url https://download.pytorch.org/whl/nightly/cpu \
        -r requirements-build.txt \
        -r requirements-cpu.txt \
    VLLM_TARGET_DEVICE=cpu python setup.py bdist_wheel && \
    pip install dist/*.whl
```

## 使用 Docker 安装

### 预编译镜像

#### Intel/AMD x86

查看  https://gallery.ecr.aws/q9t5s3a7/vllm-cpu-release-repo

### 从源码构建镜像

```plain
$ docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .
$ docker run -it \
             --rm \
             --network=host \
             --cpuset-cpus=<cpu-id-list, optional> \
             --cpuset-mems=<memory-node, optional> \
             vllm-cpu-env
```

> **提示** ARM 或 Apple 芯片使用  `Dockerfile.arm`

> **提示** IBM Z（s390x）使用  `Dockerfile.s390x`，并在  `docker run` 中添加参数  `--dtype float`

## 支持的功能

vLLM CPU 后端支持以下特性：

- 张量并行（Tensor Parallel）
- 模型量化（`INT8 W8A8`、`AWQ`、`GPTQ`）
- 分块预填充（Chunked-prefill）
- 前缀缓存（Prefix-caching）
- FP8-E5M2 KV 缓存

## 相关运行时环境变量

- `VLLM_CPU_KVCACHE_SPACE` : 指定 KV 缓存大小（例如  `VLLM_CPU_KVCACHE_SPACE=40` 表示 40 GiB 的 KV 缓存空间），设置更大的值可以让 vLLM 并行处理更多请求。该参数应根据硬件配置和用户的内存管理模式进行调整。
- `VLLM_CPU_OMP_THREADS_BIND` : 指定专用于 OpenMP 线程的 CPU 核心。例如：

`VLLM_CPU_OMP_THREADS_BIND=0-31` 表示将 32 个 OpenMP 线程绑定到 0-31 号 CPU 核心

`VLLM_CPU_OMP_THREADS_BIND=0-31|32-63` 表示启用 2 个张量并行进程，rank0 的 32 个 OpenMP 线程绑定到 0-31 号核心，rank1 的线程绑定到 32-63 号核心

- `VLLM_CPU_MOE_PREPACK` : 是否为 MoE 层使用预打包功能。该参数会传递给  `ipex.llm.modules.GatedMLPMOE` 。默认值为  `1` （启用）。在不支持的 CPU 上可能需要设置为  `0` （禁用）。

## 性能优化建议

- 我们强烈推荐使用 TCMalloc 实现高性能内存分配和更好的缓存局部性。例如，在 Ubuntu 22.4 上可执行：

```go
sudo apt-get install libtcmalloc-minimal4 # install TCMalloc library
find / -name *libtcmalloc* # find the dynamic link library path
export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:$LD_PRELOAD # prepend the library to LD_PRELOAD
python examples/offline_inference/basic/basic.py # run vLLM
```

- 使用在线服务时，建议为服务框架预留 1-2 个 CPU 核心以避免 CPU 过载。例如在 32 物理核心的平台上，预留 30 和 31 号核心给框架，0-29 号核心用于 OpenMP：

```go
export VLLM_CPU_KVCACHE_SPACE=40
export VLLM_CPU_OMP_THREADS_BIND=0-29
vllm serve facebook/opt-125m
```

- 在支持超线程的机器上使用 vLLM CPU 后端时，建议通过  `VLLM_CPU_OMP_THREADS_BIND` 将每个物理 CPU 核心只绑定一个 OpenMP 线程。在 16 逻辑核心 / 8 物理核心的超线程平台上：

```plain
$ lscpu -e # check the mapping between logical CPU cores and physical CPU cores


# The "CPU" column means the logical CPU core IDs, and the "CORE" column means the physical core IDs. On this platform, two logical cores are sharing one physical core.
# "CPU" 列表示逻辑核心 ID，"CORE" 列表示物理核心 ID。该平台上两个逻辑核心共享一个物理核心。
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE    MAXMHZ   MINMHZ      MHZ
0    0      0    0 0:0:0:0          yes 2401.0000 800.0000  800.000
1    0      0    1 1:1:1:0          yes 2401.0000 800.0000  800.000
2    0      0    2 2:2:2:0          yes 2401.0000 800.0000  800.000
3    0      0    3 3:3:3:0          yes 2401.0000 800.0000  800.000
4    0      0    4 4:4:4:0          yes 2401.0000 800.0000  800.000
5    0      0    5 5:5:5:0          yes 2401.0000 800.0000  800.000
6    0      0    6 6:6:6:0          yes 2401.0000 800.0000  800.000
7    0      0    7 7:7:7:0          yes 2401.0000 800.0000  800.000
8    0      0    0 0:0:0:0          yes 2401.0000 800.0000  800.000
9    0      0    1 1:1:1:0          yes 2401.0000 800.0000  800.000
10   0      0    2 2:2:2:0          yes 2401.0000 800.0000  800.000
11   0      0    3 3:3:3:0          yes 2401.0000 800.0000  800.000
12   0      0    4 4:4:4:0          yes 2401.0000 800.0000  800.000
13   0      0    5 5:5:5:0          yes 2401.0000 800.0000  800.000
14   0      0    6 6:6:6:0          yes 2401.0000 800.0000  800.000
15   0      0    7 7:7:7:0          yes 2401.0000 800.0000  800.000


# On this platform, it is recommend to only bind openMP threads on logical CPU cores 0-7 or 8-15
# 在此平台上，建议仅将 OpenMP 线程绑定到 0-7 或 8-15 号逻辑核心
$ export VLLM_CPU_OMP_THREADS_BIND=0-7
$ python examples/offline_inference/basic/basic.py
```

- 在多插槽 NUMA 机器上使用 vLLM CPU 后端时，应注意通过  `VLLM_CPU_OMP_THREADS_BIND` 设置 CPU 核心，避免跨 NUMA 节点的内存访问。

## 其他注意事项

- CPU 后端与 GPU 后端有显著差异，因为 vLLM 架构最初是为 GPU 优化的。需要多项优化来提升其性能。
- 建议将 HTTP 服务组件与推理组件解耦。在 GPU 后端配置中，HTTP 服务和分词任务运行在 CPU 上，而推理运行在 GPU  上，这通常不会造成问题。但在基于 CPU 的环境中，HTTP  服务和分词可能导致显著的上下文切换和缓存效率降低。因此强烈建议分离这两个组件以获得更好的性能。
- 在启用 NUMA 的 CPU 环境中，内存访问性能可能受  [拓扑结构](https://github.com/intel/intel-extension-for-pytorch/blob/main/docs/tutorials/performance_tuning/tuning_guide.inc.md#non-uniform-memory-access-numa) 影响较大。对于 NUMA 架构，推荐两种优化方案：张量并行或数据并行。
  - 延迟敏感场景使用张量并行：遵循 GPU 后端设计，基于 NUMA 节点数量（例如双 NUMA 节点系统 TP=2）使用 Megatron-LM 的并行算法切分模型。随着  [CPU 上的 TP 功能](https://github.com/vllm-project/vllm/pull/6125#) 合并，张量并行已支持服务和离线推理。通常每个 NUMA 节点被视为一个 GPU 卡。以下是启用张量并行度为 2 的服务示例：

```go
VLLM_CPU_KVCACHE_SPACE=40 VLLM_CPU_OMP_THREADS_BIND="0-31|32-63" vllm serve meta-llama/Llama-2-7b-chat-hf -tp=2 --distributed-executor-backend mp
```

- 最大吞吐量场景使用数据并行：在每个 NUMA 节点上部署一个 LLM 服务端点，并增加一个负载均衡器来分发请求到这些端点。推荐使用  [Nginx](https://docs.vllm.ai/en/latest/deployment/nginx.html#nginxloadbalancer) 或 HAProxy 等通用解决方案。Anyscale Ray 项目提供了 LLM [服务](https://docs.ray.io/en/latest/serve/index.html)功能。这里有使用  [Ray Serve](https://github.com/intel/llm-on-ray/blob/main/docs/setup.inc.md) 设置可扩展 LLM 服务的示例。

# 其他 AI 加速器

[*在线运行 vLLM 入门教程：零基础分步指南](https://openbayes.com/console/public/tutorials/rXxb5fZFr29?utm_source=vLLM-CNdoc&utm_medium=vLLM-CNdoc-V1&utm_campaign=vLLM-CNdoc-V1-25ap)

vLLM 是一个 Python 库，支持以下 AI 加速器。根据您的 AI 加速器类型查看供应商特定说明：

#### Google TPU

张量处理单元 (TPU) 是 Google 定制开发的专用集成电路 (ASIC)，用于加速机器学习工作负载。TPU 有不同的版本，每个版本具有不同的硬件规格。有关 TPU 的更多信息，请参阅 [TPU 系统架构](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)。有关 vLLM 支持的 TPU 版本信息，请参阅：

- [TPU v6e](https://cloud.google.com/tpu/docs/v6e)
- [TPU v5e](https://cloud.google.com/tpu/docs/v5e)
- [TPU v5p](https://cloud.google.com/tpu/docs/v5p)
- [TPU v4](https://cloud.google.com/tpu/docs/v4)

这些 TPU 版本允许您配置 TPU 芯片的物理排列方式。这可以提高吞吐量和网络性能。更多信息请参阅：

- [TPU v6e 拓扑结构](https://cloud.google.com/tpu/docs/v6e#configurations)
- [TPU v5e 拓扑结构](https://cloud.google.com/tpu/docs/v5e#tpu-v5e-config)
- [TPU v5p 拓扑结构](https://cloud.google.com/tpu/docs/v5p#tpu-v5p-config)
- [TPU v4 拓扑结构](https://cloud.google.com/tpu/docs/v4#tpu-v4-config)

要使用 Cloud TPU，您需要为 Google Cloud Platform 项目授予 TPU 配额。TPU 配额指定您可以在 GPC 项目中使用的 TPU 数量，并根据 TPU 版本、所需 TPU 数量和配额类型进行定义。更多信息请参阅 [TPU 配额](https://cloud.google.com/tpu/docs/quota#tpu_quota)。

有关 TPU 定价信息，请参阅 [Cloud TPU 定价](https://cloud.google.com/tpu/pricing)。

您可能需要为 TPU 虚拟机提供额外的持久存储。更多信息请参阅 [Cloud TPU 数据存储选项](https://cloud.devsite.corp.google.com/tpu/docs/storage-options)。

**注意**

此设备没有预构建的 wheels，因此您必须使用预构建的 Docker 镜像或从源代码构建 vLLM。

#### Intel Gaudi

此节提供了在 Intel Gaudi 设备上运行 vLLM 的说明。

> **注意** 此设备没有预构建的 wheels 或镜像，因此您必须从源代码构建 vLLM。

#### AWS Neuron

vLLM 0.3.3 及以上版本支持通过 Neuron SDK 在 AWS Trainium/Inferentia  上进行模型推理和服务，并支持连续批处理。分页注意力（Paged Attention）和分块预填充（Chunked  Prefill）功能目前正在开发中，即将推出。Neuron SDK 当前支持的数据类型为 FP16 和 BF16。

> **注意** 此设备没有预构建的 wheels 或镜像，因此您必须从源代码构建 vLLM。

## 环境要求

### Google TPU

- Google Cloud TPU 虚拟机
- TPU 版本：v6e、v5e、v5p、v4
- Python：3.10 或更高版本

#### 配置 Cloud TPU

您可以使用 [Cloud TPU API](https://cloud.google.com/tpu/docs/reference/rest) 或 [队列资源](https://cloud.google.com/tpu/docs/queued-resources) API 配置 Cloud TPU。本节展示如何使用队列资源 API 创建 TPU。有关使用 Cloud TPU API 的更多信息，请参阅 [使用 Create Node API 创建 Cloud TPU](https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#create-node-api)。队列资源允许您以队列方式请求 Cloud TPU 资源。当您请求队列资源时，请求会被添加到 Cloud TPU 服务维护的队列中。当请求的资源可用时，它将分配给您的 Google Cloud 项目供您独占使用。

> **注意** 在以下所有命令中，请将全大写的参数名称替换为适当的值。有关参数描述，请参阅参数描述表。

#### 使用 GKE 配置 Cloud TPU

有关在 GKE 中使用 TPU 的更多信息，请参阅：

- https://cloud.google.com/kubernetes-engine/docs/how-to/tpus
- https://cloud.google.com/kubernetes-engine/docs/concepts/tpus
- https://cloud.google.com/kubernetes-engine/docs/concepts/plan-tpus

### Intel Gaudi

- 操作系统：Ubuntu 22.04 LTS
- Python：3.10
- Intel Gaudi 加速器
- Intel Gaudi 软件版本 1.18.0

请按照 [Gaudi 安装指南](https://docs.habana.ai/en/latest/Installation_Guide/index.html) 中的说明设置执行环境。要获得最佳性能，请按照 [优化训练平台指南](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Optimization_in_Training_Platform.html) 中概述的方法操作。

### AWS Neuron

- 操作系统：Linux
- Python：3.9 – 3.11
- 加速器：NeuronCore_v2（在 trn1/inf2 实例中）
- Pytorch 2.0.1/2.1.1
- AWS Neuron SDK 2.16/2.17（已验证于 Python 3.8）

## 配置新环境

### Google TPU

#### 使用队列资源 API 配置 Cloud TPU

创建具有 4 个 TPU 芯片的 TPU v5e：

```go
gcloud alpha compute tpus queued-resources create QUEUED_RESOURCE_ID \
--node-id TPU_NAME \
--project PROJECT_ID \
--zone ZONE \
--accelerator-type ACCELERATOR_TYPE \
--runtime-version RUNTIME_VERSION \
--service-account SERVICE_ACCOUNT
```

| 参数名称           | 描述                                                         |
| :----------------- | :----------------------------------------------------------- |
| QUEUED_RESOURCE_ID | 用户分配的队列资源请求 ID。                                  |
| TPU_NAME           | 当队列资源请求被分配时创建的 TPU 的用户分配名称。            |
| PROJECT_ID         | 您的 Google Cloud 项目。                                     |
| ZONE               | 要在其中创建 Cloud TPU 的 GCP 区域。您使用的值取决于您使用的 TPU 版本。更多信息请参阅 [TPU 区域和区域](https://cloud.google.com/tpu/docs/regions-zones) |
| ACCELERATOR_TYPE   | 要使用的 TPU 版本。指定 TPU 版本，例如 v5litepod-4 指定具有 4 个核心的 v5e TPU。更多信息请参阅 [TPU 版本](https://cloud.devsite.corp.google.com/tpu/docs/system-architecture-tpu-vm#versions)。 |
| RUNTIME_VERSION    | 要使用的 TPU 虚拟机运行时版本。更多信息请参阅 [TPU 虚拟机镜像](https://cloud.google.com/tpu/docs/runtimes)。 |
| SERVICE_ACCOUNT    | 您的服务账号的电子邮件地址。您可以在 IAM 云控制台的 *服务账号* 下找到它。例如：tpu-service-account@<your_project_ID>.iam.gserviceaccount.com |

通过 SSH 连接到您的 TPU：

```plain
gcloud compute tpus tpu-vm ssh TPU_NAME --zone ZONE
```

### Intel Gaudi

#### 环境验证

要验证 Intel Gaudi 软件是否正确安装，请运行：

```go
hl-smi # 验证 hl-smi 是否在您的 PATH 中，并且每个 Gaudi 加速器可见
apt list --installed | grep habana # 验证 habanalabs-firmware-tools、habanalabs-graph、habanalabs-rdma-core、habanalabs-thunk 和 habanalabs-container-runtime 是否已安装
pip list | grep habana # 验证 habana-torch-plugin、habana-torch-dataloader、habana-pyhlml 和 habana-media-loader 是否已安装
pip list | grep neural # 验证 neural_compressor 是否已安装
```

更多详细信息请参阅 [Intel Gaudi 软件堆栈验证](https://docs.habana.ai/en/latest/Installation_Guide/SW_Verification.html#platform-upgrade)。

#### 运行 Docker 镜像

强烈建议使用来自 Intel Gaudi 仓库的最新 Docker 镜像。更多详细信息请参阅 [Intel Gaudi 文档](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html#pull-prebuilt-containers)。

使用以下命令运行 Docker 镜像：

```go
docker pull vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest
docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest
```

### AWS Neuron

#### 启动 Trn1/Inf2 实例

以下是启动 trn1/inf2 实例的步骤，以便安装 [Ubuntu 22.04 LTS 上的 PyTorch Neuron (](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu22.html)["](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu22.html)[torch-neuronx](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu22.html)["](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu22.html)[) 设置](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu22.html)。

- 请按照 [启动 Amazon EC2 实例](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html#ec2-launch-instance) 中的说明启动实例。在 EC2 控制台选择实例类型时，请确保选择正确的实例类型。
- 有关实例规格和定价的更多信息，请参阅：[Trn1 网页](https://aws.amazon.com/ec2/instance-types/trn1/)、[Inf2 网页](https://aws.amazon.com/ec2/instance-types/inf2/)
- 选择 Ubuntu Server 22.04 TLS AMI
- 启动 Trn1/Inf2 实例时，请将主 EBS 卷大小调整为至少 512GB。
- 启动实例后，按照 [连接到您的实例](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html) 中的说明连接到实例

#### 安装驱动程序和工具

如果已安装 [Deep Learning AMI Neuron](https://docs.aws.amazon.com/dlami/latest/devguide/appendix-ami-release-notes.html)，则无需安装驱动程序和工具。如果操作系统未安装驱动程序和工具，请按照以下步骤操作：

```plain
# Configure Linux for Neuron repository updates
# 为 Neuron 仓库配置 Linux
. /etc/os-release
sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF
deb https://apt.repos.neuron.amazonaws.com ${VERSION_CODENAME} main
EOF
wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -


# Update OS packages
# 更新操作系统包
sudo apt-get update -y


# Install OS headers
# 安装操作系统头文件
sudo apt-get install linux-headers-$(uname -r) -y


# Install git
# 安装 git
sudo apt-get install git -y


# install Neuron Driver
# 安装 Neuron 驱动
sudo apt-get install aws-neuronx-dkms=2.* -y


# Install Neuron Runtime
# 安装 Neuron 运行时
sudo apt-get install aws-neuronx-collectives=2.* -y
sudo apt-get install aws-neuronx-runtime-lib=2.* -y


# Install Neuron Tools
# 安装 Neuron 工具
sudo apt-get install aws-neuronx-tools=2.* -y


# Add PATH
# 添加 PATH
export PATH=/opt/aws/neuron/bin:$PATH
```

## 使用 Python 设置

### 预构建的 wheels

#### Google TPU

当前没有预构建的 TPU wheels。

#### Intel Gaudi

当前没有预构建的 Intel Gaudi wheels。

#### AWS Neuron

当前没有预构建的 Neuron wheels。

### 从源代码构建 wheel

### Google TPU

安装 Miniconda：

```plain
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc
```

创建并激活 vLLM 的 Conda 环境：

```plain
conda create -n vllm python=3.10 -y
conda activate vllm
```

克隆 vLLM 仓库并进入 vLLM 目录：

```plain
git clone https://github.com/vllm-project/vllm.git && cd vllm
```

卸载现有的 `torch` 和 `torch_xla` 包：

```plain
pip uninstall torch torch-xla -y
```

安装构建依赖项：

```plain
pip install -r requirements/tpu.txt
sudo apt-get install libopenblas-base libopenmpi-dev libomp-dev
```

运行安装脚本：

```plain
VLLM_TARGET_DEVICE="tpu" python setup.py develop
```

### Intel Gaudi

要从源代码构建并安装 vLLM，请运行：

```go
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -r requirements/hpu.txt
python setup.py develop
```

当前，最新的功能和性能优化在 Gaudi 的 [vLLM-fork](https://github.com/HabanaAI/vllm-fork) 中开发，并定期同步到 vLLM 主仓库。要安装最新的 [HabanaAI/vLLM-fork](https://github.com/HabanaAI/vllm-fork)，请运行以下命令：

```go
git clone https://github.com/HabanaAI/vllm-fork.git
cd vllm-fork
git checkout habana_main
pip install -r requirements/hpu.txt
python setup.py develop
```

### AWS Neuron

**注意**

当前支持的 Neuron Pytorch 版本安装了 `triton` 版本 `2.1.0`。这与 `vllm >= 0.5.3` 不兼容。您可能会看到错误 `cannot import name 'default_dump_dir...`。要解决此问题，请在安装 vLLM wheel 后运行 `pip install --upgrade triton==3.0.0`。

以下说明适用于 Neuron SDK 2.16 及更高版本。

#### 安装 transformers-neuronx 及其依赖项

[transformers-neuronx](https://github.com/aws-neuron/transformers-neuronx) 将作为在 trn1/inf2 实例上支持推理的后端。按照以下步骤安装 transformer-neuronx 包及其依赖项。

```plain
# Install Python venv
# 安装 Python venv
sudo apt-get install -y python3.10-venv g++


# Create Python venv
# 创建 Python venv
python3.10 -m venv aws_neuron_venv_pytorch


# Activate Python venv
# 激活 Python venv
source aws_neuron_venv_pytorch/bin/activate


# Install Jupyter notebook kernel
# 安装 Jupyter notebook 内核
pip install ipykernel
python3.10 -m ipykernel install --user --name aws_neuron_venv_pytorch --display-name "Python (torch-neuronx)"
pip install jupyter notebook
pip install environment_kernels


# Set pip repository pointing to the Neuron repository
# 将 pip 仓库指向 Neuron 仓库
python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com


# Install wget, awscli
# 安装 wget、awscli
python -m pip install wget
python -m pip install awscli


# Update Neuron Compiler and Framework
# 更新 Neuron 编译器和框架
python -m pip install --upgrade neuronx-cc==2.* --pre torch-neuronx==2.1.* torchvision transformers-neuronx
```

#### 从源代码安装 vLLM

安装 neuronx-cc 和 transformers-neuronx 包后，可按如下方式安装 vllm：

```go
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -U -r requirements/neuron.txt
VLLM_TARGET_DEVICE="neuron" pip install .
```

如果安装过程中正确检测到 neuron 包，将安装 `vllm-0.3.0+neuron212`。

## 使用 Docker 设置

### Pre-built images

### 预构建的镜像

#### Google TPU

请参阅 [使用 vLLM 的官方 Docker 镜像](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-pre-built-image) 以获取使用官方 Docker 镜像的说明，确保将镜像名称 `vllm/vllm-openai` 替换为 `vllm/vllm-tpu`。

#### Intel Gaudi

当前没有预构建的 Intel Gaudi 镜像。

#### AWS Neuron

当前没有预构建的 Neuron 镜像。

### 从源代码构建镜像

#### Google TPU

您可以使用 [Dockerfile.tpu](https://github.com/vllm-project/vllm/blob/main/Dockerfile.tpu) 构建支持 TPU 的 Docker 镜像。

```go
docker build -f Dockerfile.tpu -t vllm-tpu .
```

使用以下命令运行 Docker 镜像：

```plain
# Make sure to add `--privileged --net host --shm-size=16G`.
# 确保添加 `--privileged --net host --shm-size=16G`
docker run --privileged --net host --shm-size=16G -it vllm-tpu
```

**注意**

由于 TPU 依赖需要静态形状的 XLA，vLLM 将可能的输入形状分桶，并为每个形状编译一个 XLA 图。首次运行时编译可能需要 20~30 分钟。但由于 XLA 图会缓存到磁盘（默认在 `VLLM_XLA_CACHE_PATH` 或 `~/.cache/vllm/xla_cache`），后续编译时间将减少到约 5 分钟。

**提示**

如果遇到以下错误：

```go
from torch._C import *  # noqa: F403
ImportError: libopenblas.so.0: cannot open shared object file: No such
file or directory
```

请使用以下命令安装 OpenBLAS：

```go
sudo apt-get install libopenblas-base libopenmpi-dev libomp-dev
```

#### Intel Gaudi

```go
docker build -f Dockerfile.hpu -t vllm-hpu-env  .
docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --rm vllm-hpu-env
```

**提示**

如果遇到以下错误：`docker: Error response from daemon: Unknown runtime specified habana.`，请参阅 [Intel Gaudi 软件堆栈和驱动安装](https://docs.habana.ai/en/v1.18.0/Installation_Guide/Bare_Metal_Fresh_OS.html) 的「使用容器安装」部分。确保已安装 `habana-container-runtime` 包，并且 `habana` 容器运行时已注册。

#### AWS Neuron

有关构建 Docker 镜像的说明，请参阅 [从源代码构建 vLLM 的 Docker 镜像](https://docs.vllm.ai/en/latest/deployment/docker.html#deployment-docker-build-image-from-source)。

确保使用 [Dockerfile.neuron](https://github.com/vllm-project/vllm/blob/main/Dockerfile.neuron) 替代默认的 Dockerfile。

# Extra information

## Google TPU

此设备没有额外信息。

## Intel Gaudi

### 支持的功能

- [离线推理](https://docs.vllm.ai/en/latest/serving/offline_inference.html#offline-inference)
- 通过 [OpenAI 兼容服务器](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#openai-compatible-server) 进行在线服务
- HPU 自动检测 - 无需在 vLLM 中手动选择设备
- 针对 Intel Gaudi 加速器优化的分页 KV 缓存算法
- 针对 Intel Gaudi 的定制分页注意力、KV 缓存操作、预填充注意力、均方根层归一化、旋转位置编码实现
- 多卡推理的張量并行支持
- 使用 [HPU 图](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html) 加速低批量延迟和吞吐量的推理
- 带线性偏置的注意力（ALiBi）

### 不支持的功能

- 波束搜索
- LoRA 适配器
- 量化
- 预填充分块（混合批量推理）

### 支持的配置

以下配置已验证可在 Gaudi2 设备上运行。未列出的配置可能无法正常工作。

- [meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) 在单 HPU 上，或通过張量并行在 2x 和 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) 在单 HPU 上，或通过張量并行在 2x 和 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) 在单 HPU 上，或通过張量并行在 2x 和 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) 在单 HPU 上，或通过張量并行在 2x 和 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) 在单 HPU 上，或通过張量并行在 2x 和 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) 在单 HPU 上，或通过張量并行在 2x 和 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Llama-2-70b](https://huggingface.co/meta-llama/Llama-2-70b) 通过張量并行在 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Llama-2-70b-chat-hf](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) 通过張量并行在 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3-70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B) 通过張量并行在 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) 通过張量并行在 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3.1-70B](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B) 通过張量并行在 8x HPU 上，BF16 数据类型，随机或贪婪采样
- [meta-llama/Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct) 通过張量并行在 8x HPU 上，BF16 数据类型，随机或贪婪采样

### 性能调优

#### 执行模式

当前在 vLLM 中，HPU 支持四种执行模式，具体取决于所选的 HPU PyTorch Bridge 后端（通过 `PT_HPU_LAZY_MODE` 环境变量）和 `--enforce-eager` 标志。

| PT_HPU_LAZY_MODE | enforce_eager | 执行模式         |
| :--------------- | :------------ | :--------------- |
| 0                | 0             | torch.compile    |
| 0                | 1             | PyTorch 即时模式 |
| 1                | 0             | HPU 图           |
| 1                | 1             | PyTorch 延迟模式 |

> **警告** 在 1.18.0 版本中，所有使用 `PT_HPU_LAZY_MODE=0` 的模式均为高度实验性，仅应用于验证功能正确性。它们的性能将在后续版本中改进。要在 1.18.0 中获得最佳性能，请使用 HPU 图或 PyTorch 延迟模式。

#### 分桶机制

Intel Gaudi 加速器在操作固定张量形状的模型时表现最佳。[Intel Gaudi 图编译器](https://docs.habana.ai/en/latest/Gaudi_Overview/Intel_Gaudi_Software_Suite.html#graph-compiler-and-runtime) 负责生成在 Gaudi  上实现给定模型拓扑的优化二进制代码。在默认配置中，生成的二进制代码可能高度依赖输入和输出张量形状，并且在同一拓扑中遇到不同形状的张量时可能需要重新编译图。虽然生成的二进制代码能高效利用  Gaudi，但编译本身可能会在端到端执行中引入明显的开销。在动态推理服务场景中，需要尽量减少图编译次数，并降低图编译在服务器运行时发生的风险。目前通过「分桶」模型的向前传递在两个维度（`batch_size` 和 `sequence_length`）来实现这一点。

**注意**

分桶允许我们显著减少所需图的数量，但它不处理任何图编译和设备代码生成——这是在预热和 HPUGraph 捕获阶段完成的。

分桶范围由三个参数确定——`min`、`step` 和 `max`。它们可以分别为提示和解码阶段以及批量大小和序列长度维度单独设置。这些参数可以在 vLLM 启动期间的日志中观察到：

```plain
INFO 08-01 21:37:59 hpu_model_runner.py:493] Prompt bucket config (min, step, max_warmup) bs:[1, 32, 4], seq:[128, 128, 1024]
INFO 08-01 21:37:59 hpu_model_runner.py:499] Generated 24 prompt buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024)]
INFO 08-01 21:37:59 hpu_model_runner.py:504] Decode bucket config (min, step, max_warmup) bs:[1, 128, 4], seq:[128, 128, 2048]
INFO 08-01 21:37:59 hpu_model_runner.py:509] Generated 48 decode buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (1, 1152), (1, 1280), (1, 1408), (1, 1536), (1, 1664), (1, 1792), (1, 1920), (1, 2048), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (2, 1152), (2, 1280), (2, 1408), (2, 1536), (2, 1664), (2, 1792), (2, 1920), (2, 2048), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024), (4, 1152), (4, 1280), (4, 1408), (4, 1536), (4, 1664), (4, 1792), (4, 1920), (4, 2048)]
```

`min` 确定桶的最低值。`step` 确定桶之间的间隔，`max` 确定桶的上限。此外，`min` 和 `step` 之间的间隔有特殊处理——`min` 会被连续乘以二的幂次，直到达到 `step`。我们称此为“上升阶段”，它用于以最小浪费处理较低批量大小，同时允许在较大批量大小上进行较大填充。

示例（带上升阶段）

```plain
min = 2, step = 32, max = 64
=> ramp_up = (2, 4, 8, 16)
=> stable = (32, 64)
=> buckets = ramp_up + stable => (2, 4, 8, 16, 32, 64)
```

示例（不带上升阶段）

```plain
min = 128, step = 128, max = 512
=> ramp_up = ()
=> stable = (128, 256, 384, 512)
=> buckets = ramp_up + stable => (128, 256, 384, 512)
```

在记录的示例中，为提示（预填充）阶段生成了 24 个桶，为解码阶段生成了 48 个桶。每个桶对应具有指定张量形状的模型的单独优化设备二进制文件。每当处理一批请求时，它会在批量和序列长度维度上填充到最小的可能桶。

> **警告** 如果请求在任何维度上超过最大桶大小，则将在不填充的情况下处理，并且其处理可能需要图编译，从而显著增加端到端延迟。桶的边界可通过环境变量由用户配置，可以增加桶的上限以避免此类情况。

例如，如果空闲的 vLLM 服务器收到一个包含 3 个序列、最大序列长度为 412 的请求，则其将被填充为 `(4, 512)` 预填充桶，因为批量大小（序列数）将被填充到 4（最接近且大于 3 的批量大小维度），最大序列长度将被填充到 512（最接近且大于 412 的序列长度维度）。预填充阶段后，它将作为 `(4, 512)` 解码桶执行，并持续作为该桶，直到批量维度发生变化（由于请求完成）——此时它将变为 `(2, 512)` 桶，或上下文长度超过 512 个 token——此时它将变为 `(4, 640)` 桶。

> **注意** 分桶对客户端透明——序列长度维度的填充永远不会返回给客户端，批量维度的填充不会创建新请求。

#### 预热 (Warmup)

预热是在 vLLM 服务器开始监听之前可选但强烈推荐的步骤。它使用虚拟数据为每个桶执行向前传递。目标是在服务器运行时预编译所有图，避免在桶边界内产生任何图编译开销。每个预热步骤在 vLLM 启动期间记录：

```plain
INFO 08-01 22:26:47 hpu_model_runner.py:1066] [Warmup][Prompt][1/24] batch_size:4 seq_len:1024 free_mem:79.16 GiB
INFO 08-01 22:26:47 hpu_model_runner.py:1066] [Warmup][Prompt][2/24] batch_size:4 seq_len:896 free_mem:55.43 GiB
INFO 08-01 22:26:48 hpu_model_runner.py:1066] [Warmup][Prompt][3/24] batch_size:4 seq_len:768 free_mem:55.43 GiB
...
INFO 08-01 22:26:59 hpu_model_runner.py:1066] [Warmup][Prompt][24/24] batch_size:1 seq_len:128 free_mem:55.43 GiB
INFO 08-01 22:27:00 hpu_model_runner.py:1066] [Warmup][Decode][1/48] batch_size:4 seq_len:2048 free_mem:55.43 GiB
INFO 08-01 22:27:00 hpu_model_runner.py:1066] [Warmup][Decode][2/48] batch_size:4 seq_len:1920 free_mem:55.43 GiB
INFO 08-01 22:27:01 hpu_model_runner.py:1066] [Warmup][Decode][3/48] batch_size:4 seq_len:1792 free_mem:55.43 GiB
...
INFO 08-01 22:27:16 hpu_model_runner.py:1066] [Warmup][Decode][47/48] batch_size:2 seq_len:128 free_mem:55.43 GiB
INFO 08-01 22:27:16 hpu_model_runner.py:1066] [Warmup][Decode][48/48] batch_size:1 seq_len:128 free_mem:55.43 GiB
```

此示例使用与 [分桶机制](https://docs.vllm.ai/en/latest/getting_started/installation/ai_accelerator.html?device=hpu-gaudi#gaudi-bucketing-mechanism) 部分相同的桶。每行输出对应单个桶的执行。当桶首次执行时，其图将被编译并可在后续重复使用，跳过进一步的图编译。

> **提示** 编译所有桶可能需要一些时间，可以通过 `VLLM_SKIP_WARMUP=true` 环境变量关闭。请注意，如果这样做，您可能在首次执行给定桶时面临图编译。在开发过程中关闭预热是可以的，但在部署中强烈建议启用。

#### HPU 图捕获

[HPU 图](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html) 当前是 Intel Gaudi 上 vLLM 最高性能的执行方法。启用 HPU  图后，执行图将提前追踪（记录），以便在推理期间重放，显著减少主机开销。记录可能会占用大量内存，这在分配 KV 缓存时需要考虑。启用 HPU  图将影响可用 KV 缓存块的数量，但 vLLM 提供用户可配置的变量来控制内存管理。

当使用 HPU 图时，它们与 KV 缓存共享公共内存池（“可用内存”），由 `gpu_memory_utilization` 标志（默认为 `0.9`）确定。在分配 KV 缓存之前，模型权重将加载到设备上，并在虚拟数据上执行模型的向前传递以估计内存使用情况。之后，`gpu_memory_utilization` 标志将被使用——其默认值将标记此时设备空闲内存的 90% 为可用。接下来分配 KV 缓存，模型被预热，并捕获 HPU 图。环境变量 `VLLM_GRAPH_RESERVED_MEM` 定义保留用于 HPU 图捕获的内存比例。其默认值（`VLLM_GRAPH_RESERVED_MEM=0.1`）表示 10% 的可用内存将保留给图捕获（后称为“可用图内存”），剩余的 90% 用于 KV 缓存。环境变量 `VLLM_GRAPH_PROMPT_RATIO` 确定保留给预填充和解码图的可用图内存比例。默认情况下（`VLLM_GRAPH_PROMPT_RATIO=0.3`），两个阶段具有相同的内存约束。较低的值对应预填充阶段保留的可用图内存较少，例如 `VLLM_GRAPH_PROMPT_RATIO=0.2` 将为预填充图保留 20% 的可用图内存，为解码图保留 80%。

> **注意** >`gpu_memory_utilization` 并不对应 HPU 的绝对内存使用量。它指定在加载模型并执行性能分析运行后的内存余量。如果设备总内存为 100 GiB，加载模型权重并执行性能分析运行后空闲内存为 50 GiB，默认 `gpu_memory_utilization` 将标记 50 GiB 的 90% 为可用，留出 5 GiB 余量，无论设备总内存如何。

用户还可以分别为预填充和解码阶段配置 HPU 图的捕获策略。策略影响图的捕获顺序。已实现两种策略：- `max_bs` - 图捕获队列按批量大小降序排序。批量大小相同的桶按序列长度升序排序（例如 `(64, 128)`、`(64, 256)`、`(32, 128)`、`(32, 256)`、`(1, 128)`、`(1, 256)`），解码的默认策略 - `min_tokens` - 图捕获队列按每个图处理的 token 数量（`batch_size*sequence_length`）升序排序，预填充的默认策略。

当有大量请求挂起时，vLLM 调度器将尝试尽快填充解码的最大批量大小。当请求完成时，解码批量大小减少。此时，vLLM  将尝试为等待队列中的请求安排预填充迭代，以将解码批量大小恢复到之前的状态。这意味着在满载场景中，解码批量大小通常处于最大值，这使得捕获大批量  HPU 图至关重要，这反映在 `max_bs` 策略中。另一方面，预填充最常以极低的批量大小（1-4）执行，这反映在 `min_tokens` 策略中。

> **注意** >`VLLM_GRAPH_PROMPT_RATIO` 并未为每个阶段（预填充和解码）设置严格的内存限制。vLLM 将首先尝试为预填充 HPU 图使用全部可用预填充图内存（可用图内存 * `VLLM_GRAPH_PROMPT_RATIO`），接着对解码图执行相同操作。如果一个阶段已完全捕获，并且可用图内存池中有剩余内存，vLLM 将尝试为另一个阶段捕获更多图，直到无法在不超出保留内存池的情况下捕获更多 HPU 图。此机制的行为可在以下示例中观察到。

每个描述的步骤均由 vLLM 服务器记录，如下所示（负值表示内存被释放）：

```plain
INFO 08-02 17:37:44 hpu_model_runner.py:493] Prompt bucket config (min, step, max_warmup) bs:[1, 32, 4], seq:[128, 128, 1024]
INFO 08-02 17:37:44 hpu_model_runner.py:499] Generated 24 prompt buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024)]
INFO 08-02 17:37:44 hpu_model_runner.py:504] Decode bucket config (min, step, max_warmup) bs:[1, 128, 4], seq:[128, 128, 2048]
INFO 08-02 17:37:44 hpu_model_runner.py:509] Generated 48 decode buckets: [(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (1, 1152), (1, 1280), (1, 1408), (1, 1536), (1, 1664), (1, 1792), (1, 1920), (1, 2048), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (2, 1152), (2, 1280), (2, 1408), (2, 1536), (2, 1664), (2, 1792), (2, 1920), (2, 2048), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024), (4, 1152), (4, 1280), (4, 1408), (4, 1536), (4, 1664), (4, 1792), (4, 1920), (4, 2048)]
INFO 08-02 17:37:52 hpu_model_runner.py:430] Pre-loading model weights on hpu:0 took 14.97 GiB of device memory (14.97 GiB/94.62 GiB used) and 2.95 GiB of host memory (475.2 GiB/1007 GiB used)
INFO 08-02 17:37:52 hpu_model_runner.py:438] Wrapping in HPU Graph took 0 B of device memory (14.97 GiB/94.62 GiB used) and -252 KiB of host memory (475.2 GiB/1007 GiB used)
INFO 08-02 17:37:52 hpu_model_runner.py:442] Loading model weights took in total 14.97 GiB of device memory (14.97 GiB/94.62 GiB used) and 2.95 GiB of host memory (475.2 GiB/1007 GiB used)
INFO 08-02 17:37:54 hpu_worker.py:134] Model profiling run took 504 MiB of device memory (15.46 GiB/94.62 GiB used) and 180.9 MiB of host memory (475.4 GiB/1007 GiB used)
INFO 08-02 17:37:54 hpu_worker.py:158] Free device memory: 79.16 GiB, 39.58 GiB usable (gpu_memory_utilization=0.5), 15.83 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.4), 23.75 GiB reserved for KV cache
INFO 08-02 17:37:54 hpu_executor.py:85] # HPU blocks: 1519, # CPU blocks: 0
INFO 08-02 17:37:54 hpu_worker.py:190] Initializing cache engine took 23.73 GiB of device memory (39.2 GiB/94.62 GiB used) and -1.238 MiB of host memory (475.4 GiB/1007 GiB used)
INFO 08-02 17:37:54 hpu_model_runner.py:1066] [Warmup][Prompt][1/24] batch_size:4 seq_len:1024 free_mem:55.43 GiB
...
INFO 08-02 17:38:22 hpu_model_runner.py:1066] [Warmup][Decode][48/48] batch_size:1 seq_len:128 free_mem:55.43 GiB
INFO 08-02 17:38:22 hpu_model_runner.py:1159] Using 15.85 GiB/55.43 GiB of free device memory for HPUGraphs, 7.923 GiB for prompt and 7.923 GiB for decode (VLLM_GRAPH_PROMPT_RATIO=0.3)
INFO 08-02 17:38:22 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][1/24] batch_size:1 seq_len:128 free_mem:55.43 GiB
...
INFO 08-02 17:38:26 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][11/24] batch_size:1 seq_len:896 free_mem:48.77 GiB
INFO 08-02 17:38:27 hpu_model_runner.py:1066] [Warmup][Graph/Decode][1/48] batch_size:4 seq_len:128 free_mem:47.51 GiB
...
INFO 08-02 17:38:41 hpu_model_runner.py:1066] [Warmup][Graph/Decode][48/48] batch_size:1 seq_len:2048 free_mem:47.35 GiB
INFO 08-02 17:38:41 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][12/24] batch_size:4 seq_len:256 free_mem:47.35 GiB
INFO 08-02 17:38:42 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][13/24] batch_size:2 seq_len:512 free_mem:45.91 GiB
INFO 08-02 17:38:42 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][14/24] batch_size:1 seq_len:1024 free_mem:44.48 GiB
INFO 08-02 17:38:43 hpu_model_runner.py:1066] [Warmup][Graph/Prompt][15/24] batch_size:2 seq_len:640 free_mem:43.03 GiB
INFO 08-02 17:38:43 hpu_model_runner.py:1128] Graph/Prompt captured:15 (62.5%) used_mem:14.03 GiB buckets:[(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (4, 128), (4, 256)]
INFO 08-02 17:38:43 hpu_model_runner.py:1128] Graph/Decode captured:48 (100.0%) used_mem:161.9 MiB buckets:[(1, 128), (1, 256), (1, 384), (1, 512), (1, 640), (1, 768), (1, 896), (1, 1024), (1, 1152), (1, 1280), (1, 1408), (1, 1536), (1, 1664), (1, 1792), (1, 1920), (1, 2048), (2, 128), (2, 256), (2, 384), (2, 512), (2, 640), (2, 768), (2, 896), (2, 1024), (2, 1152), (2, 1280), (2, 1408), (2, 1536), (2, 1664), (2, 1792), (2, 1920), (2, 2048), (4, 128), (4, 256), (4, 384), (4, 512), (4, 640), (4, 768), (4, 896), (4, 1024), (4, 1152), (4, 1280), (4, 1408), (4, 1536), (4, 1664), (4, 1792), (4, 1920), (4, 2048)]
INFO 08-02 17:38:43 hpu_model_runner.py:1206] Warmup finished in 49 secs, allocated 14.19 GiB of device memory
INFO 08-02 17:38:43 hpu_executor.py:91] init_cache_engine took 37.92 GiB of device memory (53.39 GiB/94.62 GiB used) and 57.86 MiB of host memory (475.4 GiB/1007 GiB used)
```

#### 推荐的 vLLM 参数

- 我们建议在 Gaudi 2 上使用 `block_size` 为 128 进行 BF16 数据类型的推理。使用默认值（16、32）可能由于矩阵乘法引擎利用不足而导致性能欠佳（参阅 [Gaudi 架构](https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html)）。
- 对于 Llama 7B 的最大吞吐量，我们建议启用 HPU 图，批量大小为 128 或 256，最大上下文长度为 2048。如果遇到内存不足问题，请参阅故障排除部分。

#### 环境变量

**诊断和分析旋钮：**

- `VLLM_PROFILER_ENABLED`：如果为 `true`，将启用高级分析器。生成的 JSON 跟踪可在 [perfetto.habana.ai](https://perfetto.habana.ai/#!/viewer) 查看。默认禁用。
- `VLLM_HPU_LOG_STEP_GRAPH_COMPILATION`：如果为 `true`，将记录每个 vLLM 引擎步骤的图编译（仅在发生编译时）。强烈建议与 `PT_HPU_METRICS_GC_DETAILS=1` 一起使用。默认禁用。
- `VLLM_HPU_LOG_STEP_GRAPH_COMPILATION_ALL`：如果为 `true`，将始终记录每个 vLLM 引擎步骤的图编译（即使未发生）。默认禁用。
- `VLLM_HPU_LOG_STEP_CPU_FALLBACKS`：如果为 `true`，将记录每个 vLLM 引擎步骤的 CPU 回退（仅在发生回退时）。默认禁用。
- `VLLM_HPU_LOG_STEP_CPU_FALLBACKS_ALL`：如果为 `true`，将始终记录每个 vLLM 引擎步骤的 CPU 回退（即使未发生）。默认禁用。

**性能调优旋钮：**

- `VLLM_SKIP_WARMUP`：如果为 `true`，将跳过预热，默认 `false`
- `VLLM_GRAPH_RESERVED_MEM`：保留用于 HPU 图捕获的内存百分比，默认 `0.1`
- `VLLM_GRAPH_PROMPT_RATIO`：保留用于预填充图的图内存百分比，默认 `0.3`
- `VLLM_GRAPH_PROMPT_STRATEGY`：确定预填充图捕获顺序的策略，`min_tokens` 或 `max_bs`，默认 `min_tokens`
- `VLLM_GRAPH_DECODE_STRATEGY`：确定解码图捕获顺序的策略，`min_tokens` 或 `max_bs`，默认 `max_bs`
- `VLLM_{phase}_{dim}_BUCKET_{param}` - 共 12 个环境变量，用于配置分桶机制的范围
  - `{phase}` 为 `PROMPT` 或 `DECODE`
  - `{dim}` 为 `BS`、`SEQ` 或 `BLOCK`
  - `{param}` 为 `MIN`、`STEP` 或 `MAX`
  - 默认值：
    - 预填充：
      - 批量大小最小值（`VLLM_PROMPT_BS_BUCKET_MIN`）：`1`
      - 批量大小步长（`VLLM_PROMPT_BS_BUCKET_STEP`）：`min(max_num_seqs, 32)`
      - 批量大小最大值（`VLLM_PROMPT_BS_BUCKET_MAX`）：`min(max_num_seqs, 64)`
      - 序列长度最小值（`VLLM_PROMPT_SEQ_BUCKET_MIN`）：`block_size`
      - 序列长度步长（`VLLM_PROMPT_SEQ_BUCKET_STEP`）：`block_size`
      - 序列长度最大值（`VLLM_PROMPT_SEQ_BUCKET_MAX`）：`max_model_len`
    - 解码：
      - 批量大小最小值（`VLLM_DECODE_BS_BUCKET_MIN`）：`1`
      - 批量大小步长（`VLLM_DECODE_BS_BUCKET_STEP`）：`min(max_num_seqs, 32)`
      - 批量大小最大值（`VLLM_DECODE_BS_BUCKET_MAX`）：`max_num_seqs`
      - 序列长度最小值（`VLLM_DECODE_BLOCK_BUCKET_MIN`）：`block_size`
      - 序列长度步长（`VLLM_DECODE_BLOCK_BUCKET_STEP`）：`block_size`
      - 序列长度最大值（`VLLM_DECODE_BLOCK_BUCKET_MAX`）：`max(128, (max_num_seqs*max_model_len)/block_size)`

此外，以下 HPU PyTorch Bridge 环境变量会影响 vLLM 执行：

- `PT_HPU_LAZY_MODE`：如果为 `0`，将使用 Gaudi 的 PyTorch 即时后端；如果为 `1`，将使用 Gaudi 的 PyTorch 延迟后端，默认 `1`
- `PT_HPU_ENABLE_LAZY_COLLECTIVES`：必须为 `true` 以支持使用 HPU 图的张量并行推理

### 故障排除：调整 HPU 图

如果您遇到设备内存不足问题或尝试以更高批量大小进行推理，请按照以下步骤调整 HPU 图：

- 调整 `gpu_memory_utilization` 旋钮。这将减少 KV 缓存的分配，为捕获更大批量的图留出空间。默认 `gpu_memory_utilization` 设置为 0.9，尝试分配约 90% 的 HBM 剩余内存用于 KV 缓存。注意，减少此值会减少可用 KV 缓存块的数量，从而减少给定时间内可处理的最大 token 数。
- 如果此方法无效，您可以完全禁用 `HPUGraph`。禁用 HPU 图后，您将以较低批次的延迟和吞吐量为代价，换取较高批次下潜在的更高吞吐量。您可以通过向服务器添加 `--enforce-eager` 标志（用于在线服务）或向 LLM 构造函数传递 `enforce_eager=True` 参数（用于离线推理）来实现此目的。

## AWS Neuron

此设备没有额外信息。