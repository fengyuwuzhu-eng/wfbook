# 库

[TOC]

## Python

The Ollama Python library provides the easiest way to integrate Python 3.8+ projects with [Ollama](https://github.com/ollama/ollama).

## Prerequisites



- [Ollama](https://ollama.com/download) should be installed and running

- Pull a model to use with the library: 

  ```
  ollama pull <model>
  ```

   e.g. 

  ```
  ollama pull llama3.2
  ```

  - See [Ollama.com](https://ollama.com/search) for more information on the models available.

## Install



```
pip install ollama
```

​    

## Usage



```
from ollama import chat
from ollama import ChatResponse

response: ChatResponse = chat(model='llama3.2', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])
# or access fields directly from the response object
print(response.message.content)
```

​    

See [_types.py](https://github.com/ollama/ollama-python/blob/main/ollama/_types.py) for more information on the response types.

## Streaming responses



Response streaming can be enabled by setting `stream=True`.

```
from ollama import chat

stream = chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],
    stream=True,
)

for chunk in stream:
  print(chunk['message']['content'], end='', flush=True)
```

​    

## Custom client



A custom client can be created by instantiating `Client` or `AsyncClient` from `ollama`.

All extra keyword arguments are passed into the [`httpx.Client`](https://www.python-httpx.org/api/#client).

```
from ollama import Client
client = Client(
  host='http://localhost:11434',
  headers={'x-some-header': 'some-value'}
)
response = client.chat(model='llama3.2', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
```

​    

## Async client



The `AsyncClient` class is used to make asynchronous requests. It can be configured with the same fields as the `Client` class.

```
import asyncio
from ollama import AsyncClient

async def chat():
  message = {'role': 'user', 'content': 'Why is the sky blue?'}
  response = await AsyncClient().chat(model='llama3.2', messages=[message])

asyncio.run(chat())
```

​    

Setting `stream=True` modifies functions to return a Python asynchronous generator:

```
import asyncio
from ollama import AsyncClient

async def chat():
  message = {'role': 'user', 'content': 'Why is the sky blue?'}
  async for part in await AsyncClient().chat(model='llama3.2', messages=[message], stream=True):
    print(part['message']['content'], end='', flush=True)

asyncio.run(chat())
```

​    

## API



The Ollama Python library's API is designed around the [Ollama REST API](https://github.com/ollama/ollama/blob/main/docs/api.md)

### Chat



```
ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])
```

​    

### Generate



```
ollama.generate(model='llama3.2', prompt='Why is the sky blue?')
```

​    

### List



```
ollama.list()
```

​    

### Show



```
ollama.show('llama3.2')
```

​    

### Create



```
ollama.create(model='example', from_='llama3.2', system="You are Mario from Super Mario Bros.")
```

​    

### Copy



```
ollama.copy('llama3.2', 'user/llama3.2')
```

​    

### Delete



```
ollama.delete('llama3.2')
```

​    

### Pull



```
ollama.pull('llama3.2')
```

​    

### Push



```
ollama.push('user/llama3.2')
```

​    

### Embed



```
ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')
```

​    

### Embed (batch)



```
ollama.embed(model='llama3.2', input=['The sky is blue because of rayleigh scattering', 'Grass is green because of chlorophyll'])
```

​    

### Ps



```
ollama.ps()
```

​    

## Errors



Errors are raised if requests return an error status or if an error is detected while streaming.

```
model = 'does-not-yet-exist'

try:
  ollama.chat(model)
except ollama.ResponseError as e:
  print('Error:', e.error)
  if e.status_code == 404:
    ollama.pull(model)
```

​    

## Javascript

The Ollama JavaScript library provides the easiest way to integrate your JavaScript project with [Ollama](https://github.com/jmorganca/ollama).

## Getting Started



```
npm i ollama
```

​    

## Usage



```
import ollama from 'ollama'

const response = await ollama.chat({
  model: 'llama3.1',
  messages: [{ role: 'user', content: 'Why is the sky blue?' }],
})
console.log(response.message.content)
```

​    

### Browser Usage



To use the library without node, import the browser module.

```
import ollama from 'ollama/browser'
```

​    

## Streaming responses



Response streaming can be enabled by setting `stream: true`, modifying function calls to return an `AsyncGenerator` where each part is an object in the stream.

```
import ollama from 'ollama'

const message = { role: 'user', content: 'Why is the sky blue?' }
const response = await ollama.chat({ model: 'llama3.1', messages: [message], stream: true })
for await (const part of response) {
  process.stdout.write(part.message.content)
}
```

​    

## API



The Ollama JavaScript library's API is designed around the [Ollama REST API](https://github.com/jmorganca/ollama/blob/main/docs/api.md)

### chat



```
ollama.chat(request)
```

​    

- `request` `<Object>`: The request object containing chat parameters.

  - `model` `<string>` The name of the model to use for the chat.

  - ```
    messages
    ```

    ```
    <Message[]>
    ```

    : Array of message objects representing the chat history.

    - `role` `<string>`: The role of the message sender ('user', 'system', or 'assistant').
    - `content` `<string>`: The content of the message.
    - `images` `<Uint8Array[] | string[]>`: (Optional) Images to be included in the message, either as Uint8Array or base64 encoded strings.

  - `format` `<string>`: (Optional) Set the expected format of the response (`json`).

  - `stream` `<boolean>`: (Optional) When true an `AsyncGenerator` is returned.

  - `keep_alive` `<string | number>`:  (Optional) How long to keep the model loaded. A number (seconds) or a  string with a duration unit suffix ("300ms", "1.5h", "2h45m", etc.)

  - `tools` `<Tool[]>`: (Optional) A list of tool calls the model may make.

  - `options` `<Options>`: (Optional) Options to configure the runtime.

- Returns: `<ChatResponse>`

### generate



```
ollama.generate(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing generate parameters.

  - `model` `<string>` The name of the model to use for the chat.
  - `prompt` `<string>`: The prompt to send to the model.
  - `suffix` `<string>`: (Optional) Suffix is the text that comes after the inserted text.
  - `system` `<string>`: (Optional) Override the model system prompt.
  - `template` `<string>`: (Optional) Override the model template.
  - `raw` `<boolean>`: (Optional) Bypass the prompt template and pass the prompt directly to the model.
  - `images` `<Uint8Array[] | string[]>`: (Optional) Images to be included, either as Uint8Array or base64 encoded strings.
  - `format` `<string>`: (Optional) Set the expected format of the response (`json`).
  - `stream` `<boolean>`: (Optional) When true an `AsyncGenerator` is returned.
  - `keep_alive` `<string | number>`:  (Optional) How long to keep the model loaded. A number (seconds) or a  string with a duration unit suffix ("300ms", "1.5h", "2h45m", etc.)
  - `options` `<Options>`: (Optional) Options to configure the runtime.

- Returns: `<GenerateResponse>`

### pull



```
ollama.pull(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing pull parameters.

  - `model` `<string>` The name of the model to pull.
  - `insecure` `<boolean>`: (Optional) Pull from servers whose identity cannot be verified.
  - `stream` `<boolean>`: (Optional) When true an `AsyncGenerator` is returned.

- Returns: `<ProgressResponse>`

### push



```
ollama.push(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing push parameters.

  - `model` `<string>` The name of the model to push.
  - `insecure` `<boolean>`: (Optional) Push to servers whose identity cannot be verified.
  - `stream` `<boolean>`: (Optional) When true an `AsyncGenerator` is returned.

- Returns: `<ProgressResponse>`

### create



```
ollama.create(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing create parameters.

  - `model` `<string>` The name of the model to create.
  - `from` `<string>`: The base model to derive from.
  - `stream` `<boolean>`: (Optional) When true an `AsyncGenerator` is returned.
  - `quantize` `<string>`: Quanization precision level (`q8_0`, `q4_K_M`, etc.).
  - `template` `<string>`: (Optional) The prompt template to use with the model.
  - `license` `<string|string[]>`: (Optional) The license(s) associated with the model.
  - `system` `<string>`: (Optional) The system prompt for the model.
  - `parameters` `<Record<string, unknown>>`: (Optional) Additional model parameters as key-value pairs.
  - `messages` `<Message[]>`: (Optional) Initial chat messages for the model.
  - `adapters` `<Record<string, string>>`: (Optional) A key-value map of LoRA adapter configurations.

- Returns: `<ProgressResponse>`

Note: The `files` parameter is not currently supported in `ollama-js`.

### delete



```
ollama.delete(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing delete parameters.

  - `model` `<string>` The name of the model to delete.

- Returns: `<StatusResponse>`

### copy



```
ollama.copy(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing copy parameters.

  - `source` `<string>` The name of the model to copy from.
  - `destination` `<string>` The name of the model to copy to.

- Returns: `<StatusResponse>`

### list



```
ollama.list()
```

​    

- Returns: `<ListResponse>`

### show



```
ollama.show(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing show parameters.

  - `model` `<string>` The name of the model to show.
  - `system` `<string>`: (Optional) Override the model system prompt returned.
  - `template` `<string>`: (Optional) Override the model template returned.
  - `options` `<Options>`: (Optional) Options to configure the runtime.

- Returns: `<ShowResponse>`

### embed



```
ollama.embed(request)
```

​    

- ```
  request
  ```

  ```
  <Object>
  ```

  : The request object containing embedding parameters.

  - `model` `<string>` The name of the model used to generate the embeddings.
  - `input` `<string> | <string[]>`: The input used to generate the embeddings.
  - `truncate` `<boolean>`: (Optional) Truncate the input to fit the maximum context length supported by the model.
  - `keep_alive` `<string | number>`:  (Optional) How long to keep the model loaded. A number (seconds) or a  string with a duration unit suffix ("300ms", "1.5h", "2h45m", etc.)
  - `options` `<Options>`: (Optional) Options to configure the runtime.

- Returns: `<EmbedResponse>`

### ps



```
ollama.ps()
```

​    

- Returns: `<ListResponse>`

### abort



```
ollama.abort()
```

​    

This method will abort **all** streamed generations currently running with the client instance. If there is a need to manage streams with timeouts, it is recommended to have one Ollama client per stream.

All asynchronous threads listening to streams (typically the `for await (const part of response)`) will throw an `AbortError` exception. See [examples/abort/abort-all-requests.ts](https://github.com/ollama/ollama-js/blob/main/examples/abort/abort-all-requests.ts) for an example.

## Custom client



A custom client can be created with the following fields:

- `host` `<string>`: (Optional) The Ollama host address. Default: `"http://127.0.0.1:11434"`.
- `fetch` `<Object>`: (Optional) The fetch library used to make requests to the Ollama host.

```
import { Ollama } from 'ollama'

const ollama = new Ollama({ host: 'http://127.0.0.1:11434' })
const response = await ollama.chat({
  model: 'llama3.1',
  messages: [{ role: 'user', content: 'Why is the sky blue?' }],
})
```

​    

## Building



To build the project files run:

```
npm run build
```

​    