## Customize a model 自定义模型



### Import from GGUF 从 GGUF 导入



Ollama supports importing GGUF models in the Modelfile:
Ollama 支持在 Modelfile 中导入 GGUF 模型：

1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.
   创建一个名为 `Modelfile` 的文件，其中包含 `FROM` 指令，其中包含要导入的模型的本地文件路径。

   ```
   FROM ./vicuna-33b.Q4_0.gguf
   ```

   ​    

Create the model in Ollama
在 Ollama 中创建模型

```
ollama create example -f Modelfile
```

​    

Run the model 运行模型

```
ollama run example
```

​    

### Import from Safetensors 从 Safetensor 导入



See the [guide](https://github.com/ollama/ollama/blob/main/docs/import.md) on importing models for more information.
请参阅 导入模型 以了解更多信息。

### Customize a prompt 自定义提示



Models from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:
Ollama 库中的模型可以通过提示进行自定义。例如，要自定义 `llama3.2` 模型：

```
ollama pull llama3.2
```

​    

Create a `Modelfile`:
创建一个 `Modelfile`：

```
FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
"""
```

​    

Next, create and run the model:
接下来，创建并运行模型：

```
ollama create mario -f ./Modelfile
ollama run mario
>>> hi
Hello! It's your friend Mario.
```

​    

For more information on working with a Modelfile, see the [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) documentation.
有关使用 Modelfile 的更多信息，请参阅 [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) 文档。

# mporting a model 导入模型



## Table of Contents 目录



- [Importing a Safetensors adapter
  导入 Safetensors 适配器](https://github.com/ollama/ollama/blob/main/docs/import.md#Importing-a-fine-tuned-adapter-from-Safetensors-weights)
- [Importing a Safetensors model
  导入 Safetensors 模型](https://github.com/ollama/ollama/blob/main/docs/import.md#Importing-a-model-from-Safetensors-weights)
- [Importing a GGUF file 导入 GGUF 文件](https://github.com/ollama/ollama/blob/main/docs/import.md#Importing-a-GGUF-based-model-or-adapter)
- [Sharing models on ollama.com
  在 ollama.com 上共享模型](https://github.com/ollama/ollama/blob/main/docs/import.md#Sharing-your-model-on-ollamacom)

## Importing a fine tuned adapter from Safetensors weights 从 Safetensors 权重导入微调的适配器



First, create a `Modelfile` with a `FROM` command pointing at the base model you used for fine tuning, and an `ADAPTER` command which points to the directory with your Safetensors adapter:
首先，创建一个 `Modelfile`，其中包含一个指向用于微调的基本模型的 `FROM` 命令，以及一个指向带有 Safetensors 适配器的目录的 `ADAPTER` 命令：

```
FROM <base model name>
ADAPTER /path/to/safetensors/adapter/directory
```

​    

Make sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get  erratic results. Most frameworks use different quantization methods, so  it's best to use non-quantized (i.e. non-QLoRA) adapters. If your  adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.
确保在 `FROM` 命令中使用与用于创建适配器相同的基本模型，否则将得到不稳定的结果。大多数框架使用不同的量化方法，因此最好使用非量化（即非 QLoRA）适配器。如果您的适配器与 `Modelfile` 位于同一目录中，请使用 `ADAPTER 。`指定适配器路径。

Now run `ollama create` from the directory where the `Modelfile` was created:
现在从创建 `Modelfile` 的目录运行 `ollama create`：

```
ollama create my-model
```

​    

Lastly, test the model: 最后，测试模型：

```
ollama run my-model
```

​    

Ollama supports importing adapters based on several different model architectures including:
Ollama 支持基于多种不同模型架构导入适配器，包括：

- Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
  骆驼（包括骆驼 2、骆驼 3、骆驼 3.1 和骆驼 3.2）;
- Mistral (including Mistral 1, Mistral 2, and Mixtral); and
  Mistral （包括 Mistral 1、Mistral 2 和 Mixtral）;和
- Gemma (including Gemma 1 and Gemma 2)
  Gemma（包括 Gemma 1 和 Gemma 2）

You can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:
您可以使用微调框架或工具创建适配器，该框架或工具可以以 Safetensors 格式输出适配器，例如：

- Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)
  Hugging Face [微调框架](https://huggingface.co/docs/transformers/en/training)
- [Unsloth](https://github.com/unslothai/unsloth)
- [MLX MLX 系列](https://github.com/ml-explore/mlx)

## Importing a model from Safetensors weights 从 Safetensors 权重导入模型



First, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:
首先，使用 `FROM` 命令创建一个 `Modelfile`，该命令指向包含 Safetensors 权重的目录：

```
FROM /path/to/safetensors/directory
```

​    

If you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.
如果您在权重所在的目录中创建 Modelfile，则可以使用命令 `FROM .`.

Now run the `ollama create` command from the directory where you created the `Modelfile`:
现在，从您创建 `Modelfile` 的目录运行 `ollama create` 命令：

```
ollama create my-model
```

​    

Lastly, test the model: 最后，测试模型：

```
ollama run my-model
```

​    

Ollama supports importing models for several different architectures including:
Ollama 支持导入多种不同架构的模型，包括：

- Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);
  骆驼（包括骆驼 2、骆驼 3、骆驼 3.1 和骆驼 3.2）;
- Mistral (including Mistral 1, Mistral 2, and Mixtral);
  Mistral （包括 Mistral 1、Mistral 2 和 Mixtral）;
- Gemma (including Gemma 1 and Gemma 2); and
  Gemma（包括 Gemma 1 和 Gemma 2）;和
- Phi3

This includes importing foundation models as well as any fine tuned models which have been *fused* with a foundation model.
这包括导入基础模型以及已与基础模型*融合*的任何微调模型。

## Importing a GGUF based model or adapter 导入基于 GGUF 的模型或适配器



If you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:
如果您有基于 GGUF 的模型或适配器，则可以将其导入 Ollama。您可以通过以下方式获取 GGUF 型号或适配器：

- converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp;
  使用 `Llama.cpp 的 convert_hf_to_gguf.py` 转换 Safetensors 模型;
- converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or
  使用 `Llama.cpp 的 convert_lora_to_gguf.py` 转换 Safetensors 适配器;或
- downloading a model or adapter from a place such as HuggingFace
  从 HuggingFace 等位置下载模型或适配器

To import a GGUF model, create a `Modelfile` containing:
要导入 GGUF 模型，请创建一个包含以下内容的 `Modelfile`：

```
FROM /path/to/file.gguf
```

​    

For a GGUF adapter, create the `Modelfile` with:
对于 GGUF 适配器，使用以下命令创建 `Modelfile`：

```
FROM <model name>
ADAPTER /path/to/file.gguf
```

​    

When importing a GGUF adapter, it's important to use the same base model as  the base model that the adapter was created with. You can use:
导入 GGUF 适配器时，请务必使用与创建适配器时使用的基本模型相同的基本模型。您可以使用：

- a model from Ollama 来自 Ollama 的模型
- a GGUF file 一个 GGUF 文件
- a Safetensors based model
  基于 Safetensors 的模型

Once you have created your `Modelfile`, use the `ollama create` command to build the model.
创建 `Modelfile` 后，使用 `ollama create` 命令构建模型。

```
ollama create my-model
```

​    

## Quantizing a Model 量化模型



Quantizing a model allows you to run models faster and with less memory  consumption but at reduced accuracy. This allows you to run a model on  more modest hardware.
量化模型可以让您更快地运行模型，并且内存消耗更少，但准确性会降低。这允许您在更适度的硬件上运行模型。

Ollama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.
Ollama 可以在 `ollama create` 命令中使用 `-q/--quantize` 标志，将基于 FP16 和 FP32 的模型量化为不同的量化级别。

First, create a Modelfile with the FP16 or FP32 based model you wish to quantize.
首先，使用要量化的基于 FP16 或 FP32 的模型创建一个 Modelfile。

```
FROM /path/to/my/gemma/f16/model
```

​    

Use `ollama create` to then create the quantized model.
然后使用 `ollama create` 创建量化模型。

```
$ ollama create --quantize q4_K_M mymodel
transferring model data
quantizing F16 model to Q4_K_M
creating new layer sha256:735e246cc1abfd06e9cdcf95504d6789a6cd1ad7577108a70d9902fef503c1bd
creating new layer sha256:0853f0ad24e5865173bbf9ffcc7b0f5d56b66fd690ab1009867e45e7d2c4db0f
writing manifest
success
```

​    

### Supported Quantizations 支持的量化



- `q4_0`
- `q4_1`
- `q5_0`
- `q5_1`
- `q8_0`

#### K-means Quantizations K-means 量化



- `q3_K_S`
- `q3_K_M`
- `q3_K_L`
- `q4_K_S`
- `q4_K_M`
- `q5_K_S`
- `q5_K_M`
- `q6_K`

## Sharing your model on ollama.com 在 ollama.com 上共享模型



You can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.
您可以通过将您创建的任何模型推送到 [ollama.com](https://ollama.com) 来共享该模型，以便其他用户可以试用。

First, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.
首先，使用浏览器转到 [Ollama 注册](https://ollama.com/signup)页面。如果您已经有账户，则可以跳过此步骤。

[![Sign-Up](https://github.com/ollama/ollama/raw/main/docs/images/signup.png)](https://github.com/ollama/ollama/blob/main/docs/images/signup.png)

The `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.
`Username` 字段将用作模型名称的一部分（例如 `jmorganca/mymodel`），因此请确保您对所选的用户名感到满意。

Now that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.
现在，您已经创建了一个帐户并已登录，请转到 [Ollama Keys Settings](https://ollama.com/settings/keys) 页面。

Follow the directions on the page to determine where your Ollama Public Key is located.
按照页面上的说明确定您的 Ollama 公钥所在的位置。

[![Ollama Keys](https://github.com/ollama/ollama/raw/main/docs/images/ollama-keys.png)](https://github.com/ollama/ollama/blob/main/docs/images/ollama-keys.png)

Click on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.
单击 `Add Ollama Public Key` 按钮，然后将 Ollama Public Key 的内容复制并粘贴到文本字段中。

To push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy your model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).
要将模型推送到 [ollama.com](https://ollama.com)，请首先确保使用你的用户名正确命名它。您可能必须使用 `ollama cp` 命令复制模型，以便为其指定正确的名称。对模型的名称感到满意后，使用 `ollama push` 命令将其推送到 [ollama.com](https://ollama.com)。

```
ollama cp mymodel myuser/mymodel
ollama push myuser/mymodel
```

​    

Once your model has been pushed, other users can pull and run it by using the command:
推送模型后，其他用户可以使用以下命令拉取并运行它：

```
ollama run myuser/mymodel
```

​    