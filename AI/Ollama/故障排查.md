# How to troubleshoot issues 如何排查问题



Sometimes Ollama may not perform as expected. One of the best ways to figure out  what happened is to take a look at the logs. Find the logs on **Mac** by running the command:
有时 Ollama 可能无法发挥预期效果。弄清楚发生了什么的最佳方法之一是查看日志。通过运行以下命令在 **Mac** 上查找日志：

```
cat ~/.ollama/logs/server.log
```

​    

On **Linux** systems with systemd, the logs can be found with this command:
在装有 systemd 的 **Linux** 系统上，可以使用以下命令找到日志：

```
journalctl -u ollama --no-pager
```

​    

When you run Ollama in a **container**, the logs go to stdout/stderr in the container:
当您在**容器中**运行 Ollama 时，日志将转到容器中的 stdout/stderr：

```
docker logs <container-name>
```

​    

(Use `docker ps` to find the container name)
（使用 `docker ps` 查找容器名称）

If manually running `ollama serve` in a terminal, the logs will be on that terminal.
如果在终端中手动运行 `ollama serve`，则日志将位于该终端上。

When you run Ollama on **Windows**, there are a few different locations. You can view them in the explorer window by hitting `<cmd>+R` and type in:
当您在 **Windows** 上运行 Ollama 时，有几个不同的位置。您可以通过点击 `<cmd>+R` 并在资源管理器窗口中输入以下内容来查看它们：

- `explorer %LOCALAPPDATA%\Ollama` to view logs.  The most recent server logs will be in `server.log` and older logs will be in `server-#.log`
   `explorer %LOCALAPPDATA%\Ollama` 以查看日志。最新的服务器日志将位于 `server.log` 中，较旧的日志将位于 `server-#.log`
- `explorer %LOCALAPPDATA%\Programs\Ollama` to browse the binaries (The installer adds this to your user PATH)
   `explorer %LOCALAPPDATA%\Programs\Ollama` 浏览二进制文件（安装程序将此添加到您的用户 PATH）
- `explorer %HOMEPATH%\.ollama` to browse where models and configuration is stored
  `explorer %HOMEPATH%\.ollama` 浏览模型和配置的存储位置
- `explorer %TEMP%` where temporary executable files are stored in one or more `ollama*` directories
  `资源管理器 %TEMP%，`其中临时可执行文件存储在一个或多个 `ollama*` 目录中

To enable additional debug logging to help troubleshoot problems, first **Quit the running app from the tray menu** then in a powershell terminal
要启用其他调试日志记录以帮助解决问题，请先**从托盘菜单中退出正在运行的应用程序**，然后在 powershell 终端中退出

```
$env:OLLAMA_DEBUG="1"
& "ollama app.exe"
```

​    

Join the [Discord](https://discord.gg/ollama) for help interpreting the logs.
加入 [Discord](https://discord.gg/ollama) 以帮助解释日志。

## LLM libraries LLM 图书馆



Ollama includes multiple LLM libraries compiled for different GPUs and CPU  vector features. Ollama tries to pick the best one based on the  capabilities of your system. If this autodetection has problems, or you  run into other problems (e.g. crashes in your GPU) you can workaround  this by forcing a specific LLM library. `cpu_avx2` will perform the best, followed by `cpu_avx` an the slowest but most compatible is `cpu`. Rosetta emulation under MacOS will work with the `cpu` library.
Ollama 包括为不同的 GPU 和 CPU 矢量功能编译的多个LLM库。Ollama 会尝试根据您的系统功能选择最好的。如果此自动检测有问题，或者您遇到其他问题（例如 GPU 崩溃），您可以通过强制使用特定LLM库来解决此问题。`cpu_avx2` 将表现最好，其次是 `cpu_avx` 最慢但兼容性最强的是 `CPU`。MacOS 下的 Rosetta 仿真将与 `cpu` 库一起使用。

In the server log, you will see a message that looks something like this (varies from release to release):
在服务器日志中，您将看到一条类似于这样的消息（因版本而异）：

```
Dynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v11 rocm_v5]
```

​    

**Experimental LLM Library Override
实验性LLM库覆盖**

You can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to  bypass autodetection, so for example, if you have a CUDA card, but want  to force the CPU LLM library with AVX2 vector support, use:
您可以将 OLLAMA_LLM_LIBRARY 设置为任何可用LLM库以绕过自动检测，因此，例如，如果您有 CUDA 卡，但想要强制使用支持 AVX2 向量的 CPU LLM 库，请使用：

```
OLLAMA_LLM_LIBRARY="cpu_avx2" ollama serve
```

​    

You can see what features your CPU has with the following.
您可以通过以下内容查看您的 CPU 具有哪些功能。

```
cat /proc/cpuinfo| grep flags | head -1
```

​    

## Installing older or pre-release versions on Linux 在 Linux 上安装旧版本或预发布版本



If you run into problems on Linux and want to install an older version, or you'd like to try out a pre-release before it's officially released,  you can tell the install script which version to install.
如果您在 Linux 上遇到问题并想要安装旧版本，或者您想在正式发布之前试用预发布版，您可以告诉安装脚本要安装哪个版本。

```
curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh
```

​    

## Linux tmp noexec



If your system is configured with the "noexec" flag where Ollama stores  its temporary executable files, you can specify an alternate location by setting OLLAMA_TMPDIR to a location writable by the user ollama runs  as. For example OLLAMA_TMPDIR=/usr/share/ollama/
如果您的系统配置了 “noexec” 标志，Ollama 在其中存储其临时可执行文件，您可以通过将 OLLAMA_TMPDIR 设置为用户运行 ollama 时可写入的位置来指定备用位置。例如OLLAMA_TMPDIR=/usr/share/ollama/

## Linux docker Linux 码头工人



If Ollama initially works on the GPU in a docker container, but then  switches to running on CPU after some period of time with errors in the  server log reporting GPU discovery failures, this can be resolved by  disabling systemd cgroup management in Docker.  Edit `/etc/docker/daemon.json` on the host and add `"exec-opts": ["native.cgroupdriver=cgroupfs"]` to the docker configuration.
如果 Ollama 最初在 Docker 容器中的 GPU 上运行，但一段时间后切换到在 CPU 上运行，服务器日志中出现报告 GPU 发现失败的错误，则可以通过在 Docker 中禁用 systemd cgroup 管理来解决。在主机上编辑 `/etc/docker/daemon.json` 并添加到 `"exec-opts": ["native.cgroupdriver=cgroupfs"]` docker 配置。

## NVIDIA GPU Discovery NVIDIA GPU 发现



When Ollama starts up, it takes inventory of the GPUs present in the system  to determine compatibility and how much VRAM is available.  Sometimes  this discovery can fail to find your GPUs.  In general, running the  latest driver will yield the best results.
当 Ollama 启动时，它会清点系统中存在的 GPU，以确定兼容性和可用的 VRAM 量。有时，此发现可能无法找到您的 GPU。通常，运行最新的驱动程序将产生最佳结果。

### Linux NVIDIA Troubleshooting Linux NVIDIA 故障排除



If you are using a container to run Ollama, make sure you've set up the container runtime first as described in [docker.md](https://github.com/ollama/ollama/blob/main/docs/docker.md)
如果您使用容器运行 Ollama，请确保首先按照 [docker.md](https://github.com/ollama/ollama/blob/main/docs/docker.md) 中所述设置容器运行时

Sometimes the Ollama can have difficulties initializing the GPU. When you check  the server logs, this can show up as various error codes, such as "3"  (not initialized), "46" (device unavailable), "100" (no device), "999"  (unknown), or others. The following troubleshooting techniques may help  resolve the problem
有时 Ollama 在初始化 GPU 时可能会遇到困难。当您检查服务器日志时，这可能会显示为各种错误代码，例如“3”（未初始化）、“46”（设备不可用）、“100”（无设备）、“999”（未知）或其他。以下故障排除技术可能有助于解决问题

- If you are using a container, is the container runtime working?  Try `docker run --gpus all ubuntu nvidia-smi` - if this doesn't work, Ollama won't be able to see your NVIDIA GPU.
  如果您使用的是容器，容器运行时是否正常工作？尝试 `docker run --gpus all ubuntu nvidia-smi` - 如果这不起作用，Ollama 将无法看到您的 NVIDIA GPU。
- Is the uvm driver loaded? `sudo nvidia-modprobe -u`
  uvm 驱动程序是否已加载？`sudo nvidia-modprobe -u`
- Try reloading the nvidia_uvm driver - `sudo rmmod nvidia_uvm` then `sudo modprobe nvidia_uvm`
  尝试重新加载 nvidia_uvm 驱动程序 - `sudo rmmod nvidia_uvm`然后 `sudo modprobe nvidia_uvm`
- Try rebooting 尝试重新启动
- Make sure you're running the latest nvidia drivers
  确保您运行的是最新的 nvidia 驱动程序

If none of those resolve the problem, gather additional information and file an issue:
如果这些方法都无法解决问题，请收集其他信息并提交问题：

- Set `CUDA_ERROR_LEVEL=50` and try again to get more diagnostic logs
  设置 `CUDA_ERROR_LEVEL=50` 并重试以获取更多诊断日志
- Check dmesg for any errors `sudo dmesg | grep -i nvrm` and `sudo dmesg | grep -i nvidia`
  检查 dmesg 是否有任何错误 `sudo dmesg | grep -i nvrm` 和 `sudo dmesg | grep -i nvidia`

## AMD GPU Discovery AMD GPU 发现



On linux, AMD GPU access typically requires `video` and/or `render` group membership to access the `/dev/kfd` device.  If permissions are not set up correctly, Ollama will detect this and report an error in the server log.
在 Linux 上，AMD GPU 访问通常需要`视频`和/或`渲染`组成员身份才能访问 `/dev/kfd` 设备。如果权限设置不正确，Ollama 将检测到此情况并在服务器日志中报告错误。

When running in a container, in some Linux distributions and container  runtimes, the ollama process may be unable to access the GPU.  Use `ls -lnd /dev/kfd /dev/dri /dev/dri/*` on the host system to determine the **numeric** group IDs on your system, and pass additional `--group-add ...` arguments to the container so it can access the required devices.   For example, in the following output `crw-rw---- 1 0  44 226,   0 Sep 16 16:55 /dev/dri/card0` the group ID column is `44`
在容器中运行时，在某些 Linux 发行版和容器运行时中，ollama 进程可能无法访问 GPU。在主机系统上使用 `ls -lnd /dev/kfd /dev/dri /dev/dri/*` 来确定系统上**的数字**组 ID，并将额外的 `--group-add ...`参数传递给容器，以便它可以访问所需的设备。例如，在以下输出 `crw-rw---- 1 0  44 226,   0 Sep 16 16:55 /dev/dri/card0` 中，组 ID 列为 `44`

If you are experiencing problems getting Ollama to correctly discover or  use your GPU for inference, the following may help isolate the failure.
如果您在让 Ollama 正确发现或使用 GPU 进行推理时遇到问题，以下内容可能有助于隔离故障。

- `AMD_LOG_LEVEL=3` Enable info log levels in the AMD HIP/ROCm libraries.  This can help  show more detailed error codes that can help troubleshoot problems
  `AMD_LOG_LEVEL=3`在 AMD HIP/ROCm 库中启用信息日志级别。这有助于显示更详细的错误代码，从而帮助解决问题
- `OLLAMA_DEBUG=1` During GPU discovery additional information will be reported
  `OLLAMA_DEBUG=1`在 GPU 发现期间，将报告其他信息
- Check dmesg for any errors from amdgpu or kfd drivers `sudo dmesg | grep -i amdgpu` and `sudo dmesg | grep -i kfd`
  检查 dmesg 中是否有来自 amdgpu 或 kfd 驱动程序的错误 `sudo dmesg | grep -i amdgpu` 和 `sudo dmesg | grep -i kfd`

## Multiple AMD GPUs 多个 AMD GPU



If you experience gibberish responses when models load across multiple AMD GPUs on Linux, see the following guide.
如果您在 Linux 上跨多个 AMD GPU 加载模型时遇到乱码响应，请参阅以下指南。

- https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/mgpu.html#mgpu-known-issues-and-limitations

## Windows Terminal Errors Windows 终端错误



Older versions of Windows 10 (e.g., 21H1) are known to have a bug where the  standard terminal program does not display control characters correctly.  This can result in a long string of strings like `←[?25h←[?25l` being displayed, sometimes erroring with `The parameter is incorrect`  To resolve this problem, please update to Win 10 22H1 or newer.
已知旧版本的 Windows 10（例如 21H1）存在标准终端程序无法正确显示控制字符的错误。这可能会导致显示一长串字符串，例如 `←[？25h←[？25l`]，有时会显示 `The parameter is incorrect` 要解决此问题，请更新到 Win 10 22H1 或更高版本。