# 安装

[TOC]

## Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

To install Ollama, run the following command:
要安装 Ollama，请运行以下命令：

```
curl -fsSL https://ollama.com/install.sh | sh
```

​    

## Manual install 手动安装





Note 注意

If you are upgrading from a prior version, you should remove the old libraries with `sudo rm -rf /usr/lib/ollama` first.
如果要从以前的版本升级，则应先使用 `sudo rm -rf /usr/lib/ollama` 删除旧库。

Download and extract the package:
下载并解压缩包：

```
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
sudo tar -C /usr -xzf ollama-linux-amd64.tgz
```

​    

Start Ollama: 启动 Ollama：

```
ollama serve
```

​    

In another terminal, verify that Ollama is running:
在另一个终端中，验证 Ollama 是否正在运行：

```
ollama -v
```

​    

### AMD GPU install AMD GPU 安装



If you have an AMD GPU, also download and extract the additional ROCm package:
如果您有 AMD GPU，还要下载并解压缩其他 ROCm 软件包：

```
curl -L https://ollama.com/download/ollama-linux-amd64-rocm.tgz -o ollama-linux-amd64-rocm.tgz
sudo tar -C /usr -xzf ollama-linux-amd64-rocm.tgz
```

​    

### ARM64 install ARM64 安装



Download and extract the ARM64-specific package:
下载并解压缩特定于 ARM64 的包：

```
curl -L https://ollama.com/download/ollama-linux-arm64.tgz -o ollama-linux-arm64.tgz
sudo tar -C /usr -xzf ollama-linux-arm64.tgz
```

​    

### Adding Ollama as a startup service (recommended) 将 Ollama 添加为启动服务（推荐）



Create a user and group for Ollama:
为 Ollama 创建用户和组：

```
sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama
sudo usermod -a -G ollama $(whoami)
```

​    

Create a service file in `/etc/systemd/system/ollama.service`:
在 中创建 `/etc/systemd/system/ollama.service` 服务文件 ：

```
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="PATH=$PATH"

[Install]
WantedBy=default.target
```

​    

Then start the service: 然后启动服务：

```
sudo systemctl daemon-reload
sudo systemctl enable ollama
```

​    

### Install CUDA drivers (optional) 安装 CUDA 驱动程序（可选）



[Download and install](https://developer.nvidia.com/cuda-downloads) CUDA.
[下载并安装](https://developer.nvidia.com/cuda-downloads)CUDA 的 CUDA 中。

Verify that the drivers are installed by running the following command, which should print details about your GPU:
通过运行以下命令验证驱动程序是否已安装，该命令应打印有关 GPU 的详细信息：

```
nvidia-smi
```

​    

### Install AMD ROCm drivers (optional) 安装 AMD ROCm 驱动程序（可选）



[Download and Install](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html) ROCm v6.
[下载并安装](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html)ROCm v6 的。

### Start Ollama 启动 Ollama



Start Ollama and verify it is running:
启动 Ollama 并验证它是否正在运行：

```
sudo systemctl start ollama
sudo systemctl status ollama
```

​    



Note 注意

While AMD has contributed the `amdgpu` driver upstream to the official linux kernel source, the version is older and may not support all ROCm features. We recommend you install the latest driver from https://www.amd.com/en/support/linux-drivers for best support of your Radeon GPU.
虽然 AMD 已将 `amdgpu` 驱动程序贡献到官方 linux 的上游 kernel 源，版本较旧，可能不支持所有 ROCm 功能。我们 建议您安装最新的驱动程序 https://www.amd.com/en/support/linux-drivers 为您的 Radeon GPU 提供最佳支持。

## Customizing 定制



To customize the installation of Ollama, you can edit the systemd service file or the environment variables by running:
要自定义 Ollama 的安装，您可以通过运行以下命令来编辑 systemd 服务文件或环境变量：

```
sudo systemctl edit ollama
```

​    

Alternatively, create an override file manually in `/etc/systemd/system/ollama.service.d/override.conf`:
或者，在 `/etc/systemd/system/ollama.service.d/override.conf` 中手动创建覆盖文件 ：

```
[Service]
Environment="OLLAMA_DEBUG=1"
```

​    

## Updating 更新



Update Ollama by running the install script again:
通过再次运行安装脚本来更新 Ollama：

```
curl -fsSL https://ollama.com/install.sh | sh
```

​    

Or by re-downloading Ollama:
或者通过重新下载 Ollama：

```
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
sudo tar -C /usr -xzf ollama-linux-amd64.tgz
```

​    

## Installing specific versions 安装特定版本



Use `OLLAMA_VERSION` environment variable with the install script to install a specific  version of Ollama, including pre-releases. You can find the version  numbers in the [releases page](https://github.com/ollama/ollama/releases).
`OLLAMA_VERSION`环境变量与安装脚本一起使用，以安装特定版本的 Ollama，包括预发行版。您可以在 [releases 页面](https://github.com/ollama/ollama/releases)中找到版本号。

For example: 例如：

```
curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh
```

​    

## Viewing logs 查看日志



To view logs of Ollama running as a startup service, run:
要查看作为启动服务运行的 Ollama 的日志，请运行：

```
journalctl -e -u ollama
```

​    

## Uninstall 卸载



Remove the ollama service:
删除 ollama 服务：

```
sudo systemctl stop ollama
sudo systemctl disable ollama
sudo rm /etc/systemd/system/ollama.service
```

​    

Remove the ollama binary from your bin directory (either `/usr/local/bin`, `/usr/bin`, or `/bin`):
从 bin 目录（`/usr/local/bin`、`/usr/bin` 或 `/bin`）中删除 ollama 二进制文件：

```
sudo rm $(which ollama)
```

​    

Remove the downloaded models and Ollama service user and group:
删除下载的模型以及 Ollama 服务用户和组：

```
sudo rm -r /usr/share/ollama
sudo userdel ollama
sudo groupdel ollama
```

​    

Remove installed libraries:
删除已安装的库：

```
sudo rm -rf /usr/local/lib/ollama
```

​    

## Docker

The official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.

## macOS

https://ollama.com/download/Ollama-darwin.zip

## Windows

https://ollama.com/download/OllamaSetup.exe

Welcome to Ollama for Windows.

No more WSL required!

Ollama now runs as a native Windows application, including NVIDIA and AMD Radeon GPU support. After installing Ollama for Windows, Ollama will run in the background and the `ollama` command line is available in `cmd`, `powershell` or your favorite terminal application. As usual the Ollama [api](https://github.com/ollama/ollama/blob/main/docs/api.md) will be served on `http://localhost:11434`.

## System Requirements



- Windows 10 22H2 or newer, Home or Pro
- NVIDIA 452.39 or newer Drivers if you have an NVIDIA card
- AMD Radeon Driver https://www.amd.com/en/support if you have a Radeon card

Ollama uses unicode characters for progress indication,  which may render as unknown squares in some older terminal fonts in  Windows 10. If you see this, try changing your terminal font settings.

## Filesystem Requirements



The Ollama install does not require Administrator, and installs in your  home directory by default.  You'll need at least 4GB of space for the  binary install.  Once you've installed Ollama, you'll need additional  space for storing the Large Language models, which can be tens to  hundreds of GB in size.  If your home directory doesn't have enough  space, you can change where the binaries are installed, and where the  models are stored.

### Changing Install Location



To install the Ollama application in a location different than your home directory, start the installer with the following flag

```
OllamaSetup.exe /DIR="d:\some\location"
```

​    

### Changing Model Location



To change where Ollama stores the downloaded models instead of using your home directory, set the environment variable `OLLAMA_MODELS` in your user account.

1. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for *environment variables*.
2. Click on *Edit environment variables for your account*.
3. Edit or create a new variable for your user account for `OLLAMA_MODELS` where you want the models stored
4. Click OK/Apply to save.

If Ollama is already running, Quit the tray application  and relaunch it from the Start menu, or a new terminal started after you saved the environment variables.

## API Access



Here's a quick example showing API access from `powershell`

```
(Invoke-WebRequest -method POST -Body '{"model":"llama3.2", "prompt":"Why is the sky blue?", "stream": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json
```

​    

## Troubleshooting



Ollama on Windows stores files in a few different locations.  You can view them in the explorer window by hitting `<Ctrl>+R` and type in:

- ```
  explorer %LOCALAPPDATA%\Ollama
  ```

   contains logs, and downloaded updates

  - *app.log* contains most resent logs from the GUI application
  - *server.log* contains the most recent server logs
  - *upgrade.log* contains log output for upgrades

- `explorer %LOCALAPPDATA%\Programs\Ollama` contains the binaries (The installer adds this to your user PATH)

- `explorer %HOMEPATH%\.ollama` contains models and configuration

- `explorer %TEMP%` contains temporary executable files in one or more `ollama*` directories

## Uninstall



The Ollama Windows installer registers an Uninstaller application.  Under `Add or remove programs` in Windows Settings, you can uninstall Ollama.



Note

If you have [changed the OLLAMA_MODELS location](https://github.com/ollama/ollama/blob/main/docs/windows.md#changing-model-location), the installer will not remove your downloaded models

## Standalone CLI



The easiest way to install Ollama on Windows is to use the `OllamaSetup.exe` installer. It installs in your account without requiring Administrator rights. We update Ollama regularly to support the latest models, and this installer will help you keep up to date.

If you'd like to install or integrate Ollama as a service, a standalone `ollama-windows-amd64.zip` zip file is available containing only the Ollama CLI and GPU library dependencies for Nvidia and AMD. This allows for embedding Ollama in existing applications, or running it as a system service via `ollama serve` with tools such as [NSSM](https://nssm.cc/).



Note

If you are upgrading from a prior version, you should remove the old directories first.