# 支持的模型列表

* deepseek-coder-v2
* deepseek-coder
* deepseek-llm
* deepseek-r1
* deepseek-v3
* deepseek-v2
* gemma
* llama3.3
* llama3.2
* llama3.1
* llama3
* mistral
* nomic-embed-text
* phi4
* phi3
* qwen2.5
* qwen2.5-coder
* qwen2
* qwen
* llava
* gemma2
* llama2
* codellama
* mxbai-embed-large
* llama3.2-vision
* tinyllama
* mistral-nemo
* starcoder2
* snowflake-arctic-embed
* llama2-uncensored
* mixtral
* dolphin-mixtral
* codegemma
* openthinker
* phi
* bge-m3
* minicpm-v
* llava-llama3
* wizardlm2
* dolphin-mistral
* all-minilm
* smollm2          
* dolphin-llama3
* command-r
* orca-mini
* yi
* hermes3
* phi3.5
* dolphin3
* zephyr
* codestral
* mistral-small
* olmo2
* granite-code
* starcoder          
* smollm
* wizard-vicuna-uncensored
* vicuna
* mistral-openorca          
* qwq          
* llama2-chinese
* openchat
* codegeex4
* aya
* codeqwen          
* mistral-large          
* nous-hermes2
* glm4          
* stable-code    
* openhermes          
* qwen2-math
* command-r-plus
* tinydolphin
* wizardcoder
* moondream
* bakllava 
* stablelm2
* neural-chat
* reflection
* wizard-math
* llama3-gradient
* llama3-chatqa          
* sqlcoder
* bge-large          
* xwinlm          
* dolphincoder          
* nous-hermes
* phind-codellama          
* llava-phi3
* yarn-llama2
* solar
* granite3.1-dense
* starling-lm          
* athene-v2
* wizardlm
* yi-coder          
* internlm2          
* samantha-mistral
* falcon
* nemotron-mini
* nemotron
* dolphin-phi
* orca2
* deepscaler
* wizardlm-uncensored          
* stable-beluga
* granite3-dense
* llama3-groq-tool-use
* deepseek-v2.5
* medllama2
* meditron
* llama-pro
* smallthinker
* yarn-mistral
* aya-expanse
* paraphrase-multilingual
* granite3-moe
* nexusraven          
* codeup
* falcon3
* nous-hermes2-mixtral         
* everythinglm
* shieldgemma
* granite3.1-moe
* snowflake-arctic-embed2          
* falcon2
* magicoder
* mathstral      
* marco-o1
* stablelm-zephyr          
* reader-lm
* solar-pro
* codebooga    
* duckdb-nsql          
* mistrallite 
* wizard-vicuna
* llama-guard3
* exaone3.5
* megadolphin
* nuextract
* opencoder
* notux
* open-orca-platypus2
* notus
* goliath
* bespoke-minicheck
* command-r7b     
* firefunction-v2
* tulu3
* dbrx          
* granite-embedding
* granite3-guardian
* alfred
* sailor2
* r1-1776
* granite3.2

# Ollama Model File Ollama 模型文件





Note 注意

`Modelfile` syntax is in development
`Modelfile` 语法正在开发中

A model file is the blueprint to create and share models with Ollama.
模型文件是创建和与 Ollama 共享模型的蓝图。

## Table of Contents 目录



- [Format 格式](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#format)
- [Examples 例子](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#examples)
- Instructions 指示
  - FROM (Required) FROM （必填）
    - [Build from existing model
      从现有模型构建](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#build-from-existing-model)
    - [Build from a Safetensors model
      从 Safetensors 模型构建](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#build-from-a-safetensors-model)
    - [Build from a GGUF file
      从 GGUF 文件构建](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#build-from-a-gguf-file)
  - PARAMETER 参数
    - [Valid Parameters and Values
      有效参数和值](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values)
  - TEMPLATE 模板
    - [Template Variables 模板变量](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#template-variables)
  - [SYSTEM 系统](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#system)
  - [ADAPTER 适配器](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#adapter)
  - [LICENSE 许可证](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#license)
  - [MESSAGE 消息](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#message)
- [Notes 笔记](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#notes)

## Format 格式



The format of the `Modelfile`:
`Modelfile` 的格式：

```
# comment
INSTRUCTION arguments
```

​    

| Instruction 指令                                             | Description 描述                                             |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| [`FROM`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#from-required) (required) [`FROM`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#from-required) （必需） | Defines the base model to use. 定义要使用的基础模型。        |
| [`PARAMETER`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter) | Sets the parameters for how Ollama will run the model. 设置 Ollama 如何运行模型的参数。 |
| [`TEMPLATE`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#template) | The full prompt template to be sent to the model. 要发送到模型的完整提示模板。 |
| [`SYSTEM`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#system) | Specifies the system message that will be set in the template. 指定将在模板中设置的系统消息。 |
| [`ADAPTER`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#adapter) | Defines the (Q)LoRA adapters to apply to the model. 定义要应用于模型的 （Q）LoRA 适配器。 |
| [`LICENSE`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#license) | Specifies the legal license. 指定合法许可证。                |
| [`MESSAGE`](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#message) | Specify message history. 指定消息历史记录。                  |

## Examples 例子



### Basic `Modelfile` 基本`模型文件`



An example of a `Modelfile` creating a mario blueprint:
`Modelfile` 创建 mario 蓝图的示例：

```
FROM llama3.2
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are Mario from super mario bros, acting as an assistant.
```

​    

To use this: 要使用它：

1. Save it as a file (e.g. `Modelfile`)
   将其另存为文件（例如 `Modelfile`）
2. `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>`
3. `ollama run choose-a-model-name`
4. Start using the model! 开始使用该模型！

To view the Modelfile of a given model, use the `ollama show --modelfile` command.
要查看给定模型的 Modelfile，请使用 `ollama show --modelfile` 命令。

```
ollama show --modelfile llama3.2
```

​    

**Output**: **输出**：

```
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM llama3.2:latest
FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29
TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"""
PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|reserved_special_token"
```

​    

## Instructions 指示



### FROM (Required) FROM （必填）



The `FROM` instruction defines the base model to use when creating a model.
`FROM` 指令定义创建模型时要使用的基本模型。

```
FROM <model name>:<tag>
```

​    

#### Build from existing model 从现有模型构建



```
FROM llama3.2
```

​    

A list of available base models: https://github.com/ollama/ollama#model-library Additional models can be found at: https://ollama.com/library
可用基本型号列表： https://github.com/ollama/ollama#model-library 其他型号可在以下位置找到： https://ollama.com/library

#### Build from a Safetensors model 从 Safetensors 模型构建



```
FROM <model directory>
```

​    

The model directory should contain the Safetensors weights for a supported architecture.
model 目录应包含受支持架构的 Safetensors 权重。

Currently supported model architectures:
当前支持的模型架构：

- Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)
  骆驼（包括 Llama 2、Llama 3、Llama 3.1 和 Llama 3.2）
- Mistral (including Mistral 1, Mistral 2, and Mixtral)
  Mistral （包括 Mistral 1、Mistral 2 和 Mixtral）
- Gemma (including Gemma 1 and Gemma 2)
  Gemma（包括 Gemma 1 和 Gemma 2）
- Phi3

#### Build from a GGUF file 从 GGUF 文件构建



```
FROM ./ollama-model.gguf
```

​    

The GGUF file location should be specified as an absolute path or relative to the `Modelfile` location.
GGUF 文件位置应指定为绝对路径或相对于 `Modelfile` 位置。

### PARAMETER 参数



The `PARAMETER` instruction defines a parameter that can be set when the model is run.
`PARAMETER` 指令定义可在模型运行时设置的参数。

```
PARAMETER <parameter> <parametervalue>
```

​    

#### Valid Parameters and Values 有效参数和值



| Parameter 参数    | Description 描述                                             | Value Type 值类型 | Example Usage 示例用法                |
| ----------------- | ------------------------------------------------------------ | ----------------- | ------------------------------------- |
| mirostat 米罗司他 | Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) 启用 Mirostat 采样以控制困惑度。（默认值：0、0 = 禁用、1 = Mirostat、2 = Mirostat 2.0） | int               | mirostat 0 Mirostat 0 系列            |
| mirostat_eta      | Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) 影响算法响应生成文本的反馈的速度。较低的学习率将导致较慢的调整，而较高的学习率将使算法的响应速度更快。（默认值：0.1） | float 浮          | mirostat_eta 0.1                      |
| mirostat_tau      | Controls the balance between coherence and diversity of the output. A lower  value will result in more focused and coherent text. (Default: 5.0) 控制输出的连贯性和多样性之间的平衡。较低的值将导致文本更集中、更连贯。（默认值：5.0） | float 浮          | mirostat_tau 5.0                      |
| num_ctx           | Sets the size of the context window used to generate the next token. (Default: 2048) 设置用于生成下一个标记的上下文窗口的大小。（默认值：2048） | int               | num_ctx 4096                          |
| repeat_last_n     | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx) 设置模型回溯多长时间以防止重复。（默认值：64,0 = 禁用，-1 = num_ctx） | int               | repeat_last_n 64                      |
| repeat_penalty    | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will  penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1) 设置对重复项的惩罚强度。较高的值（例如 1.5）将更强烈地惩罚重复，而较低的值（例如 0.9）将更宽松。（默认值：1.1） | float 浮          | repeat_penalty 1.1                    |
| temperature 温度  | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8) 模型的温度。提高温度会使模型更有创意地回答。（默认值：0.8） | float 浮          | temperature 0.7 温度 0.7              |
| seed 种子         | Sets the random number seed to use for generation. Setting this to a  specific number will make the model generate the same text for the same  prompt. (Default: 0) 设置用于生成的随机数种子。将此设置为特定数字将使模型为同一提示生成相同的文本。（默认值：0） | int               | seed 42 种子 42                       |
| stop 停           | Sets the stop sequences to use. When this pattern is encountered the LLM  will stop generating text and return. Multiple stop patterns may be set  by specifying multiple separate `stop` parameters in a modelfile. 设置要使用的停止序列。当遇到此模式时，LLM将停止生成文本并返回。通过在模型文件中指定多个单独的`停止`参数，可以设置多个停止模式。 | string 字符串     | stop "AI assistant:" 停止 “AI 助手：” |
| num_predict       | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation) 生成文本时要预测的最大令牌数。（默认值：-1，无限生成） | int               | num_predict 42                        |
| top_k             | Reduces the probability of generating nonsense. A higher value (e.g. 100) will  give more diverse answers, while a lower value (e.g. 10) will be more  conservative. (Default: 40) 降低产生无意义的可能性。较高的值（例如 100）将给出更多样化的答案，而较低的值（例如 10）将更保守。（默认值：40） | int               | top_k 40                              |
| top_p             | Works together with top-k. A higher value (e.g., 0.95) will lead to more  diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9) 与 top-k 一起使用。较高的值（例如 0.95）将导致文本更加多样化，而较低的值（例如 0.5）将生成更集中和保守的文本。（默认值：0.9） | float 浮          | top_p 0.9                             |
| min_p             | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter *p* represents the minimum probability for a token to be considered,  relative to the probability of the most likely token. For example, with *p*=0.05 and the most likely token having a probability of 0.9, logits with a  value less than 0.045 are filtered out. (Default: 0.0) 替代top_p，旨在确保质量和多样性的平衡。参数 *p* 表示相对于最可能标记的概率，考虑标记的最小概率。例如，当 *p*=0.05 且最可能的标记的概率为 0.9 时，将筛选掉值小于 0.045 的 logit。（默认值：0.0） | float 浮          | min_p 0.05                            |

### TEMPLATE 模板



`TEMPLATE` of the full prompt template to be passed into the model. It may include (optionally) a system message, a user's message and the response from  the model. Note: syntax may be model specific. Templates use Go [template syntax](https://pkg.go.dev/text/template).
要传递到模型的完整提示模板的 `TEMPLATE`。它可能包括（可选）系统消息、用户消息和来自模型的响应。注意：语法可能特定于模型。模板使用 Go [模板语法](https://pkg.go.dev/text/template)。

#### Template Variables 模板变量



| Variable 变量                   | Description 描述                                             |
| ------------------------------- | ------------------------------------------------------------ |
| `{{ .System }}` `{{ .系统 }}`   | The system message used to specify custom behavior. 用于指定自定义行为的系统消息。 |
| `{{ .Prompt }}` `{{ .提示 }}`   | The user prompt message. 用户提示消息。                      |
| `{{ .Response }}` `{{ .响应 }}` | The response from the model. When generating a response, text after this variable is omitted. 来自模型的响应。生成响应时，将省略此变量后的文本。 |

```
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
```

​    

### SYSTEM 系统



The `SYSTEM` instruction specifies the system message to be used in the template, if applicable.
`SYSTEM` 指令指定要在模板中使用的系统消息（如果适用）。

```
SYSTEM """<system message>"""
```

​    

### ADAPTER 适配器



The `ADAPTER` instruction specifies a fine tuned LoRA adapter that should apply to  the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a `FROM` instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.
`ADAPTER` 指令指定应应用于基本模型的微调 LoRA 适配器。适配器的值应该是绝对路径或相对于 Modelfile 的路径。基本模型应使用 `FROM` 指令指定。如果基本模型与调整适配器的基本模型不同，则行为将不稳定。

#### Safetensor adapter Safetensor 适配器



```
ADAPTER <path to safetensor adapter>
```

​    

Currently supported Safetensor adapters:
当前支持的 Safetensor 适配器：

- Llama (including Llama 2, Llama 3, and Llama 3.1)
  骆驼（包括 Llama 2、Llama 3 和 Llama 3.1）
- Mistral (including Mistral 1, Mistral 2, and Mixtral)
  Mistral （包括 Mistral 1、Mistral 2 和 Mixtral）
- Gemma (including Gemma 1 and Gemma 2)
  Gemma（包括 Gemma 1 和 Gemma 2）

#### GGUF adapter GGUF 适配器



```
ADAPTER ./ollama-lora.gguf
```

​    

### LICENSE 许可证



The `LICENSE` instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.
`LICENSE` 指令允许您指定共享或分发与此 Modelfile 一起使用的模型的合法许可证。

```
LICENSE """
<license text>
"""
```

​    

### MESSAGE 消息



The `MESSAGE` instruction allows you to specify a message history for the model to  use when responding. Use multiple iterations of the MESSAGE command to  build up a conversation which will guide the model to answer in a  similar way.
`MESSAGE` 指令允许您为模型指定响应时使用的消息历史记录。使用 MESSAGE 命令的多次迭代来构建对话，这将指导模型以类似的方式回答。

```
MESSAGE <role> <message>
```

​    

#### Valid roles 有效角色



| Role 角色      | Description 描述                                             |
| -------------- | ------------------------------------------------------------ |
| system 系统    | Alternate way of providing the SYSTEM message for the model. 为模型提供 SYSTEM 消息的另一种方法。 |
| user 用户      | An example message of what the user could have asked. 用户可能询问的内容的示例消息。 |
| assistant 助理 | An example message of how the model should respond. 模型应如何响应的示例消息。 |

#### Example conversation 示例对话



```
MESSAGE user Is Toronto in Canada?
MESSAGE assistant yes
MESSAGE user Is Sacramento in Canada?
MESSAGE assistant no
MESSAGE user Is Ontario in Canada?
MESSAGE assistant yes
```

​    

## Notes 笔记



- the **`Modelfile` is not case sensitive**. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.
  **`Modelfile` 不区分大小写**。在示例中，使用大写指令来更容易将其与参数区分开来。
- Instructions can be in any order. In the examples, the `FROM` instruction is first to keep it easily readable.
  说明可以按任何顺序排列。在示例中，`FROM` 指令首先是为了使其易于阅读。