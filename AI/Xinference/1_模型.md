# 模型

[TOC]

## 模型列表

可以列出 Xinference 中所有可以启动的、某种类型的模型：

```bash
xinference registrations --model-type <MODEL_TYPE> [--endpoint "http://<XINFERENCE_HOST>:<XINFERENCE_PORT>"]
```

```bash
curl http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1/model_registrations/<MODEL_TYPE>
```

```python
from xinference.client import Client
client = Client("http://<XINFERENCE_HOST>:<XINFERENCE_PORT>")
print(client.list_model_registrations(model_type='<MODEL_TYPE>'))
```

Xinference 支持以下 `MODEL_TYPE`：

* LLM                        文本生成模型或大型语言模型
* embedding           文本嵌入模型
* image                     图像生成或处理模型
* audio                     音频模型
* rerank                    重排序模型
* video                      视频模型

如果需要的模型不可用，Xinference 还允许你注册自己的 [自定义模型](https://inference.readthedocs.io/zh-cn/latest/models/custom.html#models-custom)。

## 启动和停止模型

每个运行的模型实例将被分配一个唯一的模型 uid 。默认情况下，模型 uid 等于模型名。这个 ID 是后续使用模型实例的句柄，启动命令 `--model-uid` 选项可以手动指定它。

可以通过命令行或者 Xinference 的 Python 客户端来启动一个模型。

```bash
xinference launch --model-name <MODEL_NAME> [--model-engine <MODEL_ENGINE>] [--model-type <MODEL_TYPE>] \
                  [--model-uid <MODEL_UID>] [--endpoint "http://<XINFERENCE_HOST>:<XINFERENCE_PORT>"]
```

```python
from xinference.client import Client

client = Client("http://<XINFERENCE_HOST>:<XINFERENCE_PORT>")
model_uid = client.launch_model(
  model_name="<MODEL_NAME>",
  model_engine="<MODEL_ENGINE>",
  model_type="<MODEL_TYPE>"
  model_uid="<MODEL_UID>"
)
print(model_uid)
```

对于模型类型 `LLM`，启动模型不仅需要指定模型名称，还需要参数的大小、模型格式以及模型引擎。请参考 [大语言模型](https://inference.readthedocs.io/zh-cn/latest/models/builtin/llm/index.html#models-llm-index) 文档。

以下命令可以列出 Xinference 中正在运行的模型：

```bash
xinference list [--endpoint "http://<XINFERENCE_HOST>:<XINFERENCE_PORT>"]
```

```bash
curl http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1/models
```

```python
from xinference.client import Client

client = Client("http://<XINFERENCE_HOST>:<XINFERENCE_PORT>")
print(client.list_models())
```

当你不再需要当前正在运行的模型时，以下列方式释放其占用的资源：

```bash
xinference terminate --model-uid "<MODEL_UID>" [--endpoint "http://<XINFERENCE_HOST>:<XINFERENCE_PORT>"]
```

```bash
curl -X DELETE http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1/models/<MODEL_UID>
```

```python
from xinference.client import Client

client = Client("http://<XINFERENCE_HOST>:<XINFERENCE_PORT>")
client.terminate_model(model_uid="<MODEL_UID>")
```

## 模型使用

### 聊天 & 生成

具备 `chat` 或 `generate` 能力的模型通常被称为大型语言模型（LLM）或文本生成模型。这些模型旨在根据接收到的输入以文本输出方式进行回应，通常被称为“提示”。一般来说，可以通过特定指令或提供具体示例来引导这些模型完成任务。

具备 `generate` 能力的模型通常是预训练的大型语言模型。另一方面，配备 `chat` 功能的模型是经过精调和对齐的 LLM（Language Model），专为对话场景进行优化。在大多数情况下，以 “chat” 结尾的模型（例如 `llama-2-chat`，`qwen-chat` 等）则具有 `chat` 功能。

Chat API 和 Generate API 提供了两种不同的与 LLMs 进行交互的方法：

- Chat API（类似于 OpenAI 的 [Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create)）可以进行多轮对话。
- Generate API（类似于 OpenAI 的 [Completions API](https://platform.openai.com/docs/api-reference/completions/create) ）允许您根据文本提示生成文本。

| 模型能力 | API 端点     | OpenAI 兼容端点      |
| -------- | ------------ | -------------------- |
| chat     | Chat API     | /v1/chat/completions |
| generate | Generate API | /v1/completions      |

#### Chat API

尝试使用 cURL、OpenAI Client 或 Xinference的 Python 客户端来测试 Chat API：

```bash
curl -X 'POST' \
  'http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "<MODEL_UID>",
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is the largest animal?"
        }
    ],
    "max_tokens": 512,
    "temperature": 0.7
  }'
```

```
import openai

client = openai.Client(
    api_key="cannot be empty",
    base_url="http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1"
)
client.chat.completions.create(
    model="<MODEL_UID>",
    messages=[
        {
            "content": "What is the largest animal?",
            "role": "user",
        }
    ],
    max_tokens=512,
    temperature=0.7
)
```

```python
from xinference.client import RESTfulClient

client = RESTfulClient("http://<XINFERENCE_HOST>:<XINFERENCE_PORT>")
model = client.get_model("<MODEL_UID>")
messages = [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": "What is the largest animal?"}]
model.chat(
    messages,
    generate_config={
      "max_tokens": 512,
      "temperature": 0.7
    }
)
```

输入：

```json
{
  "id": "chatcmpl-8d76b65a-bad0-42ef-912d-4a0533d90d61",
  "model": "<MODEL_UID>",
  "object": "chat.completion",
  "created": 1688919187,
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The largest animal that has been scientifically measured is the blue whale, which has a maximum length of around 23 meters (75 feet) for adult animals and can weigh up to 150,000 pounds (68,000 kg). However, it is important to note that this is just an estimate and that the largest animal known to science may be larger still. Some scientists believe that the largest animals may not have a clear \"size\" in the same way that humans do, as their size can vary depending on the environment and the stage of their life."
      },
      "finish_reason": "None"
    }
  ],
  "usage": {
    "prompt_tokens": -1,
    "completion_tokens": -1,
    "total_tokens": -1
  }
}
```

#### Generate API

Generate API 复刻了 OpenAI 的 [Completions API](https://platform.openai.com/docs/api-reference/completions/create)。

Generate API 和 Chat API 之间的区别主要在于输入形式。Chat API 接受一个消息列表作为输入，Generate API 接受一个名为 prompt 的自由文本字符串作为输入。

```bash
curl -X 'POST' \
  'http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "<MODEL_UID>",
    "prompt": "What is the largest animal?",
    "max_tokens": 512,
    "temperature": 0.7
  }'
```

```bash
import openai

client = openai.Client(api_key="cannot be empty", base_url="http://<XINFERENCE_HOST>:<XINFERENCE_PORT>/v1")
client.chat.completions.create(
    model=("<MODEL_UID>",
    messages=[
        {"role": "user", "content": "What is the largest animal?"}
    ],
    max_tokens=512,
    temperature=0.7
)
```

```python
from xinference.client import RESTfulClient

client = RESTfulClient("http://<XINFERENCE_HOST>:<XINFERENCE_PORT>")
model = client.get_model("<MODEL_UID>")
print(model.generate(
    prompt="What is the largest animal?",
    generate_config={
      "max_tokens": 512,
      "temperature": 0.7
    }
))
```

输出：

```json
{
  "id": "cmpl-8d76b65a-bad0-42ef-912d-4a0533d90d61",
  "model": "<MODEL_UID>",
  "object": "text_completion",
  "created": 1688919187,
  "choices": [
    {
      "index": 0,
      "text": "The largest animal that has been scientifically measured is the blue whale, which has a maximum length of around 23 meters (75 feet) for adult animals and can weigh up to 150,000 pounds (68,000 kg). However, it is important to note that this is just an estimate and that the largest animal known to science may be larger still. Some scientists believe that the largest animals may not have a clear \"size\" in the same way that humans do, as their size can vary depending on the environment and the stage of their life.",
      "finish_reason": "None"
    }
  ],
  "usage": {
    "prompt_tokens": -1,
    "completion_tokens": -1,
    "total_tokens": -1
  }
}
```

#### FAQ

##### Xinference 的 LLM 是否提供与 LangChain 或 LlamaIndex 的集成方法？

是的，可以参考它们各自官方 Xinference 文档中的相关部分。以下是链接：

- [LangChain LLMs: Xinference](https://python.langchain.com/docs/integrations/llms/xinference)
- [LlamaIndex LLM integrations: Xinference](https://docs.llamaindex.ai/en/stable/examples/llm/xinference_local_deployment.html)