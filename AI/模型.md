# 模型

[TOC]

## deepseek-r1

DeepSeek's first-generation of reasoning models with comparable performance to  OpenAI-o1, including six dense models distilled from DeepSeek-R1 based  on Llama and Qwen.
DeepSeek 的第一代推理模型，性能可与 OpenAI-o1 相媲美，包括从 DeepSeek-R1 中提炼出来的基于 Llama 和 Qwen 的 6 个密集模型。

1.5b	7b	8b	14b	32b	70b	671b

##             llama3.3

New state of the art 70B model. Llama 3.3 70B offers similar performance compared to the Llama 3.1 405B model.
最先进的 70B 型号。与 Llama 3.3 70B 型号相比，Llama 3.1 405B 提供相似的性能。

tools	70b

##             phi4

Phi-4 is a 14B parameter, state-of-the-art open model from Microsoft.
Phi-4 是 Microsoft 的 14B 参数、最先进的开放模型。

​                                                              14b

##             llama3.2

Meta's Llama 3.2 goes small with 1B and 3B models. 
Meta 的 Llama 3.2 通过 1B 和 3B 模型变小。

​                                                  tools 工具                                      1b                          3b

##             llama3.1

Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.
Llama 3.1 是 Meta 推出的最先进的新型号，提供 8B、70B 和 405B 参数大小。

​                                                  tools 工具                                      8b                          70b                          405b

##             nomic-embed-text

A high-performing open embedding model with a large token context window.
具有大型 token 上下文窗口的高性能开放嵌入模型。

​                                      embedding

##             mistral

The 7B model released by Mistral AI, updated to version 0.3.
Mistral AI 发布的 7B 模型，更新到 0.3 版本。

​                                                  tools 工具                                      7b

##             llama3

Meta Llama 3: The most capable openly available LLM to date
Meta Llama 3：迄今为止最有能力的公开可用LLM

​                                                              8b                          70b

##             qwen2.5

Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset,  encompassing up to 18 trillion tokens. The model supports up to 128K  tokens and has multilingual support. 
Qwen2.5 模型在阿里巴巴最新的大规模数据集上进行了预训练，包含多达 18 万亿个代币。该模型最多支持 128K 个令牌，并具有多语言支持。

​                                                  tools 工具                                      0.5b 0.5 字节                          1.5b                          3b                          7b                          14b                          32b                          72b

##             qwen

Qwen 1.5 is a series of large language models by Alibaba Cloud spanning from 0.5B to 110B parameters
Qwen 1.5 是阿里云推出的一系列大型语言模型，参数范围从 0.5B 到 110B 不等

​                                                              0.5b 0.5 字节                          1.8b                          4b                          7b                          14b                          32b                          72b                          110b

##             gemma

Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1
Gemma 是由 Google DeepMind 构建的一系列轻量级、最先进的开放模型。更新至版本 1.1

​                                                              2b                          7b

##             qwen2

Qwen2 is a new series of large language models from Alibaba group
Qwen2 是阿里巴巴集团推出的一系列新的大型语言模型

​                                                  tools 工具                                      0.5b 0.5 字节                          1.5b                          7b                          72b

##             qwen2.5-coder

The latest series of Code-Specific Qwen models, with significant  improvements in code generation, code reasoning, and code fixing.
最新系列的特定于代码的 Qwen 模型，在代码生成、代码推理和代码修复方面有重大改进。

​                                                  tools 工具                                      0.5b 0.5 字节                          1.5b                          3b                          7b                          14b                          32b

##             llava

🌋 LLaVA is a novel end-to-end trained large multimodal model that  combines a vision encoder and Vicuna for general-purpose visual and  language understanding. Updated to version 1.6.
🌋 LLaVA 是一种新型的端到端训练大型多模态模型，它结合了视觉编码器和 Vicuna，用于通用视觉和语言理解。更新至 1.6 版本。

​                        vision 视觉                                                              7b                          13b                          34b

##             gemma2

Google Gemma 2 is a high-performing and efficient model available in three sizes: 2B, 9B, and 27B.
Google Gemma 2 是一款高性能、高效的型号，有三种尺寸可供选择：2B、9B 和 27B。

​                                                              2b                          9b                          27b

##             llama2

Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.
Llama 2 是基础语言模型的集合，参数范围从 7B 到 70B 不等。

​                                                              7b                          13b                          70b

##             phi3

Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.
Phi-3 是 Microsoft 推出的轻量级 3B （Mini） 和 14B （Medium） 先进开放式型号系列。

​                                                              3.8b                          14b

##             codellama

A large language model that can use text prompts to generate and discuss code.
一个大型语言模型，可以使用文本提示来生成和讨论代码。

​                                                              7b                          13b                          34b                          70b

##             mxbai-embed-large

State-of-the-art large embedding model from mixedbread.ai
来自 mixedbread.ai 的最先进的大型嵌入模型

​                                      embedding 嵌入

##             llama3.2-vision

Llama 3.2 Vision is a collection of instruction-tuned image reasoning generative models in 11B and 90B sizes.
Llama 3.2 Vision 是 11B 和 90B 大小的指令调整图像推理生成模型的集合。

​                        vision 视觉                                                              11b                          90b

##             tinyllama

The TinyLlama project is an open endeavor to train a compact 1.1B Llama model on 3 trillion tokens.
TinyLlama 项目是一项公开的努力，旨在在 3 万亿个代币上训练一个紧凑的 1.1B Llama 模型。

​                                                              1.1b

##             mistral-nemo

A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.
具有 128k 上下文长度的先进 12B 模型，由 Mistral AI 与 NVIDIA 合作构建。

​                                                  tools 工具                                      12b

##             starcoder2

StarCoder2 is the next generation of transparently trained open code LLMs that comes in three sizes: 3B, 7B and 15B parameters. 
StarCoder2 是下一代透明训练的开放代码LLMs，有三种大小：3B、7B 和 15B 参数。

​                                                              3b                          7b                          15b

##             snowflake-arctic-embed

A suite of text embedding models by Snowflake, optimized for performance.
Snowflake 提供的一套文本嵌入模型，针对性能进行了优化。

​                                      embedding 嵌入                                                  22m 22 分钟                          33m 33 分钟                          110m 110 分钟                          137m 137 分钟                          335m

##             deepseek-coder-v2

An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.
一种开源 Mixture-of-Experts 代码语言模型，可在特定于代码的任务中实现与 GPT4-Turbo 相当的性能。

​                                                              16b                          236b

##             deepseek-v3

A strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.
一个强大的专家混合 （MoE） 语言模型，总共有 671B 个参数，每个标记激活了 37B。

​                                                              671b

##             llama2-uncensored

Uncensored Llama 2 model by George Sung and Jarrad Hope.
未经审查的 Llama 2 模型，由 George Sung 和 Jarrad Hope 设计。

​                                                              7b                          70b

##             deepseek-coder

DeepSeek Coder is a capable coding model trained on two trillion code and natural language tokens.
DeepSeek Coder 是一个功能强大的编码模型，使用 2 万亿个代码和自然语言标记进行训练。

​                                                              1.3b                          6.7b 6.7 字节                          33b

##             mixtral

A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes.
一组专家混合 （MoE） 模型，由 Mistral AI 以 8x7b 和 8x22b 参数大小提供开放权重。

​                                                  tools 工具                                      8x7b                          8x22b

##             dolphin-mixtral

Uncensored, 8x7b and 8x22b fine-tuned models based on the Mixtral mixture of  experts models that excels at coding tasks. Created by Eric Hartford.
未经审查的 8x7b 和 8x22b 微调模型，基于 Mixtral 混合专家模型，擅长编码任务。由 Eric Hartford 创建。

​                                                              8x7b                          8x22b

##             codegemma

CodeGemma is a collection of powerful, lightweight models that can perform a  variety of coding tasks like fill-in-the-middle code completion, code  generation, natural language understanding, mathematical reasoning, and  instruction following.
CodeGemma 是一组功能强大的轻量级模型，可以执行各种编码任务，如填充中间代码完成、代码生成、自然语言理解、数学推理和指令跟踪。

​                                                              2b                          7b

##             openthinker

A fully open-source family of reasoning models built using a dataset derived by distilling DeepSeek-R1.
一个完全开源的推理模型系列，使用通过蒸馏 DeepSeek-R1 得出的数据集构建。

​                                                              7b                          32b

##             phi

Phi-2: a 2.7B language model by Microsoft Research that demonstrates  outstanding reasoning and language understanding capabilities.
Phi-2：Microsoft Research 的 2.7B 语言模型，展示了出色的推理和语言理解能力。

​                                                              2.7b

##             bge-m3

BGE-M3 is a new model from BAAI distinguished for its versatility in  Multi-Functionality, Multi-Linguality, and Multi-Granularity.
BGE-M3 是 BAAI 的一款新型号，以其在多功能、多语言和多粒度方面的多功能性而著称。

​                                      embedding 嵌入                                                  567m

##             minicpm-v

A series of multimodal LLMs (MLLMs) designed for vision-language understanding.
一系列专为视觉-语言理解而设计的多模态 LLMs （MLLM）。

​                        vision 视觉                                                              8b

##             llava-llama3

A LLaVA model fine-tuned from Llama 3 Instruct with better scores in several benchmarks.
从 Llama 3 Instruct 微调的 LLaVA 模型，在多个基准测试中得分更高。

​                        vision 视觉                                                              8b

##             wizardlm2

State of the art large language model from Microsoft AI with improved  performance on complex chat, multilingual, reasoning and agent use  cases.
来自 Microsoft AI 的最先进的大型语言模型，在复杂聊天、多语言、推理和代理用例上的性能得到了改进。

​                                                              7b                          8x22b

##             dolphin-mistral

The uncensored Dolphin model based on Mistral that excels at coding tasks. Updated to version 2.8.
基于 Mistral 的未经审查的 Dolphin 模型，擅长编码任务。更新至 2.8 版。

​                                                              7b

##             all-minilm

Embedding models on very large sentence level datasets.
将模型嵌入到非常大的句子级数据集上。

​                                      embedding 嵌入                                                  22m 22 分钟                          33m

##             smollm2          

SmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters.
SmolLM2 是一系列紧凑的语言模型，有三种尺寸可供选择：135M、360M 和 1.7B 参数。

​                                                  tools 工具                                      135m 135 分钟                          360m 360 米                          1.7b 

##             dolphin-llama3

Dolphin 2.9 is a new model with 8B and 70B sizes by Eric Hartford based on  Llama 3 that has a variety of instruction, conversational, and coding  skills.
Dolphin 2.9 是 Eric Hartford 基于 Llama 8 开发的 70B 和 3B 大小的新模型，具有多种教学、对话和编码技能。

​                                                              8b                          70b

##             command-r

Command R is a Large Language Model optimized for conversational interaction and long context tasks.
Command R 是一种大型语言模型，针对对话交互和长上下文任务进行了优化。

​                                                  tools 工具                                      35b

##             orca-mini

A general-purpose model ranging from 3 billion parameters to 70 billion, suitable for entry-level hardware.
从 30 亿个参数到 700 亿个参数不等的通用模型，适用于入门级硬件。

​                                                              3b                          7b                          13b                          70b

##             yi

Yi 1.5 is a high-performing, bilingual language model.
Yi 1.5 是一种高性能的双语语言模型。

​                                                              6b                          9b                          34b

##             hermes3

Hermes 3 is the latest version of the flagship Hermes series of LLMs by Nous Research
爱马仕 3 是 Nous Research LLMs 的旗舰爱马仕系列的最新版本

​                                                  tools 工具                                      3b                          8b                          70b                          405b

##             phi3.5

A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.
一个轻量级的 AI 模型，具有 38 亿个参数，其性能超过了类似和更大尺寸的模型。

​                                                              3.8b

##             dolphin3

Dolphin 3.0 Llama 3.1 8B 🐬 is the next generation of the Dolphin series of  instruct-tuned models designed to be the ultimate general purpose local  model, enabling coding, math, agentic, function calling, and general use cases.
Dolphin 3.0 Llama 3.1 8B 🐬 是 Dolphin 系列指令调优模型的下一代产品，旨在成为终极通用本地模型，支持编码、数学、代理、函数调用和一般用例。

​                                                              8b

##             zephyr

Zephyr is a series of fine-tuned versions of the Mistral and Mixtral models that are trained to act as helpful assistants.
Zephyr 是 Mistral 和 Mixtral 模型的一系列微调版本，经过训练，可以充当有用的助手。

​                                                              7b                          141b

##             codestral

Codestral is Mistral AI’s first-ever code model designed for code generation tasks.
Codestral 是 Mistral AI 有史以来第一个专为代码生成任务而设计的代码模型。

​                                                              22b

##             mistral-small

Mistral Small 3 sets a new benchmark in the “small” Large Language Models category below 70B.
Mistral Small 3 在 70B 以下的“小型”大型语言模型类别中树立了新的标杆。

​                                                  tools 工具                                      22b                          24b

##             olmo2

OLMo 2 is a new family of 7B and 13B models trained on up to 5T tokens.  These models are on par with or better than equivalently sized fully  open models, and competitive with open-weight models such as Llama 3.1  on English academic benchmarks.
OLMo 2 是一个新的 7B 和 13B 模型系列，可在高达 5T 的令牌上进行训练。这些模型与同等大小的完全开放模型相当或更好，并且在英语学术基准测试中与 Llama 3.1 等开放重量模型竞争。

​                                                              7b                          13b

##             granite-code

A family of open foundation models by IBM for Code Intelligence
IBM 面向 Code Intelligence 的一系列开放基础模型

​                                                              3b                          8b                          20b                          34b

##             starcoder          

StarCoder is a code generation model trained on 80+ programming languages.
StarCoder 是一种在 80+ 编程语言上训练的代码生成模型。

​                                                              1b                          3b                          7b                          15b

##             smollm

🪐 A family of small models with 135M, 360M, and 1.7B parameters, trained on a new high-quality dataset.
🪐 一系列具有 135M、360M 和 1.7B 参数的小模型，在新的高质量数据集上进行训练。

​                                                              135m 135 分钟                          360m 360 米                          1.7b

##             wizard-vicuna-uncensored

Wizard Vicuna Uncensored is a 7B, 13B, and 30B parameter model based on Llama 2 uncensored by Eric Hartford.
Wizard Vicuna Uncensored 是一个基于 Eric Hartford 的 Llama 2 uncensored 的 7B、13B 和 30B 参数模型。

​                                                              7b                          13b                          30b

##             vicuna

General use chat model based on Llama and Llama 2 with 2K to 16K context sizes.
基于 Llama 和 Llama 2 的通用聊天模型，上下文大小为 2K 到 16K。

​                                                              7b                          13b                          33b

##             mistral-openorca          

Mistral OpenOrca is a 7 billion parameter model, fine-tuned on top of the Mistral 7B model using the OpenOrca dataset.
Mistral OpenOrca 是一个 70 亿参数模型，使用 OpenOrca 数据集在 Mistral 7B 模型之上进行了微调。

​                                                              7b

##             qwq          

QwQ is an experimental research model focused on advancing AI reasoning capabilities.
QwQ 是一种实验研究模型，专注于提高 AI 推理能力。

​                                                  tools 工具                                      32b

##             llama2-chinese

Llama 2 based model fine tuned to improve Chinese dialogue ability.
基于 Llama 2 的模型进行了微调，以提高中文对话能力。

​                                                              7b                          13b

##             openchat

A family of open-source models trained on a wide variety of data,  surpassing ChatGPT on various benchmarks. Updated to version 3.5-0106.
一系列基于各种数据进行训练的开源模型，在各种基准测试中都超过了 ChatGPT。更新至版本 3.5-0106。

​                                                              7b

##             codegeex4

A versatile model for AI software development scenarios, including code completion.
适用于 AI 软件开发场景的通用模型，包括代码完成。

​                                                              9b

##             aya

Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. 
Aya 23 由 Cohere 发布，是支持 23 种语言的最先进的多语言模型的新系列。

​                                                              8b                          35b

##             codeqwen          

CodeQwen1.5 is a large language model pretrained on a large amount of code data.
CodeQwen1.5 是一个基于大量代码数据进行预训练的大型语言模型。

​                                                              7b

##             deepseek-llm

An advanced language model crafted with 2 trillion bilingual tokens.
使用 2 万亿个双语标记制作的高级语言模型。

7b                          67b

##             mistral-large          

Mistral Large 2 is Mistral's new flagship model that is significantly more  capable in code generation, mathematics, and reasoning with 128k context window and support for dozens of languages.
Mistral Large 2 是 Mistral 的新旗舰模型，它在代码生成、数学和推理方面的能力要强得多，具有 128k 上下文窗口，并支持数十种语言。

tools 工具                                      123b

##             deepseek-v2

A strong, economical, and efficient Mixture-of-Experts language model.
一个强大、经济且高效的 Mixture-of-Experts 语言模型。

16b                          236b

##             nous-hermes2

The powerful family of models by Nous Research that excels at scientific discussion and coding tasks.
Nous Research 强大的模型系列，擅长科学讨论和编码任务。

10.7b                          34b

##             glm4          

A strong multi-lingual general language model with competitive performance to Llama 3.
强大的多语言通用语言模型，具有与 Llama 3 相比具有竞争力的性能。

9b

##             stable-code 稳定代码          

Stable Code 3B is a coding model with instruct and code completion variants on par with models such as Code Llama 7B that are 2.5x larger.
Stable Code 3B 是一种编码模型，具有 instruct 和 code completion 变体，与 Code Llama 7B 等模型相同，后者的尺寸是 2.5 倍。

3b

##             openhermes          

OpenHermes 2.5 is a 7B model fine-tuned by Teknium on Mistral with fully open datasets.
OpenHermes 2.5 是由 Teknium 在 Mistral 上微调的 7B 模型，具有完全开放的数据集。

##             qwen2-math

Qwen2 Math is a series of specialized math language models built upon the  Qwen2 LLMs, which significantly outperforms the mathematical  capabilities of open-source models and even closed-source models (e.g.,  GPT4o).
Qwen2 Math 是一系列基于 Qwen2 构建的专用数学语言模型LLMs，其数学能力明显优于开源模型甚至闭源模型（例如 GPT4o）。

1.5b                          7b                          72b

##             command-r-plus

Command R+ is a powerful, scalable large language model purpose-built to excel at real-world enterprise use cases.
Command R+ 是一个功能强大、可扩展的大型语言模型，专为在实际企业用例中脱颖而出而构建。

tools 工具                                      104b

##             tinydolphin

An experimental 1.1B parameter model trained on the new Dolphin 2.8 dataset by Eric Hartford and based on TinyLlama.
由 Eric Hartford 在新的 Dolphin 2.8 数据集上训练的实验性 1.1B 参数模型，该模型基于 TinyLlama。

1.1b

##             wizardcoder

State-of-the-art code generation model
最先进的代码生成模型

33b 

##             moondream

moondream2 is a small vision language model designed to run efficiently on edge devices.
MoonDream2 是一种小型视觉语言模型，旨在在边缘设备上高效运行。

vision 视觉                                                              1.8b

##             bakllava

BakLLaVA is a multimodal model consisting of the Mistral 7B base model augmented with the LLaVA  architecture.
BakLLaVA 是一个多模态模型，由 Mistral 7B 基本模型组成，并增强了 LLaVA 架构。

vision 视觉                                                              7b

##             stablelm2

Stable LM 2 is a state-of-the-art 1.6B and 12B parameter language model  trained on multilingual data in English, Spanish, German, Italian,  French, Portuguese, and Dutch.
Stable LM 2 是最先进的 1.6B 和 12B 参数语言模型，基于英语、西班牙语、德语、意大利语、法语、葡萄牙语和荷兰语的多语言数据进行训练。

1.6b                          12b

##             neural-chat

A fine-tuned model based on Mistral with good coverage of domain and language.
基于 Mistral 的微调模型，具有良好的域和语言覆盖率。

7b

##             reflection

A high-performing model trained with a new technique called  Reflection-tuning that teaches a LLM to detect mistakes in its reasoning and correct course.
一种高性能模型，使用一种称为 Reflection-tuning 的新技术进行训练，该技术教 a LLM 检测其推理中的错误并纠正过程。

70b

##             wizard-math

Model focused on math and logic problems
模型侧重于数学和逻辑问题

7b                          13b                          70b

##             llama3-gradient

This model extends LLama-3 8B's context length from 8k to over 1m tokens.
此模型将 LLama-3 8B 的上下文长度从 8k 扩展到超过 1m 令牌。

8b                          70b

##             llama3-chatqa          

A model from NVIDIA based on Llama 3 that excels at conversational  question answering (QA) and retrieval-augmented generation (RAG).
NVIDIA 基于 Llama 3 的模型，擅长对话问答 （QA） 和检索增强生成 （RAG）。

8b                          70b

##             sqlcoder

SQLCoder is a code completion model fined-tuned on StarCoder for SQL generation tasks
SQLCoder 是一种代码完成模型，在 StarCoder 上针对 SQL 生成任务进行了微调

7b                          15b

##             bge-large          

Embedding model from BAAI mapping texts to vectors.
将 BAAI 中的模型嵌入到向量。

embedding 嵌入                                                  335m

##             xwinlm          

Conversational model based on Llama 2 that performs competitively on various benchmarks.
基于 Llama 2 的对话模型，在各种基准测试中表现具有竞争力。

7b                          13b

##             dolphincoder          

A 7B and 15B uncensored variant of the Dolphin model family that excels at coding, based on StarCoder2.
Dolphin 模型系列的 7B 和 15B变体，擅长编码，基于 StarCoder2。

7b                          15b

##             nous-hermes

General use models based on Llama and Llama 2 from Nous Research.
基于 Nous Research 的 Llama 和 Llama 2 的通用模型。

7b                          13b

##             phind-codellama          

Code generation model based on Code Llama.
基于 Code Llama 的代码生成模型。

34b       

##             llava-phi3

A new small LLaVA model fine-tuned from Phi 3 Mini.
从 Phi 3 Mini 微调而来的新型小型 LLaVA 型号。

​                        vision 视觉                                                              3.8b

##             yarn-llama2

An extension of Llama 2 that supports a context of up to 128k tokens.
Llama 2 的扩展，支持最多 128k 个令牌的上下文。

7b                          13b

##             solar

A compact, yet powerful 10.7B large language model designed for single-turn conversation.
一个紧凑但功能强大的 10.7B 大型语言模型，专为单轮对话而设计。

10.7b

##             granite3.1-dense

The IBM Granite 2B and 8B models are text-only dense LLMs trained on over  12 trillion tokens of data, demonstrated significant improvements over  their predecessors in performance and speed in IBM’s initial testing.
IBM Granite 2B 和 8B 模型是纯文本密集LLMs模型，在超过 12 万亿个数据令牌上进行了密集训练，在 IBM 的初始测试中，其性能和速度比其前身有了显著改进。

tools	2b                          8b

##             starling-lm          

Starling is a large language model trained by reinforcement learning from AI feedback focused on improving chatbot helpfulness.
Starling 是一种大型语言模型，通过对 AI 反馈进行强化学习进行训练，专注于提高聊天机器人的实用性。

​                                                              7b

##             athene-v2

Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.
Athene-V2 是一个 72B 参数模型，擅长代码完成、数学和对数提取任务。

​                                                  tools 工具                                      72b

##             wizardlm

General use model based on Llama 2.
基于 Llama 2 的通用模型。

##             yi-coder          

Yi-Coder is a series of open-source code language models that delivers  state-of-the-art coding performance with fewer than 10 billion  parameters.
Yi-Coder 是一系列开源代码语言模型，可提供最先进的编码性能，参数少于 100 亿个。

1.5b                          9b

##             internlm2          

InternLM2.5 is a 7B parameter model tailored for practical scenarios with outstanding reasoning capability.
InternLM2.5 是一个为实际场景量身定制的 7B 参数模型，具有出色的推理能力。

1m	1.8b	7b	20b

##             samantha-mistral

A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.
接受过哲学、心理学和人际关系培训的同伴助理。基于 Mistral。

​                                                              7b

##             falcon

A large language model built by the Technology Innovation Institute (TII) for use in summarization, text generation, and chat bots.
由技术创新研究所 （TII） 构建的大型语言模型，用于摘要、文本生成和聊天机器人。

​                                                              7b                          40b 40 字节                          180b

##             nemotron-mini

A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.
NVIDIA 的商业友好型小语言模型，针对角色扮演、RAG QA 和函数调用进行了优化。

​                                                  tools 工具                                      4b

##             nemotron

Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the  helpfulness of LLM generated responses to user queries.
Llama-3.1-Nemotron-70B-Instruct 是由 NVIDIA 定制的大型语言模型，用于提高生成的响应对用户查询的LLM帮助性。

​                                                  tools 工具                                      70b

##             dolphin-phi

2.7B uncensored Dolphin model by Eric Hartford, based on the Phi language model by Microsoft Research.
2.7B 由 Eric Hartford 提供的未经审查的 Dolphin 模型，基于 Microsoft Research 的 Phi 语言模型。

​                                                              2.7b

##             orca2

Orca 2 is built by Microsoft research, and are a fine-tuned version of  Meta's Llama 2 models.  The model is designed to excel particularly in  reasoning.
Orca 2 由 Microsoft Research 构建，是 Meta 的 Llama 2 模型的微调版本。该模型旨在特别擅长推理。

​                                                              7b                          13b

##             deepscaler

A fine-tuned version of Deepseek-R1-Distilled-Qwen-1.5B that surpasses  the performance of OpenAI’s o1-preview with just 1.5B parameters on  popular math evaluations.
Deepseek-R1-Distilled-Qwen-1.5B 的微调版本，在流行的数学评估中仅具有 1.5B 参数，其性能超过了 OpenAI 的 o1-preview。

​                                                              1.5b

##             wizardlm-uncensored          

Uncensored version of Wizard LM model 
Wizard LM 模型的未经审查版本

​                                                              13b

##             stable-beluga

Llama 2 based model fine tuned on an Orca-style dataset. Originally called Free Willy.
基于 Llama 2 的模型在 Orca 风格的数据集上进行了微调。原名 Free Willy。

​                                                              7b                          13b                          70b

##             granite3-dense

The IBM Granite 2B and 8B models are designed to support tool-based use  cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.
IBM Granite 2B 和 8B 模型旨在支持基于工具的用例，并支持检索增强生成 （RAG），从而简化代码生成、转换和错误修复。

​                                                  tools 工具                                      2b                          8b

##             llama3-groq-tool-use

A series of models from Groq that represent a significant advancement in  open-source AI capabilities for tool use/function calling.
来自 Groq 的一系列模型，代表了用于工具使用/函数调用的开源 AI 功能的重大进步。

​                                                  tools 工具                                      8b                          70b

##             deepseek-v2.5

An upgraded version of DeekSeek-V2  that integrates the general and coding abilities of both DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct.
DeekSeek-V2 的升级版本，集成了 DeepSeek-V2-Chat 和 DeepSeek-Coder-V2-Struct 的通用和编码能力。

236b       

##             medllama2

Fine-tuned Llama 2 model to answer medical questions based on an open source medical dataset. 
微调 Llama 2 模型，以基于开源医学数据集回答医学问题。

7b

##             meditron

Open-source medical large language model adapted from Llama 2 to the medical domain.
从 Llama 2 改编到医学领域的开源医学大语言模型。

7b	70b

##             llama-pro

An expansion of Llama 2 that specializes in integrating both general  language understanding and domain-specific knowledge, particularly in  programming and mathematics.
Llama 2 的扩展，专门用于整合一般语言理解和特定领域的知识，特别是在编程和数学方面。

##             smallthinker

A new small reasoning model fine-tuned from the Qwen 2.5 3B Instruct model.
从 Qwen 2.5 3B Instruct 模型微调而来的全新小推理模型。

3b

##             yarn-mistral

An extension of Mistral to support context windows of 64K or 128K.
Mistral 的扩展，可支持 64K 或 128K 的上下文窗口。

7b

##             aya-expanse

Cohere For AI's language models trained to perform well across 23 different languages.
Cohere For AI 的语言模型经过训练，可在 23 种不同的语言中表现良好。

tools 工具	8b	32b

##             paraphrase-multilingual

Sentence-transformers model that can be used for tasks like clustering or semantic search.
可用于聚类分析或语义搜索等任务的句子转换器模型。

embedding

##             granite3-moe

The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.
IBM Granite 1B 和 3B 模型是 IBM 的首个专家 （MoE） Granite 模型混合，专为低延迟使用而设计。

tools	1b	3b

##             nexusraven          

Nexus Raven is a 13B instruction tuned model for function calling tasks. 
Nexus Raven 是一个用于函数调用任务的 13B 指令调优模型。

13b

##             codeup

Great code generation model based on Llama2.
基于 Llama2 的出色代码生成模型。

13b            

##             falcon3

A family of efficient AI models under 10B parameters performant in  science, math, and coding through innovative training techniques.
一系列低于 10B 参数的高效 AI 模型，通过创新的训练技术在科学、数学和编码方面表现出色。

1b	3b	7b	10b

##             nous-hermes2-mixtral          

The Nous Hermes 2 model from Nous Research, now trained over Mixtral.
来自 Nous Research 的 Nous Hermes 2 模型，现在在 Mixtral 上训练。

8x7b

##             everythinglm

Uncensored Llama2 based model with support for a 16K context window.
基于 Llama2 的未经审查的模型，支持 16K 上下文窗口。

13b

##             shieldgemma

ShieldGemma is set of instruction tuned models for evaluating the safety of text  prompt input and text output responses against a set of defined safety  policies.
ShieldGemma 是一组指令调整模型，用于根据一组定义的安全策略评估文本提示输入和文本输出响应的安全性。

2b	9b	27b

##             granite3.1-moe

The IBM Granite 1B and 3B models are long-context mixture of experts (MoE)  Granite models from IBM designed for low latency usage.
IBM Granite 1B 和 3B 模型是 IBM 专家 （MoE） Granite 模型的长期上下文混合，专为低延迟使用而设计。

tools	1b	3b

##             snowflake-arctic-embed2          

Snowflake's frontier embedding model. Arctic Embed 2.0 adds multilingual support  without sacrificing English performance or scalability.
Snowflake 的 frontier 嵌入模型。Arctic Embed 2.0 增加了多语言支持，而不会牺牲英语性能或可扩展性。

embedding

##             falcon2

Falcon2 is an 11B parameters causal decoder-only model built by TII and trained over 5T tokens.
Falcon2 是由 TII 构建的 11B 参数因果解码器专用模型，并通过 5T 令牌进行训练。

11b

##             magicoder

🎩 Magicoder is a family of 7B parameter models trained on 75K synthetic  instruction data using OSS-Instruct, a novel approach to enlightening  LLMs with open-source code snippets.
🎩 Magicoder 是一系列 7B 参数模型，使用 OSS-Struct 在 75K 合成指令数据上进行训练，OSS-Struct 是一种LLMs使用开源代码片段进行启发的新方法。

7b

##             mathstral

MathΣtral: a 7B model designed for math reasoning and scientific discovery by Mistral AI.
MathΣtral：由 Mistral AI 为数学推理和科学发现而设计的 7B 模型。

7b                

##             marco-o1

An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).
阿里巴巴国际数字商务集团 （AIDC-AI） 为实际解决方案提供的开放式大型推理模型。

7b

##             stablelm-zephyr          

A lightweight chat model allowing accurate, and responsive output without requiring high-end hardware.
轻量级聊天模型，无需高端硬件即可实现准确、响应迅速的输出。

3b

##             reader-lm

A series of models that convert HTML content to Markdown content, which is useful for content conversion tasks.
将 HTML 内容转换为 Markdown 内容的一系列模型，这对于内容转换任务非常有用。

0.5b	1.5b

##             solar-pro

Solar Pro Preview: an advanced large language model (LLM) with 22 billion parameters designed to fit into a single GPU
Solar Pro Preview：具有 220 亿个参数的高级大型语言模型 （LLM），旨在适应单个 GPU

22b

##             codebooga

A high-performing code instruct model created by merging two existing code models.
通过合并两个现有代码模型创建的高性能代码指令模型。

34b             

##             duckdb-nsql          

7B parameter text-to-SQL model made by MotherDuck and Numbers Station.
由 MotherDuck 和 Numbers Station 制作的 7B 参数文本到 SQL 模型。

7b            

##             mistrallite

MistralLite is a fine-tuned model based on Mistral with enhanced capabilities of processing long contexts.
MistralLite 是基于 Mistral 的微调模型，具有处理长上下文的增强能力。

7b            

##             wizard-vicuna

Wizard Vicuna is a 13B parameter model based on Llama 2 trained by MelodysDreamj.
Wizard Vicuna 是一个基于 Llama 2 的 13B 参数模型，由 MelodysDreamj 训练。

13b

##             llama-guard3

Llama Guard 3 is a series of models fine-tuned for content safety classification of LLM inputs and responses.
Llama Guard 3 是一系列针对LLM输入和响应的内容安全分类进行微调的模型。

1b	8b                       

##             exaone3.5

EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and  released by LG AI Research. 
EXAONE 3.5 是由 LG AI Research 开发和发布的一系列教学调整的双语（英语和韩语）生成模型，范围从 2.4B 到 32B 参数。

2.4b	7.8b	32b

##             megadolphin

MegaDolphin-2.2-120b is a transformation of Dolphin-2.2-70b created by interleaving the model with itself.
MegaDolphin-2.2-120b 是 Dolphin-2.2-70b 的转换，通过模型与自身交错创建。

120b

##             nuextract          

A 3.8B model fine-tuned on a private high-quality synthetic dataset for information extraction, based on Phi-3.
一个 3.8B 模型，基于 Phi-3 在私有高质量合成数据集上进行微调，用于信息提取。

3.8b

##             opencoder

OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B  models, supporting chat in English and Chinese languages.
OpenCoder 是一个开放且可复现的代码LLM系列，包括 1.5B 和 8B 模型，支持英文和中文聊天。

1.5b	8b

##             notux          

A top-performing mixture of experts model, fine-tuned with high-quality data.
性能最佳的专家模型组合，使用高质量数据进行微调。

8x7b

##             open-orca-platypus2

Merge of the Open Orca OpenChat model and the Garage-bAInd Platypus 2 model. Designed for chat and code generation.
Open Orca OpenChat 模型和 Garage-bAInd Platypus 2 模型的合并。专为聊天和代码生成而设计。

13b       

##             notus

A 7B chat model fine-tuned with high-quality data and based on Zephyr.
一个基于 Zephyr 的 7B 聊天模型，使用高质量数据进行微调。

7b

##             goliath

A language model created by combining two fine-tuned Llama 2 70B models into one.
通过将两个微调的 Llama 2 70B 模型合并为一个模型而创建的语言模型。 

##             bespoke-minicheck

A state-of-the-art fact-checking model developed by Bespoke Labs.
由 Bespoke Labs 开发的最先进的事实核查模型。

7b

##             command-r7b     

The smallest model in Cohere's R series delivers top-tier speed,  efficiency, and quality to build powerful AI applications on commodity  GPUs and edge devices.
Cohere 的 R 系列中最小的型号可提供一流的速度、效率和质量，以在商用 GPU 和边缘设备上构建强大的 AI 应用程序。

tools	7b

##             firefunction-v2

An open weights function calling model based on Llama 3, competitive with GPT-4o function calling capabilities.
一种基于 Llama 3 的开放权重函数调用模型，与 GPT-4o 函数调用能力相媲美。

tools	70b

##             tulu3

Tülu 3 is a leading instruction following model family, offering fully  open-source data, code, and recipes by the The Allen Institute for AI.
Tülu 3 是遵循模型系列的领先指令，提供由 Allen Institute for AI 提供的完全开源数据、代码和配方。

8b	70b

##             dbrx          

DBRX is an open, general-purpose LLM created by Databricks.
DBRX 是由 Databricks LLM 创建的开放式通用工具。

132b

##             granite-embedding

The IBM Granite Embedding 30M and 278M models models are text-only dense  biencoder embedding models, with 30M available in English only and 278M  serving multilingual use cases.
IBM Granite Embedding 30M 和 278M 模型模型是纯文本密集双编码器嵌入模型，其中 30M 仅提供英文版本，278M 服务于多语言用例。

embedding          

##             granite3-guardian

The IBM Granite Guardian 3.0 2B and 8B models are designed to detect risks in prompts and/or responses.
IBM Granite Guardian 3.0 2B 和 8B 模型旨在检测提示和/或响应中的风险。

2b	8b

##             alfred

A robust conversational model designed to be used for both chat and instruct use cases.
一个强大的对话模型，旨在用于聊天和指导使用案例。

40b

##             sailor2

Sailor2 是专为东南亚打造的多语言语言模型。提供 1B、8B 和 20B 参数大小。

1b	8b	20b

##             r1-1776

A version of the DeepSeek-R1 model that has been post trained to provide  unbiased, accurate, and factual information by Perplexity. 
Perplexity 已经过后训练的 DeepSeek-R1 模型版本，可提供公正、准确和真实的信息。

70b	671b

##             granite3.2

Granite-3.2 is a family of long-context AI models from IBM Granite fine-tuned for thinking capabilities.
Granite-3.2 是 IBM Granite 的一系列长上下文 AI 模型，针对思维能力进行了微调。

tools	2b	8b
