# Upgrade etcd from 2.3 to 3.0 将 etcd 从 2.3 升级到 3.0

Processes, checklists, and notes on upgrading etcd from 2.3 to 3.0
将 etcd 从 2.3 升级到 3.0 的流程、清单和注意事项



In the general case, upgrading from etcd 2.3 to 3.0 can be a zero-downtime, rolling upgrade:
在一般情况下，从 etcd 2.3 升级到 3.0 可以是一个零停机时间的滚动升级：

- one by one, stop the etcd v2.3 processes and replace them with etcd v3.0 processes
  逐一停止 etcd v2.3 进程，替换为 etcd v3.0 进程
- after running all v3.0 processes, new features in v3.0 are available to the cluster
  运行所有 v3.0 进程后，v3.0 中的新功能可供集群使用

Before [starting an upgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_0/#upgrade-procedure), read through the rest of this guide to prepare.
在开始升级之前，请通读本指南的其余部分以做好准备。

### Upgrade checklists 升级清单

**NOTE:** When [migrating from v2 with no v3 data](https://github.com/etcd-io/etcd/issues/9480), etcd server v3.2+ panics when etcd restores from existing snapshots but no v3 `ETCD_DATA_DIR/member/snap/db` file. This happens when the server had migrated from v2 with no  previous v3 data. This also prevents accidental v3 data loss (e.g. `db` file might have been moved). etcd requires that post v3 migration can  only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.
注意：从没有 v3 数据的 v2 迁移时，当 etcd 从现有快照恢复但没有 v3 `ETCD_DATA_DIR/member/snap/db` 文件时，etcd 服务器 v3.2+ 会崩溃。当服务器从没有以前的 v3 数据的 v2 迁移时，会发生这种情况。这还可以防止意外的 v3 数据丢失（例如， `db` 文件可能已被移动）。etcd 要求 v3 后迁移只能对 v3 数据进行。在 v3.0 服务器包含 v3 数据之前，不要升级到较新的 v3 版本。

#### Upgrade requirements 升级要求

To upgrade an existing etcd deployment to 3.0, the running cluster must be 2.3 or greater. If it’s before 2.3, please upgrade to [2.3](https://github.com/etcd-io/etcd/releases/tag/v2.3.8) before upgrading to 3.0.
要将现有 etcd 部署升级到 3.0，正在运行的集群必须为 2.3 或更高版本。如果是 2.3 之前版本，请先升级到 2.3，然后再升级到 3.0。

Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using the `etcdctl cluster-health` command before proceeding.
此外，为确保顺利滚动升级，正在运行的群集必须运行正常。在继续操作之前，请使用命令 `etcdctl cluster-health` 检查群集的运行状况。

#### Preparation 制备

Before upgrading etcd, always test the services relying on etcd in a staging  environment before deploying the upgrade to the production environment.
在升级 etcd 之前，在将升级部署到生产环境之前，请始终在暂存环境中测试依赖于 etcd 的服务。

Before beginning, [backup the etcd data directory](https://etcd.io/docs/v2.3/admin_guide#backing-up-the-datastore). Should something go wrong with the upgrade, it is possible to use this backup to [downgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_0/#downgrade) back to existing etcd version.
在开始之前，请备份 etcd 数据目录。如果升级出现问题，可以使用此备份降级回现有的 etcd 版本。

#### Mixed versions 混合版本

While upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to  version 3.0. Internally, etcd members negotiate with each other to  determine the overall cluster version, which controls the reported  version and the supported features.
在升级时，etcd 集群支持 etcd 成员的混合版本，并使用最低通用版本的协议运行。只有当集群的所有成员都升级到版本 3.0 后，集群才会被视为已升级。在内部，etcd 成员相互协商以确定整体集群版本，该版本控制报告的版本和支持的功能。

#### Limitations 局限性

It might take up to 2 minutes for the newly upgraded member to catch up  with the existing cluster when the total data size is larger than 50MB.  Check the size of a recent snapshot to estimate the total data size. In  other words, it is safest to wait for 2 minutes between upgrading each  member.
当总数据大小大于 50MB 时，新升级的成员最多可能需要 2 分钟才能赶上现有集群。检查最近快照的大小以估计总数据大小。换句话说，在升级每个成员之间等待 2 分钟是最安全的。

For a much larger total data size, 100MB or more , this one-time process  might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact the [etcd team](https://groups.google.com/g/etcd-dev) before upgrading, and we’ll be happy to provide advice on the procedure.
对于更大的总数据大小，100MB或更多，此一次性过程可能需要更多时间。这种规模的超大型 etcd 集群的管理员可以在升级之前随时联系 etcd 团队，我们很乐意提供有关该过程的建议。

#### Downgrade 降级

If all members have been upgraded to v3.0, the cluster will be upgraded to v3.0, and downgrade from this completed state is **not possible**. If any single member is still v2.3, however, the cluster and its  operations remains “v2.3”, and it is possible from this mixed cluster  state to return to using a v2.3 etcd binary on all members.
如果所有成员都已升级到 v3.0，则集群将升级到 v3.0，并且无法从此完成状态降级。但是，如果任何一个成员仍然是 v2.3，则集群及其操作仍为“v2.3”，并且可以从这种混合集群状态返回到在所有成员上使用 v2.3 etcd 二进制文件。

Please [backup the data directory](https://etcd.io/docs/v2.3/admin_guide#backing-up-the-datastore) of all etcd members to make downgrading the cluster possible even after it has been completely upgraded.
请备份所有 etcd 成员的数据目录，以便即使在集群完全升级后也可以降级集群。

### Upgrade procedure 升级过程

This example details the upgrade of a three-member v2.3 etcd cluster running on a local machine.
此示例详细介绍了在本地计算机上运行的三成员 v2.3 etcd 集群的升级。

#### 1. Check upgrade requirements. 1. 检查升级要求。

Is the cluster healthy and running v.2.3.x?
集群是否正常运行并运行 v.2.3.x？

```
$ etcdctl cluster-health
member 6e3bd23ae5f1eae0 is healthy: got healthy result from http://localhost:22379
member 924e2e83e93f2560 is healthy: got healthy result from http://localhost:32379
member 8211f1d0f64f3269 is healthy: got healthy result from http://localhost:12379
cluster is healthy

$ curl http://localhost:2379/version
{"etcdserver":"2.3.x","etcdcluster":"2.3.8"}
```

#### 2. Stop the existing etcd process 2. 停止现有的 etcd 进程

When each etcd process is stopped, expected errors will be logged by other  cluster members. This is normal since a cluster member connection has  been (temporarily) broken:
当每个 etcd 进程停止时，其他集群成员将记录预期的错误。这是正常现象，因为集群成员连接已（暂时）中断：

```
2016-06-27 15:21:48.624124 E | rafthttp: failed to dial 8211f1d0f64f3269 on stream Message (dial tcp 127.0.0.1:12380: getsockopt: connection refused)
2016-06-27 15:21:48.624175 I | rafthttp: the connection with 8211f1d0f64f3269 became inactive
```

It’s a good idea at this point to [backup the etcd data directory](https://etcd.io/docs/v2.3/admin_guide#backing-up-the-datastore) to provide a downgrade path should any problems occur:
此时，最好备份 etcd 数据目录，以便在出现任何问题时提供降级路径：

```
$ etcdctl backup \
      --data-dir /var/lib/etcd \
      --backup-dir /tmp/etcd_backup
```

#### 3. Drop-in etcd v3.0 binary and start the new etcd process 3. 插入 etcd v3.0 二进制文件并启动新的 etcd 进程

The new v3.0 etcd will publish its information to the cluster:
新的 v3.0 etcd 会将其信息发布到集群：

```
09:58:25.938673 I | etcdserver: published {Name:infra1 ClientURLs:[http://localhost:12379]} to cluster 524400597fb1d5f6
```

Verify that each member, and then the entire cluster, becomes healthy with the new v3.0 etcd binary:
使用新的 v3.0 etcd 二进制文件验证每个成员以及整个集群是否正常运行：

```
$ etcdctl cluster-health
member 6e3bd23ae5f1eae0 is healthy: got healthy result from http://localhost:22379
member 924e2e83e93f2560 is healthy: got healthy result from http://localhost:32379
member 8211f1d0f64f3269 is healthy: got healthy result from http://localhost:12379
cluster is healthy
```

Upgraded members will log warnings like the following until the entire cluster  is upgraded. This is expected and will cease after all etcd cluster  members are upgraded to v3.0:
升级后的成员将记录如下警告，直到整个集群升级为止。这是意料之中的，并且在所有 etcd 集群成员升级到 v3.0 后将停止：

```
2016-06-27 15:22:05.679644 W | etcdserver: the local etcd version 2.3.7 is not up-to-date
2016-06-27 15:22:05.679660 W | etcdserver: member 8211f1d0f64f3269 has a higher version 3.0.0
```

#### 4. Repeat step 2 to step 3 for all other members 4. 对所有其他成员重复步骤 2 到步骤 3

#### 5. Finish 5. 完成

When all members are upgraded, the cluster will report upgrading to 3.0 successfully:
升级所有成员后，群集将报告已成功升级到 3.0：

```
2016-06-27 15:22:19.873751 N | membership: updated the cluster version from 2.3 to 3.0
2016-06-27 15:22:19.914574 I | api: enabled capabilities for version 3.0.0
$ ETCDCTL_API=3 etcdctl endpoint health
127.0.0.1:12379 is healthy: successfully committed proposal: took = 18.440155ms
127.0.0.1:32379 is healthy: successfully committed proposal: took = 13.651368ms
127.0.0.1:22379 is healthy: successfully committed proposal: took = 18.513301ms
```

## Further considerations 其他注意事项

- etcdctl environment variables have been updated. If `ETCDCTL_API=2 etcdctl cluster-health` works properly but `ETCDCTL_API=3 etcdctl endpoints health` responds with `Error: grpc: timed out when dialing`, be sure to use the [new variable names](https://github.com/etcd-io/etcd/tree/master/etcdctl#etcdctl).
  etcdctl 环境变量已更新。如果 `ETCDCTL_API=2 etcdctl cluster-health` 工作正常，但 `ETCDCTL_API=3 etcdctl endpoints health` 响应 `Error: grpc: timed out when dialing` ，请确保使用新的变量名称。

## Known Issues 已知问题

- etcd < v3.1 does not work properly if built with Go > v1.7. See [Issue 6951](https://github.com/etcd-io/etcd/issues/6951) for additional information.
  etcd < v3.1 如果使用 Go > v1.7 构建，则无法正常工作。有关其他信息，请参阅问题 6951。
- If an error such as `transport: http2Client.notifyError got notified that the client transport was broken unexpected EOF.` shows up in the etcd server logs, be sure etcd is a pre-built release  or built with (etcd v3.1+ & go v1.7+) or (etcd <v3.1 & go  v1.6.x).
  如果 etcd 服务器日志中 `transport: http2Client.notifyError got notified that the client transport was broken unexpected EOF.` 出现错误，请确保 etcd 是预构建版本或使用 （etcd v3.1+ & go v1.7+） 或 （etcd 
- Adding a v3 node to v2.3 cluster during upgrades is not supported and could trigger panics. See [Issue 7249](https://github.com/etcd-io/etcd/issues/7429) for additional information. Mixed versions of etcd members are only  allowed during v3 migration. Finish upgrades before making any  membership changes.
  不支持在升级期间将 v3 节点添加到 v2.3 集群，这可能会触发崩溃。有关其他信息，请参阅问题 7249。只有在 v3 迁移期间才允许使用 etcd 成员的混合版本。在进行任何成员资格更改之前完成升级。

# Upgrade etcd from 3.0 to 3.1 将 etcd 从 3.0 升级到 3.1

Processes, checklists, and notes on upgrading etcd from 3.0 to 3.1
将 etcd 从 3.0 升级到 3.1 的流程、清单和注意事项



In the general case, upgrading from etcd 3.0 to 3.1 can be a zero-downtime, rolling upgrade:
在一般情况下，从 etcd 3.0 升级到 3.1 可以是一个零停机时间的滚动升级：

- one by one, stop the etcd v3.0 processes and replace them with etcd v3.1 processes
  逐一停止 etcd v3.0 进程，替换为 etcd v3.1 进程
- after running all v3.1 processes, new features in v3.1 are available to the cluster
  运行所有 v3.1 进程后，v3.1 中的新功能可供集群使用

Before [starting an upgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_1/#upgrade-procedure), read through the rest of this guide to prepare.
在开始升级之前，请通读本指南的其余部分以做好准备。

### Upgrade checklists 升级清单

**NOTE:** When [migrating from v2 with no v3 data](https://github.com/etcd-io/etcd/issues/9480), etcd server v3.2+ panics when etcd restores from existing snapshots but no v3 `ETCD_DATA_DIR/member/snap/db` file. This happens when the server had migrated from v2 with no  previous v3 data. This also prevents accidental v3 data loss (e.g. `db` file might have been moved). etcd requires that post v3 migration can  only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.
注意：从没有 v3 数据的 v2 迁移时，当 etcd 从现有快照恢复但没有 v3 `ETCD_DATA_DIR/member/snap/db` 文件时，etcd 服务器 v3.2+ 会崩溃。当服务器从没有以前的 v3 数据的 v2 迁移时，会发生这种情况。这还可以防止意外的 v3 数据丢失（例如， `db` 文件可能已被移动）。etcd 要求 v3 后迁移只能对 v3 数据进行。在 v3.0 服务器包含 v3 数据之前，不要升级到较新的 v3 版本。

#### Monitoring 监测

Following metrics from v3.0.x have been deprecated in favor of [go-grpc-prometheus](https://github.com/grpc-ecosystem/go-grpc-prometheus):
v3.0.x 中的以下指标已被弃用，取而代之的是 go-grpc-prometheus：

- `etcd_grpc_requests_total`
- `etcd_grpc_requests_failed_total`
- `etcd_grpc_active_streams`
- `etcd_grpc_unary_requests_duration_seconds`

#### Upgrade requirements 升级要求

To upgrade an existing etcd deployment to 3.1, the running cluster must be 3.0 or greater. If it’s before 3.0, please [upgrade to 3.0](https://etcd.io/docs/v3.5/upgrades/upgrade_3_0) before upgrading to 3.1.
要将现有 etcd 部署升级到 3.1，正在运行的集群必须为 3.0 或更高版本。如果低于 3.0，请先升级到 3.0，然后再升级到 3.1。

Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using the `etcdctl endpoint health` command before proceeding.
此外，为确保顺利滚动升级，正在运行的群集必须运行正常。在继续操作之前，请使用命令 `etcdctl endpoint health` 检查群集的运行状况。

#### Preparation 制备

Before upgrading etcd, always test the services relying on etcd in a staging  environment before deploying the upgrade to the production environment.
在升级 etcd 之前，在将升级部署到生产环境之前，请始终在暂存环境中测试依赖于 etcd 的服务。

Before beginning, [backup the etcd data](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup). Should something go wrong with the upgrade, it is possible to use this backup to [downgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_1/#downgrade) back to existing etcd version. Please note that the `snapshot` command only backs up the v3 data. For v2 data, see [backing up v2 datastore](https://etcd.io/docs/v2.3/admin_guide#backing-up-the-datastore).
在开始之前，请备份 etcd 数据。如果升级出现问题，可以使用此备份降级回现有的 etcd 版本。请注意，该 `snapshot` 命令仅备份 v3 数据。有关 v2 数据，请参阅备份 v2 数据存储。

#### Mixed versions 混合版本

While upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to  version 3.1. Internally, etcd members negotiate with each other to  determine the overall cluster version, which controls the reported  version and the supported features.
在升级时，etcd 集群支持 etcd 成员的混合版本，并使用最低通用版本的协议运行。只有当集群的所有成员都升级到版本 3.1 后，集群才会被视为已升级。在内部，etcd 成员相互协商以确定整体集群版本，该版本控制报告的版本和支持的功能。

#### Limitations 局限性

Note: If the cluster only has v3 data and no v2 data, it is not subject to this limitation.
注意：如果集群只有 v3 数据而没有 v2 数据，则不受此限制。

If the cluster is serving a v2 data set larger than 50MB, each newly  upgraded member may take up to two minutes to catch up with the existing cluster. Check the size of a recent snapshot to estimate the total data size. In other words, it is safest to wait for 2 minutes between  upgrading each member.
如果群集提供的 v2 数据集大于 50MB，则每个新升级的成员最多可能需要两分钟才能赶上现有群集。检查最近快照的大小以估计总数据大小。换句话说，在升级每个成员之间等待 2 分钟是最安全的。

For a much larger total data size, 100MB or more , this one-time process  might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact the [etcd team](https://groups.google.com/g/etcd-dev) before upgrading, and we’ll be happy to provide advice on the procedure.
对于更大的总数据大小，100MB或更多，此一次性过程可能需要更多时间。这种规模的超大型 etcd 集群的管理员可以在升级之前随时联系 etcd 团队，我们很乐意提供有关该过程的建议。

#### Downgrade 降级

If all members have been upgraded to v3.1, the cluster will be upgraded to v3.1, and downgrade from this completed state is **not possible**. If any single member is still v3.0, however, the cluster and its  operations remains “v3.0”, and it is possible from this mixed cluster  state to return to using a v3.0 etcd binary on all members.
如果所有成员都已升级到 v3.1，则集群将升级到 v3.1，并且无法从此完成状态降级。但是，如果任何一个成员仍然是 v3.0，则集群及其操作仍为“v3.0”，并且可以从这种混合集群状态返回到在所有成员上使用 v3.0 etcd 二进制文件。

Please [backup the data directory](https://etcd.io/docs/v3.5/op-guide/maintenance#snapshot-backup) of all etcd members to make downgrading the cluster possible even after it has been completely upgraded.
请备份所有 etcd 成员的数据目录，以便即使在集群完全升级后也可以降级集群。

### Upgrade procedure 升级过程

This example shows how to upgrade a 3-member v3.0 etcd cluster running on a local machine.
此示例说明如何升级在本地计算机上运行的 3 成员 v3.0 etcd 集群。

#### 1. Check upgrade requirements 1. 检查升级要求

Is the cluster healthy and running v3.0.x?
群集是否正常运行并运行 v3.0.x？

```
$ ETCDCTL_API=3 etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 6.600684ms
localhost:22379 is healthy: successfully committed proposal: took = 8.540064ms
localhost:32379 is healthy: successfully committed proposal: took = 8.763432ms

$ curl http://localhost:2379/version
{"etcdserver":"3.0.16","etcdcluster":"3.0.0"}
```

#### 2. Stop the existing etcd process 2. 停止现有的 etcd 进程

When each etcd process is stopped, expected errors will be logged by other  cluster members. This is normal since a cluster member connection has  been (temporarily) broken:
当每个 etcd 进程停止时，其他集群成员将记录预期的错误。这是正常现象，因为集群成员连接已（暂时）中断：

```
2017-01-17 09:34:18.352662 I | raft: raft.node: 1640829d9eea5cfb elected leader 1640829d9eea5cfb at term 5
2017-01-17 09:34:18.359630 W | etcdserver: failed to reach the peerURL(http://localhost:2380) of member fd32987dcd0511e0 (Get http://localhost:2380/version: dial tcp 127.0.0.1:2380: getsockopt: connection refused)
2017-01-17 09:34:18.359679 W | etcdserver: cannot get the version of member fd32987dcd0511e0 (Get http://localhost:2380/version: dial tcp 127.0.0.1:2380: getsockopt: connection refused)
2017-01-17 09:34:18.548116 W | rafthttp: lost the TCP streaming connection with peer fd32987dcd0511e0 (stream Message writer)
2017-01-17 09:34:19.147816 W | rafthttp: lost the TCP streaming connection with peer fd32987dcd0511e0 (stream MsgApp v2 writer)
2017-01-17 09:34:34.364907 W | etcdserver: failed to reach the peerURL(http://localhost:2380) of member fd32987dcd0511e0 (Get http://localhost:2380/version: dial tcp 127.0.0.1:2380: getsockopt: connection refused)
```

It’s a good idea at this point to [backup the etcd data](https://etcd.io/docs/v3.5/op-guide/maintenance#snapshot-backup) to provide a downgrade path should any problems occur:
此时，最好备份 etcd 数据，以便在出现任何问题时提供降级路径：

```
$ etcdctl snapshot save backup.db
```

#### 3. Drop-in etcd v3.1 binary and start the new etcd process 3. 插入 etcd v3.1 二进制文件并启动新的 etcd 进程

The new v3.1 etcd will publish its information to the cluster:
新的 v3.1 etcd 会将其信息发布到集群：

```
2017-01-17 09:36:00.996590 I | etcdserver: published {Name:my-etcd-1 ClientURLs:[http://localhost:2379]} to cluster 46bc3ce73049e678
```

Verify that each member, and then the entire cluster, becomes healthy with the new v3.1 etcd binary:
使用新的 v3.1 etcd 二进制文件验证每个成员以及整个集群是否正常运行：

```
$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:22379 is healthy: successfully committed proposal: took = 5.540129ms
localhost:32379 is healthy: successfully committed proposal: took = 7.321671ms
localhost:2379 is healthy: successfully committed proposal: took = 10.629901ms
```

Upgraded members will log warnings like the following until the entire cluster  is upgraded. This is expected and will cease after all etcd cluster  members are upgraded to v3.1:
升级后的成员将记录如下警告，直到整个集群升级为止。这是意料之中的，并且在所有 etcd 集群成员升级到 v3.1 后将停止：

```
2017-01-17 09:36:38.406268 W | etcdserver: the local etcd version 3.0.16 is not up-to-date
2017-01-17 09:36:38.406295 W | etcdserver: member fd32987dcd0511e0 has a higher version 3.1.0
2017-01-17 09:36:42.407695 W | etcdserver: the local etcd version 3.0.16 is not up-to-date
2017-01-17 09:36:42.407730 W | etcdserver: member fd32987dcd0511e0 has a higher version 3.1.0
```

#### 4. Repeat step 2 to step 3 for all other members 4. 对所有其他成员重复步骤 2 到步骤 3

#### 5. Finish 5. 完成

When all members are upgraded, the cluster will report upgrading to 3.1 successfully:
升级所有成员后，集群将报告已成功升级到 3.1：

```
2017-01-17 09:37:03.100015 I | etcdserver: updating the cluster version from 3.0 to 3.1
2017-01-17 09:37:03.104263 N | etcdserver/membership: updated the cluster version from 3.0 to 3.1
2017-01-17 09:37:03.104374 I | etcdserver/api: enabled capabilities for version 3.1
$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 2.312897ms
localhost:22379 is healthy: successfully committed proposal: took = 2.553476ms
localhost:32379 is healthy: successfully committed proposal: took = 2.516902ms
```

# Upgrade etcd from 3.1 to 3.2 将 etcd 从 3.1 升级到 3.2

Processes, checklists, and notes on upgrading etcd from 3.1 to 3.2
将 etcd 从 3.1 升级到 3.2 的流程、清单和注意事项



In the general case, upgrading from etcd 3.1 to 3.2 can be a zero-downtime, rolling upgrade:
在一般情况下，从 etcd 3.1 升级到 3.2 可以是一个零停机时间的滚动升级：

- one by one, stop the etcd v3.1 processes and replace them with etcd v3.2 processes
  逐个停止 etcd v3.1 进程，替换为 etcd v3.2 进程
- after running all v3.2 processes, new features in v3.2 are available to the cluster
  运行所有 v3.2 进程后，v3.2 中的新功能可供集群使用

Before [starting an upgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_2/#upgrade-procedure), read through the rest of this guide to prepare.
在开始升级之前，请通读本指南的其余部分以做好准备。

### Upgrade checklists 升级清单

**NOTE:** When [migrating from v2 with no v3 data](https://github.com/etcd-io/etcd/issues/9480), etcd server v3.2+ panics when etcd restores from existing snapshots but no v3 `ETCD_DATA_DIR/member/snap/db` file. This happens when the server had migrated from v2 with no  previous v3 data. This also prevents accidental v3 data loss (e.g. `db` file might have been moved). etcd requires that post v3 migration can  only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.
注意：从没有 v3 数据的 v2 迁移时，当 etcd 从现有快照恢复但没有 v3 `ETCD_DATA_DIR/member/snap/db` 文件时，etcd 服务器 v3.2+ 会崩溃。当服务器从没有以前的 v3 数据的 v2 迁移时，会发生这种情况。这还可以防止意外的 v3 数据丢失（例如， `db` 文件可能已被移动）。etcd 要求 v3 后迁移只能对 v3 数据进行。在 v3.0 服务器包含 v3 数据之前，不要升级到较新的 v3 版本。

Highlighted breaking changes in 3.2.
突出显示了 3.2 中的重大更改。

#### Changed default `snapshot-count` value 更改了默认值 `snapshot-count` 

Higher `--snapshot-count` holds more Raft entries in memory until snapshot, thus causing [recurrent higher memory usage](https://github.com/kubernetes/kubernetes/issues/60589#issuecomment-371977156). Since leader retains latest Raft entries for longer, a slow follower has more time to catch up before leader snapshot. `--snapshot-count` is a tradeoff between higher memory usage and better availabilities of slow followers.
在快照之前，较高 `--snapshot-count` 的内存中会保留更多的 Raft 条目，从而导致经常性的较高内存使用率。由于领导者将最新的 Raft 条目保留更长时间，因此速度较慢的追随者在领导者快照之前有更多时间赶上。 `--snapshot-count` 是在更高的内存使用率和较慢的追随者的可用性之间进行权衡。

Since v3.2, the default value of `--snapshot-count` has [changed from from 10,000 to 100,000](https://github.com/etcd-io/etcd/pull/7160).
从 v3.2 开始，默认值 `--snapshot-count` 从 10,000 更改为 100,000。

#### Changed gRPC dependency (>=3.2.10) 更改了 gRPC 依赖项 （>=3.2.10）

3.2.10 or later now requires [grpc/grpc-go](https://github.com/grpc/grpc-go/releases) `v1.7.5` (<=3.2.9 requires `v1.2.1`).
3.2.10 或更高版本现在需要 grpc/grpc-go `v1.7.5` （<=3.2.9 需要 `v1.2.1` ）。

##### Deprecated `grpclog.Logger` 荒废的 `grpclog.Logger` 

`grpclog.Logger` has been deprecated in favor of [`grpclog.LoggerV2`](https://github.com/grpc/grpc-go/blob/master/grpclog/loggerv2.go). `clientv3.Logger` is now `grpclog.LoggerV2`.
 `grpclog.Logger` 已被弃用 `grpclog.LoggerV2` ，取而代之的是 。 `clientv3.Logger` 现在是 `grpclog.LoggerV2` .

Before 以前

```go
import "github.com/coreos/etcd/clientv3"
clientv3.SetLogger(log.New(os.Stderr, "grpc: ", 0))
```

After 后

```go
import "github.com/coreos/etcd/clientv3"
import "google.golang.org/grpc/grpclog"
clientv3.SetLogger(grpclog.NewLoggerV2(os.Stderr, os.Stderr, os.Stderr))

// log.New above cannot be used (not implement grpclog.LoggerV2 interface)
```

##### Deprecated `grpc.ErrClientConnTimeout` 荒废的 `grpc.ErrClientConnTimeout` 

Previously, `grpc.ErrClientConnTimeout` error is returned on client dial time-outs. 3.2 instead returns `context.DeadlineExceeded` (see [#8504](https://github.com/etcd-io/etcd/issues/8504)).
以前， `grpc.ErrClientConnTimeout` 在客户端拨号超时时返回错误。3.2 改为返回 `context.DeadlineExceeded` （参见 #8504）。

Before 以前

```go
// expect dial time-out on ipv4 blackhole
_, err := clientv3.New(clientv3.Config{
    Endpoints:   []string{"http://254.0.0.1:12345"},
    DialTimeout: 2 * time.Second
})
if err == grpc.ErrClientConnTimeout {
	// handle errors
}
```

After 后

```go
_, err := clientv3.New(clientv3.Config{
    Endpoints:   []string{"http://254.0.0.1:12345"},
    DialTimeout: 2 * time.Second
})
if err == context.DeadlineExceeded {
	// handle errors
}
```

#### Changed maximum request size limits (>=3.2.10) 更改了最大请求大小限制 （>=3.2.10）

3.2.10 and 3.2.11 allow custom request size limits in server side. >=3.2.12 allows custom request size limits for both server and **client side**. In previous versions(v3.2.10, v3.2.11), client response size was limited to only 4 MiB.
3.2.10 和 3.2.11 允许在服务器端限制自定义请求大小。>=3.2.12 允许对服务器端和客户端进行自定义请求大小限制。在以前的版本（v3.2.10、v3.2.11）中，客户端响应大小限制为仅 4 MiB。

Server-side request limits can be configured with `--max-request-bytes` flag:
服务器端请求限制可以通过以下标志 `--max-request-bytes` 进行配置：

```bash
# limits request size to 1.5 KiB
etcd --max-request-bytes 1536

# client writes exceeding 1.5 KiB will be rejected
etcdctl put foo [LARGE VALUE...]
# etcdserver: request is too large
```

Or configure `embed.Config.MaxRequestBytes` field:
或配置 `embed.Config.MaxRequestBytes` 字段：

```go
import "github.com/coreos/etcd/embed"
import "github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"

// limit requests to 5 MiB
cfg := embed.NewConfig()
cfg.MaxRequestBytes = 5 * 1024 * 1024

// client writes exceeding 5 MiB will be rejected
_, err := cli.Put(ctx, "foo", [LARGE VALUE...])
err == rpctypes.ErrRequestTooLarge
```

**If not specified, server-side limit defaults to 1.5 MiB**.
如果未指定，服务器端限制默认为 1.5 MiB。

Client-side request limits must be configured based on server-side limits.
客户端请求限制必须根据服务器端限制进行配置。

```bash
# limits request size to 1 MiB
etcd --max-request-bytes 1048576
import "github.com/coreos/etcd/clientv3"

cli, _ := clientv3.New(clientv3.Config{
    Endpoints: []string{"127.0.0.1:2379"},
    MaxCallSendMsgSize: 2 * 1024 * 1024,
    MaxCallRecvMsgSize: 3 * 1024 * 1024,
})


// client writes exceeding "--max-request-bytes" will be rejected from etcd server
_, err := cli.Put(ctx, "foo", strings.Repeat("a", 1*1024*1024+5))
err == rpctypes.ErrRequestTooLarge


// client writes exceeding "MaxCallSendMsgSize" will be rejected from client-side
_, err = cli.Put(ctx, "foo", strings.Repeat("a", 5*1024*1024))
err.Error() == "rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (5242890 vs. 2097152)"


// some writes under limits
for i := range []int{0,1,2,3,4} {
    _, err = cli.Put(ctx, fmt.Sprintf("foo%d", i), strings.Repeat("a", 1*1024*1024-500))
    if err != nil {
        panic(err)
    }
}
// client reads exceeding "MaxCallRecvMsgSize" will be rejected from client-side
_, err = cli.Get(ctx, "foo", clientv3.WithPrefix())
err.Error() == "rpc error: code = ResourceExhausted desc = grpc: received message larger than max (5240509 vs. 3145728)"
```

**If not specified, client-side send limit defaults to 2 MiB (1.5 MiB + gRPC overhead bytes) and receive limit to `math.MaxInt32`**. Please see [clientv3 godoc](https://pkg.go.dev/github.com/etcd-io/etcd/clientv3#Config) for more detail.
如果未指定，则客户端发送限制默认为 2 MiB（1.5 MiB + gRPC 开销字节），接收限制为 `math.MaxInt32` 。有关更多详细信息，请参阅 clientv3 godoc。

#### Changed raw gRPC client wrappers 更改了原始 gRPC 客户端包装器

3.2.12 or later changes the function signatures of `clientv3` gRPC client wrapper. This change was needed to support [custom `grpc.CallOption` on message size limits](https://github.com/etcd-io/etcd/pull/9047).
3.2.12 或更高版本更改 gRPC 客户端包装器的 `clientv3` 函数签名。需要进行此更改以支持自定义 `grpc.CallOption` 消息大小限制。

Before and after 之前和之后

```diff
-func NewKVFromKVClient(remote pb.KVClient) KV {
+func NewKVFromKVClient(remote pb.KVClient, c *Client) KV {

-func NewClusterFromClusterClient(remote pb.ClusterClient) Cluster {
+func NewClusterFromClusterClient(remote pb.ClusterClient, c *Client) Cluster {

-func NewLeaseFromLeaseClient(remote pb.LeaseClient, keepAliveTimeout time.Duration) Lease {
+func NewLeaseFromLeaseClient(remote pb.LeaseClient, c *Client, keepAliveTimeout time.Duration) Lease {

-func NewMaintenanceFromMaintenanceClient(remote pb.MaintenanceClient) Maintenance {
+func NewMaintenanceFromMaintenanceClient(remote pb.MaintenanceClient, c *Client) Maintenance {

-func NewWatchFromWatchClient(wc pb.WatchClient) Watcher {
+func NewWatchFromWatchClient(wc pb.WatchClient, c *Client) Watcher {
```

#### Changed `clientv3.Lease.TimeToLive` API 更改的 `clientv3.Lease.TimeToLive` API

Previously, `clientv3.Lease.TimeToLive` API returned `lease.ErrLeaseNotFound` on non-existent lease ID. 3.2 instead returns TTL=-1 in its response and no error (see [#7305](https://github.com/etcd-io/etcd/pull/7305)).
以前， `clientv3.Lease.TimeToLive` 在不存在的租约 ID 上返回 `lease.ErrLeaseNotFound` 的 API 3.2 在其响应中返回 TTL=-1，并且没有错误（参见 #7305）。

Before 以前

```go
// when leaseID does not exist
resp, err := TimeToLive(ctx, leaseID)
resp == nil
err == lease.ErrLeaseNotFound
```

After 后

```go
// when leaseID does not exist
resp, err := TimeToLive(ctx, leaseID)
resp.TTL == -1
err == nil
```

#### Moved `clientv3.NewFromConfigFile` to `clientv3.yaml.NewConfig` 移至 `clientv3.NewFromConfigFile` `clientv3.yaml.NewConfig` 

`clientv3.NewFromConfigFile` is moved to `yaml.NewConfig`.
 `clientv3.NewFromConfigFile` 已移至 `yaml.NewConfig` 。

Before 以前

```go
import "github.com/coreos/etcd/clientv3"
clientv3.NewFromConfigFile
```

After 后

```go
import clientv3yaml "github.com/coreos/etcd/clientv3/yaml"
clientv3yaml.NewConfig
```

#### Change in `--listen-peer-urls` and `--listen-client-urls` 更改和 `--listen-peer-urls` `--listen-client-urls` 

3.2 now rejects domains names for `--listen-peer-urls` and `--listen-client-urls` (3.1 only prints out warnings), since domain name is invalid for  network interface binding. Make sure that those URLs are properly  formatted as `scheme://IP:port`.
3.2 现在拒绝 AND `--listen-client-urls` 的 `--listen-peer-urls` 域名（3.1 仅打印警告），因为域名对于网络接口绑定无效。请确保这些 URL 的格式正确为 `scheme://IP:port` 。

See [issue #6336](https://github.com/etcd-io/etcd/issues/6336) for more contexts.
有关更多上下文，请参阅问题 #6336。

### Server upgrade checklists 服务器升级清单

#### Upgrade requirements 升级要求

To upgrade an existing etcd deployment to 3.2, the running cluster must be 3.1 or greater. If it’s before 3.1, please [upgrade to 3.1](https://etcd.io/docs/v3.5/upgrades/upgrade_3_1) before upgrading to 3.2.
要将现有 etcd 部署升级到 3.2，正在运行的集群必须为 3.1 或更高版本。如果低于 3.1，请先升级到 3.1，然后再升级到 3.2。

Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using the `etcdctl endpoint health` command before proceeding.
此外，为确保顺利滚动升级，正在运行的群集必须运行正常。在继续操作之前，请使用命令 `etcdctl endpoint health` 检查群集的运行状况。

#### Preparation 制备

Before upgrading etcd, always test the services relying on etcd in a staging  environment before deploying the upgrade to the production environment.
在升级 etcd 之前，在将升级部署到生产环境之前，请始终在暂存环境中测试依赖于 etcd 的服务。

Before beginning, [backup the etcd data](https://etcd.io/docs/v3.5/op-guide/maintenance#snapshot-backup). Should something go wrong with the upgrade, it is possible to use this backup to [downgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_2/#downgrade) back to existing etcd version. Please note that the `snapshot` command only backs up the v3 data. For v2 data, see [backing up v2 datastore](https://etcd.io/docs/v2.3/admin_guide#backing-up-the-datastore).
在开始之前，请备份 etcd 数据。如果升级出现问题，可以使用此备份降级回现有的 etcd 版本。请注意，该 `snapshot` 命令仅备份 v3 数据。有关 v2 数据，请参阅备份 v2 数据存储。

#### Mixed versions 混合版本

While upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to  version 3.2. Internally, etcd members negotiate with each other to  determine the overall cluster version, which controls the reported  version and the supported features.
在升级时，etcd 集群支持 etcd 成员的混合版本，并使用最低通用版本的协议运行。只有当集群的所有成员都升级到版本 3.2 时，集群才会被视为已升级。在内部，etcd 成员相互协商以确定整体集群版本，该版本控制报告的版本和支持的功能。

#### Limitations 局限性

Note: If the cluster only has v3 data and no v2 data, it is not subject to this limitation.
注意：如果集群只有 v3 数据而没有 v2 数据，则不受此限制。

If the cluster is serving a v2 data set larger than 50MB, each newly  upgraded member may take up to two minutes to catch up with the existing cluster. Check the size of a recent snapshot to estimate the total data size. In other words, it is safest to wait for 2 minutes between  upgrading each member.
如果群集提供的 v2 数据集大于 50MB，则每个新升级的成员最多可能需要两分钟才能赶上现有群集。检查最近快照的大小以估计总数据大小。换句话说，在升级每个成员之间等待 2 分钟是最安全的。

For a much larger total data size, 100MB or more , this one-time process  might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact the [etcd team](https://groups.google.com/g/etcd-dev) before upgrading, and we’ll be happy to provide advice on the procedure.
对于更大的总数据大小，100MB或更多，此一次性过程可能需要更多时间。这种规模的超大型 etcd 集群的管理员可以在升级之前随时联系 etcd 团队，我们很乐意提供有关该过程的建议。

#### Downgrade 降级

If all members have been upgraded to v3.2, the cluster will be upgraded to v3.2, and downgrade from this completed state is **not possible**. If any single member is still v3.1, however, the cluster and its  operations remains “v3.1”, and it is possible from this mixed cluster  state to return to using a v3.1 etcd binary on all members.
如果所有成员都已升级到 v3.2，则集群将升级到 v3.2，并且无法从此完成状态降级。但是，如果任何一个成员仍然是 v3.1，则集群及其操作仍为“v3.1”，并且可以从这种混合集群状态返回到在所有成员上使用 v3.1 etcd 二进制文件。

Please [backup the data directory](https://etcd.io/docs/v3.5/op-guide/maintenance#snapshot-backup) of all etcd members to make downgrading the cluster possible even after it has been completely upgraded.
请备份所有 etcd 成员的数据目录，以便即使在集群完全升级后也可以降级集群。

### Upgrade procedure 升级过程

This example shows how to upgrade a 3-member v3.1 etcd cluster running on a local machine.
此示例说明如何升级在本地计算机上运行的 3 成员 v3.1 etcd 集群。

#### 1. Check upgrade requirements 1. 检查升级要求

Is the cluster healthy and running v3.1.x?
群集是否正常运行并运行 v3.1.x？

```
$ ETCDCTL_API=3 etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 6.600684ms
localhost:22379 is healthy: successfully committed proposal: took = 8.540064ms
localhost:32379 is healthy: successfully committed proposal: took = 8.763432ms

$ curl http://localhost:2379/version
{"etcdserver":"3.1.7","etcdcluster":"3.1.0"}
```

#### 2. Stop the existing etcd process 2. 停止现有的 etcd 进程

When each etcd process is stopped, expected errors will be logged by other  cluster members. This is normal since a cluster member connection has  been (temporarily) broken:
当每个 etcd 进程停止时，其他集群成员将记录预期的错误。这是正常现象，因为集群成员连接已（暂时）中断：

```
2017-04-27 14:13:31.491746 I | raft: c89feb932daef420 [term 3] received MsgTimeoutNow from 6d4f535bae3ab960 and starts an election to get leadership.
2017-04-27 14:13:31.491769 I | raft: c89feb932daef420 became candidate at term 4
2017-04-27 14:13:31.491788 I | raft: c89feb932daef420 received MsgVoteResp from c89feb932daef420 at term 4
2017-04-27 14:13:31.491797 I | raft: c89feb932daef420 [logterm: 3, index: 9] sent MsgVote request to 6d4f535bae3ab960 at term 4
2017-04-27 14:13:31.491805 I | raft: c89feb932daef420 [logterm: 3, index: 9] sent MsgVote request to 9eda174c7df8a033 at term 4
2017-04-27 14:13:31.491815 I | raft: raft.node: c89feb932daef420 lost leader 6d4f535bae3ab960 at term 4
2017-04-27 14:13:31.524084 I | raft: c89feb932daef420 received MsgVoteResp from 6d4f535bae3ab960 at term 4
2017-04-27 14:13:31.524108 I | raft: c89feb932daef420 [quorum:2] has received 2 MsgVoteResp votes and 0 vote rejections
2017-04-27 14:13:31.524123 I | raft: c89feb932daef420 became leader at term 4
2017-04-27 14:13:31.524136 I | raft: raft.node: c89feb932daef420 elected leader c89feb932daef420 at term 4
2017-04-27 14:13:31.592650 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream MsgApp v2 reader)
2017-04-27 14:13:31.592825 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream Message reader)
2017-04-27 14:13:31.693275 E | rafthttp: failed to dial 6d4f535bae3ab960 on stream Message (dial tcp [::1]:2380: getsockopt: connection refused)
2017-04-27 14:13:31.693289 I | rafthttp: peer 6d4f535bae3ab960 became inactive
2017-04-27 14:13:31.936678 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream Message writer)
```

It’s a good idea at this point to [backup the etcd data](https://etcd.io/docs/v3.5/op-guide/maintenance#snapshot-backup) to provide a downgrade path should any problems occur:
此时，最好备份 etcd 数据，以便在出现任何问题时提供降级路径：

```
$ etcdctl snapshot save backup.db
```

#### 3. Drop-in etcd v3.2 binary and start the new etcd process 3. 插入 etcd v3.2 二进制文件并启动新的 etcd 进程

The new v3.2 etcd will publish its information to the cluster:
新的 v3.2 etcd 会将其信息发布到集群：

```
2017-04-27 14:14:25.363225 I | etcdserver: published {Name:s1 ClientURLs:[http://localhost:2379]} to cluster a9ededbffcb1b1f1
```

Verify that each member, and then the entire cluster, becomes healthy with the new v3.2 etcd binary:
使用新的 v3.2 etcd 二进制文件验证每个成员以及整个集群是否正常运行：

```
$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:22379 is healthy: successfully committed proposal: took = 5.540129ms
localhost:32379 is healthy: successfully committed proposal: took = 7.321771ms
localhost:2379 is healthy: successfully committed proposal: took = 10.629901ms
```

Upgraded members will log warnings like the following until the entire cluster  is upgraded. This is expected and will cease after all etcd cluster  members are upgraded to v3.2:
升级后的成员将记录如下警告，直到整个集群升级为止。这是意料之中的，并将在所有 etcd 集群成员升级到 v3.2 后停止：

```
2017-04-27 14:15:17.071804 W | etcdserver: member c89feb932daef420 has a higher version 3.2.0
2017-04-27 14:15:21.073110 W | etcdserver: the local etcd version 3.1.7 is not up-to-date
2017-04-27 14:15:21.073142 W | etcdserver: member 6d4f535bae3ab960 has a higher version 3.2.0
2017-04-27 14:15:21.073157 W | etcdserver: the local etcd version 3.1.7 is not up-to-date
2017-04-27 14:15:21.073164 W | etcdserver: member c89feb932daef420 has a higher version 3.2.0
```

#### 4. Repeat step 2 to step 3 for all other members 4. 对所有其他成员重复步骤 2 到步骤 3

#### 5. Finish 5. 完成

When all members are upgraded, the cluster will report upgrading to 3.2 successfully:
升级所有成员后，集群将报告已成功升级到 3.2：

```
2017-04-27 14:15:54.536901 N | etcdserver/membership: updated the cluster version from 3.1 to 3.2
2017-04-27 14:15:54.537035 I | etcdserver/api: enabled capabilities for version 3.2
$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 2.312897ms
localhost:22379 is healthy: successfully committed proposal: took = 2.553476ms
localhost:32379 is healthy: successfully committed proposal: took = 2.517902ms
```

# Upgrade etcd from 3.2 to 3.3 将 etcd 从 3.2 升级到 3.3

Processes, checklists, and notes on upgrading etcd from 3.2 to 3.3
将 etcd 从 3.2 升级到 3.3 的流程、清单和注意事项



In the general case, upgrading from etcd 3.2 to 3.3 can be a zero-downtime, rolling upgrade:
在一般情况下，从 etcd 3.2 升级到 3.3 可以是一个零停机时间的滚动升级：

- one by one, stop the etcd v3.2 processes and replace them with etcd v3.3 processes
  逐个停止 etcd v3.2 进程，用 etcd v3.3 进程替换
- after running all v3.3 processes, new features in v3.3 are available to the cluster
  运行所有 v3.3 进程后，v3.3 中的新功能可供集群使用

Before [starting an upgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_3/#upgrade-procedure), read through the rest of this guide to prepare.
在开始升级之前，请通读本指南的其余部分以做好准备。

### Upgrade checklists 升级清单

**NOTE:** When [migrating from v2 with no v3 data](https://github.com/etcd-io/etcd/issues/9480), etcd server v3.2+ panics when etcd restores from existing snapshots but no v3 `ETCD_DATA_DIR/member/snap/db` file. This happens when the server had migrated from v2 with no  previous v3 data. This also prevents accidental v3 data loss (e.g. `db` file might have been moved). etcd requires that post v3 migration can  only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.
注意：从没有 v3 数据的 v2 迁移时，当 etcd 从现有快照恢复但没有 v3 `ETCD_DATA_DIR/member/snap/db` 文件时，etcd 服务器 v3.2+ 会崩溃。当服务器从没有以前的 v3 数据的 v2 迁移时，会发生这种情况。这还可以防止意外的 v3 数据丢失（例如， `db` 文件可能已被移动）。etcd 要求 v3 后迁移只能对 v3 数据进行。在 v3.0 服务器包含 v3 数据之前，不要升级到较新的 v3 版本。

**NOTE:** if you enable auth and use lease(lease ttl is small), it has a high probability to encounter [issue](https://github.com/etcd-io/etcd/issues/11689) that will result in data inconsistency. It is strongly recommended  upgrading to 3.2.31+ firstly to fix this problem, and then upgrade to  3.3. In addition, if the user without permission sends a `LeaseRevoke` request to the 3.3 node during the upgrade process, it may still cause  data corruption, so it is best to ensure that your environment doesn’t  exist such abnormal calls before upgrading, see [#11691](https://github.com/etcd-io/etcd/pull/11691) for detail.
注意：如果启用身份验证并使用lease（lease ttl较小），则很有可能遇到导致数据不一致的问题。强烈建议先升级到3.2.31+解决此问题，再升级到3.3。另外，如果用户在升级过程中擅自向 3.3 节点发送 `LeaseRevoke` 请求，仍可能造成数据损坏，因此最好在升级前确保您的环境不存在此类异常调用，详见 #11691。

Highlighted breaking changes in 3.3.
突出显示了 3.3 中的重大更改。

#### Changed value type of `etcd --auto-compaction-retention` flag to `string` 将标志的 `etcd --auto-compaction-retention` 值类型更改为 `string` 

Changed `--auto-compaction-retention` flag to [accept string values](https://github.com/etcd-io/etcd/pull/8563) with [finer granularity](https://github.com/etcd-io/etcd/issues/8503). Now that `--auto-compaction-retention` accepts string values, etcd configuration YAML file `auto-compaction-retention` field must be changed to `string` type. Previously, `--config-file etcd.config.yaml` can have `auto-compaction-retention: 24` field, now must be `auto-compaction-retention: "24"` or `auto-compaction-retention: "24h"`. If configured as `--auto-compaction-mode periodic --auto-compaction-retention "24h"`, the time duration value for `--auto-compaction-retention` flag must be valid for [`time.ParseDuration`](https://golang.org/pkg/time/#ParseDuration) function in Go.
更改了 `--auto-compaction-retention` 标志以接受更精细粒度的字符串值。现在 `--auto-compaction-retention` 接受字符串值，etcd 配置 YAML 文件 `auto-compaction-retention` 字段必须更改为 `string` type。以前， `--config-file etcd.config.yaml` 可以有 `auto-compaction-retention: 24` 字段，现在必须是 `auto-compaction-retention: "24"` 或 `auto-compaction-retention: "24h"` 。如果配置为 `--auto-compaction-mode periodic --auto-compaction-retention "24h"` ，则 flag 的 `--auto-compaction-retention` 持续时间值必须对 `time.ParseDuration` Go 中的函数有效。

```diff
# etcd.config.yaml
+auto-compaction-mode: periodic
-auto-compaction-retention: 24
+auto-compaction-retention: "24"
+# Or
+auto-compaction-retention: "24h"
```

#### Changed `etcdserver.EtcdServer.ServerConfig` to `*etcdserver.EtcdServer.ServerConfig` 更改 `etcdserver.EtcdServer.ServerConfig` 为 `*etcdserver.EtcdServer.ServerConfig` 

`etcdserver.EtcdServer` has changed the type of its member field `*etcdserver.ServerConfig` to `etcdserver.ServerConfig`. And `etcdserver.NewServer` now takes `etcdserver.ServerConfig`, instead of `*etcdserver.ServerConfig`.
 `etcdserver.EtcdServer` 已将其成员字段 `*etcdserver.ServerConfig` 的类型更改为 `etcdserver.ServerConfig` 。现在 `etcdserver.NewServer` 取 `etcdserver.ServerConfig` ，而不是 `*etcdserver.ServerConfig` 。

Before and after (e.g. [k8s.io/kubernetes/test/e2e_node/services/etcd.go](https://github.com/kubernetes/kubernetes/blob/release-1.8/test/e2e_node/services/etcd.go#L50-L55))
之前和之后（例如 k8s.io/kubernetes/test/e2e_node/services/etcd.go）

```diff
import "github.com/coreos/etcd/etcdserver"

type EtcdServer struct {
	*etcdserver.EtcdServer
-	config *etcdserver.ServerConfig
+	config etcdserver.ServerConfig
}

func NewEtcd(dataDir string) *EtcdServer {
-	config := &etcdserver.ServerConfig{
+	config := etcdserver.ServerConfig{
		DataDir: dataDir,
        ...
	}
	return &EtcdServer{config: config}
}

func (e *EtcdServer) Start() error {
	var err error
	e.EtcdServer, err = etcdserver.NewServer(e.config)
    ...
```

#### Added `embed.Config.LogOutput` struct 添加了结构 `embed.Config.LogOutput` 体

**Note that this field has been renamed to `embed.Config.LogOutputs` in `[]string` type in v3.4. Please see [v3.4 upgrade guide](https://etcd.io/docs/v3.5/upgrades/upgrade_3_4/) for more details.
请注意，此字段已在 v3.4 中重命名为 `embed.Config.LogOutputs` in `[]string` type。有关详细信息，请参阅 v3.4 升级指南。**

Field `LogOutput` is added to `embed.Config`:
字段 `LogOutput` 被添加到 `embed.Config` ：

```diff
package embed

type Config struct {
 	Debug bool `json:"debug"`
 	LogPkgLevels string `json:"log-package-levels"`
+	LogOutput string `json:"log-output"`
 	...
```

Before gRPC server warnings were logged in etcdserver.
在 etcdserver 中记录 gRPC 服务器警告之前。

```
WARNING: 2017/11/02 11:35:51 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp: operation was canceled"; Reconnecting to {localhost:2379 <nil>}
WARNING: 2017/11/02 11:35:51 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp: operation was canceled"; Reconnecting to {localhost:2379 <nil>}
```

From v3.3, gRPC server logs are disabled by default.
从 v3.3 开始，gRPC 服务器日志默认处于禁用状态。

**Note that `embed.Config.SetupLogging` method has been deprecated in v3.4. Please see [v3.4 upgrade guide](https://etcd.io/docs/v3.5/upgrades/upgrade_3_4/) for more details.
请注意，该 `embed.Config.SetupLogging` 方法已在 v3.4 中弃用。有关详细信息，请参阅 v3.4 升级指南。**

```go
import "github.com/coreos/etcd/embed"

cfg := &embed.Config{Debug: false}
cfg.SetupLogging()
```

Set `embed.Config.Debug` field to `true` to enable gRPC server logs.
将字段设置为 `embed.Config.Debug`  `true` 以启用 gRPC 服务器日志。

#### Changed `/health` endpoint response 更改的 `/health` 终结点响应

Previously, `[endpoint]:[client-port]/health` returned manually marshaled JSON value. 3.3 now defines [`etcdhttp.Health`](https://pkg.go.dev/github.com/etcd-io/etcd/etcdserver/api/etcdhttp#Health) struct.
以前， `[endpoint]:[client-port]/health` 返回手动封送的 JSON 值。3.3 现在定义 `etcdhttp.Health` 了结构。

Note that in v3.3.0-rc.0, v3.3.0-rc.1, and v3.3.0-rc.2, `etcdhttp.Health` has boolean type `"health"` and `"errors"` fields. For backward compatibilities, we reverted `"health"` field to `string` type and removed `"errors"` field. Further health information will be provided in separate APIs.
请注意，在 v3.3.0-rc.0、v3.3.0-rc.1 和 v3.3.0-rc.2 中， `etcdhttp.Health` 具有布尔类型 `"health"` 和 `"errors"` 字段。为了向后兼容，我们将 `"health"` 字段恢复为 `string` 类型并删除 `"errors"` 了字段。进一步的健康信息将在单独的 API 中提供。

```bash
$ curl http://localhost:2379/health
{"health":"true"}
```

#### Changed gRPC gateway HTTP endpoints (replaced `/v3alpha` with `/v3beta`) 更改了 gRPC 网关 HTTP 终结点（替换 `/v3alpha` 为 `/v3beta` ）

Before 以前

```bash
curl -L http://localhost:2379/v3alpha/kv/put \
  -X POST -d '{"key": "Zm9v", "value": "YmFy"}'
```

After 后

```bash
curl -L http://localhost:2379/v3beta/kv/put \
  -X POST -d '{"key": "Zm9v", "value": "YmFy"}'
```

Requests to `/v3alpha` endpoints will redirect to `/v3beta`, and `/v3alpha` will be removed in 3.4 release.
对 `/v3alpha` 端点的请求将重定向到 `/v3beta` ，并将 `/v3alpha` 在 3.4 版本中删除。

#### Changed maximum request size limits 更改了最大请求大小限制

3.3 now allows custom request size limits for both server and **client side**. In previous versions(v3.2.10, v3.2.11), client response size was limited to only 4 MiB.
3.3 现在允许对服务器端和客户端进行自定义请求大小限制。在以前的版本（v3.2.10、v3.2.11）中，客户端响应大小限制为仅 4 MiB。

Server-side request limits can be configured with `--max-request-bytes` flag:
服务器端请求限制可以通过以下标志 `--max-request-bytes` 进行配置：

```bash
# limits request size to 1.5 KiB
etcd --max-request-bytes 1536

# client writes exceeding 1.5 KiB will be rejected
etcdctl put foo [LARGE VALUE...]
# etcdserver: request is too large
```

Or configure `embed.Config.MaxRequestBytes` field:
或配置 `embed.Config.MaxRequestBytes` 字段：

```go
import "github.com/coreos/etcd/embed"
import "github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes"

// limit requests to 5 MiB
cfg := embed.NewConfig()
cfg.MaxRequestBytes = 5 * 1024 * 1024

// client writes exceeding 5 MiB will be rejected
_, err := cli.Put(ctx, "foo", [LARGE VALUE...])
err == rpctypes.ErrRequestTooLarge
```

**If not specified, server-side limit defaults to 1.5 MiB**.
如果未指定，服务器端限制默认为 1.5 MiB。

Client-side request limits must be configured based on server-side limits.
客户端请求限制必须根据服务器端限制进行配置。

```bash
# limits request size to 1 MiB
etcd --max-request-bytes 1048576
import "github.com/coreos/etcd/clientv3"

cli, _ := clientv3.New(clientv3.Config{
    Endpoints: []string{"127.0.0.1:2379"},
    MaxCallSendMsgSize: 2 * 1024 * 1024,
    MaxCallRecvMsgSize: 3 * 1024 * 1024,
})


// client writes exceeding "--max-request-bytes" will be rejected from etcd server
_, err := cli.Put(ctx, "foo", strings.Repeat("a", 1*1024*1024+5))
err == rpctypes.ErrRequestTooLarge


// client writes exceeding "MaxCallSendMsgSize" will be rejected from client-side
_, err = cli.Put(ctx, "foo", strings.Repeat("a", 5*1024*1024))
err.Error() == "rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (5242890 vs. 2097152)"


// some writes under limits
for i := range []int{0,1,2,3,4} {
    _, err = cli.Put(ctx, fmt.Sprintf("foo%d", i), strings.Repeat("a", 1*1024*1024-500))
    if err != nil {
        panic(err)
    }
}
// client reads exceeding "MaxCallRecvMsgSize" will be rejected from client-side
_, err = cli.Get(ctx, "foo", clientv3.WithPrefix())
err.Error() == "rpc error: code = ResourceExhausted desc = grpc: received message larger than max (5240509 vs. 3145728)"
```

**If not specified, client-side send limit defaults to 2 MiB (1.5 MiB + gRPC overhead bytes) and receive limit to `math.MaxInt32`**. Please see [clientv3 godoc](https://pkg.go.dev/github.com/etcd-io/etcd/clientv3#Config) for more detail.
如果未指定，则客户端发送限制默认为 2 MiB（1.5 MiB + gRPC 开销字节），接收限制为 `math.MaxInt32` 。有关更多详细信息，请参阅 clientv3 godoc。

#### Changed raw gRPC client wrapper function signatures 更改了原始 gRPC 客户端包装器函数签名

3.3 changes the function signatures of `clientv3` gRPC client wrapper. This change was needed to support [custom `grpc.CallOption` on message size limits](https://github.com/etcd-io/etcd/pull/9047).
3.3 更改 gRPC 客户端包装器的 `clientv3` 函数签名。需要进行此更改以支持自定义 `grpc.CallOption` 消息大小限制。

Before and after 之前和之后

```diff
-func NewKVFromKVClient(remote pb.KVClient) KV {
+func NewKVFromKVClient(remote pb.KVClient, c *Client) KV {

-func NewClusterFromClusterClient(remote pb.ClusterClient) Cluster {
+func NewClusterFromClusterClient(remote pb.ClusterClient, c *Client) Cluster {

-func NewLeaseFromLeaseClient(remote pb.LeaseClient, keepAliveTimeout time.Duration) Lease {
+func NewLeaseFromLeaseClient(remote pb.LeaseClient, c *Client, keepAliveTimeout time.Duration) Lease {

-func NewMaintenanceFromMaintenanceClient(remote pb.MaintenanceClient) Maintenance {
+func NewMaintenanceFromMaintenanceClient(remote pb.MaintenanceClient, c *Client) Maintenance {

-func NewWatchFromWatchClient(wc pb.WatchClient) Watcher {
+func NewWatchFromWatchClient(wc pb.WatchClient, c *Client) Watcher {
```

#### Changed clientv3 `Snapshot` API error type 更改了 clientv3 `Snapshot` API 错误类型

Previously, clientv3 `Snapshot` API returned raw [`grpc/*status.statusError`] type error. v3.3 now translates those errors to corresponding public error types, to be consistent with other APIs.
以前，clientv3 `Snapshot` API 返回原始 [ `grpc/*status.statusError` ] 类型错误。v3.3 现在将这些错误转换为相应的公共错误类型，以与其他 API 保持一致。

Before 以前

```go
import "context"

// reading snapshot with canceled context should error out
ctx, cancel := context.WithCancel(context.Background())
rc, _ := cli.Snapshot(ctx)
cancel()
_, err := io.Copy(f, rc)
err.Error() == "rpc error: code = Canceled desc = context canceled"

// reading snapshot with deadline exceeded should error out
ctx, cancel = context.WithTimeout(context.Background(), time.Second)
defer cancel()
rc, _ = cli.Snapshot(ctx)
time.Sleep(2 * time.Second)
_, err = io.Copy(f, rc)
err.Error() == "rpc error: code = DeadlineExceeded desc = context deadline exceeded"
```

After 后

```go
import "context"

// reading snapshot with canceled context should error out
ctx, cancel := context.WithCancel(context.Background())
rc, _ := cli.Snapshot(ctx)
cancel()
_, err := io.Copy(f, rc)
err == context.Canceled

// reading snapshot with deadline exceeded should error out
ctx, cancel = context.WithTimeout(context.Background(), time.Second)
defer cancel()
rc, _ = cli.Snapshot(ctx)
time.Sleep(2 * time.Second)
_, err = io.Copy(f, rc)
err == context.DeadlineExceeded
```

#### Changed `etcdctl lease timetolive` command output 更改了 `etcdctl lease timetolive` 命令输出

Previously, `lease timetolive LEASE_ID` command on expired lease prints `-1s` for remaining seconds. 3.3 now outputs clearer messages.
以前， `lease timetolive LEASE_ID` 对过期租约的命令会 `-1s` 打印剩余的秒数。3.3 现在输出更清晰的消息。

Before 以前

```bash
lease 2d8257079fa1bc0c granted with TTL(0s), remaining(-1s)
```

After 后

```bash
lease 2d8257079fa1bc0c already expired
```

#### Changed `golang.org/x/net/context` imports 更改的 `golang.org/x/net/context` 导入

`clientv3` has deprecated `golang.org/x/net/context`. If a project vendors `golang.org/x/net/context` in other code (e.g. etcd generated protocol buffer code) and imports `github.com/coreos/etcd/clientv3`, it requires Go 1.9+ to compile.
 `clientv3` 已弃用 `golang.org/x/net/context` 。如果一个项目供应商 `golang.org/x/net/context` 用其他代码（例如etcd生成的协议缓冲区代码）并导入 `github.com/coreos/etcd/clientv3` ，则需要Go 1.9+来编译。

Before 以前

```go
import "golang.org/x/net/context"
cli.Put(context.Background(), "f", "v")
```

After 后

```go
import "context"
cli.Put(context.Background(), "f", "v")
```

#### Changed gRPC dependency 更改了 gRPC 依赖项

3.3 now requires [grpc/grpc-go](https://github.com/grpc/grpc-go/releases) `v1.7.5`.
3.3 现在需要 grpc/grpc-go `v1.7.5` 。

##### Deprecated `grpclog.Logger` 荒废的 `grpclog.Logger` 

`grpclog.Logger` has been deprecated in favor of [`grpclog.LoggerV2`](https://github.com/grpc/grpc-go/blob/master/grpclog/loggerv2.go). `clientv3.Logger` is now `grpclog.LoggerV2`.
 `grpclog.Logger` 已被弃用 `grpclog.LoggerV2` ，取而代之的是 。 `clientv3.Logger` 现在是 `grpclog.LoggerV2` .

Before 以前

```go
import "github.com/coreos/etcd/clientv3"
clientv3.SetLogger(log.New(os.Stderr, "grpc: ", 0))
```

After 后

```go
import "github.com/coreos/etcd/clientv3"
import "google.golang.org/grpc/grpclog"
clientv3.SetLogger(grpclog.NewLoggerV2(os.Stderr, os.Stderr, os.Stderr))

// log.New above cannot be used (not implement grpclog.LoggerV2 interface)
```

##### Deprecated `grpc.ErrClientConnTimeout` 荒废的 `grpc.ErrClientConnTimeout` 

Previously, `grpc.ErrClientConnTimeout` error is returned on client dial time-outs. 3.3 instead returns `context.DeadlineExceeded` (see [#8504](https://github.com/etcd-io/etcd/issues/8504)).
以前， `grpc.ErrClientConnTimeout` 在客户端拨号超时时返回错误。3.3 改为返回 `context.DeadlineExceeded` （参见 #8504）。

Before 以前

```go
// expect dial time-out on ipv4 blackhole
_, err := clientv3.New(clientv3.Config{
    Endpoints:   []string{"http://254.0.0.1:12345"},
    DialTimeout: 2 * time.Second
})
if err == grpc.ErrClientConnTimeout {
	// handle errors
}
```

After 后

```go
_, err := clientv3.New(clientv3.Config{
    Endpoints:   []string{"http://254.0.0.1:12345"},
    DialTimeout: 2 * time.Second
})
if err == context.DeadlineExceeded {
	// handle errors
}
```

#### Changed official container registry 更改了官方容器注册表

etcd now uses [`gcr.io/etcd-development/etcd`](https://gcr.io/etcd-development/etcd) as a primary container registry, and [`quay.io/coreos/etcd`](https://quay.io/coreos/etcd) as secondary.
etcd 现在用作 `gcr.io/etcd-development/etcd` 主容器注册表和 `quay.io/coreos/etcd` 辅助容器注册表。

Before 以前

```bash
docker pull quay.io/coreos/etcd:v3.2.5
```

After 后

```bash
docker pull gcr.io/etcd-development/etcd:v3.3.0
```

### Upgrades to >= v3.3.14 升级到 >= v3.3.14

[v3.3.14](https://github.com/etcd-io/etcd/releases/tag/v3.3.14) had to include some features from 3.4, while trying to minimize the  difference between client balancer implementation. This release fixes [“kube-apiserver 1.13.x refuses to work when first etcd-server is not available” (kubernetes#72102)](https://github.com/kubernetes/kubernetes/issues/72102).
v3.3.14 必须包含 3.4 中的一些功能，同时尽量减少客户端均衡器实现之间的差异。此版本修复了“kube-apiserver 1.13.x 在第一个 etcd-server 不可用时拒绝工作”的问题 （kubernetes#72102）。

`grpc.ErrClientConnClosing` has been [deprecated in gRPC >= 1.10](https://github.com/grpc/grpc-go/pull/1854).
 `grpc.ErrClientConnClosing` 已在 gRPC >= 1.10 中弃用。

```diff
import (
+	"go.etcd.io/etcd/clientv3"

	"google.golang.org/grpc"
+	"google.golang.org/grpc/codes"
+	"google.golang.org/grpc/status"
)

_, err := kvc.Get(ctx, "a")
-if err == grpc.ErrClientConnClosing {
+if clientv3.IsConnCanceled(err) {

// or
+s, ok := status.FromError(err)
+if ok {
+  if s.Code() == codes.Canceled
```

[The new client balancer](https://etcd.io/docs/v3.5/learning/design-client/) uses an asynchronous resolver to pass endpoints to the gRPC dial function. As a result, [v3.3.14](https://github.com/etcd-io/etcd/releases/tag/v3.3.14) or later requires `grpc.WithBlock` dial option to wait until the underlying connection is up.
新的客户端均衡器使用异步解析程序将终结点传递给 gRPC 拨号函数。因此，v3.3.14 或更高版本需要 `grpc.WithBlock` 拨号选项才能等到基础连接启动。

```diff
import (
	"time"
	"go.etcd.io/etcd/clientv3"
+	"google.golang.org/grpc"
)

+// "grpc.WithBlock()" to block until the underlying connection is up
ccfg := clientv3.Config{
  Endpoints:            []string{"localhost:2379"},
  DialTimeout:          time.Second,
+ DialOptions:          []grpc.DialOption{grpc.WithBlock()},
  DialKeepAliveTime:    time.Second,
  DialKeepAliveTimeout: 500 * time.Millisecond,
}
```

Please see [CHANGELOG](https://github.com/etcd-io/etcd/blob/master/CHANGELOG-3.3.md) for a full list of changes.
请参阅更改日志以获取完整的更改列表。

### Server upgrade checklists 服务器升级清单

#### Upgrade requirements 升级要求

To upgrade an existing etcd deployment to 3.3, the running cluster must be 3.2 or greater. If it’s before 3.2, please [upgrade to 3.2](https://etcd.io/docs/v3.5/upgrades/upgrade_3_2/) before upgrading to 3.3.
要将现有 etcd 部署升级到 3.3，正在运行的集群必须为 3.2 或更高版本。如果是 3.2 之前版本，请先升级到 3.2，然后再升级到 3.3。

Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using the `etcdctl endpoint health` command before proceeding.
此外，为确保顺利滚动升级，正在运行的群集必须运行正常。在继续操作之前，请使用命令 `etcdctl endpoint health` 检查群集的运行状况。

#### Preparation 制备

Before upgrading etcd, always test the services relying on etcd in a staging  environment before deploying the upgrade to the production environment.
在升级 etcd 之前，在将升级部署到生产环境之前，请始终在暂存环境中测试依赖于 etcd 的服务。

Before beginning, [backup the etcd data](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup). Should something go wrong with the upgrade, it is possible to use this backup to [downgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_3/#downgrade) back to existing etcd version. Please note that the `snapshot` command only backs up the v3 data. For v2 data, see [backing up v2 datastore](https://etcd.io/docs/v2.3/admin_guide/#backing-up-the-datastore).
在开始之前，请备份 etcd 数据。如果升级出现问题，可以使用此备份降级回现有的 etcd 版本。请注意，该 `snapshot` 命令仅备份 v3 数据。有关 v2 数据，请参阅备份 v2 数据存储。

#### Mixed versions 混合版本

While upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to  version 3.3. Internally, etcd members negotiate with each other to  determine the overall cluster version, which controls the reported  version and the supported features.
在升级时，etcd 集群支持 etcd 成员的混合版本，并使用最低通用版本的协议运行。只有当集群的所有成员都升级到版本 3.3 后，集群才会被视为已升级。在内部，etcd 成员相互协商以确定整体集群版本，该版本控制报告的版本和支持的功能。

#### Limitations 局限性

Note: If the cluster only has v3 data and no v2 data, it is not subject to this limitation.
注意：如果集群只有 v3 数据而没有 v2 数据，则不受此限制。

If the cluster is serving a v2 data set larger than 50MB, each newly  upgraded member may take up to two minutes to catch up with the existing cluster. Check the size of a recent snapshot to estimate the total data size. In other words, it is safest to wait for 2 minutes between  upgrading each member.
如果群集提供的 v2 数据集大于 50MB，则每个新升级的成员最多可能需要两分钟才能赶上现有群集。检查最近快照的大小以估计总数据大小。换句话说，在升级每个成员之间等待 2 分钟是最安全的。

For a much larger total data size, 100MB or more , this one-time process  might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact the [etcd team](https://groups.google.com/g/etcd-dev) before upgrading, and we’ll be happy to provide advice on the procedure.
对于更大的总数据大小，100MB或更多，此一次性过程可能需要更多时间。这种规模的超大型 etcd 集群的管理员可以在升级之前随时联系 etcd 团队，我们很乐意提供有关该过程的建议。

#### Downgrade 降级

If all members have been upgraded to v3.3, the cluster will be upgraded to v3.3, and downgrade from this completed state is **not possible**. If any single member is still v3.2, however, the cluster and its  operations remains “v3.2”, and it is possible from this mixed cluster  state to return to using a v3.2 etcd binary on all members.
如果所有成员都已升级到 v3.3，则集群将升级到 v3.3，并且无法从此完成状态降级。但是，如果任何一个成员仍然是 v3.2，则集群及其操作将保持为“v3.2”，并且有可能从这种混合集群状态返回到在所有成员上使用 v3.2 etcd 二进制文件。

Please [backup the data directory](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup) of all etcd members to make downgrading the cluster possible even after it has been completely upgraded.
请备份所有 etcd 成员的数据目录，以便即使在集群完全升级后也可以降级集群。

### Upgrade procedure 升级过程

This example shows how to upgrade a 3-member v3.2 etcd cluster running on a local machine.
此示例说明如何升级在本地计算机上运行的 3 成员 v3.2 etcd 集群。

#### 1. Check upgrade requirements 1. 检查升级要求

Is the cluster healthy and running v3.2.x?
群集是否正常运行并运行 v3.2.x？

```
$ ETCDCTL_API=3 etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 6.600684ms
localhost:22379 is healthy: successfully committed proposal: took = 8.540064ms
localhost:32379 is healthy: successfully committed proposal: took = 8.763432ms

$ curl http://localhost:2379/version
{"etcdserver":"3.2.7","etcdcluster":"3.2.0"}
```

#### 2. Stop the existing etcd process 2. 停止现有的 etcd 进程

When each etcd process is stopped, expected errors will be logged by other  cluster members. This is normal since a cluster member connection has  been (temporarily) broken:
当每个 etcd 进程停止时，其他集群成员将记录预期的错误。这是正常现象，因为集群成员连接已（暂时）中断：

```
14:13:31.491746 I | raft: c89feb932daef420 [term 3] received MsgTimeoutNow from 6d4f535bae3ab960 and starts an election to get leadership.
14:13:31.491769 I | raft: c89feb932daef420 became candidate at term 4
14:13:31.491788 I | raft: c89feb932daef420 received MsgVoteResp from c89feb932daef420 at term 4
14:13:31.491797 I | raft: c89feb932daef420 [logterm: 3, index: 9] sent MsgVote request to 6d4f535bae3ab960 at term 4
14:13:31.491805 I | raft: c89feb932daef420 [logterm: 3, index: 9] sent MsgVote request to 9eda174c7df8a033 at term 4
14:13:31.491815 I | raft: raft.node: c89feb932daef420 lost leader 6d4f535bae3ab960 at term 4
14:13:31.524084 I | raft: c89feb932daef420 received MsgVoteResp from 6d4f535bae3ab960 at term 4
14:13:31.524108 I | raft: c89feb932daef420 [quorum:2] has received 2 MsgVoteResp votes and 0 vote rejections
14:13:31.524123 I | raft: c89feb932daef420 became leader at term 4
14:13:31.524136 I | raft: raft.node: c89feb932daef420 elected leader c89feb932daef420 at term 4
14:13:31.592650 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream MsgApp v2 reader)
14:13:31.592825 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream Message reader)
14:13:31.693275 E | rafthttp: failed to dial 6d4f535bae3ab960 on stream Message (dial tcp [::1]:2380: getsockopt: connection refused)
14:13:31.693289 I | rafthttp: peer 6d4f535bae3ab960 became inactive
14:13:31.936678 W | rafthttp: lost the TCP streaming connection with peer 6d4f535bae3ab960 (stream Message writer)
```

It’s a good idea at this point to [backup the etcd data](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup) to provide a downgrade path should any problems occur:
此时，最好备份 etcd 数据，以便在出现任何问题时提供降级路径：

```
$ etcdctl snapshot save backup.db
```

#### 3. Drop-in etcd v3.3 binary and start the new etcd process 3. 插入 etcd v3.3 二进制文件并启动新的 etcd 进程

The new v3.3 etcd will publish its information to the cluster:
新的 v3.3 etcd 会将其信息发布到集群：

```
14:14:25.363225 I | etcdserver: published {Name:s1 ClientURLs:[http://localhost:2379]} to cluster a9ededbffcb1b1f1
```

Verify that each member, and then the entire cluster, becomes healthy with the new v3.3 etcd binary:
使用新的 v3.3 etcd 二进制文件验证每个成员以及整个集群是否正常运行：

```
$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:22379 is healthy: successfully committed proposal: took = 5.540129ms
localhost:32379 is healthy: successfully committed proposal: took = 7.321771ms
localhost:2379 is healthy: successfully committed proposal: took = 10.629901ms
```

Upgraded members will log warnings like the following until the entire cluster  is upgraded. This is expected and will cease after all etcd cluster  members are upgraded to v3.3:
升级后的成员将记录如下警告，直到整个集群升级为止。这是意料之中的，并且在所有 etcd 集群成员升级到 v3.3 后将停止：

```
14:15:17.071804 W | etcdserver: member c89feb932daef420 has a higher version 3.3.0
14:15:21.073110 W | etcdserver: the local etcd version 3.2.7 is not up-to-date
14:15:21.073142 W | etcdserver: member 6d4f535bae3ab960 has a higher version 3.3.0
14:15:21.073157 W | etcdserver: the local etcd version 3.2.7 is not up-to-date
14:15:21.073164 W | etcdserver: member c89feb932daef420 has a higher version 3.3.0
```

#### 4. Repeat step 2 to step 3 for all other members 4. 对所有其他成员重复步骤 2 到步骤 3

#### 5. Finish 5. 完成

When all members are upgraded, the cluster will report upgrading to 3.3 successfully:
升级所有成员后，群集将报告已成功升级到 3.3：

```
14:15:54.536901 N | etcdserver/membership: updated the cluster version from 3.2 to 3.3
14:15:54.537035 I | etcdserver/api: enabled capabilities for version 3.3
$ ETCDCTL_API=3 /etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
localhost:2379 is healthy: successfully committed proposal: took = 2.312897ms
localhost:22379 is healthy: successfully committed proposal: took = 2.553476ms
localhost:32379 is healthy: successfully committed proposal: took = 2.517902ms
```

# Upgrade etcd from 3.3 to 3.4 将 etcd 从 3.3 升级到 3.4

Processes, checklists, and notes on upgrading etcd from 3.3 to 3.4
将 etcd 从 3.3 升级到 3.4 的流程、清单和注意事项



In the general case, upgrading from etcd 3.3 to 3.4 can be a zero-downtime, rolling upgrade:
在一般情况下，从 etcd 3.3 升级到 3.4 可以是一个零停机时间的滚动升级：

- one by one, stop the etcd v3.3 processes and replace them with etcd v3.4 processes
  逐个停止 etcd v3.3 进程，用 etcd v3.4 进程替换
- after running all v3.4 processes, new features in v3.4 are available to the cluster
  运行所有 v3.4 进程后，v3.4 中的新功能可供集群使用

Before [starting an upgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_4/#upgrade-procedure), read through the rest of this guide to prepare.
在开始升级之前，请通读本指南的其余部分以做好准备。

### Upgrade checklists 升级清单

**NOTE:** When [migrating from v2 with no v3 data](https://github.com/etcd-io/etcd/issues/9480), etcd server v3.2+ panics when etcd restores from existing snapshots but no v3 `ETCD_DATA_DIR/member/snap/db` file. This happens when the server had migrated from v2 with no  previous v3 data. This also prevents accidental v3 data loss (e.g. `db` file might have been moved). etcd requires that post v3 migration can  only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.
注意：从没有 v3 数据的 v2 迁移时，当 etcd 从现有快照恢复但没有 v3 `ETCD_DATA_DIR/member/snap/db` 文件时，etcd 服务器 v3.2+ 会崩溃。当服务器从没有以前的 v3 数据的 v2 迁移时，会发生这种情况。这还可以防止意外的 v3 数据丢失（例如， `db` 文件可能已被移动）。etcd 要求 v3 后迁移只能对 v3 数据进行。在 v3.0 服务器包含 v3 数据之前，不要升级到较新的 v3 版本。

Highlighted breaking changes in 3.4.
突出显示了 3.4 中的重大更改。

#### Make `ETCDCTL_API=3 etcdctl` default 设 `ETCDCTL_API=3 etcdctl` 为默认值

`ETCDCTL_API=3` is now the default.
 `ETCDCTL_API=3` 现在是默认值。

```diff
etcdctl set foo bar
Error: unknown command "set" for "etcdctl"

-etcdctl set foo bar
+ETCDCTL_API=2 etcdctl set foo bar
bar

ETCDCTL_API=3 etcdctl put foo bar
OK

-ETCDCTL_API=3 etcdctl put foo bar
+etcdctl put foo bar
```

#### Make `etcd --enable-v2=false` default 设 `etcd --enable-v2=false` 为默认值

[`etcd --enable-v2=false`](https://github.com/etcd-io/etcd/pull/10935) is now the default.
 `etcd --enable-v2=false` 现在是默认值。

This means, unless `etcd --enable-v2=true` is specified, etcd v3.4 server would not serve v2 API requests.
这意味着，除非指定，否则 `etcd --enable-v2=true` etcd v3.4 服务器不会提供 v2 API 请求。

If v2 API were used, make sure to enable v2 API in v3.4:
如果使用了 v2 API，请确保在 v3.4 中启用 v2 API：

```diff
-etcd
+etcd --enable-v2=true
```

Other HTTP APIs will still work (e.g. `[CLIENT-URL]/metrics`, `[CLIENT-URL]/health`, v3 gRPC gateway).
其他 HTTP API 仍将有效（例如 `[CLIENT-URL]/metrics` 、 `[CLIENT-URL]/health` 、 v3 gRPC 网关）。

#### Deprecated `etcd --ca-file` and `etcd --peer-ca-file` flags 已 `etcd --ca-file` 弃用和 `etcd --peer-ca-file` 标志

`--ca-file` and `--peer-ca-file` flags are deprecated; they have been deprecated since v2.1.
 `--ca-file` 并且 `--peer-ca-file` 标志已弃用;自 v2.1 起，它们已被弃用。

Note setting this parameter will also automatically enable client cert authentication no matter what value is set for `--client-cert-auth`.
注意 设置此参数也将自动启用客户端证书身份验证，无论为 `--client-cert-auth` 设置了什么值。

```diff
-etcd --ca-file ca-client.crt
+etcd --trusted-ca-file ca-client.crt
-etcd --peer-ca-file ca-peer.crt
+etcd --peer-trusted-ca-file ca-peer.crt
```

#### Deprecated `grpc.ErrClientConnClosing` error 已 `grpc.ErrClientConnClosing` 弃用的错误

`grpc.ErrClientConnClosing` has been [deprecated in gRPC >= 1.10](https://github.com/grpc/grpc-go/pull/1854).
 `grpc.ErrClientConnClosing` 已在 gRPC >= 1.10 中弃用。

```diff
import (
+	"go.etcd.io/etcd/clientv3"

	"google.golang.org/grpc"
+	"google.golang.org/grpc/codes"
+	"google.golang.org/grpc/status"
)

_, err := kvc.Get(ctx, "a")
-if err == grpc.ErrClientConnClosing {
+if clientv3.IsConnCanceled(err) {

// or
+s, ok := status.FromError(err)
+if ok {
+  if s.Code() == codes.Canceled
```

#### Require `grpc.WithBlock` for client dial 客户端拨号需要 `grpc.WithBlock` 

[The new client balancer](https://etcd.io/docs/v3.5/learning/design-client/) uses an asynchronous resolver to pass endpoints to the gRPC dial function. As a result, v3.4 client requires `grpc.WithBlock` dial option to wait until the underlying connection is up.
新的客户端均衡器使用异步解析程序将终结点传递给 gRPC 拨号函数。因此，v3.4 客户端需要 `grpc.WithBlock` 拨号选项来等待基础连接启动。

```diff
import (
	"time"
	"go.etcd.io/etcd/clientv3"
+	"google.golang.org/grpc"
)

+// "grpc.WithBlock()" to block until the underlying connection is up
ccfg := clientv3.Config{
  Endpoints:            []string{"localhost:2379"},
  DialTimeout:          time.Second,
+ DialOptions:          []grpc.DialOption{grpc.WithBlock()},
  DialKeepAliveTime:    time.Second,
  DialKeepAliveTimeout: 500 * time.Millisecond,
}
```

#### Deprecating `etcd_debugging_mvcc_db_total_size_in_bytes` Prometheus metrics 弃用 `etcd_debugging_mvcc_db_total_size_in_bytes` Prometheus 指标

v3.4 promotes `etcd_debugging_mvcc_db_total_size_in_bytes` Prometheus metrics to `etcd_mvcc_db_total_size_in_bytes`, in order to encourage etcd storage monitoring.
v3.4 将 Prometheus 指标提升 `etcd_debugging_mvcc_db_total_size_in_bytes` 为 `etcd_mvcc_db_total_size_in_bytes` ，以鼓励 etcd 存储监控。

`etcd_debugging_mvcc_db_total_size_in_bytes` is still served in v3.4 for backward compatibilities. It will be completely deprecated in v3.5.
 `etcd_debugging_mvcc_db_total_size_in_bytes` 在 v3.4 中仍提供向后兼容性。它将在 v3.5 中被完全弃用。

```diff
-etcd_debugging_mvcc_db_total_size_in_bytes
+etcd_mvcc_db_total_size_in_bytes
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecating `etcd_debugging_mvcc_put_total` Prometheus metrics 弃用 `etcd_debugging_mvcc_put_total` Prometheus 指标

v3.4 promotes `etcd_debugging_mvcc_put_total` Prometheus metrics to `etcd_mvcc_put_total`, in order to encourage etcd storage monitoring.
v3.4 将 Prometheus 指标提升 `etcd_debugging_mvcc_put_total` 为 `etcd_mvcc_put_total` ，以鼓励 etcd 存储监控。

`etcd_debugging_mvcc_put_total` is still served in v3.4 for backward compatibilities. It will be completely deprecated in v3.5.
 `etcd_debugging_mvcc_put_total` 在 v3.4 中仍提供向后兼容性。它将在 v3.5 中被完全弃用。

```diff
-etcd_debugging_mvcc_put_total
+etcd_mvcc_put_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecating `etcd_debugging_mvcc_delete_total` Prometheus metrics 弃用 `etcd_debugging_mvcc_delete_total` Prometheus 指标

v3.4 promotes `etcd_debugging_mvcc_delete_total` Prometheus metrics to `etcd_mvcc_delete_total`, in order to encourage etcd storage monitoring.
v3.4 将 Prometheus 指标提升 `etcd_debugging_mvcc_delete_total` 为 `etcd_mvcc_delete_total` ，以鼓励 etcd 存储监控。

`etcd_debugging_mvcc_delete_total` is still served in v3.4 for backward compatibilities. It will be completely deprecated in v3.5.
 `etcd_debugging_mvcc_delete_total` 在 v3.4 中仍提供向后兼容性。它将在 v3.5 中被完全弃用。

```diff
-etcd_debugging_mvcc_delete_total
+etcd_mvcc_delete_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecating `etcd_debugging_mvcc_txn_total` Prometheus metrics 弃用 `etcd_debugging_mvcc_txn_total` Prometheus 指标

v3.4 promotes `etcd_debugging_mvcc_txn_total` Prometheus metrics to `etcd_mvcc_txn_total`, in order to encourage etcd storage monitoring.
v3.4 将 Prometheus 指标提升 `etcd_debugging_mvcc_txn_total` 为 `etcd_mvcc_txn_total` ，以鼓励 etcd 存储监控。

`etcd_debugging_mvcc_txn_total` is still served in v3.4 for backward compatibilities. It will be completely deprecated in v3.5.
 `etcd_debugging_mvcc_txn_total` 在 v3.4 中仍提供向后兼容性。它将在 v3.5 中被完全弃用。

```diff
-etcd_debugging_mvcc_txn_total
+etcd_mvcc_txn_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecating `etcd_debugging_mvcc_range_total` Prometheus metrics 弃用 `etcd_debugging_mvcc_range_total` Prometheus 指标

v3.4 promotes `etcd_debugging_mvcc_range_total` Prometheus metrics to `etcd_mvcc_range_total`, in order to encourage etcd storage monitoring.
v3.4 将 Prometheus 指标提升 `etcd_debugging_mvcc_range_total` 为 `etcd_mvcc_range_total` ，以鼓励 etcd 存储监控。

`etcd_debugging_mvcc_range_total` is still served in v3.4 for backward compatibilities. It will be completely deprecated in v3.5.
 `etcd_debugging_mvcc_range_total` 在 v3.4 中仍提供向后兼容性。它将在 v3.5 中被完全弃用。

```diff
-etcd_debugging_mvcc_range_total
+etcd_mvcc_range_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecating `etcd --log-output` flag (now `--log-outputs`) 弃用 `etcd --log-output` 标志（现在 `--log-outputs` ）

Rename [`etcd --log-output` to `--log-outputs`](https://github.com/etcd-io/etcd/pull/9624) to support multiple log outputs. **`etcd --logger=capnslog` does not support multiple log outputs.**
重命名为 `etcd --log-output`  `--log-outputs` 以支持多个日志输出。 `etcd --logger=capnslog` 不支持多个日志输出。

**`etcd --log-output`** will be deprecated in v3.5. **`etcd --logger=capnslog` will be deprecated in v3.5**.
 `etcd --log-output` 将在 v3.5 中弃用。 `etcd --logger=capnslog` 将在 v3.5 中弃用。

```diff
-etcd --log-output=stderr
+etcd --log-outputs=stderr

+# to write logs to stderr and a.log file at the same time
+# only "--logger=zap" supports multiple writers
+etcd --logger=zap --log-outputs=stderr,a.log
```

v3.4 adds `etcd --logger=zap --log-outputs=stderr` support for structured logging and multiple log outputs. Main  motivation is to promote automated etcd monitoring, rather than looking  back server logs when it starts breaking. Future development will make  etcd log as few as possible, and make etcd easier to monitor with  metrics and alerts. **`etcd --logger=capnslog` will be deprecated in v3.5**.
v3.4 增加了 `etcd --logger=zap --log-outputs=stderr` 对结构化日志记录和多个日志输出的支持。主要动机是促进自动化的 etcd 监控，而不是在服务器日志开始崩溃时回顾它。未来的开发将使 etcd 日志尽可能少，并使 etcd 更容易通过指标和警报进行监控。 `etcd --logger=capnslog` 将在 v3.5 中弃用。

#### Changed `log-outputs` field type in `etcd --config-file` to `[]string` 将字段类型更改 `log-outputs` 为 `etcd --config-file` `[]string` 

Now that `log-outputs` (old field name `log-output`) accepts multiple writers, etcd configuration YAML file `log-outputs` field must be changed to `[]string` type as below:
现在 `log-outputs` （old field name `log-output` ） 接受多个写入器，etcd 配置 YAML 文件 `log-outputs` 字段必须更改为 `[]string` 类型，如下所示：

```diff
 # Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.
-log-output: default
+log-outputs: [default]
```

#### Renamed `embed.Config.LogOutput` to `embed.Config.LogOutputs` 重命名为 `embed.Config.LogOutput` `embed.Config.LogOutputs` 

Renamed [**`embed.Config.LogOutput`** to **`embed.Config.LogOutputs`**](https://github.com/etcd-io/etcd/pull/9624) to support multiple log outputs. And changed [`embed.Config.LogOutput` type from `string` to `[\]string`](https://github.com/etcd-io/etcd/pull/9579) to support multiple log outputs.
重命名为 `embed.Config.LogOutput`  `embed.Config.LogOutputs` 支持多个日志输出。并将类型从 `embed.Config.LogOutput`  `string` 更改 `[]string` 为支持多个日志输出。

```diff
import "github.com/coreos/etcd/embed"

cfg := &embed.Config{Debug: false}
-cfg.LogOutput = "stderr"
+cfg.LogOutputs = []string{"stderr"}
```

#### v3.5 deprecates `capnslog` v3.5 弃用 `capnslog` 

**v3.5 will deprecate `etcd --log-package-levels` flag for `capnslog`**; `etcd --logger=zap --log-outputs=stderr` will the default. **v3.5 will deprecate `[CLIENT-URL]/config/local/log` endpoint.**
v3.5 将弃用 `etcd --log-package-levels` `capnslog` ; `etcd --logger=zap --log-outputs=stderr` 将默认。v3.5 将弃用 `[CLIENT-URL]/config/local/log` endpoint。

```diff
-etcd
+etcd --logger zap
```

#### Deprecating `etcd --debug` flag (now `--log-level=debug`) 弃用 `etcd --debug` 标志（现在 `--log-level=debug` ）

v3.4 deprecates [`etcd --debug`](https://github.com/etcd-io/etcd/pull/10947) flag. Instead, use `etcd --log-level=debug` flag.
v3.4 弃用了标志 `etcd --debug` 。请改用 `etcd --log-level=debug` flag。

```diff
-etcd --debug
+etcd --logger zap --log-level debug
```

#### Deprecated `pkg/transport.TLSInfo.CAFile` field 已弃用的 `pkg/transport.TLSInfo.CAFile` 字段

Deprecated `pkg/transport.TLSInfo.CAFile` field.
已弃用的 `pkg/transport.TLSInfo.CAFile` 字段。

```diff
import "github.com/coreos/etcd/pkg/transport"

tlsInfo := transport.TLSInfo{
    CertFile: "/tmp/test-certs/test.pem",
    KeyFile: "/tmp/test-certs/test-key.pem",
-   CAFile: "/tmp/test-certs/trusted-ca.pem",
+   TrustedCAFile: "/tmp/test-certs/trusted-ca.pem",
}
tlsConfig, err := tlsInfo.ClientConfig()
if err != nil {
    panic(err)
}
```

#### Changed `embed.Config.SnapCount` to `embed.Config.SnapshotCount` 更改 `embed.Config.SnapCount` 为 `embed.Config.SnapshotCount` 

To be consistent with the flag name `etcd --snapshot-count`, `embed.Config.SnapCount` field has been renamed to `embed.Config.SnapshotCount`:
为了与标志名称 `etcd --snapshot-count` 保持一致， `embed.Config.SnapCount` 字段已重命名为 `embed.Config.SnapshotCount` ：

```diff
import "github.com/coreos/etcd/embed"

cfg := embed.NewConfig()
-cfg.SnapCount = 100000
+cfg.SnapshotCount = 100000
```

#### Changed `etcdserver.ServerConfig.SnapCount` to `etcdserver.ServerConfig.SnapshotCount` 更改 `etcdserver.ServerConfig.SnapCount` 为 `etcdserver.ServerConfig.SnapshotCount` 

To be consistent with the flag name `etcd --snapshot-count`, `etcdserver.ServerConfig.SnapCount` field has been renamed to `etcdserver.ServerConfig.SnapshotCount`:
为了与标志名称 `etcd --snapshot-count` 保持一致， `etcdserver.ServerConfig.SnapCount` 字段已重命名为 `etcdserver.ServerConfig.SnapshotCount` ：

```diff
import "github.com/coreos/etcd/etcdserver"

srvcfg := etcdserver.ServerConfig{
-  SnapCount: 100000,
+  SnapshotCount: 100000,
```

#### Changed function signature in package `wal` 更改了包 `wal` 中的函数签名

Changed `wal` function signatures to support structured logger.
更改了 `wal` 函数签名以支持结构化记录器。

```diff
import "github.com/coreos/etcd/wal"
+import "go.uber.org/zap"

+lg, _ = zap.NewProduction()

-wal.Open(dirpath, snap)
+wal.Open(lg, dirpath, snap)

-wal.OpenForRead(dirpath, snap)
+wal.OpenForRead(lg, dirpath, snap)

-wal.Repair(dirpath)
+wal.Repair(lg, dirpath)

-wal.Create(dirpath, metadata)
+wal.Create(lg, dirpath, metadata)
```

#### Changed `IntervalTree` type in package `pkg/adt` 更改 `IntervalTree` 了包 `pkg/adt` 中的类型

`pkg/adt.IntervalTree` is now defined as an `interface`.
 `pkg/adt.IntervalTree` 现在被定义为 `interface` .

```diff
import (
    "fmt"

    "go.etcd.io/etcd/pkg/adt"
)

func main() {
-    ivt := &adt.IntervalTree{}
+    ivt := adt.NewIntervalTree()
```

#### Deprecated `embed.Config.SetupLogging` 荒废的 `embed.Config.SetupLogging` 

`embed.Config.SetupLogging` has been removed in order to prevent wrong logging configuration, and now set up automatically.
 `embed.Config.SetupLogging` 已删除以防止错误的日志记录配置，现在会自动设置。

```diff
import "github.com/coreos/etcd/embed"

cfg := &embed.Config{Debug: false}
-cfg.SetupLogging()
```

#### Changed gRPC gateway HTTP endpoints (replaced `/v3beta` with `/v3`) 更改了 gRPC 网关 HTTP 终结点（替换 `/v3beta` 为 `/v3` ）

Before 以前

```bash
curl -L http://localhost:2379/v3beta/kv/put \
  -X POST -d '{"key": "Zm9v", "value": "YmFy"}'
```

After 后

```bash
curl -L http://localhost:2379/v3/kv/put \
  -X POST -d '{"key": "Zm9v", "value": "YmFy"}'
```

Requests to `/v3beta` endpoints will redirect to `/v3`, and `/v3beta` will be removed in 3.5 release.
对 `/v3beta` 端点的请求将重定向到 `/v3` ，并将 `/v3beta` 在 3.5 版本中删除。

#### Deprecated container image tags 已弃用的容器映像标记

`latest` and minor version images tags are deprecated:
 `latest` 次要版本图像标记已弃用：

```diff
-docker pull gcr.io/etcd-development/etcd:latest
+docker pull gcr.io/etcd-development/etcd:v3.4.0

-docker pull gcr.io/etcd-development/etcd:v3.4
+docker pull gcr.io/etcd-development/etcd:v3.4.0

-docker pull gcr.io/etcd-development/etcd:v3.4
+docker pull gcr.io/etcd-development/etcd:v3.4.1

-docker pull gcr.io/etcd-development/etcd:v3.4
+docker pull gcr.io/etcd-development/etcd:v3.4.2
```

### Server upgrade checklists 服务器升级清单

#### Upgrade requirements 升级要求

To upgrade an existing etcd deployment to 3.4, the running cluster must be 3.3 or greater. If it’s before 3.3, please [upgrade to 3.3](https://etcd.io/docs/v3.5/upgrades/upgrade_3_3/) before upgrading to 3.4.
要将现有 etcd 部署升级到 3.4，正在运行的集群必须为 3.3 或更高版本。如果是 3.3 之前版本，请先升级到 3.3，然后再升级到 3.4。

Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using the `etcdctl endpoint health` command before proceeding.
此外，为确保顺利滚动升级，正在运行的群集必须运行正常。在继续操作之前，请使用命令 `etcdctl endpoint health` 检查群集的运行状况。

#### Preparation 制备

Before upgrading etcd, always test the services relying on etcd in a staging  environment before deploying the upgrade to the production environment.
在升级 etcd 之前，在将升级部署到生产环境之前，请始终在暂存环境中测试依赖于 etcd 的服务。

Before beginning, [download the snapshot backup](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup). Should something go wrong with the upgrade, it is possible to use this backup to [downgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_4/#downgrade) back to existing etcd version. Please note that the `snapshot` command only backs up the v3 data. For v2 data, see [backing up v2 datastore](https://etcd.io/docs/v2.3/admin_guide/#backing-up-the-datastore).
在开始之前，请下载快照备份。如果升级出现问题，可以使用此备份降级回现有的 etcd 版本。请注意，该 `snapshot` 命令仅备份 v3 数据。有关 v2 数据，请参阅备份 v2 数据存储。

#### Mixed versions 混合版本

While upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to  version 3.4. Internally, etcd members negotiate with each other to  determine the overall cluster version, which controls the reported  version and the supported features.
在升级时，etcd 集群支持 etcd 成员的混合版本，并使用最低通用版本的协议运行。只有当集群的所有成员都升级到版本 3.4 时，集群才会被视为已升级。在内部，etcd 成员相互协商以确定整体集群版本，该版本控制报告的版本和支持的功能。

#### Limitations 局限性

Note: If the cluster only has v3 data and no v2 data, it is not subject to this limitation.
注意：如果集群只有 v3 数据而没有 v2 数据，则不受此限制。

If the cluster is serving a v2 data set larger than 50MB, each newly  upgraded member may take up to two minutes to catch up with the existing cluster. Check the size of a recent snapshot to estimate the total data size. In other words, it is safest to wait for 2 minutes between  upgrading each member.
如果群集提供的 v2 数据集大于 50MB，则每个新升级的成员最多可能需要两分钟才能赶上现有群集。检查最近快照的大小以估计总数据大小。换句话说，在升级每个成员之间等待 2 分钟是最安全的。

For a much larger total data size, 100MB or more , this one-time process  might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact the [etcd team](https://groups.google.com/g/etcd-dev) before upgrading, and we’ll be happy to provide advice on the procedure.
对于更大的总数据大小，100MB或更多，此一次性过程可能需要更多时间。这种规模的超大型 etcd 集群的管理员可以在升级之前随时联系 etcd 团队，我们很乐意提供有关该过程的建议。

#### Downgrade 降级

If all members have been upgraded to v3.4, the cluster will be upgraded to v3.4, and downgrade from this completed state is **not possible**. If any single member is still v3.3, however, the cluster and its  operations remains “v3.3”, and it is possible from this mixed cluster  state to return to using a v3.3 etcd binary on all members.
如果所有成员都已升级到 v3.4，则集群将升级到 v3.4，并且无法从此完成状态降级。但是，如果任何一个成员仍是 v3.3，则集群及其操作仍为“v3.3”，并且可以从这种混合集群状态返回到在所有成员上使用 v3.3 etcd 二进制文件。

Please [download the snapshot backup](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup) to make downgrading the cluster possible even after it has been completely upgraded.
请下载快照备份，以便在集群完全升级后仍可以降级集群。

### Upgrade procedure 升级过程

This example shows how to upgrade a 3-member v3.3 etcd cluster running on a local machine.
此示例说明如何升级在本地计算机上运行的 3 成员 v3.3 etcd 集群。

#### Step 1: check upgrade requirements 第 1 步：检查升级要求

Is the cluster healthy and running v3.3.x?
群集是否正常运行并运行 v3.3.x？

```bash
etcdctl --endpoints=localhost:2379,localhost:22379,localhost:32379 endpoint health
<<COMMENT
localhost:2379 is healthy: successfully committed proposal: took = 2.118638ms
localhost:22379 is healthy: successfully committed proposal: took = 3.631388ms
localhost:32379 is healthy: successfully committed proposal: took = 2.157051ms
COMMENT

curl http://localhost:2379/version
<<COMMENT
{"etcdserver":"3.3.5","etcdcluster":"3.3.0"}
COMMENT

curl http://localhost:22379/version
<<COMMENT
{"etcdserver":"3.3.5","etcdcluster":"3.3.0"}
COMMENT

curl http://localhost:32379/version
<<COMMENT
{"etcdserver":"3.3.5","etcdcluster":"3.3.0"}
COMMENT
```

#### Step 2: download snapshot backup from leader 第 2 步：从 leader 下载快照备份

[Download the snapshot backup](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup) to provide a downgrade path should any problems occur.
下载快照备份，以便在出现任何问题时提供降级路径。

etcd leader is guaranteed to have the latest application data, thus fetch snapshot from leader:
etcd leader 保证拥有最新的应用程序数据，因此从 leader 获取快照：

```bash
curl -sL http://localhost:2379/metrics | grep etcd_server_is_leader
<<COMMENT
# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.
# TYPE etcd_server_is_leader gauge
etcd_server_is_leader 1
COMMENT

curl -sL http://localhost:22379/metrics | grep etcd_server_is_leader
<<COMMENT
etcd_server_is_leader 0
COMMENT

curl -sL http://localhost:32379/metrics | grep etcd_server_is_leader
<<COMMENT
etcd_server_is_leader 0
COMMENT

etcdctl --endpoints=localhost:2379 snapshot save backup.db
<<COMMENT
{"level":"info","ts":1526585787.148433,"caller":"snapshot/v3_snapshot.go:109","msg":"created temporary db file","path":"backup.db.part"}
{"level":"info","ts":1526585787.1485257,"caller":"snapshot/v3_snapshot.go:120","msg":"fetching snapshot","endpoint":"localhost:2379"}
{"level":"info","ts":1526585787.1519694,"caller":"snapshot/v3_snapshot.go:133","msg":"fetched snapshot","endpoint":"localhost:2379","took":0.003502721}
{"level":"info","ts":1526585787.1520295,"caller":"snapshot/v3_snapshot.go:142","msg":"saved","path":"backup.db"}
Snapshot saved at backup.db
COMMENT
```

#### Step 3: stop one existing etcd server 第 3 步：停止一个现有的 etcd 服务器

When each etcd process is stopped, expected errors will be logged by other  cluster members. This is normal since a cluster member connection has  been (temporarily) broken:
当每个 etcd 进程停止时，其他集群成员将记录预期的错误。这是正常现象，因为集群成员连接已（暂时）中断：

```bash
10.237579 I | etcdserver: updating the cluster version from 3.0 to 3.3
10.238315 N | etcdserver/membership: updated the cluster version from 3.0 to 3.3
10.238451 I | etcdserver/api: enabled capabilities for version 3.3


^C21.192174 N | pkg/osutil: received interrupt signal, shutting down...
21.192459 I | etcdserver: 7339c4e5e833c029 starts leadership transfer from 7339c4e5e833c029 to 729934363faa4a24
21.192569 I | raft: 7339c4e5e833c029 [term 8] starts to transfer leadership to 729934363faa4a24
21.192619 I | raft: 7339c4e5e833c029 sends MsgTimeoutNow to 729934363faa4a24 immediately as 729934363faa4a24 already has up-to-date log
WARNING: 2018/05/17 12:45:21 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: Error while dialing dial tcp: operation was canceled"; Reconnecting to {localhost:2379 0  <nil>}
WARNING: 2018/05/17 12:45:21 grpc: addrConn.transportMonitor exits due to: grpc: the connection is closing
21.193589 I | raft: 7339c4e5e833c029 [term: 8] received a MsgVote message with higher term from 729934363faa4a24 [term: 9]
21.193626 I | raft: 7339c4e5e833c029 became follower at term 9
21.193651 I | raft: 7339c4e5e833c029 [logterm: 8, index: 9, vote: 0] cast MsgVote for 729934363faa4a24 [logterm: 8, index: 9] at term 9
21.193675 I | raft: raft.node: 7339c4e5e833c029 lost leader 7339c4e5e833c029 at term 9
21.194424 I | raft: raft.node: 7339c4e5e833c029 elected leader 729934363faa4a24 at term 9
21.292898 I | etcdserver: 7339c4e5e833c029 finished leadership transfer from 7339c4e5e833c029 to 729934363faa4a24 (took 100.436391ms)
21.292975 I | rafthttp: stopping peer 729934363faa4a24...
21.293206 I | rafthttp: closed the TCP streaming connection with peer 729934363faa4a24 (stream MsgApp v2 writer)
21.293225 I | rafthttp: stopped streaming with peer 729934363faa4a24 (writer)
21.293437 I | rafthttp: closed the TCP streaming connection with peer 729934363faa4a24 (stream Message writer)
21.293459 I | rafthttp: stopped streaming with peer 729934363faa4a24 (writer)
21.293514 I | rafthttp: stopped HTTP pipelining with peer 729934363faa4a24
21.293590 W | rafthttp: lost the TCP streaming connection with peer 729934363faa4a24 (stream MsgApp v2 reader)
21.293610 I | rafthttp: stopped streaming with peer 729934363faa4a24 (stream MsgApp v2 reader)
21.293680 W | rafthttp: lost the TCP streaming connection with peer 729934363faa4a24 (stream Message reader)
21.293700 I | rafthttp: stopped streaming with peer 729934363faa4a24 (stream Message reader)
21.293711 I | rafthttp: stopped peer 729934363faa4a24
21.293720 I | rafthttp: stopping peer b548c2511513015...
21.293987 I | rafthttp: closed the TCP streaming connection with peer b548c2511513015 (stream MsgApp v2 writer)
21.294063 I | rafthttp: stopped streaming with peer b548c2511513015 (writer)
21.294467 I | rafthttp: closed the TCP streaming connection with peer b548c2511513015 (stream Message writer)
21.294561 I | rafthttp: stopped streaming with peer b548c2511513015 (writer)
21.294742 I | rafthttp: stopped HTTP pipelining with peer b548c2511513015
21.294867 W | rafthttp: lost the TCP streaming connection with peer b548c2511513015 (stream MsgApp v2 reader)
21.294892 I | rafthttp: stopped streaming with peer b548c2511513015 (stream MsgApp v2 reader)
21.294990 W | rafthttp: lost the TCP streaming connection with peer b548c2511513015 (stream Message reader)
21.295004 E | rafthttp: failed to read b548c2511513015 on stream Message (context canceled)
21.295013 I | rafthttp: peer b548c2511513015 became inactive
21.295024 I | rafthttp: stopped streaming with peer b548c2511513015 (stream Message reader)
21.295035 I | rafthttp: stopped peer b548c2511513015
```

#### Step 4: restart the etcd server with same configuration 第 4 步：使用相同的配置重新启动 etcd 服务器

Restart the etcd server with same configuration but with the new etcd binary.
使用相同的配置重新启动 etcd 服务器，但使用新的 etcd 二进制文件。

```diff
-etcd-old --name s1 \
+etcd-new --name s1 \
  --data-dir /tmp/etcd/s1 \
  --listen-client-urls http://localhost:2379 \
  --advertise-client-urls http://localhost:2379 \
  --listen-peer-urls http://localhost:2380 \
  --initial-advertise-peer-urls http://localhost:2380 \
  --initial-cluster s1=http://localhost:2380,s2=http://localhost:22380,s3=http://localhost:32380 \
  --initial-cluster-token tkn \
+ --initial-cluster-state new \
+ --logger zap \
+ --log-outputs stderr
```

The new v3.4 etcd will publish its information to the cluster. At this  point, cluster still operates as v3.3 protocol, which is the lowest  common version.
新的 v3.4 etcd 会将其信息发布到集群。此时，集群仍以 v3.3 协议运行，这是最低的通用版本。

> ```
> {"level":"info","ts":1526586617.1647713,"caller":"membership/cluster.go:485","msg":"set initial cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"7339c4e5e833c029","cluster-version":"3.0"}
> ```

> ```
> {"level":"info","ts":1526586617.1648536,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.0"}
> ```

> ```
> {"level":"info","ts":1526586617.1649303,"caller":"membership/cluster.go:473","msg":"updated cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"7339c4e5e833c029","from":"3.0","from":"3.3"}
> ```

> ```
> {"level":"info","ts":1526586617.1649797,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.3"}
> ```

> ```
> {"level":"info","ts":1526586617.2107732,"caller":"etcdserver/server.go:1770","msg":"published local member to cluster through  raft","local-member-id":"7339c4e5e833c029","local-member-attributes":"{Name:s1  ClientURLs:[http://localhost:2379]}","request-path":"/0/members/7339c4e5e833c029/attributes","cluster-id":"7dee9ba76d59ed53","publish-timeout":7}
> ```

Verify that each member, and then the entire cluster, becomes healthy with the new v3.4 etcd binary:
使用新的 v3.4 etcd 二进制文件验证每个成员以及整个集群是否正常运行：

```bash
etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
<<COMMENT
localhost:32379 is healthy: successfully committed proposal: took = 2.337471ms
localhost:22379 is healthy: successfully committed proposal: took = 1.130717ms
localhost:2379 is healthy: successfully committed proposal: took = 2.124843ms
COMMENT
```

Un-upgraded members will log warnings like the following until the entire cluster is upgraded.
未升级的成员将记录如下所示的警告，直到整个集群升级为止。

This is expected and will cease after all etcd cluster members are upgraded to v3.4:
这是意料之中的，并且在所有 etcd 集群成员升级到 v3.4 后将停止：

```
:41.942121 W | etcdserver: member 7339c4e5e833c029 has a higher version 3.4.0
:45.945154 W | etcdserver: the local etcd version 3.3.5 is not up-to-date
```

#### Step 5: repeat *step 3* and *step 4* for rest of the members 第 5 步：对其余成员重复第 3 步和第 4 步

When all members are upgraded, the cluster will report upgrading to 3.4 successfully:
升级所有成员后，集群将报告已成功升级到 3.4：

Member 1: 会员 1：

> ```
> {"level":"info","ts":1526586949.0920913,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.4"}` `{"level":"info","ts":1526586949.0921566,"caller":"etcdserver/server.go:2272","msg":"cluster version is updated","cluster-version":"3.4"}
> ```

Member 2: 会员2：

> ```
> {"level":"info","ts":1526586949.092117,"caller":"membership/cluster.go:473","msg":"updated cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"729934363faa4a24","from":"3.3","from":"3.4"}` `{"level":"info","ts":1526586949.0923078,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.4"}
> ```

Member 3: 会员3：

> ```
> {"level":"info","ts":1526586949.0921423,"caller":"membership/cluster.go:473","msg":"updated cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"b548c2511513015","from":"3.3","from":"3.4"}` `{"level":"info","ts":1526586949.0922918,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.4"}
> ```

```bash
endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
<<COMMENT
localhost:2379 is healthy: successfully committed proposal: took = 492.834µs
localhost:22379 is healthy: successfully committed proposal: took = 1.015025ms
localhost:32379 is healthy: successfully committed proposal: took = 1.853077ms
COMMENT

curl http://localhost:2379/version
<<COMMENT
{"etcdserver":"3.4.0","etcdcluster":"3.4.0"}
COMMENT

curl http://localhost:22379/version
<<COMMENT
{"etcdserver":"3.4.0","etcdcluster":"3.4.0"}
COMMENT

curl http://localhost:32379/version
<<COMMENT
{"etcdserver":"3.4.0","etcdcluster":"3.4.0"}
COMMENT
```

# Upgrade etcd from 3.4 to 3.5 将 etcd 从 3.4 升级到 3.5

Processes, checklists, and notes on upgrading etcd from 3.4 to 3.5
将 etcd 从 3.4 升级到 3.5 的流程、清单和注意事项



In the general case, upgrading from etcd 3.4 to 3.5 can be a zero-downtime, rolling upgrade:
在一般情况下，从 etcd 3.4 升级到 3.5 可以是零停机时间的滚动升级：

- one by one, stop the etcd v3.4 processes and replace them with etcd v3.5 processes
  逐一停止 etcd v3.4 进程，替换为 etcd v3.5 进程
- after running all v3.5 processes, new features in v3.5 are available to the cluster
  运行所有 v3.5 进程后，v3.5 中的新功能可供集群使用

Before [starting an upgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_5/#upgrade-procedure), read through the rest of this guide to prepare.
在开始升级之前，请通读本指南的其余部分以做好准备。

### Upgrade checklists 升级清单

**NOTE:** When [migrating from v2 with no v3 data](https://github.com/etcd-io/etcd/issues/9480), etcd server v3.2+ panics when etcd restores from existing snapshots but no v3 `ETCD_DATA_DIR/member/snap/db` file. This happens when the server had migrated from v2 with no  previous v3 data. This also prevents accidental v3 data loss (e.g. `db` file might have been moved). etcd requires that post v3 migration can  only happen with v3 data. Do not upgrade to newer v3 versions until v3.0 server contains v3 data.
注意：从没有 v3 数据的 v2 迁移时，当 etcd 从现有快照恢复但没有 v3 `ETCD_DATA_DIR/member/snap/db` 文件时，etcd 服务器 v3.2+ 会崩溃。当服务器从没有以前的 v3 数据的 v2 迁移时，会发生这种情况。这还可以防止意外的 v3 数据丢失（例如， `db` 文件可能已被移动）。etcd 要求 v3 后迁移只能对 v3 数据进行。在 v3.0 服务器包含 v3 数据之前，不要升级到较新的 v3 版本。

**NOTE:** If your cluster enables auth, rolling upgrade from 3.4 or older version isn’t supported because 3.5 [changes a format of WAL entries related to auth](https://github.com/etcd-io/etcd/pull/11943).
注意：如果您的集群启用了身份验证，则不支持从 3.4 或更低版本滚动升级，因为 3.5 会更改与身份验证相关的 WAL 条目的格式。

Highlighted breaking changes in 3.5.
突出显示了 3.5 中的重大更改。

#### Deprecated `etcd_debugging_mvcc_db_total_size_in_bytes` Prometheus metrics 已弃用的 `etcd_debugging_mvcc_db_total_size_in_bytes` Prometheus 指标

v3.5 promoted `etcd_debugging_mvcc_db_total_size_in_bytes` Prometheus metrics to `etcd_mvcc_db_total_size_in_bytes`, in order to encourage etcd storage monitoring. And v3.5 completely deprecates `etcd_debugging_mvcc_db_total_size_in_bytes`.
v3.5 将 Prometheus 指标提升 `etcd_debugging_mvcc_db_total_size_in_bytes` 为 `etcd_mvcc_db_total_size_in_bytes` ，以鼓励 etcd 存储监控。而 v3.5 完全 `etcd_debugging_mvcc_db_total_size_in_bytes` 弃用了 .

```diff
-etcd_debugging_mvcc_db_total_size_in_bytes
+etcd_mvcc_db_total_size_in_bytes
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecated `etcd_debugging_mvcc_put_total` Prometheus metrics 已弃用的 `etcd_debugging_mvcc_put_total` Prometheus 指标

v3.5 promoted `etcd_debugging_mvcc_put_total` Prometheus metrics to `etcd_mvcc_put_total`, in order to encourage etcd storage monitoring. And v3.5 completely deprecates `etcd_debugging_mvcc_put_total`.
v3.5 将 Prometheus 指标提升 `etcd_debugging_mvcc_put_total` 为 `etcd_mvcc_put_total` ，以鼓励 etcd 存储监控。而 v3.5 完全 `etcd_debugging_mvcc_put_total` 弃用了 .

```diff
-etcd_debugging_mvcc_put_total
+etcd_mvcc_put_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecated `etcd_debugging_mvcc_delete_total` Prometheus metrics 已弃用的 `etcd_debugging_mvcc_delete_total` Prometheus 指标

v3.5 promoted `etcd_debugging_mvcc_delete_total` Prometheus metrics to `etcd_mvcc_delete_total`, in order to encourage etcd storage monitoring. And v3.5 completely deprecates `etcd_debugging_mvcc_delete_total`.
v3.5 将 Prometheus 指标提升 `etcd_debugging_mvcc_delete_total` 为 `etcd_mvcc_delete_total` ，以鼓励 etcd 存储监控。而 v3.5 完全 `etcd_debugging_mvcc_delete_total` 弃用了 .

```diff
-etcd_debugging_mvcc_delete_total
+etcd_mvcc_delete_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecated `etcd_debugging_mvcc_txn_total` Prometheus metrics 已弃用的 `etcd_debugging_mvcc_txn_total` Prometheus 指标

v3.5 promoted `etcd_debugging_mvcc_txn_total` Prometheus metrics to `etcd_mvcc_txn_total`, in order to encourage etcd storage monitoring. And v3.5 completely deprecates `etcd_debugging_mvcc_txn_total`.
v3.5 将 Prometheus 指标提升 `etcd_debugging_mvcc_txn_total` 为 `etcd_mvcc_txn_total` ，以鼓励 etcd 存储监控。而 v3.5 完全 `etcd_debugging_mvcc_txn_total` 弃用了 .

```diff
-etcd_debugging_mvcc_txn_total
+etcd_mvcc_txn_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecated `etcd_debugging_mvcc_range_total` Prometheus metrics 已弃用的 `etcd_debugging_mvcc_range_total` Prometheus 指标

v3.5 promoted `etcd_debugging_mvcc_range_total` Prometheus metrics to `etcd_mvcc_range_total`, in order to encourage etcd storage monitoring. And v3.5 completely deprecates `etcd_debugging_mvcc_range_total`.
v3.5 将 Prometheus 指标提升 `etcd_debugging_mvcc_range_total` 为 `etcd_mvcc_range_total` ，以鼓励 etcd 存储监控。而 v3.5 完全 `etcd_debugging_mvcc_range_total` 弃用了 .

```diff
-etcd_debugging_mvcc_range_total
+etcd_mvcc_range_total
```

Note that `etcd_debugging_*` namespace metrics have been marked as experimental. As we improve monitoring guide, we may promote more metrics.
请注意， `etcd_debugging_*` 命名空间指标已被标记为实验性指标。随着我们改进监控指南，我们可能会推广更多指标。

#### Deprecated `etcd --logger capnslog` 荒废的 `etcd --logger capnslog` 

v3.4 defaults to `--logger=zap` in order to support multiple log outputs and structured logging.
v3.4 默认为 `--logger=zap` 为了支持多个日志输出和结构化日志记录。

**`etcd --logger=capnslog` has been deprecated in v3.5**, and now `--logger=zap` is the default.
 `etcd --logger=capnslog` 已在 v3.5 中弃用，现在是 `--logger=zap` 默认值。

```diff
-etcd --logger=capnslog
+etcd --logger=zap --log-outputs=stderr

+# to write logs to stderr and a.log file at the same time
+etcd --logger=zap --log-outputs=stderr,a.log
```

v3.4 adds `etcd --logger=zap` support for structured logging and multiple log outputs. Main  motivation is to promote automated etcd monitoring, rather than looking  back server logs when it starts breaking. Future development will make  etcd log as few as possible, and make etcd easier to monitor with  metrics and alerts. **`etcd --logger=capnslog` will be deprecated in v3.5.**
v3.4 增加了 `etcd --logger=zap` 对结构化日志记录和多个日志输出的支持。主要动机是促进自动化的 etcd 监控，而不是在服务器日志开始崩溃时回顾它。未来的开发将使 etcd 日志尽可能少，并使 etcd 更容易通过指标和警报进行监控。 `etcd --logger=capnslog` 将在 v3.5 中弃用。

#### Deprecated `etcd --log-output` 荒废的 `etcd --log-output` 

v3.4 renamed [`etcd --log-output` to `--log-outputs`](https://github.com/etcd-io/etcd/pull/9624) to support multiple log outputs.
v3.4 重命名为 `etcd --log-output`  `--log-outputs` 支持多个日志输出。

**`etcd --log-output` has been deprecated in v3.5.
 `etcd --log-output` 已在 v3.5 中弃用。**

```diff
-etcd --log-output=stderr
+etcd --log-outputs=stderr
```

#### Deprecated `etcd --debug` flag (now `--log-level=debug`) 已弃用 `etcd --debug` 的标志（现在 `--log-level=debug` ）

**`etcd --debug` flag has been deprecated.
 `etcd --debug` 标志已被弃用。**

```diff
-etcd --debug
+etcd --log-level debug
```

#### Deprecated `etcd --log-package-levels` 荒废的 `etcd --log-package-levels` 

**`etcd --log-package-levels` flag for `capnslog` has been deprecated.
 `etcd --log-package-levels` 标志 for `capnslog` 已被弃用。**

Now, **`etcd --logger=zap`** is the default.
现在， `etcd --logger=zap` 是默认值。

```diff
-etcd --log-package-levels 'etcdmain=CRITICAL,etcdserver=DEBUG'
+etcd --logger=zap --log-outputs=stderr
```

#### Deprecated `[CLIENT-URL]/config/local/log` 荒废的 `[CLIENT-URL]/config/local/log` 

**`/config/local/log` endpoint is being deprecated in v3.5, as is `etcd --log-package-levels` flag.
 `/config/local/log` endpoint 在 v3.5 中被弃用，标志也是如此 `etcd --log-package-levels` 。**

```diff
-$ curl http://127.0.0.1:2379/config/local/log -XPUT -d '{"Level":"DEBUG"}'
-# debug logging enabled
```

#### Changed gRPC gateway HTTP endpoints (deprecated `/v3beta`) 更改了 gRPC 网关 HTTP 终结点（已弃用 `/v3beta` ）

Before 以前

```bash
curl -L http://localhost:2379/v3beta/kv/put \
  -X POST -d '{"key": "Zm9v", "value": "YmFy"}'
```

After 后

```bash
curl -L http://localhost:2379/v3/kv/put \
  -X POST -d '{"key": "Zm9v", "value": "YmFy"}'
```

`/v3beta` has been removed in 3.5 release.
 `/v3beta` 已在 3.5 版本中删除。

### Server upgrade checklists 服务器升级清单

#### Upgrade requirements 升级要求

To upgrade an existing etcd deployment to 3.5, the running cluster must be 3.4 or greater. If it’s before 3.4, please [upgrade to 3.4](https://etcd.io/docs/v3.5/upgrades/upgrade_3_3/) before upgrading to 3.5.
要将现有 etcd 部署升级到 3.5，正在运行的集群必须为 3.4 或更高版本。如果是 3.4 之前版本，请先升级到 3.4，然后再升级到 3.5。

Also, to ensure a smooth rolling upgrade, the running cluster must be healthy. Check the health of the cluster by using the `etcdctl endpoint health` command before proceeding.
此外，为确保顺利滚动升级，正在运行的群集必须运行正常。在继续操作之前，请使用命令 `etcdctl endpoint health` 检查群集的运行状况。

#### Preparation 制备

Before upgrading etcd, always test the services relying on etcd in a staging  environment before deploying the upgrade to the production environment.
在升级 etcd 之前，在将升级部署到生产环境之前，请始终在暂存环境中测试依赖于 etcd 的服务。

Before beginning, [download the snapshot backup](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup). Should something go wrong with the upgrade, it is possible to use this backup to [downgrade](https://etcd.io/docs/v3.5/upgrades/upgrade_3_5/#downgrade) back to existing etcd version. Please note that the `snapshot` command only backs up the v3 data. For v2 data, see [backing up v2 datastore](https://etcd.io/docs/v2.3/admin_guide#backing-up-the-datastore).
在开始之前，请下载快照备份。如果升级出现问题，可以使用此备份降级回现有的 etcd 版本。请注意，该 `snapshot` 命令仅备份 v3 数据。有关 v2 数据，请参阅备份 v2 数据存储。

#### Mixed versions 混合版本

While upgrading, an etcd cluster supports mixed versions of etcd members, and operates with the protocol of the lowest common version. The cluster is only considered upgraded once all of its members are upgraded to  version 3.5. Internally, etcd members negotiate with each other to  determine the overall cluster version, which controls the reported  version and the supported features.
在升级时，etcd 集群支持 etcd 成员的混合版本，并使用最低通用版本的协议运行。只有当集群的所有成员都升级到版本 3.5 时，集群才会被视为已升级。在内部，etcd 成员相互协商以确定整体集群版本，该版本控制报告的版本和支持的功能。

#### Limitations 局限性

Note: If the cluster only has v3 data and no v2 data, it is not subject to this limitation.
注意：如果集群只有 v3 数据而没有 v2 数据，则不受此限制。

If the cluster is serving a v2 data set larger than 50MB, each newly  upgraded member may take up to two minutes to catch up with the existing cluster. Check the size of a recent snapshot to estimate the total data size. In other words, it is safest to wait for 2 minutes between  upgrading each member.
如果群集提供的 v2 数据集大于 50MB，则每个新升级的成员最多可能需要两分钟才能赶上现有群集。检查最近快照的大小以估计总数据大小。换句话说，在升级每个成员之间等待 2 分钟是最安全的。

For a much larger total data size, 100MB or more , this one-time process  might take even more time. Administrators of very large etcd clusters of this magnitude can feel free to contact the [etcd team](https://groups.google.com/g/etcd-dev) before upgrading, and we’ll be happy to provide advice on the procedure.
对于更大的总数据大小，100MB或更多，此一次性过程可能需要更多时间。这种规模的超大型 etcd 集群的管理员可以在升级之前随时联系 etcd 团队，我们很乐意提供有关该过程的建议。

#### Downgrade 降级

If all members have been upgraded to v3.5, the cluster will be upgraded to v3.5, and downgrade from this completed state is **not possible**. If any single member is still v3.4, however, the cluster and its  operations remains “v3.4”, and it is possible from this mixed cluster  state to return to using a v3.4 etcd binary on all members.
如果所有成员都已升级到 v3.5，则集群将升级到 v3.5，并且无法从此完成状态降级。但是，如果任何一个成员仍然是 v3.4，则集群及其操作仍为“v3.4”，并且可以从这种混合集群状态返回到在所有成员上使用 v3.4 etcd 二进制文件。

Please [download the snapshot backup](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup) to make downgrading the cluster possible even after it has been completely upgraded.
请下载快照备份，以便在集群完全升级后仍可以降级集群。

### Upgrade procedure 升级过程

This example shows how to upgrade a 3-member v3.4 etcd cluster running on a local machine.
此示例说明如何升级在本地计算机上运行的 3 成员 v3.4 etcd 集群。

#### Step 1: check upgrade requirements 第 1 步：检查升级要求

Is the cluster healthy and running v3.4.x?
群集是否正常运行并运行 v3.4.x？

```bash
etcdctl --endpoints=localhost:2379,localhost:22379,localhost:32379 endpoint health
<<COMMENT
localhost:2379 is healthy: successfully committed proposal: took = 2.118638ms
localhost:22379 is healthy: successfully committed proposal: took = 3.631388ms
localhost:32379 is healthy: successfully committed proposal: took = 2.157051ms
COMMENT

curl http://localhost:2379/version
<<COMMENT
{"etcdserver":"3.4.0","etcdcluster":"3.4.0"}
COMMENT

curl http://localhost:22379/version
<<COMMENT
{"etcdserver":"3.4.0","etcdcluster":"3.4.0"}
COMMENT

curl http://localhost:32379/version
<<COMMENT
{"etcdserver":"3.4.0","etcdcluster":"3.4.0"}
COMMENT
```

#### Step 2: download snapshot backup from leader 第 2 步：从 leader 下载快照备份

[Download the snapshot backup](https://etcd.io/docs/v3.5/op-guide/maintenance/#snapshot-backup) to provide a downgrade path should any problems occur.
下载快照备份，以便在出现任何问题时提供降级路径。

etcd leader is guaranteed to have the latest application data, thus fetch snapshot from leader:
etcd leader 保证拥有最新的应用程序数据，因此从 leader 获取快照：

```bash
curl -sL http://localhost:2379/metrics | grep etcd_server_is_leader
<<COMMENT
# HELP etcd_server_is_leader Whether or not this member is a leader. 1 if is, 0 otherwise.
# TYPE etcd_server_is_leader gauge
etcd_server_is_leader 1
COMMENT

curl -sL http://localhost:22379/metrics | grep etcd_server_is_leader
<<COMMENT
etcd_server_is_leader 0
COMMENT

curl -sL http://localhost:32379/metrics | grep etcd_server_is_leader
<<COMMENT
etcd_server_is_leader 0
COMMENT

etcdctl --endpoints=localhost:2379 snapshot save backup.db
<<COMMENT
{"level":"info","ts":1526585787.148433,"caller":"snapshot/v3_snapshot.go:109","msg":"created temporary db file","path":"backup.db.part"}
{"level":"info","ts":1526585787.1485257,"caller":"snapshot/v3_snapshot.go:120","msg":"fetching snapshot","endpoint":"localhost:2379"}
{"level":"info","ts":1526585787.1519694,"caller":"snapshot/v3_snapshot.go:133","msg":"fetched snapshot","endpoint":"localhost:2379","took":0.003502721}
{"level":"info","ts":1526585787.1520295,"caller":"snapshot/v3_snapshot.go:142","msg":"saved","path":"backup.db"}
Snapshot saved at backup.db
COMMENT
```

#### Step 3: stop one existing etcd server 第 3 步：停止一个现有的 etcd 服务器

When each etcd process is stopped, expected errors will be logged by other  cluster members. This is normal since a cluster member connection has  been (temporarily) broken:
当每个 etcd 进程停止时，其他集群成员将记录预期的错误。这是正常现象，因为集群成员连接已（暂时）中断：

```bash
{"level":"info","ts":1526587281.2001143,"caller":"etcdserver/server.go:2249","msg":"updating cluster version","from":"3.0","to":"3.4"}
{"level":"info","ts":1526587281.2010646,"caller":"membership/cluster.go:473","msg":"updated cluster version","cluster-id":"7dee9ba76d59ed53","local-member-id":"7339c4e5e833c029","from":"3.0","from":"3.4"}
{"level":"info","ts":1526587281.2012327,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.4"}
{"level":"info","ts":1526587281.2013083,"caller":"etcdserver/server.go:2272","msg":"cluster version is updated","cluster-version":"3.4"}



^C{"level":"info","ts":1526587299.0717514,"caller":"osutil/interrupt_unix.go:63","msg":"received signal; shutting down","signal":"interrupt"}
{"level":"info","ts":1526587299.0718873,"caller":"embed/etcd.go:285","msg":"closing etcd server","name":"s1","data-dir":"/tmp/etcd/s1","advertise-peer-urls":["http://localhost:2380"],"advertise-client-urls":["http://localhost:2379"]}
{"level":"info","ts":1526587299.0722554,"caller":"etcdserver/server.go:1341","msg":"leadership transfer starting","local-member-id":"7339c4e5e833c029","current-leader-member-id":"7339c4e5e833c029","transferee-member-id":"729934363faa4a24"}
{"level":"info","ts":1526587299.0723994,"caller":"raft/raft.go:1107","msg":"7339c4e5e833c029 [term 3] starts to transfer leadership to 729934363faa4a24"}
{"level":"info","ts":1526587299.0724802,"caller":"raft/raft.go:1113","msg":"7339c4e5e833c029 sends MsgTimeoutNow to 729934363faa4a24 immediately as 729934363faa4a24 already has up-to-date log"}
{"level":"info","ts":1526587299.0737045,"caller":"raft/raft.go:797","msg":"7339c4e5e833c029 [term: 3] received a MsgVote message with higher term from 729934363faa4a24 [term: 4]"}
{"level":"info","ts":1526587299.0737681,"caller":"raft/raft.go:656","msg":"7339c4e5e833c029 became follower at term 4"}
{"level":"info","ts":1526587299.073831,"caller":"raft/raft.go:882","msg":"7339c4e5e833c029 [logterm: 3, index: 9, vote: 0] cast MsgVote for 729934363faa4a24 [logterm: 3, index: 9] at term 4"}
{"level":"info","ts":1526587299.0738947,"caller":"raft/node.go:312","msg":"raft.node: 7339c4e5e833c029 lost leader 7339c4e5e833c029 at term 4"}
{"level":"info","ts":1526587299.0748374,"caller":"raft/node.go:306","msg":"raft.node: 7339c4e5e833c029 elected leader 729934363faa4a24 at term 4"}
{"level":"info","ts":1526587299.1726425,"caller":"etcdserver/server.go:1362","msg":"leadership transfer finished","local-member-id":"7339c4e5e833c029","old-leader-member-id":"7339c4e5e833c029","new-leader-member-id":"729934363faa4a24","took":0.100389359}
{"level":"info","ts":1526587299.1728148,"caller":"rafthttp/peer.go:333","msg":"stopping remote peer","remote-peer-id":"b548c2511513015"}
{"level":"warn","ts":1526587299.1751974,"caller":"rafthttp/stream.go:291","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"b548c2511513015"}
{"level":"warn","ts":1526587299.1752589,"caller":"rafthttp/stream.go:301","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"b548c2511513015"}
{"level":"warn","ts":1526587299.177348,"caller":"rafthttp/stream.go:291","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"b548c2511513015"}
{"level":"warn","ts":1526587299.1774004,"caller":"rafthttp/stream.go:301","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"b548c2511513015"}
{"level":"info","ts":1526587299.177515,"caller":"rafthttp/pipeline.go:86","msg":"stopped HTTP pipelining with remote peer","local-member-id":"7339c4e5e833c029","remote-peer-id":"b548c2511513015"}
{"level":"warn","ts":1526587299.1777067,"caller":"rafthttp/stream.go:436","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"7339c4e5e833c029","remote-peer-id":"b548c2511513015","error":"read tcp 127.0.0.1:34636->127.0.0.1:32380: use of closed network connection"}
{"level":"info","ts":1526587299.1778402,"caller":"rafthttp/stream.go:459","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"7339c4e5e833c029","remote-peer-id":"b548c2511513015"}
{"level":"warn","ts":1526587299.1780295,"caller":"rafthttp/stream.go:436","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"7339c4e5e833c029","remote-peer-id":"b548c2511513015","error":"read tcp 127.0.0.1:34634->127.0.0.1:32380: use of closed network connection"}
{"level":"info","ts":1526587299.1780987,"caller":"rafthttp/stream.go:459","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"7339c4e5e833c029","remote-peer-id":"b548c2511513015"}
{"level":"info","ts":1526587299.1781602,"caller":"rafthttp/peer.go:340","msg":"stopped remote peer","remote-peer-id":"b548c2511513015"}
{"level":"info","ts":1526587299.1781986,"caller":"rafthttp/peer.go:333","msg":"stopping remote peer","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.1802843,"caller":"rafthttp/stream.go:291","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.1803446,"caller":"rafthttp/stream.go:301","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.1824749,"caller":"rafthttp/stream.go:291","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.18255,"caller":"rafthttp/stream.go:301","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"729934363faa4a24"}
{"level":"info","ts":1526587299.18261,"caller":"rafthttp/pipeline.go:86","msg":"stopped HTTP pipelining with remote peer","local-member-id":"7339c4e5e833c029","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.1827736,"caller":"rafthttp/stream.go:436","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"7339c4e5e833c029","remote-peer-id":"729934363faa4a24","error":"read tcp 127.0.0.1:51482->127.0.0.1:22380: use of closed network connection"}
{"level":"info","ts":1526587299.182845,"caller":"rafthttp/stream.go:459","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"7339c4e5e833c029","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.1830168,"caller":"rafthttp/stream.go:436","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"7339c4e5e833c029","remote-peer-id":"729934363faa4a24","error":"context canceled"}
{"level":"warn","ts":1526587299.1831107,"caller":"rafthttp/peer_status.go:65","msg":"peer became inactive","peer-id":"729934363faa4a24","error":"failed to read 729934363faa4a24 on stream Message (context canceled)"}
{"level":"info","ts":1526587299.1831737,"caller":"rafthttp/stream.go:459","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"7339c4e5e833c029","remote-peer-id":"729934363faa4a24"}
{"level":"info","ts":1526587299.1832306,"caller":"rafthttp/peer.go:340","msg":"stopped remote peer","remote-peer-id":"729934363faa4a24"}
{"level":"warn","ts":1526587299.1837125,"caller":"rafthttp/http.go:424","msg":"failed to find remote peer in cluster","local-member-id":"7339c4e5e833c029","remote-peer-id-stream-handler":"7339c4e5e833c029","remote-peer-id-from":"b548c2511513015","cluster-id":"7dee9ba76d59ed53"}
{"level":"warn","ts":1526587299.1840093,"caller":"rafthttp/http.go:424","msg":"failed to find remote peer in cluster","local-member-id":"7339c4e5e833c029","remote-peer-id-stream-handler":"7339c4e5e833c029","remote-peer-id-from":"b548c2511513015","cluster-id":"7dee9ba76d59ed53"}
{"level":"warn","ts":1526587299.1842315,"caller":"rafthttp/http.go:424","msg":"failed to find remote peer in cluster","local-member-id":"7339c4e5e833c029","remote-peer-id-stream-handler":"7339c4e5e833c029","remote-peer-id-from":"729934363faa4a24","cluster-id":"7dee9ba76d59ed53"}
{"level":"warn","ts":1526587299.1844475,"caller":"rafthttp/http.go:424","msg":"failed to find remote peer in cluster","local-member-id":"7339c4e5e833c029","remote-peer-id-stream-handler":"7339c4e5e833c029","remote-peer-id-from":"729934363faa4a24","cluster-id":"7dee9ba76d59ed53"}
{"level":"info","ts":1526587299.2056687,"caller":"embed/etcd.go:473","msg":"stopping serving peer traffic","address":"127.0.0.1:2380"}
{"level":"info","ts":1526587299.205819,"caller":"embed/etcd.go:480","msg":"stopped serving peer traffic","address":"127.0.0.1:2380"}
{"level":"info","ts":1526587299.2058413,"caller":"embed/etcd.go:289","msg":"closed etcd server","name":"s1","data-dir":"/tmp/etcd/s1","advertise-peer-urls":["http://localhost:2380"],"advertise-client-urls":["http://localhost:2379"]}
```

#### Step 4: restart the etcd server with same configuration 第 4 步：使用相同的配置重新启动 etcd 服务器

Restart the etcd server with same configuration but with the new etcd binary.
使用相同的配置重新启动 etcd 服务器，但使用新的 etcd 二进制文件。

```diff
-etcd-old --name s1 \
+etcd-new --name s1 \
  --data-dir /tmp/etcd/s1 \
  --listen-client-urls http://localhost:2379 \
  --advertise-client-urls http://localhost:2379 \
  --listen-peer-urls http://localhost:2380 \
  --initial-advertise-peer-urls http://localhost:2380 \
  --initial-cluster s1=http://localhost:2380,s2=http://localhost:22380,s3=http://localhost:32380 \
  --initial-cluster-token tkn \
  --initial-cluster-state new
```

The new v3.5 etcd will publish its information to the cluster. At this  point, cluster still operates as v3.4 protocol, which is the lowest  common version.
新的 v3.5 etcd 会将其信息发布到集群。此时，集群仍以 v3.4 协议运行，这是最低的通用版本。

> ```
> {"level":"info","ts":1526586617.1647713,"caller":"membership/cluster.go:485","msg":"set initial cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"7339c4e5e833c029","cluster-version":"3.0"}
> ```

> ```
> {"level":"info","ts":1526586617.1648536,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.0"}
> ```

> ```
> {"level":"info","ts":1526586617.1649303,"caller":"membership/cluster.go:473","msg":"updated cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"7339c4e5e833c029","from":"3.0","from":"3.4"}
> ```

> ```
> {"level":"info","ts":1526586617.1649797,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.4"}
> ```

> ```
> {"level":"info","ts":1526586617.2107732,"caller":"etcdserver/server.go:1770","msg":"published local member to cluster through  raft","local-member-id":"7339c4e5e833c029","local-member-attributes":"{Name:s1  ClientURLs:[http://localhost:2379]}","request-path":"/0/members/7339c4e5e833c029/attributes","cluster-id":"7dee9ba76d59ed53","publish-timeout":7}
> ```

Verify that each member, and then the entire cluster, becomes healthy with the new v3.5 etcd binary:
使用新的 v3.5 etcd 二进制文件验证每个成员以及整个集群是否正常运行：

```bash
etcdctl endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
<<COMMENT
localhost:32379 is healthy: successfully committed proposal: took = 2.337471ms
localhost:22379 is healthy: successfully committed proposal: took = 1.130717ms
localhost:2379 is healthy: successfully committed proposal: took = 2.124843ms
COMMENT
```

Un-upgraded members will log warnings like the following until the entire cluster is upgraded.
未升级的成员将记录如下所示的警告，直到整个集群升级为止。

This is expected and will cease after all etcd cluster members are upgraded to v3.5:
这是意料之中的，并将在所有 etcd 集群成员升级到 v3.5 后停止：

```
:41.942121 W | etcdserver: member 7339c4e5e833c029 has a higher version 3.5.0
:45.945154 W | etcdserver: the local etcd version 3.4.0 is not up-to-date
```

#### Step 5: repeat *step 3* and *step 4* for rest of the members 第 5 步：对其余成员重复第 3 步和第 4 步

When all members are upgraded, the cluster will report upgrading to 3.5 successfully:
升级所有成员后，群集将报告已成功升级到 3.5：

Member 1: 会员 1：

> ```
> {"level":"info","ts":1526586949.0920913,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.5"}` `{"level":"info","ts":1526586949.0921566,"caller":"etcdserver/server.go:2272","msg":"cluster version is updated","cluster-version":"3.5"}
> ```

Member 2: 会员2：

> ```
> {"level":"info","ts":1526586949.092117,"caller":"membership/cluster.go:473","msg":"updated cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"729934363faa4a24","from":"3.4","from":"3.5"}` `{"level":"info","ts":1526586949.0923078,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.5"}
> ```

Member 3: 会员3：

> ```
> {"level":"info","ts":1526586949.0921423,"caller":"membership/cluster.go:473","msg":"updated cluster  version","cluster-id":"7dee9ba76d59ed53","local-member-id":"b548c2511513015","from":"3.4","from":"3.5"}` `{"level":"info","ts":1526586949.0922918,"caller":"api/capability.go:76","msg":"enabled capabilities for version","cluster-version":"3.5"}
> ```

```bash
endpoint health --endpoints=localhost:2379,localhost:22379,localhost:32379
<<COMMENT
localhost:2379 is healthy: successfully committed proposal: took = 492.834µs
localhost:22379 is healthy: successfully committed proposal: took = 1.015025ms
localhost:32379 is healthy: successfully committed proposal: took = 1.853077ms
COMMENT

curl http://localhost:2379/version
<<COMMENT
{"etcdserver":"3.5.0","etcdcluster":"3.5.0"}
COMMENT

curl http://localhost:22379/version
<<COMMENT
{"etcdserver":"3.5.0","etcdcluster":"3.5.0"}
COMMENT

curl http://localhost:32379/version
<<COMMENT
{"etcdserver":"3.5.0","etcdcluster":"3.5.0"}
COMMENT
```