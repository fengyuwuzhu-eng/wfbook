# 安装

[TOC]

## 概述

官方提供了一个部署工具 http://play.etcd.io/install			 	

## 要求

### 支持的平台

#### Support tiers 支持层

etcd 在不同的平台上运行，但它提供的保证取决于平台的支持层：

- **Tier 1**

  由 etcd 维护者全面支持；保证通过所有测试，包括功能和鲁棒性测试。

- **Tier 2**

  保证通过集成和端到端测试，但不一定通过功能或鲁棒性测试。

- **Tier 3**

  可以保证构建，可以进行少量测试（或不测试），因此应该认为它不稳定。

#### 当前支持

下表列出了当前支持的平台及其对应的 etcd 支持层：

| Architecture | 操作系统 | Support tier |   Maintainers    |
| :----------: | :------: | :----------: | :--------------: |
|    AMD64     |  Linux   |      1       | etcd maintainers |
|    ARM64     |  Linux   |      1       | etcd maintainers |
|    AMD64     |  Darwin  |      3       |                  |
|    AMD64     | Windows  |      3       |                  |
|     ARM      |  Linux   |      3       |                  |
|     386      |  Linux   |      3       |                  |
|   ppc64le    |  Linux   |      3       |                  |
|    s390x     |  Linux   |      3       |                  |

不支持未列出的平台。

#### 支持新平台

Want to contribute to etcd as the “official” maintainer of a new platform? In addition to committing to support the platform, you must setup etcd continuous integration (CI) satisfying the following requirements, depending on the support tier:
想作为新平台的“官方”维护者为 etcd 做出贡献吗？除了承诺支持平台外，您还必须设置满足以下要求的 etcd 持续集成 （CI），具体取决于支持层：

| etcd continuous integration etcd 持续集成                  | Tier 1 第 1 层 | Tier 2 第 2 层 | Tier 3 第 3 层 |
| ---------------------------------------------------------- | :------------: | :------------: | :------------: |
| Build passes 构建通道                                      |       ✓        |       ✓        |       ✓        |
| Unit tests pass 单元测试通过                               |       ✓        |       ✓        |                |
| Integration and end-to-end tests pass 集成和端到端测试通过 |       ✓        |       ✓        |                |
| Robustness tests pass 鲁棒性测试通过                       |       ✓        |                |                |

For an example of setting up tier-2 CI for ARM64, see [etcd PR #12928](https://github.com/etcd-io/etcd/pull/12928).
有关为 ARM64 设置第 2 层 CI 的示例，请参阅 etcd PR #12928。

### 不支持的平台

为了避免无意中在不受支持的平台上运行 etcd 服务器，etcd 会打印一条警告消息并立即退出，除非将环境变量 `ETCD_UNSUPPORTED_ARCH` 设置为目标架构。

#### 32 位系统

etcd 在 32 位系统上存在已知问题，这是由于 Go runtime 中的一个 bug 。

### 硬件

sually runs well with limited resources for development or testing  purposes.
etcd 通常运行良好，资源有限，用于开发或测试目的;在笔记本电脑或廉价的云主机上使用 etcd 进行开发是很常见的。但是，在生产环境中运行 etcd  集群时，一些硬件准则对于正确管理很有用。这些建议不是硬性规定。它们是可靠的生产部署的良好起点。在生产环境运行之前，应使用模拟工作负载进行测试。

#### CPU

很少有 etcd 部署需要大量的 CPU 容量。Typical clusters need  two to four cores to run smoothly. 典型的集群需要两到四个内核才能平稳运行。负载较重的 etcd  部署，每秒服务数千个客户端或数万个请求，tend to be CPU bound since etcd can  serve requests from memory.往往受 CPU 限制，因为 etcd 可以从内存中处理请求。这种繁重的部署通常需要 8 到 16  个专用内核。

#### 内存

etcd has a relatively small memory footprint but its performance still  depends on having enough memory. An etcd server will aggressively cache  key-value data and spends most of the rest of its memory tracking  watchers. For heavy deployments with thousands  of watchers and millions of keys, allocate 16GB to 64GB memory  accordingly.
etcd 的内存占用相对较小，但其性能仍然取决于是否有足够的内存。etcd 服务器将积极缓存键值数据，并将其余大部分内存跟踪观察器花费在上。通常 8GB 就足够了。对于具有数千个观察程序和数百万个密钥的繁重部署，请相应地分配 16GB 到 64GB 内存。

#### 磁盘

Fast disks are the most critical factor for etcd deployment performance and stability.
快速磁盘是 etcd 部署性能和稳定性的最关键因素。

Since etcd’s consensus protocol depends on  persistently storing metadata to a log, a majority of etcd cluster  members must write every request down to disk. Additionally, etcd will  also incrementally checkpoint its state to disk so it can truncate this  log. If these writes take too long, heartbeats may time out and trigger  an election, undermining the stability of the cluster. In general, to  tell whether a disk is fast enough for etcd, a benchmarking tool such as [fio](https://github.com/axboe/fio) can be used. Read [here](https://prog.world/is-storage-speed-suitable-for-etcd-ask-fio/) for an example.
磁盘速度较慢会增加 etcd 请求延迟，并可能损害集群稳定性。由于 etcd 的共识协议依赖于将元数据持久地存储到日志中，因此大多数 etcd  集群成员必须将每个请求写入磁盘。此外，etcd  还会以增量方式将其状态检查到磁盘，以便截断此日志。如果这些写入时间过长，检测信号可能会超时并触发选举，从而破坏群集的稳定性。一般来说，要判断一个磁盘是否足够快，可以使用 fio 等基准测试工具。阅读此处的示例。

etcd is very sensitive to disk write latency. Typically 50 sequential IOPS  (e.g., a 7200 RPM disk) is required. For heavily loaded clusters, 500  sequential IOPS (e.g., a typical local SSD or a high performance  virtualized block device) is recommended. Note that most cloud providers publish concurrent IOPS rather than sequential IOPS; the published  concurrent IOPS can be 10x greater than the sequential IOPS. To measure  actual sequential IOPS, we suggest using a disk benchmarking tool such  as [diskbench](https://github.com/ongardie/diskbenchmark) or [fio](https://github.com/axboe/fio).
etcd 对磁盘写入延迟非常敏感。通常需要 50 个顺序 IOPS（例如，一个 7200 RPM 的磁盘）。对于负载较重的集群，建议使用 500 顺序  IOPS（例如，典型的本地 SSD 或高性能虚拟化块设备）。请注意，大多数云提供商发布并发 IOPS，而不是顺序 IOPS;发布的并发 IOPS 可以比顺序 IOPS 高 10 倍。要测量实际的顺序 IOPS，我们建议使用磁盘基准测试工具，例如 diskbench 或 fio。

etcd requires only modest disk bandwidth but more disk bandwidth buys faster recovery times when a failed member has to catch up with the cluster.  Typically 10MB/s will recover 100MB data within 15 seconds. For large  clusters, 100MB/s or higher is suggested for recovering 1GB data within  15 seconds.
etcd 只需要适度的磁盘带宽，但当故障成员必须赶上集群时，更多的磁盘带宽可以带来更快的恢复时间。通常，10MB/s 将在 15 秒内恢复 100MB 数据。对于大型集群，建议在 15 秒内恢复 1GB 数据，建议使用 100MB/s 或更高。

When possible, back etcd’s storage with a SSD. A SSD usually provides lower  write latencies and with less variance than a spinning disk, thus  improving the stability and reliability of etcd. If using spinning disk, get the fastest disks possible (15,000 RPM). Using RAID 0 is also an  effective way to increase disk speed, for both spinning disks and SSD.  With at least three cluster members, mirroring and/or parity variants of RAID are unnecessary; etcd’s consistent replication already gets high  availability.
如果可能，请使用 SSD 备份 etcd 的存储。与旋转盘相比，SSD 通常提供更低的写入延迟和更小的方差，从而提高了 etcd  的稳定性和可靠性。如果使用旋转磁盘，请获得尽可能快的磁盘 （15,000 RPM）。使用 RAID 0  也是提高磁盘速度的有效方法，适用于旋转磁盘和 SSD。对于至少三个集群成员，RAID 的镜像和/或奇偶校验变体是不必要的;etcd  的一致复制已经获得了高可用性。

#### 网络

Multi-member etcd deployments benefit from a fast and reliable network. In order for etcd to be both consistent and partition tolerant, an unreliable  network with partitioning outages will lead to poor availability. Low  latency ensures etcd members can communicate fast. High bandwidth can  reduce the time to recover a failed etcd member. 1GbE is sufficient for  common etcd deployments. For large etcd clusters, a 10GbE network will  reduce mean time to recovery.
多成员 etcd 部署受益于快速可靠的网络。为了使 etcd 既一致又能容忍分区，一个不可靠的网络和分区中断将导致可用性差。低延迟确保 etcd  成员可以快速通信。高带宽可以减少恢复故障 etcd 成员的时间。1GbE 对于常见的 etcd 部署来说已经足够了。对于大型 etcd  集群，10GbE 网络将缩短平均恢复时间。

Deploy etcd members within a single data center when possible to avoid latency overheads and lessen the possibility of partitioning events. If a  failure domain in another data center is required, choose a data center  closer to the existing one. Please also read the [tuning](https://etcd.io/docs/v3.5/tuning/) documentation for more information on cross data center deployment.
尽可能在单个数据中心内部署 etcd 成员，以避免延迟开销并减少对事件进行分区的可能性。如果需要另一个数据中心中的故障域，请选择更靠近现有数据中心的数据中心。另请阅读调优文档，了解有关跨数据中心部署的更多信息。

#### 硬件配置示例

Here are a few example hardware setups on AWS and GCE environments. As  mentioned before, but must be stressed regardless, administrators should test an etcd deployment with a simulated workload before putting it  into production.
以下是 AWS 和 GCE 环境中的一些硬件设置示例。如前所述，但无论如何都必须强调，管理员应该在将 etcd 部署投入生产之前使用模拟工作负载对其进行测试。

Note that these configurations assume these machines are totally dedicated  to etcd. Running other applications along with etcd on these machines  may cause resource contentions and lead to cluster instability.
请注意，这些配置假设这些机器完全专用于 etcd。在这些机器上运行其他应用程序以及 etcd 可能会导致资源争用并导致集群不稳定。

##### 小型集群

小型集群服务少于 100 个客户端，每秒少于 200 个请求，存储的数据不超过 100MB。

Example application workload: A 50-node Kubernetes cluster
示例应用程序工作负载：50 节点的 Kubernetes 集群

| Provider | 类型                        | vCPU | 内存 （GB） | 最大并发 IOPS | 磁盘带宽 （MB/s） |
| -------- | --------------------------- | ---- | ----------- | ------------- | ----------------- |
| AWS      | m4.large m4                 | 2    | 8           | 3600          | 56.25             |
| GCE      | n1-standard-2 + 50GB PD SSD | 2    | 7.5         | 1500          | 25                |

##### 中型集群

A medium cluster serves fewer than 500 clients, fewer than 1,000 of requests per second, and stores no more than 500MB of data.
一个中型集群服务少于 500 个客户端，每秒少于 1,000 个请求，存储的数据不超过 500MB。

Example application workload: A 250-node Kubernetes cluster
示例应用程序工作负载：250 节点的 Kubernetes 集群

| Provider | 类型                         | vCPU | 内存 （GB） | 最大并发 IOPS | 磁盘带宽 （MB/s） |
| -------- | ---------------------------- | ---- | ----------- | ------------- | ----------------- |
| AWS      | m4.xlarge                    | 4    | 16          | 6000          | 93.75             |
| GCE      | n1-standard-4 + 150GB PD SSD | 4    | 15          | 4500          | 75                |

##### 大型集群

A large cluster serves fewer than 1,500 clients, fewer than 10,000 of requests per second, and stores no more than 1GB of data.
大型集群服务少于 1,500 个客户端，每秒少于 10,000 个请求，存储的数据不超过 1GB。

Example application workload: A 1,000-node Kubernetes cluster
示例应用程序工作负载：包含 1,000 个节点的 Kubernetes 集群

| Provider | 类型                         | vCPU | 内存 （GB） | 最大并发 IOPS | 磁盘带宽 （MB/s） |
| -------- | ---------------------------- | ---- | ----------- | ------------- | ----------------- |
| AWS      | m4.2xlarge                   | 8    | 32          | 8000          | 125               |
| GCE      | n1-standard-8 + 250GB PD SSD | 8    | 30          | 7500          | 125               |

#####  超大型集群

An xLarge cluster serves more than 1,500 clients, more than 10,000 of requests per second, and stores more than 1GB data.
一个超大型集群服务于超过1,500个客户端，每秒超过10,000个请求，并存储超过1GB的数据。

Example application workload: A 3,000 node Kubernetes cluster
示例应用程序工作负载：3,000 个节点的 Kubernetes 集群

| Provider | 类型                          | vCPU | 内存 （GB） | 最大并发 IOPS | 磁盘带宽 （MB/s） |
| -------- | ----------------------------- | ---- | ----------- | ------------- | ----------------- |
| AWS      | m4.4xlarge                    | 16   | 64          | 16,000        | 250               |
| GCE      | n1-standard-16 + 500GB PD SSD | 16   | 60          | 15,000        | 250               |

## 预构建的二进制文件

1. 从 [Releases](https://github.com/etcd-io/etcd/releases/) 下载适用于您平台的压缩存档文件，选择 release v3.5.11 或更高版本。
2. 解压缩存档文件。将生成一个包含二进制文件的目录。
3. 将可执行二进制文件添加到 PATH 中。例如，重命名和/或将二进制文件移动到 path 中的目录（如 `/usr/local/bin` ），或将上一步创建的目录添加到 path 中。

## 源代码构建

如果有 Go 版本 1.2+，您可以按照以下步骤从源代码构建 etcd：

1. [Download the etcd repo as a zip file](https://github.com/etcd-io/etcd/archive/v3.5.11.zip) and unzip it, or clone the repo using the following command.
   将 etcd 存储库下载为 zip 文件并解压缩，或使用以下命令克隆存储库。

   ```sh
   $ git clone -b v3.5.11 https://github.com/etcd-io/etcd.git
   ```

   To build from `main@HEAD`, omit the `-b v3.5.11` flag.
   若要从 `main@HEAD` 生成，请省略该 `-b v3.5.11` 标志。

2. 更改目录：

   ```bash
   $ cd etcd
   ```

3. 运行脚本：

   ```bash
   $ ./build.sh
   ```

   二进制文件位于 `bin` 该目录下。
   
4. 将 `bin` 目录的完整路径添加到 path 中，例如：
   
   ```bash
   $ export PATH="$PATH:`pwd`/bin"
   ```

## 通过操作系统软件包安装

*Disclaimer: etcd installations through OS package managers can deliver outdated  versions since they are not being automatically maintained nor  officially supported by etcd project. *

*免责声明：通过操作系统包管理器安装的 etcd 可能会提供过时的版本，因为它们不会被自动维护，也不会得到 etcd 项目的官方支持。因此，请谨慎使用操作系统包。*

### MacOS (Homebrew)

1. 更新 homebrew：

   ```bash
   $ brew update
   ```

2. 安装 etcd：

   ```bash
   $ brew install etcd
   ```

### Linux

TBD

### 作为 Kubernetes 安装的一部分进行安装

TBD—需要帮助

### 在 Kubernetes 上安装，使用 statefulset 或 helm chart

The etcd project does not currently maintain a helm chart, however you can follow the instructions provided by [Bitnami’s etcd Helm chart](https://bitnami.com/stack/etcd/helm).
etcd 项目目前没有 helm chart，但您可以按照 Bitnami 的 etcd Helm chart 提供的说明进行操作。

## 安装检查

在本地安装、运行和测试 etcd 的单成员集群：

1. 从预构建的二进制文件或源代码安装 etcd。

   **重要提示**: 请确保执行安装说明的最后一步，以验证是否 `etcd` 在您的路径中。

2. 运行 `etcd` ：

   ```bash
   $ etcd
   {"level":"info","ts":"2021-09-17T09:19:32.783-0400","caller":"etcdmain/etcd.go:72","msg":... }
   ⋮
   ```

   **Note**: `etcd` 生成的输出是日志 — 可以忽略 info 级日志。

## systemd

/usr/lib/systemd/system/etcd.service

```ini
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/bin/etcd

[Install]
WantedBy=multi-user.target
```

