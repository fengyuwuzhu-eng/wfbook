# 监控

[TOC]

## 概述

Ceph Dashboard 使用 Prometheus、Grafana 和相关工具来存储和可视化集群利用率和性能的详细指标。Ceph 用户有三种选择：

1. 让 cephadm 部署和配置这些服务。这是引导新集群时的默认设置，除非使用 `--skip-monitoring-stack` 选项。
2. 手动部署和配置这些服务。这是推荐给在其环境中具有现有 prometheus 服务的用户（以及 Ceph 使用 Rook 在 Kubernetes 中运行的情况下）。
3. 完全跳过监视堆栈。一些 Ceph dashboard 的图形将不可用。

监控堆栈由 Prometheus 、Prometheus exporter（ Prometheus Module 、Node exporter ）、Prometheus Alert Manager 和 Grafana 组成。

> **Note:**
>
> Prometheus 的安全模型假定不受信任的用户可以访问 Prometheus 的 HTTP endpoint 和 log 。Untrusted users have access to all the (meta)data Prometheus collects that is contained in the database, plus a variety of operational and debugging information.不受信任的用户可以访问 Prometheus 收集的数据库中的所有（元）数据，以及各种操作和调试信息。
>
> 然而，Prometheus 的 HTTP API 仅限于只读操作。不能使用 API 更改配置，and secrets are not exposed并且不会泄露秘密。此外，Prometheus 有一些内置的措施来减轻拒绝服务攻击的影响。
>

## 使用 cephadm 部署监控

(Here are some ways that you might have come to have a Ceph cluster without a monitoring stack: You might have passed the `--skip-monitoring stack` option to `cephadm` during the installation of the cluster, or you might have converted an existing cluster (which had no monitoring stack) to cephadm management.)

cephadm 的默认行为是部署基本的监视堆栈。然而，可能有一个没有监视堆栈的 Ceph 集群，并且希望向其中添加监视堆栈。（这里有一些方法可以使 Ceph 集群没有监视堆栈：可能在集群安装期间将 `--skip-monitoring stack` 选项传递给 cephadm，或者可能将现有集群（没有监视堆栈）转换为 cephadm 管理。）

要在没有监控的 Ceph 集群上设置监控，请执行以下步骤：

1. 在集群的每个节点上部署 node-exporter 服务。node-exporter 提供主机级指标，如 CPU 和内存利用率：

   ```bash
   ceph orch apply node-exporter
   ```

2. 部署 alertmanager：

   ```bash
   ceph orch apply alertmanager
   ```

3. 部署 Prometheus 。一个 Prometheus 实例就足够了，但对于高可用性（HA），可能需要部署两个：

   ```bash
   ceph orch apply prometheus
   ```

   或者:

   ```bash
   ceph orch apply prometheus --placement 'count:2'
   ```

4. 部署 grafana：

   ```bash
   ceph orch apply grafana
   ```

### 为监控堆栈启用安全性

默认情况下，在 cephadm 托管的集群中，无需启用安全措施即可设置和配置监控组件。 虽然这对于某些部署来说已经足够了，但其他具有严格安全需求的部署可能会发现有必要保护监控堆栈，防止未经授权的访问。在这种情况下，cephadm 依赖于特定的配置参数 mgr/cephadm/secure_monitoring_stack，用于切换所有监控组件的安全设置。要激活安全措施，请使用以下形式的命令将此选项设置为 `true`：

```bash
ceph config set mgr mgr/cephadm/secure_monitoring_stack true
```

此更改将触发所有监控守护程序中的一系列重新配置，通常需要几分钟才能所有组件完全运行。更新后的安全配置包括以下修改：

1. Prometheus：访问 Web 门户需要基本身份验证，并且启用了 TLS 以实现安全通信。
2. Alertmanager：访问 Web 门户需要基本身份验证，并且启用了 TLS 以实现安全通信。
3. Node Exporter：启用 TLS 以实现安全通信。
4. Grafana：已启用 TLS，并且需要身份验证才能访问数据源信息。

在此安全设置中，用户需要为 Prometheus 和 Alertmanager 设置身份验证（用户名/密码）。默认情况下，用户名和密码设置为 `admin`/`admin`。用户可以分别使用命令 `ceph orch prometheus set-credentials` 和 `ceph orch alertmanager set-credentials` 更改这些值。These commands offer the flexibility to input the username/password either as parameters or via a JSON file, which enhances security. 这些命令提供了灵活地以参数形式或通过 JSON 输入用户名/密码 文件，以增强安全性。此外，Cephadm 还提供了命令 `ceph orch prometheus get-credentials` 和 `ceph orch alertmanager get-credentials` 来检索当前凭证。

### Ceph 中的集中日志记录

Ceph now provides centralized logging with Loki & Promtail.  Centralized Log Management (CLM) consolidates all log data and pushes it to a central repository, with an accessible and easy-to-use interface. Centralized logging is  designed to make your life easier. Some of the advantages are:

Ceph 现在使用 Loki 和 Promtail 提供集中日志记录。集中式日志管理（CLM）整合所有日志数据，并通过可访问且易于使用的界面将其推送到中央存储库。集中日志记录旨在让您的生活更轻松。其优点包括：

1. **Linear event timeline**: it is easier to troubleshoot issues analyzing a single chain of events than thousands of different logs from a hundred nodes.
   **线性事件时间线**：与分析来自 100 个节点的数千个不同日志相比，分析单个事件链更容易解决问题。
2. **Real-time live log monitoring**: it is impractical to follow logs from thousands of different sources.
   **实时实时日志监控**：跟踪来自数千个不同来源的日志是不切实际的。
3. **灵活的保留策略**：对于每个守护进程日志，日志轮换通常设置为较短的间隔（1-2 周）以节省磁盘使用量。
4. **提高安全性和备份性**：日志可能包含敏感信息并暴露使用模式。此外，集中式日志记录允许 HA 等。

Ceph 中的集中日志记录是使用两个新服务—— `loki` 和 `promtail` 实现的。

Loki：它基本上是一个日志聚合系统，用于查询日志。它可以配置为 Grafana 中的数据源。

Promtail： 它充当代理，从系统收集日志并将其提供给 Loki。

默认情况下，这两项服务不会部署在 Ceph 集群中。要启用集中日志记录，您可以按照此处提到的步骤作：[在 Dashboard 中启用集中日志记录](https://docs.ceph.com/en/latest/mgr/dashboard/#centralized-logging)。

### 网络和端口

所有监控服务都可以使用 yaml 服务规范配置它们绑定到的网络和端口。默认情况下，cephadm 在配置 Grafana 守护进程时将使用 `https` 协议，除非用户明确将协议设置为 `http` 。

```yaml
service_type: grafana
service_name: grafana
placement:
  count: 1
networks:
- 192.169.142.0/24
spec:
  port: 4200
  protocol: http
```

### 默认镜像

*The information in this section was developed by Eugen Block in a thread on the [ceph-users] mailing list in April of 2024. The thread can be viewed here: ``https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/QGC66QIFBKRTPZAQMQEYFXOGZJ7RLWBN/``.
本节中的信息由 Eugen Block 于 2024 年 4 月在 [ceph-users] 邮件列表上的一个帖子中开发。可以在此处查看线程：  ''https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/QGC66QIFBKRTPZAQMQEYFXOGZJ7RLWBN/'' 。*

`cephadm` 将 `cephadm` 二进制文件的本地副本存储在 `var/lib/ceph/{FSID}/cephadm.{DIGEST}` ，其中 `{DIGEST}` 是一个字母数字字符串，表示当前运行的 Ceph 版本。

要查看默认容器镜像，请运行以下命令：

```bash
cephadm list-images
```

Default monitoring images are specified in `/src/python-common/ceph/cephadm/images.py`.
默认监控镜像在 `/src/python-common/ceph/cephadm/images.py` .

*class* ceph.cephadm.images.DefaultImages(*value*)

An enumeration. 一个枚举。

* ALERTMANAGER *= quay.io/prometheus/alertmanager:v0.27.0*
* ELASTICSEARCH *= quay.io/omrizeneva/elasticsearch:6.8.23*
* GRAFANA *= quay.io/ceph/grafana:10.4.8*
* HAPROXY *= quay.io/ceph/haproxy:2.3*
* JAEGER_AGENT *= quay.io/jaegertracing/jaeger-agent:1.29*
* JAEGER_COLLECTOR *= quay.io/jaegertracing/jaeger-collector:1.29*
* JAEGER_QUERY *= quay.io/jaegertracing/jaeger-query:1.29*
* KEEPALIVED *= quay.io/ceph/keepalived:2.2.4*
* LOKI *= docker.io/grafana/loki:3.0.0*
* NGINX *= quay.io/ceph/nginx:sclorg-nginx-126*
* NODE_EXPORTER *= quay.io/prometheus/node-exporter:v1.7.0*
* NVMEOF *= quay.io/ceph/nvmeof:1.5*
* OAUTH2_PROXY *= quay.io/oauth2-proxy/oauth2-proxy:v7.6.0*  
* PROMETHEUS *= quay.io/prometheus/prometheus:v2.51.0*
* PROMTAIL *= docker.io/grafana/promtail:3.0.0*
* SAMBA *= quay.io/samba.org/samba-server:devbuilds-centos-amd64*
* SAMBA_METRICS *= quay.io/samba.org/samba-metrics:latest*
* SNMP_GATEWAY *= docker.io/maxwo/snmp-notifier:v1.2.1*

### 使用自定义镜像

可以基于其他镜像安装或升级监控组件。为此，需要首先将要使用的镜像的名称存储在配置中。以下配置选项可用。

- `container_image_prometheus`
- `container_image_grafana`
- `container_image_alertmanager`
- `container_image_node_exporter`
- `container_image_loki`
- `container_image_promtail`
- `container_image_haproxy`
- `container_image_keepalived`
- `container_image_snmp_gateway`
- `container_image_elasticsearch`
- `container_image_jaeger_agent`
- `container_image_jaeger_collector`
- `container_image_jaeger_query`

可以使用 `ceph config` 命令设置自定义镜像

```bash
ceph config set mgr mgr/cephadm/<option_name> <value>
```

示例：

```bash
ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1
```

如果已经运行了您更改了其映像类型的监视堆栈守护程序，If there were already running monitoring stack daemon(s) of the type whose image you’ve changed, 则必须重新部署守护程序，以便它们实际使用新镜像。

例如，如果改变了 prometheus 的镜像

```bash
ceph orch redeploy prometheus
```

> **Note:**
>
> By setting a custom image, the default value will be overridden (but not overwritten).  The default value changes when updates become available. By setting a custom image, you will not be able to update the component you have set the custom image for automatically.  You will need to manually update the configuration (image name and tag) to be able to install updates.
>
> If you choose to go with the recommendations instead, you can reset the custom image you have set before.  After that, the default value will be used again.  Use `ceph config rm` to reset the configuration option
>
> 通过设置自定义图像，将覆盖（但不覆盖）默认值。当更新可用时，默认值会更改。通过设置自定义图像，您将无法自动更新为其设置自定义图像的组件。您需要手动更新配置（图像名称和标记），以便能够安装更新。
>
> 如果您选择使用建议，则可以重置之前设置的自定义图像。之后，将再次使用默认值。使用ceph-config rm重置配置选项
>
> ```bash
> ceph config rm mgr mgr/cephadm/<option_name>
> ```
>
> 例如：
>
> ```bash
> ceph config rm mgr mgr/cephadm/container_image_prometheus
> ```

### 使用自定义配置文件

通过覆盖 cephadm 模板，可以完全定制监控服务的配置文件。

Starting from version 17.2.3, cephadm supports Prometheus http service discovery, and uses this endpoint for the definition and management of the embedded Prometheus service. The endpoint listens on `https://<mgr-ip>:8765/sd/` (the port is configurable through the variable `service_discovery_port`) and returns scrape target information in [http_sd_config format](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config)
在内部，cephadm 已经使用 [Jinja2](https://jinja.palletsprojects.com/en/2.11.x/) 模板来生成所有监控组件的配置文件。从版本 17.2.3 开始， cephadm 支持 Prometheus http 服务发现，并将此端点用于 嵌入式 Prometheus 服务的定义和管理。终端节点监听 `https：//<mgr-ip>:8765/sd/` （端口可通过变量 `service_discovery_port` 进行配置）并以 [http_sd_config 格式](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config)返回抓取目标信息

Customers with external monitoring stack can use ceph-mgr service discovery endpoint to get scraping configuration. 

具有外部监控堆栈的客户可以使用 ceph-mgr 服务发现端点来获取抓取配置。可以通过以下命令获取服务器的根证书：

```bash
ceph orch sd dump cert
```

Prometheus、Grafana 或 Alertmanager 的配置可以通过为每个服务存储 Jinja2 模板来定制。每次部署或重新配置此类服务时都会评估此模板。这样，将保留自定义配置并自动应用于这些服务的未来部署。

> **Note：**
>
> The configuration of the custom template is also preserved when the default configuration of cephadm changes. If the updated configuration is to be used, the custom template needs to be migrated *manually* after each upgrade of Ceph.
>
> 当 cephadm 的默认配置更改时，也会保留自定义模板的配置。如果要使用更新的配置，则需要在每次 Ceph 升级后手动迁移自定义模板。

#### 选项名称

The following templates for files that will be generated by cephadm can be overridden. These are the names to be used when storing with `ceph config-key set`:

可以覆盖 cephadm 生成的文件的以下模板。以下是使用 `ceph config-key set` 进行存储时要使用的名称：

- `services/alertmanager/alertmanager.yml`
- `services/alertmanager/web.yml`
- `services/grafana/ceph-dashboard.yml`
- `services/grafana/grafana.ini`
- `services/ingress/haproxy.cfg`
- `services/ingress/keepalived.conf`
- `services/iscsi/iscsi-gateway.cfg`
- `services/mgmt-gateway/external_server.conf`
- `services/mgmt-gateway/internal_server.conf`
- `services/mgmt-gateway/nginx.conf`
- `services/nfs/ganesha.conf`
- `services/node-exporter/web.yml`
- `services/nvmeof/ceph-nvmeof.conf`
- `services/oauth2-proxy/oauth2-proxy.conf`
- `services/prometheus/prometheus.yml`
- `services/prometheus/web.yml`
- `services/loki.yml`
- `services/promtail.yml`

You can look up the file templates that are currently used by cephadm in `src/pybind/mgr/cephadm/templates`:您可以在 cephadm 中查找当前使用 的文件模板 `src/pybind/mgr/cephadm/templates` :

- `services/alertmanager/alertmanager.yml.j2`
- `services/alertmanager/web.yml.j2`
- `services/grafana/ceph-dashboard.yml.j2`
- `services/grafana/grafana.ini.j2`
- `services/ingress/haproxy.cfg.j2`
- `services/ingress/keepalived.conf.j2`
- `services/iscsi/iscsi-gateway.cfg.j2`
- `services/mgmt-gateway/external_server.conf.j2`
- `services/mgmt-gateway/internal_server.conf.j2`
- `services/mgmt-gateway/nginx.conf.j2`
- `services/nfs/ganesha.conf.j2`
- `services/node-exporter/web.yml.j2`
- `services/nvmeof/ceph-nvmeof.conf.j2`
- `services/oauth2-proxy/oauth2-proxy.conf.j2`
- `services/prometheus/prometheus.yml.j2`
- `services/prometheus/web.yml.j2`
- `services/loki.yml.j2`
- `services/promtail.yml.j2`

#### 用法

The following command applies a single line value:以下命令应用单个行值：

```bash
ceph config-key set mgr/cephadm/<option_name> <value>
```

To set contents of files as template use the `-i` argument:要将文件内容设置为模板，请使用 `-i` 参数：

```bash
ceph config-key set mgr/cephadm/<option_name> -i $PWD/<filename>
```

> **Note：**
>
> 当使用文件作为 `config-key` 的输入时，必须使用文件的绝对路径。

然后，需要重新创建服务的配置文件。这是使用 reconfig 完成的。

#### 示例

```bash
# set the contents of ./prometheus.yml.j2 as template
ceph config-key set mgr/cephadm/services/prometheus/prometheus.yml -i $PWD/prometheus.yml.j2

# reconfig the prometheus service
ceph orch reconfig prometheus
```

```bash
# set additional custom alerting rules for Prometheus
ceph config-key set mgr/cephadm/services/prometheus/alerting/custom_alerts.yml -i $PWD/custom_alerts.yml

# Note that custom alerting rules are not parsed by Jinja and hence escaping
# will not be an issue.
```

## 不使用 cephadm 部署监控

如果您已有 prometheus 监控基础设施，或者想要自己管理它，则需要对其进行配置以与您的 Ceph 集群集成。

- 在 ceph-mgr 守护进程中启用 prometheus 模块

  ```bash
  ceph mgr module enable prometheus
  ```

  默认情况下，ceph-mgr 在运行 ceph-mgr 守护进程的每台主机上的端口 9283 上显示 prometheus 指标。配置 prometheus 以抓取这些。

To make this integration easier, Ceph provides by means of ceph-mgr a service discovery endpoint at <https://<mgr-ip>:8765/sd/ which can be used by an external Prometheus to retrieve targets information. Information reported by this EP used the format specified by http_sd_config <https://prometheus.io/docs/prometheus/2.28/configuration/configuration/#http_sd_config>

为了简化此集成，cephadm 在 `https：//<mgr-ip>：8765/sd/`.外部 Prometheus 服务器可以使用此终端节点来检索特定服务的目标信息。此终端节点返回的信息使用 Prometheus [http_sd_config 选项](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#http_sd_config/)指定的格式

Here’s an example prometheus job definition that uses the cephadm service discovery endpoint
下面是一个使用 cephadm 服务发现端点的 prometheus 作业定义示例

```yaml
- job_name: 'ceph-exporter'
  http_sd_configs:
  - url: http://<mgr-ip>:8765/sd/prometheus/sd-config?service=ceph-exporter
```

* To enable the dashboard’s prometheus-based alerting, see [Enabling Prometheus Alerting](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-alerting).要启用控制面板基于 Prometheus 的警报，请参阅[启用 Prometheus 警报](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-alerting)。

- 要启用控制面板与 Grafana 的集成，请参阅[启用 Grafana 控制面板的嵌入](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-grafana)。

## 禁用监控

要禁用监控并删除支持监控的软件，请运行以下命令：

```bash
ceph orch rm grafana
ceph orch rm prometheus --force   # this will delete metrics data collected so far
ceph orch rm node-exporter
ceph orch rm alertmanager
ceph mgr module disable prometheus
```

## 设置 RBD-Image 监控

由于性能原因，默认情况下禁用 RBD 镜像的监视。有关更多信息，请参阅 [Ceph Health Checks](https://docs.ceph.com/en/latest/mgr/prometheus/#prometheus-rbd-io-statistics) 。如果禁用，Grafana 中的概览和详细信息仪表板将保持空白，Prometheus 中的指标将不可见。

## 设置 Prometheus

### 设置 Prometheus 保留大小和时间

Cephadm 可以通过在 Prometheus 服务规范中指定 `retention_time` 和 `retention_size` 值来配置 Prometheus TSDB 保留。保留时间值默认为 15 天（15d）。用户可以设置不同的值/单位，其中支持的单位为：“y”、“w”、“d”、“h”、“m” 和 “s” 。保留大小值默认为 0（禁用）。本例中支持的单位为：“B”、“KB”、“MB”、“GB”、“TB”、“PB” 和 “EB” 。

在下面的示例规范中，我们将保留时间设置为 1 年，大小设置为 1GB 。

```yaml
service_type: prometheus
placement:
  count: 1
spec:
  retention_time: "1y"
  retention_size: "1GB"
```

> **Note：**
>
> If you already had Prometheus daemon(s) deployed before and are updating an existent spec as opposed to doing a fresh Prometheus deployment, you must also tell cephadm to redeploy the Prometheus daemon(s) to put this change into effect. This can be done with a `ceph orch redeploy prometheus` command.
>
> 如果您之前已经部署了 Prometheus 守护进程，并且正在更新现有规范，而不是执行新的 Prometheus 部署，则还必须告诉 cephadm 重新部署 Prometheus 守护进程以使此更改生效。这可以通过 `ceph orch redeploy prometheus` 命令来完成。

## 设置 Grafana

### 手动设置 Grafana URL

Cephadm 会在除一种情况之外的所有情况下自动配置 Prometheus、Grafana 和 Alertmanager。

在某些设置中，Dashboard 用户的浏览器可能无法访问在 Ceph Dashboard 中配置的 Grafana URL 。当集群和访问用户位于不同的 DNS 区域时，可能会发生这种情况。

如果是这种情况，您可以使用 Ceph Dashboard 的配置选项来设置用户浏览器将用于访问 Grafana 的 URL。cephadm 永远不会更改此值。要设置此配置选项，请发出以下命令：

```bash
ceph dashboard set-grafana-frontend-api-url <grafana-server-api>
```

部署服务可能需要一两分钟。部署服务后，当您发出命令 `ceph orch ls` 时，应该会看到如下内容：

```bash
ceph orch ls
NAME           RUNNING  REFRESHED  IMAGE NAME                                      IMAGE ID        SPEC
alertmanager       1/1  6s ago     docker.io/prom/alertmanager:latest              0881eb8f169f  present
crash              2/2  6s ago     docker.io/ceph/daemon-base:latest-master-devel  mix           present
grafana            1/1  0s ago     docker.io/pcuzner/ceph-grafana-el8:latest       f77afcf0bcf6   absent
node-exporter      2/2  6s ago     docker.io/prom/node-exporter:latest             e5a616e4b9cf  present
prometheus         1/1  6s ago     docker.io/prom/prometheus:latest                e935122ab143  present
```

### 为 Grafana 配置 SSL/TLS

`cephadm` 使用 ceph 键/值存储中定义的证书部署 Grafana。如果未指定证书，则 ` cephadm` 会在部署 Grafana 服务期间生成自签名证书。每个证书都特定于生成它的主机。

可以使用以下命令配置自定义证书：

```bash
ceph config-key set mgr/cephadm/{hostname}/grafana_key -i $PWD/key.pem
ceph config-key set mgr/cephadm/{hostname}/grafana_crt -i $PWD/certificate.pem
```

其中 hostname 是部署 grafana 服务的主机的主机名。

如果您已经部署了 Grafana，请在服务上运行 `reconfig` 以更新其配置：

```bash
ceph orch reconfig grafana
```

`reconfig` 命令还为 Ceph Dashboard 设置正确的 URL 。

### 设置初始管理员密码

默认情况下，Grafana 不会创建初始 admin 用户。要创建 admin 用户，请创建一个文件 `grafana.yaml` 替换为以下内容：

```yaml
service_type: grafana
spec:
  initial_admin_password: mypassword
```

然后应用此规范：

```bash
ceph orch apply -i grafana.yaml
ceph orch redeploy grafana
```

Grafana 现在将使用给定的密码创建一个名为 `admin` 的管理员用户。

### 关闭匿名访问

cephadm allows anonymous users (users who have not provided any login information) limited, viewer only access to the grafana dashboard. 
默认情况下，cephadm 允许匿名用户（未提供任何登录信息的用户）访问仅限查看者访问 grafana 仪表板。为了将 grafana 设置为仅允许已登录用户查看，可以在 grafana 规范中设置 `anonymous_access： False`。

```yaml
service_type: grafana
placement:
  hosts:
  - host1
spec:
  anonymous_access: False
  initial_admin_password: "mypassword"
```

Since deploying grafana with anonymous access set to false without an initial admin password set would make the dashboard inaccessible, cephadm requires setting the `initial_admin_password` when `anonymous_access` is set to false.
由于在部署匿名访问权限设置为 false 且未设置初始管理员密码的情况下部署 grafana 会导致仪表板无法访问，因此 cephadm 需要在 `anonymous_access` 设置为 false 时设置 `initial_admin_password`。

## 设置 Alertmanager

### 添加 Alertmanager webhook

要将新的 webhook 添加到 Alertmanager 配置中，请添加额外的 webhook URL，如下所示：

```yaml
service_type: alertmanager
spec:
  user_data:
    default_webhook_urls:
    - "https://foo"
    - "https://bar"
```

Where `default_webhook_urls` is a list of additional URLs that are added to the default receivers’ `<webhook_configs>` configuration.

其中 `default_webhook_urls` 是添加到默认接收器的 `<webhook_configs>` 配置中的其他 URL 的列表。

在服务上运行 `reconfig` 以更新其配置：

```bash
ceph orch reconfig alertmanager
```

### 打开 Certificate Validation 证书验证

如果您正在为 alertmanager 使用证书并希望确保这些证书经过验证，您应该在 alertmanager 规范中将 “secure” 选项设置为 true（默认为 false）。

```yaml
service_type: alertmanager
spec:
  secure: true
```

如果您在应用 spec 之前已经运行了 alertmanager 守护进程，则必须重新配置它们以更新其配置

```bash
ceph orch reconfig alertmanager
```

## 扩展阅读

- [Prometheus Module](https://docs.ceph.com/en/latest/mgr/prometheus/#mgr-prometheus)



## Other

开源的ceph-web项目，是非常简单的Web前端，通过ceph-rest-api获得数据并展示。

## Ceph-web

开源地址 https://github.com/tobegit3hub/ceph-web 。

目前ceph-web已经支持通过容器运行，执行下述命令即可一键启动Ceph监控工具。

```bash
docker run -d --net=host tobegit3hub/ceph-web
```

这样通过浏览器打开 http://127.0.0.1:8080 就可以看到以下管理界面。

![](../../Image/c/ceph_web.png)

![](../../Image/c/ceph_inkscope.jpg)