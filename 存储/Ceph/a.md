





- **Monitors**: A [Ceph Monitor](https://docs.ceph.com/docs/master/glossary/#term-Ceph-Monitor) (`ceph-mon`) maintains maps of the cluster state, including the monitor map, manager map, the OSD map, the MDS map, and the CRUSH map.  These maps are critical cluster state required for Ceph daemons to coordinate with each other. Monitors are also responsible for managing authentication between daemons and clients.  At least three monitors are normally required for redundancy and high availability.
- **Managers**: A [Ceph Manager](https://docs.ceph.com/docs/master/glossary/#term-Ceph-Manager) daemon (`ceph-mgr`) is responsible for keeping track of runtime metrics and the current state of the Ceph cluster, including storage utilization, current performance metrics, and system load.  The Ceph Manager daemons also host python-based modules to manage and expose Ceph cluster information, including a web-based [Ceph Dashboard](https://docs.ceph.com/docs/master/mgr/dashboard/#mgr-dashboard) and [REST API](https://docs.ceph.com/docs/master/mgr/restful).  At least two managers are normally required for high availability.
- **Ceph OSDs**: A [Ceph OSD](https://docs.ceph.com/docs/master/glossary/#term-Ceph-OSD) (object storage daemon, `ceph-osd`) stores data, handles data replication, recovery, rebalancing, and provides some monitoring information to Ceph Monitors and Managers by checking other Ceph OSD Daemons for a heartbeat. At least 3 Ceph OSDs are normally required for redundancy and high availability.
- **MDSs**: A [Ceph Metadata Server](https://docs.ceph.com/docs/master/glossary/#term-Ceph-Metadata-Server) (MDS, `ceph-mds`) stores metadata on behalf of the [Ceph File System](https://docs.ceph.com/docs/master/glossary/#term-Ceph-File-System) (i.e., Ceph Block Devices and Ceph Object Storage do not use MDS). Ceph Metadata Servers allow POSIX file system users to execute basic commands (like `ls`, `find`, etc.) without placing an enormous burden on the Ceph Storage Cluster.

Ceph stores data as objects within logical storage pools. Using the [CRUSH](https://docs.ceph.com/docs/master/glossary/#term-CRUSH) algorithm, Ceph calculates which placement group should contain the object, and further calculates which Ceph OSD Daemon should store the placement group.  The CRUSH algorithm enables the Ceph Storage Cluster to scale, rebalance, and recover dynamically.

Intro to Ceph  Whether you want to provide Ceph Object Storage and/or Ceph Block Device services to Cloud Platforms, deploy a Ceph File System or use Ceph for  another purpose, all Ceph Storage Cluster deployments begin with setting up each Ceph Node, your network, and the CephStorage Cluster.A Ceph  Storage Cluster requires at least one Ceph Monitor, Ceph Manager, and  Ceph OSD (Object Storage Daemon).The Ceph Metadata Server is also  required when running Ceph File System clients.      Monitors: A Ceph Monitor (ceph-mon) maintains maps of the cluster  state, including the monitor map, manager map, the OSD map, the MDS map, and the CRUSH map.These maps are critical cluster state required for  Ceph daemons to coordinate with each other.Monitors are also responsible for managing authentication between daemons and clients.At least three  monitors are normally required for redundancy and high availability.      Managers: A Ceph Manager daemon (ceph-mgr) is responsible for  keeping track of runtime metrics and the current state of the Ceph  cluster, including storage utilization, current performance metrics, and system load.The Ceph Manager daemons also host python-based modules to  manage and expose Ceph cluster information, including a web-based Ceph  Dashboard and REST API.At least two managers are normally required for  high availability.      Ceph OSDs: A Ceph OSD (object storage daemon, ceph-osd) stores data, handles data replication, recovery, rebalancing, and provides some  monitoring information to Ceph Monitors and Managers by checking other  Ceph OSD Daemons for a heartbeat.At least 3 Ceph OSDs are normally  required for redundancy and high availability.      MDSs: A Ceph Metadata Server (MDS, ceph-mds) stores metadata on  behalf of the Ceph File System (i.e., Ceph Block Devices and Ceph Object Storage do not use MDS).Ceph Metadata Servers allow POSIX file system  users to execute basic commands (like ls, find, etc.) without placing an enormous burden on the Ceph Storage Cluster.  Ceph stores data as objects within logical storage pools.Using the CRUSH algorithm, Ceph calculates which placement group should contain the  object, and further calculates which Ceph OSD Daemon should store the  placement group.The CRUSH algorithm enables the Ceph Storage Cluster to  scale, rebalance, and recover dynamically. Recommendations  To begin using Ceph in production, you should review our hardware  recommendations and operating system recommendations.      Hardware Recommendations         CPU         RAM         Memory         Data Storage         Networks         Failure Domains         Minimum Hardware Recommendations     OS Recommendations         Ceph Dependencies         Platforms   Get Involved You can avail yourself of help or contribute documentation, source code  or bugs by getting involved in the Ceph community.      Get Involved in the Ceph Community!     Documenting Ceph         Making Contributions         Documentation Style Guide

无论您是要向Cloud  Platform提供Ceph对象存储和/或Ceph块设备服务，部署Ceph文件系统还是出于其他目的使用Ceph，所有Ceph Storage  Cluster部署都要从设置每个Ceph节点，您的网络和Ceph开始存储集群。一个Ceph存储群集至少需要一个Ceph监视器，Ceph管理器和Ceph OSD（对象存储守护程序）。运行Ceph文件系统客户端时，也需要Ceph Metadata Server。 

​     监视器：Ceph监视器（ceph-mon）维护集群状态的映射，包括监视器映射，管理器映射，OSD映射，MDS映射和CRUSH映射。这些映射是Ceph守护程序相互协调所需的关键群集状态。监视器还负责管理守护程序和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。 

​    管理器：Ceph  Manager守护进程（ceph-mgr）负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。 Ceph  Manager守护程序还托管基于python的模块，以管理和公开Ceph集群信息，包括基于Web的Ceph仪表板和REST  API。高可用性通常至少需要两个管理器。 

​    Ceph OSD：Ceph  OSD（对象存储守护程序，ceph-osd）存储数据，处理数据复制，恢复，重新平衡，并通过检查其他Ceph  OSD守护程序的心跳来向Ceph监视器和管理器提供一些监视信息。通常至少需要3个Ceph OSD才能实现冗余和高可用性。 

​     MDS：Ceph元数据服务器（MDS，ceph-mds）代表Ceph文件系统存储元数据（即，Ceph块设备和Ceph对象存储不使用MDS）。  Ceph元数据服务器允许POSIX文件系统用户执行基本命令（如ls，find等），而不会给Ceph存储集群带来巨大负担。 

Ceph将数据作为对象存储在逻辑存储池中。使用CRUSH算法，Ceph计算哪个放置组应包含该对象，并进一步计算哪个Ceph OSD守护程序应存储该放置组。 CRUSH算法使Ceph存储集群能够动态扩展，重新平衡和恢复。 