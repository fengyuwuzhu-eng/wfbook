# Snapshot Scheduling Module[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#snapshot-scheduling-module)

This module implements scheduled snapshots for CephFS. It provides a user interface to add, query and remove snapshots schedules and retention policies, as well as a scheduler that takes the snapshots and prunes existing snapshots accordingly.

## How to enable[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#how-to-enable)

The *snap_schedule* module is enabled with:

```
ceph mgr module enable snap_schedule
```

## Usage[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#usage)

This module uses [CephFS Snapshots](https://docs.ceph.com/en/latest/dev/cephfs-snapshots/), please consider this documentation as well.

This module’s subcommands live under the ceph fs snap-schedule namespace. Arguments can either be supplied as positional arguments or as keyword arguments. Once a keyword argument was encountered, all following arguments are assumed to be keyword arguments too.

Snapshot schedules are identified by path, their repeat interval and their start time. The repeat interval defines the time between two subsequent snapshots. It is specified by a number and a period multiplier, one of h(our), d(ay), w(eek), M(onth) and Y(ear). E.g. a repeat interval of 12h specifies one snapshot every 12 hours. The start time is specified as a time string (more details about passing times below). By default the start time is last midnight. So when a snapshot schedule with repeat interval 1h is added at 13:50 with the default start time, the first snapshot will be taken at 14:00. The time zone is assumed to be UTC if none is explicitly included in the string. An explicit time zone will be mapped to UTC at execution. The start time must be in ISO8601 format. Examples below:

UTC: 2022-08-08T05:30:00 i.e. 5:30 AM UTC, without explicit time zone offset IDT: 2022-08-08T09:00:00+03:00 i.e. 6:00 AM UTC EDT: 2022-08-08T05:30:00-04:00 i.e. 9:30 AM UTC

Retention specifications are identified by path and the retention spec itself. A retention spec consists of either a number and a time period separated by a space or concatenated pairs of <number><time period>. The semantics are that a spec will ensure <number> snapshots are kept that are at least <time period> apart. For Example 7d means the user wants to keep 7 snapshots that are at least one day (but potentially longer) apart from each other. The following time periods are recognized: h(our), d(ay), w(eek), M(onth), Y(ear) and n. The latter is a special modifier where e.g. 10n means keep the last 10 snapshots regardless of timing,

All subcommands take optional fs argument to specify paths in multi-fs setups and [FS volumes and subvolumes](https://docs.ceph.com/en/latest/cephfs/fs-volumes/) managed setups. If not passed fs defaults to the first file system listed in the fs_map. When using [FS volumes and subvolumes](https://docs.ceph.com/en/latest/cephfs/fs-volumes/) the argument fs is equivalent to a volume.

When a timestamp is passed (the start argument in the add, remove, activate and deactivate subcommands) the ISO format %Y-%m-%dT%H:%M:%S will always be accepted. When either python3.7 or newer is used or https://github.com/movermeyer/backports.datetime_fromisoformat is installed, any valid ISO timestamp that is parsed by python’s datetime.fromisoformat is valid.

When no subcommand is supplied a synopsis is printed:

```
#> ceph fs snap-schedule
no valid command found; 8 closest matches:
fs snap-schedule status [<path>] [<fs>] [<format>]
fs snap-schedule list <path> [--recursive] [<fs>] [<format>]
fs snap-schedule add <path> <snap_schedule> [<start>] [<fs>]
fs snap-schedule remove <path> [<repeat>] [<start>] [<fs>]
fs snap-schedule retention add <path> <retention_spec_or_period> [<retention_count>] [<fs>]
fs snap-schedule retention remove <path> <retention_spec_or_period> [<retention_count>] [<fs>]
fs snap-schedule activate <path> [<repeat>] [<start>] [<fs>]
fs snap-schedule deactivate <path> [<repeat>] [<start>] [<fs>]
Error EINVAL: invalid command
```

### Note:[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#note)

A subvolume argument is no longer accepted by the commands.

#### Inspect snapshot schedules[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#inspect-snapshot-schedules)

The module offers two subcommands to inspect existing schedules: list and status. Bother offer plain and json output via the optional format argument. The default is plain. The list sub-command will list all schedules on a path in a short single line format. It offers a recursive argument to list all schedules in the specified directory and all contained directories. The status subcommand prints all available schedules and retention specs for a path.

Examples:

```
ceph fs snap-schedule status /
ceph fs snap-schedule status /foo/bar --format=json
ceph fs snap-schedule list /
ceph fs snap-schedule list / --recursive=true # list all schedules in the tree
```

#### Add and remove schedules[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-schedules)

The add and remove subcommands add and remove snapshots schedules respectively. Both require at least a path argument, add additionally requires a schedule argument as described in the USAGE section.

Multiple different schedules can be added to a path. Two schedules are considered different from each other if they differ in their repeat interval and their start time.

If multiple schedules have been set on a path, remove can remove individual schedules on a path by specifying the exact repeat interval and start time, or the subcommand can remove all schedules on a path when just a path is specified.

Examples:

```
ceph fs snap-schedule add / 1h
ceph fs snap-schedule add / 1h 11:55
ceph fs snap-schedule add / 2h 11:55
ceph fs snap-schedule remove / 1h 11:55 # removes one single schedule
ceph fs snap-schedule remove / 1h # removes all schedules with --repeat=1h
ceph fs snap-schedule remove / # removes all schedules on path /
```

#### Add and remove retention policies[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#add-and-remove-retention-policies)

The retention add and retention remove subcommands allow to manage retention policies. One path has exactly one retention policy. A policy can however contain multiple count-time period pairs in order to specify complex retention policies. Retention policies can be added and removed individually or in bulk via the forms ceph fs snap-schedule retention add <path> <time period> <count> and ceph fs snap-schedule retention add <path> <countTime period>[countTime period]

Examples:

```
ceph fs snap-schedule retention add / h 24 # keep 24 snapshots at least an hour apart
ceph fs snap-schedule retention add / d 7 # and 7 snapshots at least a day apart
ceph fs snap-schedule retention remove / h 24 # remove retention for 24 hourlies
ceph fs snap-schedule retention add / 24h4w # add 24 hourly and 4 weekly to retention
ceph fs snap-schedule retention remove / 7d4w # remove 7 daily and 4 weekly, leaves 24 hourly
```

#### Active and inactive schedules[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#active-and-inactive-schedules)

Snapshot schedules can be added for a path that doesn’t exist yet in the directory tree. Similarly a path can be removed without affecting any snapshot schedules on that path. If a directory is not present when a snapshot is scheduled to be taken, the schedule will be set to inactive and will be excluded from scheduling until it is activated again. A schedule can manually be set to inactive to pause the creating of scheduled snapshots. The module provides the activate and deactivate subcommands for this purpose.

Examples:

```
ceph fs snap-schedule activate / # activate all schedules on the root directory
ceph fs snap-schedule deactivate / 1d # deactivates daily snapshots on the root directory
```

#### Limitations[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#limitations)

Snapshots are scheduled using python Timers. Under normal circumstances specifying 1h as the schedule will result in snapshots 1 hour apart fairly precisely. If the mgr daemon is under heavy load however, the Timer threads might not get scheduled right away, resulting in a slightly delayed snapshot. If this happens, the next snapshot will be schedule as if the previous one was not delayed, i.e. one or more delayed snapshots will not cause drift in the overall schedule.

In order to somewhat limit the overall number of snapshots in a file system, the module will only keep a maximum of 50 snapshots per directory. If the retention policy results in more then 50 retained snapshots, the retention list will be shortened to the newest 50 snapshots.

#### Data storage[](https://docs.ceph.com/en/latest/cephfs/snap-schedule/#data-storage)

The snapshot schedule data is stored in a rados object in the cephfs metadata pool. At runtime all data lives in a sqlite database that is serialized and stored as a rados object.

Brought to you by the Ceph Foundation

The Ceph Documentation is a community resource funded and hosted by the non-profit [Ceph Foundation](https://ceph.io/en/foundation/). If you would like to support this and our other efforts, please consider [joining now](https://ceph.io/en/foundation/join/).

# CephFS Snapshot Mirroring[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#cephfs-snapshot-mirroring)

CephFS supports asynchronous replication of snapshots to a remote CephFS file system via the cephfs-mirror tool. Snapshots are synchronized by mirroring snapshot data followed by creating a remote snapshot with the same name (for a given directory on the remote file system) as the source snapshot.

## Requirements[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#requirements)

The primary (local) and secondary (remote) Ceph clusters version should be Pacific or later.



## Creating Users[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#creating-users)

Start by creating a Ceph user (on the primary/local cluster) for the cephfs-mirror daemon. This user requires write capability on the metadata pool to create RADOS objects (index objects) for watch/notify operation and read capability on the data pool(s):

```
$ ceph auth get-or-create client.mirror mon 'profile cephfs-mirror' mds 'allow r' osd 'allow rw tag cephfs metadata=*, allow r tag cephfs data=*' mgr 'allow r'
```

Create a Ceph user for each file system peer (on the secondary/remote cluster). This user needs to have full capabilities on the MDS (to take snapshots) and the OSDs:

```
$ ceph fs authorize <fs_name> client.mirror_remote / rwps
```

This user will be supplied as part of the peer specification when adding a peer.

## Starting Mirror Daemon[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#starting-mirror-daemon)

The mirror daemon should be spawned using systemctl(1) unit files:

```
$ systemctl enable cephfs-mirror@mirror
$ systemctl start cephfs-mirror@mirror
```

cephfs-mirror daemon can be run in foreground using:

```
$ cephfs-mirror --id mirror --cluster site-a -f
```

Note

The user specified here is mirror, the creation of which is described in the [Creating Users](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#cephfs-mirroring-creating-users) section.

Multiple `cephfs-mirror` daemons may be deployed for concurrent synchronization and high availability. Mirror daemons share the synchronization load using a simple `M/N` policy, where `M` is the number of directories and `N` is the number of `cephfs-mirror` daemons.

When `cephadm` is used to manage a Ceph cluster, `cephfs-mirror` daemons can be deployed by running the following command:

```
ceph orch apply cephfs-mirror
```

To deploy multiple mirror daemons, run a command of the following form:

```
ceph orch apply cephfs-mirror --placement=<placement-spec>
```

For example, to deploy 3 cephfs-mirror daemons on different hosts, run a command of the following form:

```
$ ceph orch apply cephfs-mirror --placement="3 host1,host2,host3"
```

## Interface[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#interface)

The Mirroring module (manager plugin) provides interfaces for managing directory snapshot mirroring. These are (mostly) wrappers around monitor commands for managing file system mirroring and is the recommended control interface.

## Mirroring Module[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#mirroring-module)

The mirroring module is responsible for assigning directories to mirror daemons for synchronization. Multiple mirror daemons can be spawned to achieve concurrency in directory snapshot synchronization. When mirror daemons are spawned (or terminated), the mirroring module discovers the modified set of mirror daemons and rebalances directory assignments across the new set, thus providing high-availability.

Note

Deploying a single mirror daemon is recommended. Running multiple daemons is untested.

The mirroring module is disabled by default. To enable the mirroring module, run the following command:

```
ceph mgr module enable mirroring
```

The mirroring module provides a family of commands that can be used to control the mirroring of directory snapshots. To add or remove directories, mirroring must be enabled for a given file system. To enable mirroring for a given file system, run a command of the following form:

```
ceph fs snapshot mirror enable <fs_name>
```

Note

“Mirroring module” commands are prefixed with `fs snapshot mirror`. This distinguishes them from “monitor commands”, which are prefixed with `fs mirror`. Be sure (in this context) to use module commands.

To disable mirroring for a given file system, run a command of the following form:

```
ceph fs snapshot mirror disable <fs_name>
```

After mirroring is enabled, add a peer to which directory snapshots are to be mirrored. Peers are specified by the `<client>@<cluster>` format, which is referred to elsewhere in this document as the `remote_cluster_spec`. Peers are assigned a unique-id (UUID) when added. See the [Creating Users](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#cephfs-mirroring-creating-users) section for instructions that describe how to create Ceph users for mirroring.

To add a peer, run a command of the following form:

```
ceph fs snapshot mirror peer_add <fs_name> <remote_cluster_spec> [<remote_fs_name>] [<remote_mon_host>] [<cephx_key>]
```

`<remote_cluster_spec>` is of the format `client.<id>@<cluster_name>`.

`<remote_fs_name>` is optional, and defaults to <fs_name> (on the remote cluster).

For this command to succeed, the remote cluster’s Ceph configuration and user keyring must be available in the primary cluster. For example, if a user named `client_mirror` is created on the remote cluster which has `rwps` permissions for the remote file system named `remote_fs` (see Creating Users) and the remote cluster is named `remote_ceph` (that is, the remote cluster configuration file is named `remote_ceph.conf` on the primary cluster), run the following command to add the remote filesystem as a peer to the primary filesystem `primary_fs`:

```
ceph fs snapshot mirror peer_add primary_fs client.mirror_remote@remote_ceph remote_fs
```

To avoid having to maintain the remote cluster configuration file and remote ceph user keyring in the primary cluster, users can bootstrap a peer (which stores the relevant remote cluster details in the monitor config store on the primary cluster). See the [Bootstrap Peers](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#cephfs-mirroring-bootstrap-peers) section.

The `peer_add` command supports passing the remote cluster monitor address and the user key. However, bootstrapping a peer is the recommended way to add a peer.

Note

Only a single peer is currently supported.

To remove a peer, run a command of the following form:

```
ceph fs snapshot mirror peer_remove <fs_name> <peer_uuid>
```

To list file system mirror peers, run a command of the following form:

```
ceph fs snapshot mirror peer_list <fs_name>
```

To configure a directory for mirroring, run a command of the following form:

```
ceph fs snapshot mirror add <fs_name> <path>
```

To stop mirroring directory snapshots, run a command of the following form:

```
ceph fs snapshot mirror remove <fs_name> <path>
```

Only absolute directory paths are allowed.

Paths are normalized by the mirroring module. This means that `/a/b/../b` is equivalent to `/a/b`. Paths always start from the CephFS file-system root and not from the host system mount point.

For example:

```
$ mkdir -p /d0/d1/d2
$ ceph fs snapshot mirror add cephfs /d0/d1/d2
{}
$ ceph fs snapshot mirror add cephfs /d0/d1/../d1/d2
Error EEXIST: directory /d0/d1/d2 is already tracked
```

After a directory is added for mirroring, the additional mirroring of subdirectories or ancestor directories is disallowed:

```
$ ceph fs snapshot mirror add cephfs /d0/d1
Error EINVAL: /d0/d1 is a ancestor of tracked path /d0/d1/d2
$ ceph fs snapshot mirror add cephfs /d0/d1/d2/d3
Error EINVAL: /d0/d1/d2/d3 is a subtree of tracked path /d0/d1/d2
```

The [Mirroring Status](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#cephfs-mirroring-mirroring-status) section contains information about the commands for checking the directory mapping (to mirror daemons) and for checking the directory distribution.



## Bootstrap Peers[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#bootstrap-peers)

Adding a peer (via peer_add) requires the peer cluster configuration and user keyring to be available in the primary cluster (manager host and hosts running the mirror daemon). This can be avoided by bootstrapping and importing a peer token. Peer bootstrap involves creating a bootstrap token on the peer cluster via:

```
$ ceph fs snapshot mirror peer_bootstrap create <fs_name> <client_entity> <site-name>
```

e.g.:

```
$ ceph fs snapshot mirror peer_bootstrap create backup_fs client.mirror_remote site-remote
{"token": "eyJmc2lkIjogIjBkZjE3MjE3LWRmY2QtNDAzMC05MDc5LTM2Nzk4NTVkNDJlZiIsICJmaWxlc3lzdGVtIjogImJhY2t1cF9mcyIsICJ1c2VyIjogImNsaWVudC5taXJyb3JfcGVlcl9ib290c3RyYXAiLCAic2l0ZV9uYW1lIjogInNpdGUtcmVtb3RlIiwgImtleSI6ICJBUUFhcDBCZ0xtRmpOeEFBVnNyZXozai9YYUV0T2UrbUJEZlJDZz09IiwgIm1vbl9ob3N0IjogIlt2MjoxOTIuMTY4LjAuNTo0MDkxOCx2MToxOTIuMTY4LjAuNTo0MDkxOV0ifQ=="}
```

site-name refers to a user-defined string to identify the remote filesystem. In context of peer_add interface, site-name is the passed in cluster name from remote_cluster_spec.

Import the bootstrap token in the primary cluster via:

```
$ ceph fs snapshot mirror peer_bootstrap import <fs_name> <token>
```

e.g.:

```
$ ceph fs snapshot mirror peer_bootstrap import cephfs eyJmc2lkIjogIjBkZjE3MjE3LWRmY2QtNDAzMC05MDc5LTM2Nzk4NTVkNDJlZiIsICJmaWxlc3lzdGVtIjogImJhY2t1cF9mcyIsICJ1c2VyIjogImNsaWVudC5taXJyb3JfcGVlcl9ib290c3RyYXAiLCAic2l0ZV9uYW1lIjogInNpdGUtcmVtb3RlIiwgImtleSI6ICJBUUFhcDBCZ0xtRmpOeEFBVnNyZXozai9YYUV0T2UrbUJEZlJDZz09IiwgIm1vbl9ob3N0IjogIlt2MjoxOTIuMTY4LjAuNTo0MDkxOCx2MToxOTIuMTY4LjAuNTo0MDkxOV0ifQ==
```



## Mirroring Status[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#mirroring-status)

CephFS mirroring module provides mirror daemon status interface to check mirror daemon status:

```
$ ceph fs snapshot mirror daemon status
[
  {
    "daemon_id": 284167,
    "filesystems": [
      {
        "filesystem_id": 1,
        "name": "a",
        "directory_count": 1,
        "peers": [
          {
            "uuid": "02117353-8cd1-44db-976b-eb20609aa160",
            "remote": {
              "client_name": "client.mirror_remote",
              "cluster_name": "ceph",
              "fs_name": "backup_fs"
            },
            "stats": {
              "failure_count": 1,
              "recovery_count": 0
            }
          }
        ]
      }
    ]
  }
]
```

An entry per mirror daemon instance is displayed along with information such as configured peers and basic stats. For more detailed stats, use the admin socket interface as detailed below.

CephFS mirror daemons provide admin socket commands for querying mirror status. To check available commands for mirror status use:

```
$ ceph --admin-daemon /path/to/mirror/daemon/admin/socket help
{
    ....
    ....
    "fs mirror status cephfs@360": "get filesystem mirror status",
    ....
    ....
}
```

Commands prefixed with`fs mirror status` provide mirror status for mirror enabled file systems. Note that cephfs@360 is of format filesystem-name@filesystem-id. This format is required since mirror daemons get asynchronously notified regarding file system mirror status (A file system can be deleted and recreated with the same name).

This command currently provides minimal information regarding mirror status:

```
$ ceph --admin-daemon /var/run/ceph/cephfs-mirror.asok fs mirror status cephfs@360
{
  "rados_inst": "192.168.0.5:0/1476644347",
  "peers": {
      "a2dc7784-e7a1-4723-b103-03ee8d8768f8": {
          "remote": {
              "client_name": "client.mirror_remote",
              "cluster_name": "site-a",
              "fs_name": "backup_fs"
          }
      }
  },
  "snap_dirs": {
      "dir_count": 1
  }
}
```

The Peers section in the command output above shows the peer information including the unique peer-id (UUID) and specification. The peer-id is required when removing an existing peer as mentioned in the Mirror Module and Interface section.

Commands prefixed with fs mirror peer status provide peer synchronization status. This command is of format filesystem-name@filesystem-id peer-uuid:

```
$ ceph --admin-daemon /var/run/ceph/cephfs-mirror.asok fs mirror peer status cephfs@360 a2dc7784-e7a1-4723-b103-03ee8d8768f8
{
  "/d0": {
      "state": "idle",
      "last_synced_snap": {
          "id": 120,
          "name": "snap1",
          "sync_duration": 0.079997898999999997,
          "sync_time_stamp": "274900.558797s"
      },
      "snaps_synced": 2,
      "snaps_deleted": 0,
      "snaps_renamed": 0
  }
}
```

Synchronization stats including snaps_synced, snaps_deleted and snaps_renamed are reset on daemon restart and/or when a directory is reassigned to another mirror daemon (when multiple mirror daemons are deployed).

A directory can be in one of the following states:

```
- `idle`: The directory is currently not being synchronized
- `syncing`: The directory is currently being synchronized
- `failed`: The directory has hit upper limit of consecutive failures
```

When a directory experiences a configured number of consecutive synchronization failures, the mirror daemon marks it as failed. Synchronization for these directories is retried. By default, the number of consecutive failures before a directory is marked as failed is controlled by cephfs_mirror_max_consecutive_failures_per_directory configuration option (default: 10) and the retry interval for failed directories is controlled via cephfs_mirror_retry_failed_directories_interval configuration option (default: 60s).

E.g., adding a regular file for synchronization would result in failed status:

```
$ ceph fs snapshot mirror add cephfs /f0
$ ceph --admin-daemon /var/run/ceph/cephfs-mirror.asok fs mirror peer status cephfs@360 a2dc7784-e7a1-4723-b103-03ee8d8768f8
{
  "/d0": {
      "state": "idle",
      "last_synced_snap": {
          "id": 120,
          "name": "snap1",
          "sync_duration": 0.079997898999999997,
          "sync_time_stamp": "274900.558797s"
      },
      "snaps_synced": 2,
      "snaps_deleted": 0,
      "snaps_renamed": 0
  },
  "/f0": {
      "state": "failed",
      "snaps_synced": 0,
      "snaps_deleted": 0,
      "snaps_renamed": 0
  }
}
```

This allows a user to add a non-existent directory for synchronization. The mirror daemon will mark such a directory as failed and retry (less frequently). When the directory is created, the mirror daemon will clear the failed state upon successful synchronization.

When mirroring is disabled, the respective fs mirror status command for the file system will not show up in command help.

## Configuration Options[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#configuration-options)

- cephfs_mirror_max_concurrent_directory_syncs[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_max_concurrent_directory_syncs)

  maximum number of directory snapshots that can be synchronized concurrently by cephfs-mirror daemon. Controls the number of synchronization threads. type `uint` default `3` min `1`

- cephfs_mirror_action_update_interval[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_action_update_interval)

  Interval in seconds to process pending mirror update actions. type `secs` default `2` min `1`

- cephfs_mirror_restart_mirror_on_blocklist_interval[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_restart_mirror_on_blocklist_interval)

  Interval in seconds to restart blocklisted mirror instances. Setting to zero (0) disables restarting blocklisted instances. type `secs` default `30` min `0`

- cephfs_mirror_max_snapshot_sync_per_cycle[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_max_snapshot_sync_per_cycle)

  maximum number of snapshots to mirror when a directory is picked up for mirroring by worker threads. type `uint` default `3` min `1`

- cephfs_mirror_directory_scan_interval[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_directory_scan_interval)

  interval in seconds to scan configured directories for snapshot mirroring. type `uint` default `10` min `1`

- cephfs_mirror_max_consecutive_failures_per_directory[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_max_consecutive_failures_per_directory)

  number of consecutive snapshot synchronization failures to mark a directory as “failed”. failed directories are retried for synchronization less frequently. type `uint` default `10` min `0`

- cephfs_mirror_retry_failed_directories_interval[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_retry_failed_directories_interval)

  interval in seconds to retry synchronization for failed directories. type `uint` default `60` min `1`

- cephfs_mirror_restart_mirror_on_failure_interval[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_restart_mirror_on_failure_interval)

  Interval in seconds to restart failed mirror instances. Setting to zero (0) disables restarting failed mirror instances. type `secs` default `20` min `0`

- cephfs_mirror_mount_timeout[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#confval-cephfs_mirror_mount_timeout)

  Timeout in seconds for mounting primary or secondary (remote) ceph file system by the cephfs-mirror daemon. Setting this to a higher value could result in the mirror daemon getting stalled when mounting a file system if the cluster is not reachable. This option is used to override the usual client_mount_timeout. type `secs` default `10` min `0`

## Re-adding Peers[](https://docs.ceph.com/en/latest/cephfs/cephfs-mirroring/#re-adding-peers)

When re-adding (reassigning) a peer to a file system in another cluster, ensure that all mirror daemons have stopped synchronization to the peer. This can be checked via fs mirror status admin socket command (the Peer UUID should not show up in the command output). Also, it is recommended to purge synchronized directories from the peer  before re-adding it to another file system (especially those directories which might exist in the new primary file system). This is not required if re-adding a peer to the same primary file system it was earlier synchronized from.

Brought to you by the Ceph Foundation

The Ceph Documentation is a community resource funded and hosted by the non-profit [Ceph Foundation](https://ceph.io/en/foundation/). If you would like to support this and our other efforts, please consider [joining now](https://ceph.io/en/foundation/join/).