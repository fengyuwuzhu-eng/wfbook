# 服务管理

[TOC]

## 服务状态

A service is a group of daemons that are configured together.服务是一起配置的一组守护进程。

Print a list of services known to the orchestrator. The list can be limited to services on a particular host with the optional `–-host` parameter and/or services of a particular type via optional `–-type` parameter (mon, osd, mgr, mds, rgw):

以下命令打印编排器已知的服务列表。要将输出限制为仅在指定主机上的服务，请使用可选的--host参数。要将输出限制为仅特定类型的服务，请使用可选的--type参数（mon、osd、mgr、mds、rgw）

```bash
ceph orch ls [--service_type type] [--service_name name] [--export] [--format f] [--refresh]
```

Discover the status of a particular service or daemons:

发现特定服务或守护程序的状态：

```bash
ceph orch ls --service_type type --service_name <name> [--refresh]
```

Export the service specs known to the orchestrator:

```bash
ceph orch ls --export
```

## Daemon Status

A daemon is a running systemd unit and is part of a service.

Print a list of all daemons known to the orchestrator:

```bash
ceph orch ps [--hostname host] [--daemon_type type] [--service_name name] [--daemon_id id] [--format f] [--refresh]
```

Query the status of a particular service instance (mon, osd, mds, rgw).  For OSDs the id is the numeric OSD ID, for MDS services it is the file system name:

```bash
ceph orch ps --daemon_type osd --daemon_id 0
```

## Service Specification

A *Service Specification* is a data structure to specify the deployment of services.  For example in YAML:

服务规范是用于指定服务部署的数据结构。以下是YAML中的服务规范示例：

```yaml
service_type: rgw
service_id: realm.zone
placement:
  hosts:
    - host1
    - host2
    - host3
unmanaged: false
networks:
- 192.169.142.0/24
spec:
  # Additional service specific attributes.
```

where the properties of a service specification are:

- `networks: List[str]`

  A list of network identities instructing the daemons to only bind on the particular networks in that list. In case the cluster is distributed across multiple networks, you can add multiple networks. See [Networks and Ports](https://docs.ceph.com/en/latest/cephadm/services/monitoring/#cephadm-monitoring-networks-ports), [Specifying Networks](https://docs.ceph.com/en/latest/cephadm/services/rgw/#cephadm-rgw-networks) and [Specifying Networks](https://docs.ceph.com/en/latest/cephadm/services/mgr/#cephadm-mgr-networks).

- `service_type`

  The type of the service. Needs to be either a Ceph service (`mon`, `crash`, `mds`, `mgr`, `osd` or `rbd-mirror`), a gateway (`nfs` or `rgw`), part of the monitoring stack (`alertmanager`, `grafana`, `node-exporter` or `prometheus`) or (`container`) for custom containers.

- `service_id`

  The name of the service.Required for `iscsi`, `mds`, `nfs`, `osd`, `rgw`, `container`, `ingress`

- `placement`

  See [Placement Specification](https://docs.ceph.com/en/latest/cephadm/service-management/#orchestrator-cli-placement-spec).

- `unmanaged`

  If set to `true`, the orchestrator will not deploy nor remove any daemon associated with this service. Placement and all other properties will be ignored. This is useful, if this service should not be managed temporarily. For cephadm, See [Disable automatic deployment of daemons](https://docs.ceph.com/en/latest/cephadm/service-management/#cephadm-spec-unmanaged)

Each service type can have additional service specific properties.

Service specifications of type `mon`, `mgr`, and the monitoring types do not require a `service_id`.

A service of type `osd` is described in [Advanced OSD Service Specifications](https://docs.ceph.com/en/latest/cephadm/osd/#drivegroups)

Many service specifications can be applied at once using `ceph orch apply -i` by submitting a multi-document YAML file:

```yaml
cat <<EOF | ceph orch apply -i -
service_type: mon
placement:
  host_pattern: "mon*"
---
service_type: mgr
placement:
  host_pattern: "mgr*"
---
service_type: osd
service_id: default_drive_group
placement:
  host_pattern: "osd*"
data_devices:
  all: true
EOF
```

### Retrieving the running Service Specification

If the services have been started via `ceph orch apply...`, then directly changing the Services Specification is complicated. we suggest exporting the running Service Specification by following these instructions:

```bash
ceph orch ls --service-name rgw.<realm>.<zone> --export > rgw.<realm>.<zone>.yaml
ceph orch ls --service-type mgr --export > mgr.yaml
ceph orch ls --export > cluster.yaml
```

The Specification can then be changed and re-applied as above.

- https://docs.ceph.com/en/latest/cephadm/troubleshooting/#cephadm-pause)

### Updating Service Specifications

The Ceph Orchestrator maintains a declarative state of each service in a `ServiceSpec`. For certain operations, like updating the RGW HTTP port, we need to update the existing specification.

1. List the current `ServiceSpec`:

   ```
   ceph orch ls --service_name=<service-name> --export > myservice.yaml
   ```

2. Update the yaml file:

   ```
   vi myservice.yaml
   ```

3. Apply the new `ServiceSpec`:

   ```
   ceph orch apply -i myservice.yaml [--dry-run]
   ```



## Daemon Placement

For the orchestrator to deploy a *service*, it needs to know where to deploy *daemons*, and how many to deploy.  This is the role of a placement specification.  Placement specifications can either be passed as command line arguments or in a YAML files.

Note

cephadm will not deploy daemons on hosts with the `_no_schedule` label; see [Special host labels](https://docs.ceph.com/en/latest/cephadm/host-management/#cephadm-special-host-labels).

Note

The **apply** command can be confusing. For this reason, we recommend using YAML specifications.

Each `ceph orch apply <service-name>` command supersedes the one before it. If you do not use the proper syntax, you will clobber your work as you go.

For example:

```
ceph orch apply mon host1
ceph orch apply mon host2
ceph orch apply mon host3
```

This results in only one host having a monitor applied to it: host 3.

(The first command creates a monitor on host1. Then the second command clobbers the monitor on host1 and creates a monitor on host2. Then the third command clobbers the monitor on host2 and creates a monitor on host3. In this scenario, at this point, there is a monitor ONLY on host3.)

To make certain that a monitor is applied to each of these three hosts, run a command like this:

```
ceph orch apply mon "host1,host2,host3"
```

There is another way to apply monitors to multiple hosts: a `yaml` file can be used. Instead of using the “ceph orch apply mon” commands, run a command of this form:

```
ceph orch apply -i file.yaml
```

Here is a sample **file.yaml** file

```
service_type: mon
placement:
  hosts:
   - host1
   - host2
   - host3
```

### Explicit placements

Daemons can be explicitly placed on hosts by simply specifying them:

> ```
> ceph orch apply prometheus --placement="host1 host2 host3"
> ```

Or in YAML:

```
service_type: prometheus
placement:
  hosts:
    - host1
    - host2
    - host3
```

MONs and other services may require some enhanced network specifications:

> ```
> ceph orch daemon add mon --placement="myhost:[v2:1.2.3.4:3300,v1:1.2.3.4:6789]=name"
> ```

where `[v2:1.2.3.4:3300,v1:1.2.3.4:6789]` is the network address of the monitor and `=name` specifies the name of the new monitor.



### Placement by labels

Daemon placement can be limited to hosts that match a specific label. To set a label `mylabel` to the appropriate hosts, run this command:

> ```
> ceph orch host label add *<hostname>* mylabel
> ```
>
> To view the current hosts and labels, run this command:
>
> ```
> ceph orch host ls
> ```
>
> For example:
>
> ```
> ceph orch host label add host1 mylabel
> ceph orch host label add host2 mylabel
> ceph orch host label add host3 mylabel
> ceph orch host ls
> HOST   ADDR   LABELS  STATUS
> host1         mylabel
> host2         mylabel
> host3         mylabel
> host4
> host5
> ```

Now, Tell cephadm to deploy daemons based on the label by running this command:

> ```
> ceph orch apply prometheus --placement="label:mylabel"
> ```

Or in YAML:

```
service_type: prometheus
placement:
  label: "mylabel"
```

- See [Host labels](https://docs.ceph.com/en/latest/cephadm/host-management/#orchestrator-host-labels)

### Placement by pattern matching

Daemons can be placed on hosts as well:

> ```
> ceph orch apply prometheus --placement='myhost[1-3]'
> ```

Or in YAML:

```
service_type: prometheus
placement:
  host_pattern: "myhost[1-3]"
```

To place a service on *all* hosts, use `"*"`:

> ```
> ceph orch apply node-exporter --placement='*'
> ```

Or in YAML:

```
service_type: node-exporter
placement:
  host_pattern: "*"
```

### Changing the number of daemons

By specifying `count`, only the number of daemons specified will be created:

> ```
> ceph orch apply prometheus --placement=3
> ```

To deploy *daemons* on a subset of hosts, specify the count:

> ```
> ceph orch apply prometheus --placement="2 host1 host2 host3"
> ```

If the count is bigger than the amount of hosts, cephadm deploys one per host:

> ```
> ceph orch apply prometheus --placement="3 host1 host2"
> ```

The command immediately above results in two Prometheus daemons.

YAML can also be used to specify limits, in the following way:

```
service_type: prometheus
placement:
  count: 3
```

YAML can also be used to specify limits on hosts:

```
service_type: prometheus
placement:
  count: 2
  hosts:
    - host1
    - host2
    - host3
```

### Co-location of daemons

Cephadm supports the deployment of multiple daemons on the same host:

```
service_type: rgw
placement:
  label: rgw
  count-per-host: 2
```

The main reason for deploying multiple daemons per host is an additional performance benefit for running multiple RGW and MDS daemons on the same host.

See also:

- [Allow co-location of MGR daemons](https://docs.ceph.com/en/latest/cephadm/services/mgr/#cephadm-mgr-co-location).
- [Designated gateways](https://docs.ceph.com/en/latest/cephadm/services/rgw/#cephadm-rgw-designated-gateways).

This feature was introduced in Pacific.

### Algorithm description

Cephadm’s declarative state consists of a list of service specifications containing placement specifications.

Cephadm continually compares a list of daemons actually running in the cluster against the list in the service specifications. Cephadm adds new daemons and removes old daemons as necessary in order to conform to the service specifications.

Cephadm does the following to maintain compliance with the service specifications.

Cephadm first selects a list of candidate hosts. Cephadm seeks explicit host names and selects them. If cephadm finds no explicit host names, it looks for label specifications. If no label is defined in the specification, cephadm selects hosts based on a host pattern. If no host pattern is defined, as a last resort, cephadm selects all known hosts as candidates.

Cephadm is aware of existing daemons running services and tries to avoid moving them.

Cephadm supports the deployment of a specific amount of services. Consider the following service specification:

```
service_type: mds
service_name: myfs
placement:
  count: 3
  label: myfs
```

This service specification instructs cephadm to deploy three daemons on hosts labeled `myfs` across the cluster.

If there are fewer than three daemons deployed on the candidate hosts, cephadm randomly chooses hosts on which to deploy new daemons.

If there are more than three daemons deployed on the candidate hosts, cephadm removes existing daemons.

Finally, cephadm removes daemons on hosts that are outside of the list of candidate hosts.

Note

There is a special case that cephadm must consider.

If there are fewer hosts selected by the placement specification than demanded by `count`, cephadm will deploy only on the selected hosts.

## Extra Container Arguments

Warning

The arguments provided for extra container args are limited to whatever arguments are available for a run command from whichever container engine you are using. Providing any arguments the run command does not support (or invalid values for arguments) will cause the daemon to fail to start.

Cephadm supports providing extra miscellaneous container arguments for specific cases when they may be necessary. For example, if a user needed to limit the amount of cpus their mon daemons make use of they could apply a spec like

```
service_type: mon
service_name: mon
placement:
  hosts:
    - host1
    - host2
    - host3
extra_container_args:
  -  "--cpus=2"
```

which would cause each mon daemon to be deployed with –cpus=2.

## Removing a Service

In order to remove a service including the removal of all daemons of that service, run

```
ceph orch rm <service-name>
```

For example:

```
ceph orch rm rgw.myrgw
```

## Disabling automatic deployment of daemons

Cephadm supports disabling the automated deployment and removal of daemons on a per service basis. The CLI supports two commands for this.

In order to fully remove a service, see [Removing a Service](https://docs.ceph.com/en/latest/cephadm/services/#orch-rm).

### Disabling automatic management of daemons

To disable the automatic management of dameons, set `unmanaged=True` in the [Service Specification](https://docs.ceph.com/en/latest/cephadm/services/#orchestrator-cli-service-spec) (`mgr.yaml`).

`mgr.yaml`:

```
service_type: mgr
unmanaged: true
placement:
  label: mgr
ceph orch apply -i mgr.yaml
```

Note

After you apply this change in the Service Specification, cephadm will no longer deploy any new daemons (even if the placement specification matches additional hosts).

### Deploying a daemon on a host manually

Note

This workflow has a very limited use case and should only be used in rare circumstances.

To manually deploy a daemon on a host, follow these steps:

Modify the service spec for a service by getting the existing spec, adding `unmanaged: true`, and applying the modified spec.

Then manually deploy the daemon using the following:

> ```
> ceph orch daemon add <daemon-type>  --placement=<placement spec>
> ```

For example :

> ```
> ceph orch daemon add mgr --placement=my_host
> ```

Note

Removing `unmanaged: true` from the service spec will enable the reconciliation loop for this service and will potentially lead to the removal of the daemon, depending on the placement spec.

### Removing a daemon from a host manually

To manually remove a daemon, run a command of the following form:

> ```
> ceph orch daemon rm <daemon name>... [--force]
> ```

For example:

> ```
> ceph orch daemon rm mgr.my_host.xyzxyz
> ```

Note

For managed services (`unmanaged=False`), cephadm will automatically deploy a new daemon a few seconds later.

### See also

- See [Declarative State](https://docs.ceph.com/en/latest/cephadm/services/osd/#cephadm-osd-declarative) for special handling of unmanaged OSDs.
- See also [Pausing or disabling cephadm](https://docs.ceph.com/en/latest/cephadm/troubleshooting/#cephadm-pause)

## Custom Container Service

The orchestrator enables custom containers to be deployed using a YAML file. A corresponding [Service Specification](https://docs.ceph.com/en/latest/cephadm/services/#orchestrator-cli-service-spec) must look like:

```yaml
service_type: container
service_id: foo
placement:
    ...
spec:
  image: docker.io/library/foo:latest
  entrypoint: /usr/bin/foo
  uid: 1000
  gid: 1000
  args:
    - "--net=host"
    - "--cpus=2"
  ports:
    - 8080
    - 8443
  envs:
    - SECRET=mypassword
    - PORT=8080
    - PUID=1000
    - PGID=1000
  volume_mounts:
    CONFIG_DIR: /etc/foo
  bind_mounts:
    - ['type=bind', 'source=lib/modules', 'destination=/lib/modules', 'ro=true']
  dirs:
    - CONFIG_DIR
  files:
    CONFIG_DIR/foo.conf:
      - refresh=true
      - username=xyz
      - "port: 1234"
```

where the properties of a service specification are:

- `service_id`

  A unique name of the service.

- `image`

  The name of the Docker image.

- `uid`

  The UID to use when creating directories and files in the host system.

- `gid`

  The GID to use when creating directories and files in the host system.

- `entrypoint`

  Overwrite the default ENTRYPOINT of the image.

- `args`

  A list of additional Podman/Docker command line arguments.

- `ports`

  A list of TCP ports to open in the host firewall.

- `envs`

  A list of environment variables.

- `bind_mounts`

  When you use a bind mount, a file or directory on the host machine is mounted into the container. Relative source=… paths will be located below /var/lib/ceph/<cluster-fsid>/<daemon-name>.

- `volume_mounts`

  When you use a volume mount, a new directory is created within Docker’s storage directory on the host machine, and Docker manages that directory’s contents. Relative source paths will be located below /var/lib/ceph/<cluster-fsid>/<daemon-name>.

- `dirs`

  A list of directories that are created below /var/lib/ceph/<cluster-fsid>/<daemon-name>.

- `files`

  A dictionary, where the key is the relative path of the file and the value the file content. The content must be double quoted when using a string. Use ‘\n’ for line breaks in that case. Otherwise define multi-line content as list of strings. The given files will be created below the directory /var/lib/ceph/<cluster-fsid>/<daemon-name>. The absolute path of the directory where the file will be created must exist. Use the dirs property to create them if necessary.