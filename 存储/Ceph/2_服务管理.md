# 服务管理

[TOC]

A service is a group of daemons that are configured together.服务是一起配置的一组守护进程。

## 服务状态

要查看Ceph 集群中运行的某个服务的状态，请执行以下操作：

1. 使用命令行打印服务列表。
2. 找到要检查其状态的服务。
3. 打印服务状态。

以下命令打印编排器已知的服务列表。要将输出限制为指定主机上的服务，请使用可选的 `--host` 参数。要将输出限制为仅特定类型的服务，请使用可选的 `--type` 参数（mon、osd、mgr、mds、rgw）：

```bash
ceph orch ls [--service_type type] [--service_name name] [--export] [--format f] [--refresh]
```

发现特定服务或守护程序的状态：

```bash
ceph orch ls --service_type type --service_name <name> [--refresh]
```

To export the service specifications knows to the orchestrator, 要将服务规范知识导出到编排器：

```bash
ceph orch ls --export
```

使用此命令导出的服务规范将导出为 yaml ，可以与 `ceph orch apply -i` 命令一起使用。

## 守护进程状态

守护程序是一个正在运行的系统单元，是服务的一部分。

要查看守护程序的状态，请执行以下操作：

1. 打印编排器已知的所有守护程序的列表。
2. 查询目标守护程序的状态。

打印编排器已知的所有守护进程的列表：

```bash
ceph orch ps [--hostname host] [--daemon_type type] [--service_name name] [--daemon_id id] [--format f] [--refresh]
```

查询特定服务实例（mon、osd、mds、rgw）的状态。对于 OSD，id 是 numeric OSD ID 。对于MDS 服务，id 是文件系统名称：

```bash
ceph orch ps --daemon_type osd --daemon_id 0
```

> **Note：**
>
> 命令 `ceph orch ps` 的输出可能无法反映后台进程的当前状态。默认情况下，状态每 10 分钟更新一次。可以通过修改 `mgr/cephadm/daemon_cache_timeout` 配置变量（以秒为单位）来缩短此时间间隔。例如： `ceph config set mgr mgr/cephadm/daemon_cache_timeout 60` 将刷新时间间隔缩短为一分钟。除非使用 `--refresh` 选项，否则信息将在每个 `daemon_cache_timeout` 秒更新一次。此选项将触发刷新信息的请求，这可能需要一些时间，具体取决于集群的大小。通常，`REFRESHED` 值表示 `ceph orch ps` 和类似命令显示的信息的更新程度。

## Service Specification

服务规范是用于指定服务部署的数据结构。除了 placement 或 network 等参数之外，用户还可以通过 config 部分设置服务配置参数的初始值。对于每个参数 / 值配置对，cephadm 调用以下命令来设置其值：

```bash
ceph config set <service-name> <param> <value>
```

如果在规范中发现无效的配置参数（CEPHADM_INVALID_CONFIG_OPTION），或者在尝试应用新的配置选项（CEPHADM_FAILED_SET_OPTION）时出现任何错误，cephadm 会引发健康警告。

以下是服务规范示例：

```yaml
service_type: rgw
service_id: realm.zone
placement:
  hosts:
    - host1
    - host2
    - host3
config:
  param_1: val_1
  ...
  param_N: val_N
unmanaged: false
networks:
- 192.169.142.0/24
spec:
  # Additional service specific attributes.
```

在此示例中，此服务规范的属性为：

```c++
class ceph.deployment.service_spec.ServiceSpec(service_type, service_id=None, placement=None, count=None, config=None, unmanaged=False, preview_only=False, networks=None, extra_container_args=None, custom_configs=None)
```

服务创建的详细信息。

向编排器请求一组守护程序，如 MDS、RGW、iscsi 网关、MON、MGR、Prometheus 。

这个结构应该有足够的信息来启动服务。

- `networks: List[str]`

  一个网络标识列表，指示守护进程仅绑定在该列表中的特定网络上。如果集群分布在多个网络上，则可以添加多个网络。

- `placement:ceph.deployment.service_spec.PlacementSpec`

  查看 [Daemon Placement](https://docs.ceph.com/en/latest/cephadm/services/#orchestrator-cli-placement-spec).

- `service_type`

  服务的类型。需要是 Ceph 服务（`mon`、`crash`、`mds`、`mgr`、`osd` 或 `rbd-mirror`）、网关（`nfs` 或 `rgw`）、监视堆栈的一部分（`alertmanager`、`grafana`、`node-exporter` 或 `prometheus`）或自定义容器的（`container`）。

- `service_id`

  服务的名称。被 `iscsi`, `mds`, `nfs`, `osd`, `rgw`, `container`, `ingress` 需要。

- `unmanaged`

  如果设置为 `true`，则编排器将不会部署或删除与此服务关联的任何守护程序。放置和所有其他属性将被忽略。如果暂时不想管理此服务，这很有用。

每种服务类型都可以具有其他特定于服务的属性。

`mon`、`mgr` 和监视类型的服务规范不需要 `service_id` 。

许多服务规范可以使用 `ceph orch apply -i` 命令通过提交一个 multi-document YAML 文件同时应用：

```yaml
cat <<EOF | ceph orch apply -i -
service_type: mon
placement:
  host_pattern: "mon*"
---
service_type: mgr
placement:
  host_pattern: "mgr*"
---
service_type: osd
service_id: default_drive_group
placement:
  host_pattern: "osd*"
data_devices:
  all: true
EOF
```

### 检索正在运行的服务规范

如果服务是通过 `ceph orch apply...` 启动的，那么直接更改服务规范是很复杂的。建议按照以下说明导出正在运行的服务规范，而不是尝试直接更改服务规范：

```bash
ceph orch ls --service-name rgw.<realm>.<zone> --export > rgw.<realm>.<zone>.yaml
ceph orch ls --service-type mgr --export > mgr.yaml
ceph orch ls --export > cluster.yaml
```

然后，可以按照上述方式更改和重新应用本规范。

### 更新服务规范

Ceph Orchestrator 在 `ServiceSpec` 中维护每个服务的声明性状态。对于某些操作，如更新 RGW HTTP 端口，我们需要更新现有规范。

1. 列出当前的 `ServiceSpec` :

   ```bash
   ceph orch ls --service_name=<service-name> --export > myservice.yaml
   ```

2. 更新 yaml 文件:

   ```bash
   vi myservice.yaml
   ```

3. 应用新的 `ServiceSpec`:

   ```bash
   ceph orch apply -i myservice.yaml [--dry-run]
   ```

## 守护程序放置

为了让编排器部署服务，它需要知道在哪里部署守护进程，以及部署多少守护进程。这是放置规范的作用。放置规范可以作为命令行参数传递，也可以在 YAML 文件中传递。

> **Note:**
>
> cephadm 不会在带有 `_no_schedule` 标签的主机上部署守护进程。

> **Note:**
>
> **apply** 命令可能会令人困惑。因此，我们建议使用 YAML 规范。
>
> 每个 `ceph orch apply <service-name>` 命令将取代之前的命令。如果你不使用正确的语法，你会在工作中出错。
>
> 例如：
>
> ```bash
> ceph orch apply mon host1
> ceph orch apply mon host2
> ceph orch apply mon host3
> ```
>
> 最终只有一个主机应用了 MON ：host3 。（第一个命令在 host1 上创建一个 MON 。然后第二个命令在 host1 上删除 MON ，并在 host2 上创建 MON 。然后，第三个命令删除 host2 上的 MON，并创建 host3 上的MON 。在这种情况下，此时只有 host3 上有 MON 。）
>
> 要确保 MON 应用于这三个主机中的每一个，请运行如下命令：
>
> ```bash
> ceph orch apply mon "host1,host2,host3"
> ```

另一种方法可以将 MON 应用于多个主机：使用 yaml 文件。不要使用 `ceph orch apply mon` 命令，而是运行以下形式的命令：

```bash
ceph orch apply -i file.yaml
```

样例文件 **file.yaml** ：

```yaml
service_type: mon
placement:
  hosts:
   - host1
   - host2
   - host3
```

### 显式布局

可以通过简单地指定主机来显式地将守护进程放置在主机上：

```bash
ceph orch apply prometheus --placement="host1 host2 host3"
```

或者在一个 YAML 文件中：

```yaml
service_type: prometheus
placement:
  hosts:
    - host1
    - host2
    - host3
```

MON 和其他服务可能需要一些增强的网络规范：

```bash
ceph orch daemon add mon --placement="myhost:[v2:1.2.3.4:3300,v1:1.2.3.4:6789]=name"
```

其中 `[v2:1.2.3.4:3300,v1:1.2.3.4:6789]` 是 MON 的网络地址， `=name` 指定新 MON 的名称。

### 按标签放置

守护程序的放置可以限于与特定标签匹配的主机。

现在，通过运行以下命令，告诉 cephadm 根据标签部署守护进程：

```bash
ceph orch apply prometheus --placement="label:mylabel"
```

或者使用一个 YAML 文件：

```yaml
service_type: prometheus
placement:
  label: "mylabel"
```

### Placement by pattern matching

守护程序也可以放置在主机上：

```bash
ceph orch apply prometheus --placement='myhost[1-3]'
```



```yaml
service_type: prometheus
placement:
  host_pattern: "myhost[1-3]"
```

要在所有主机上放置服务，使用 `*`：

```bash
ceph orch apply node-exporter --placement='*'
```

或者使用一个 YAML 文件：

```yaml
service_type: node-exporter
placement:
  host_pattern: "*"
```

### 更改守护程序的数量

通过指定 `count` ，将只创建指定数量的守护进程：

```bash
ceph orch apply prometheus --placement=3
```

要在主机子集上部署守护程序，请指定计数：

```bash
ceph orch apply prometheus --placement="2 host1 host2 host3"
```

如果计数大于主机数量，cephadm 将为每个主机部署一个：

```bash
ceph orch apply prometheus --placement="3 host1 host2"
```

上面的命令会导致部署两个 Prometheus 守护进程。

YAML 也可用于指定限制，具体方式如下：

```yaml
service_type: prometheus
placement:
  count: 3
```

YAML 还可用于指定主机的限制：

```yaml
service_type: prometheus
placement:
  count: 2
  hosts:
    - host1
    - host2
    - host3
```

### Co-location of daemons

Cephadm 支持在同一主机上部署多个守护程序：

```yaml
service_type: rgw
placement:
  label: rgw
  count-per-host: 2
```

每个主机部署多个守护程序的主要原因是在同一主机上运行多个 RGW 和 MDS 守护程序会带来额外的性能优势。

参考：

- [Allow co-location of MGR daemons](https://docs.ceph.com/en/latest/cephadm/services/mgr/#cephadm-mgr-co-location).
- [Designated gateways](https://docs.ceph.com/en/latest/cephadm/services/rgw/#cephadm-rgw-designated-gateways).

这项功能是在 Pacific 版本推出的。

### 算法描述

Cephadm 的声明性状态由包含放置规范的服务规范列表组成。

Cephadm 不断将集群中实际运行的守护进程列表与服务规范中的列表进行比较。Cephadm 根据需要添加新的守护程序并删除旧的守护程序，以符合服务规范。

Cephadm 执行以下操作以保持符合服务规范：

Cephadm 首先选择候选主机列表。Cephadm 查找显式主机名并选择它们。如果 cephadm 没有找到明确的主机名，它会查找标签规范。如果规范中没有定义标签，cephadm 将根据主机模式选择主机。如果没有定义主机模式，作为最后的手段，cephadm 将选择所有已知的主机作为候选。

Cephadm 知道运行服务的现有守护进程，并试图避免移动它们。

Cephadm 支持部署特定数量的服务。考虑以下服务规范：

```yaml
service_type: mds
service_name: myfs
placement:
  count: 3
  label: myfs
```

该服务规范指示 cephadm 在集群中标记为 `myfs` 的主机上部署三个守护进程。

如果候选主机上部署的守护程序少于三个，cephadm 会随机选择要在其上部署新守护程序的主机。

如果候选主机上部署了三个以上的守护程序，cephadm 将删除现有的守护程序。

最后，cephadm 删除候选主机列表之外的主机上的守护进程。

> **Note：**
>
> cephadm 必须考虑一个特殊情况。如果放置规范选择的主机少于计数要求的主机，则 cephadm 将仅部署在选定的主机上。
>

## 额外容器参数

> **Warning：**
>
> The arguments provided for extra container args are limited to whatever arguments are available for a run command from whichever container engine you are using. Providing any arguments the run command does not support (or invalid values for arguments)
>
> 为额外的容器参数提供的参数仅限于您使用的任何容器引擎中的运行命令可用的任何参数。提供run命令不支持的任何参数（或参数值无效）将导致守护程序无法启动。

Cephadm 支持在必要时为特定情况提供额外的各种容器参数。例如，如果用户需要限制其 mon 守护进程使用的 cpu 数量，则可以应用如下规范

```yaml
service_type: mon
service_name: mon
placement:
  hosts:
    - host1
    - host2
    - host3
extra_container_args:
  -  "--cpus=2"
```

这将导致每个 mon 守护进程使用 `--cpus=2` 参数部署。

### 装载带有额外容器参数的文件

A common use case for extra container arguments is to mount additional files within the container. 额外容器参数的一个常见用例是在容器中装载其他文件。The recommended syntax for mounting a file with extra container arguments is:

然而，some intuitive formats for doing so can cause deployment to fail这样做的一些直观格式可能会导致部署失败（请参见 https://tracker.ceph.com/issues/57338 )。挂载带有额外容器参数的文件的建议语法是：

```yaml
extra_container_args:
  - "-v"
  - "/absolute/file/path/on/host:/absolute/file/path/in/container"
```

例如：

```yaml
extra_container_args:
  - "-v"
  - "/opt/ceph_cert/host.cert:/etc/grafana/certs/cert_file:ro"
```

### 额外入口点参数

与容器运行时的额外容器参数类似，Cephadm 支持附加到传递给容器内运行的入口点进程的参数。例如，要为 node-exporter 服务设置收集器文本文件目录，可以应用如下服务规范：

```yaml
service_type: node-exporter
service_name: node-exporter
placement:
  host_pattern: '*'
extra_entrypoint_args:
  - "--collector.textfile.directory=/var/lib/node_exporter/textfile_collector2"
```

## 自定义配置文件

Cephadm 支持为守护程序指定其他配置文件。要做到这一点，用户必须提供配置文件的内容和守护程序容器中应该挂载它的位置。在应用指定了自定义配置文件的 YAML 规范并让 cephadm 重新部署为其指定了配置文件的守护程序之后，这些文件将挂载在守护程序容器中的指定位置。

服务规范示例：

```yaml
service_type: grafana
service_name: grafana
custom_configs:
  - mount_path: /etc/example.conf
    content: |
      setting1 = value1
      setting2 = value2
  - mount_path: /usr/share/grafana/example.cert
    content: |
      -----BEGIN PRIVATE KEY-----
      V2VyIGRhcyBsaWVzdCBpc3QgZG9vZi4gTG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFt
      ZXQsIGNvbnNldGV0dXIgc2FkaXBzY2luZyBlbGl0ciwgc2VkIGRpYW0gbm9udW15
      IGVpcm1vZCB0ZW1wb3IgaW52aWR1bnQgdXQgbGFib3JlIGV0IGRvbG9yZSBtYWdu
      YSBhbGlxdXlhbSBlcmF0LCBzZWQgZGlhbSB2b2x1cHR1YS4gQXQgdmVybyBlb3Mg
      ZXQgYWNjdXNhbSBldCBqdXN0byBkdW8=
      -----END PRIVATE KEY-----
      -----BEGIN CERTIFICATE-----
      V2VyIGRhcyBsaWVzdCBpc3QgZG9vZi4gTG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFt
      ZXQsIGNvbnNldGV0dXIgc2FkaXBzY2luZyBlbGl0ciwgc2VkIGRpYW0gbm9udW15
      IGVpcm1vZCB0ZW1wb3IgaW52aWR1bnQgdXQgbGFib3JlIGV0IGRvbG9yZSBtYWdu
      YSBhbGlxdXlhbSBlcmF0LCBzZWQgZGlhbSB2b2x1cHR1YS4gQXQgdmVybyBlb3Mg
      ZXQgYWNjdXNhbSBldCBqdXN0byBkdW8=
      -----END CERTIFICATE-----
```

要使这些新的配置文件真正装入后台进程的容器中：

```bash
ceph orch redeploy <service-name>
```

例如：

```bash
ceph orch redeploy grafana
```

## 删除服务

要删除服务，包括删除该服务的所有守护程序，请运行：

```bash
ceph orch rm <service-name>
```

例如：

```bash
ceph orch rm rgw.myrgw
```

## 禁用守护进程的自动部署

Cephadm 支持基于每个服务禁用后台进程的自动部署和删除。CLI 为此支持两个命令。

### 禁用守护进程的自动管理

要禁用 damon 的自动管理，请在服务规范（`mgr.yaml`）中设置 `unmanaged=True` 。

```yaml
service_type: mgr
unmanaged: true
placement:
  label: mgr
```

```bash
ceph orch apply -i mgr.yaml
```

> **Note：**
>
> 在服务规范中应用此更改后，cephadm 将不再部署任何新的守护程序（即使放置规范与其他主机匹配）。

### 在主机上手动部署守护程序

> **Note：**
>
> 此工作流的用例非常有限，只能在极少数情况下使用。

要在主机上手动部署守护程序，请执行以下步骤：

通过获取现有规范、添加 `unmanaged: true` ，并应用修改后的规范来修改服务。

然后使用以下方法手动部署守护程序：

```bash
ceph orch daemon add <daemon-type>  --placement=<placement spec>
```

例如：

```bash
ceph orch daemon add mgr --placement=my_host
```

> **Note：**
>
> 从服务规范中删除 `unmanaged: true` 将启用此服务的the reconciliation loop 协调循环，并可能导致删除守护程序，具体取决于放置规范。

### 手动从主机中删除守护程序

要手动删除守护程序，请运行以下形式的命令：

```bash
ceph orch daemon rm <daemon name>... [--force]
```

例如：

```bash
ceph orch daemon rm mgr.my_host.xyzxyz
```

> Note
>
> 对于受托管的服务 (`unmanaged=False`)，cephadm 将在几秒钟后自动部署新的守护进程。
