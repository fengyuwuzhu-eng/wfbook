# Ceph file system client eviction[](https://docs.ceph.com/en/latest/cephfs/eviction/#ceph-file-system-client-eviction)

When a file system client is unresponsive or otherwise misbehaving, it may be necessary to forcibly terminate its access to the file system.  This process is called *eviction*.

Evicting a CephFS client prevents it from communicating further with MDS daemons and OSD daemons.  If a client was doing buffered IO to the file system, any un-flushed data will be lost.

Clients may either be evicted automatically (if they fail to communicate promptly with the MDS), or manually (by the system administrator).

The client eviction process applies to clients of all kinds, this includes FUSE mounts, kernel mounts, nfs-ganesha gateways, and any process using libcephfs.

## Automatic client eviction[](https://docs.ceph.com/en/latest/cephfs/eviction/#automatic-client-eviction)

There are three situations in which a client may be evicted automatically.

1. On an active MDS daemon, if a client has not communicated with the MDS for over `session_autoclose` (a file system variable) seconds (300 seconds by default), then it will be evicted automatically.
2. On an active MDS daemon, if a client has not responded to cap revoke messages for over `mds_cap_revoke_eviction_timeout` (configuration option) seconds. This is disabled by default.
3. During MDS startup (including on failover), the MDS passes through a state called `reconnect`.  During this state, it waits for all the clients to connect to the new MDS daemon.  If any clients fail to do so within the time window (`mds_reconnect_timeout`, 45 seconds by default) then they will be evicted.

A warning message is sent to the cluster log if either of these situations arises.

## Manual client eviction[](https://docs.ceph.com/en/latest/cephfs/eviction/#manual-client-eviction)

Sometimes, the administrator may want to evict a client manually.  This could happen if a client has died and the administrator does not want to wait for its session to time out, or it could happen if a client is misbehaving and the administrator does not have access to the client node to unmount it.

It is useful to inspect the list of clients first:

```
ceph tell mds.0 client ls

[
    {
        "id": 4305,
        "num_leases": 0,
        "num_caps": 3,
        "state": "open",
        "replay_requests": 0,
        "completed_requests": 0,
        "reconnecting": false,
        "inst": "client.4305 172.21.9.34:0/422650892",
        "client_metadata": {
            "ceph_sha1": "ae81e49d369875ac8b569ff3e3c456a31b8f3af5",
            "ceph_version": "ceph version 12.0.0-1934-gae81e49 (ae81e49d369875ac8b569ff3e3c456a31b8f3af5)",
            "entity_id": "0",
            "hostname": "senta04",
            "mount_point": "/tmp/tmpcMpF1b/mnt.0",
            "pid": "29377",
            "root": "/"
        }
    }
]
```

Once you have identified the client you want to evict, you can do that using its unique ID, or various other attributes to identify it:

```
# These all work
ceph tell mds.0 client evict id=4305
ceph tell mds.0 client evict client_metadata.=4305
```

## Advanced: Un-blocklisting a client[](https://docs.ceph.com/en/latest/cephfs/eviction/#advanced-un-blocklisting-a-client)

Ordinarily, a blocklisted client may not reconnect to the servers: it must be unmounted and then mounted anew.

However, in some situations it may be useful to permit a client that was evicted to attempt to reconnect.

Because CephFS uses the RADOS OSD blocklist to control client eviction, CephFS clients can be permitted to reconnect by removing them from the blocklist:

```
$ ceph osd blocklist ls
listed 1 entries
127.0.0.1:0/3710147553 2018-03-19 11:32:24.716146
$ ceph osd blocklist rm 127.0.0.1:0/3710147553
un-blocklisting 127.0.0.1:0/3710147553
```

Doing this may put data integrity at risk if other clients have accessed files that the blocklisted client was doing buffered IO to.  It is also not guaranteed to result in a fully functional client -- the best way to get a fully healthy client back after an eviction is to unmount the client and do a fresh mount.

If you are trying to reconnect clients in this way, you may also find it useful to set `client_reconnect_stale` to true in the FUSE client, to prompt the client to try to reconnect.

## Advanced: Configuring blocklisting[](https://docs.ceph.com/en/latest/cephfs/eviction/#advanced-configuring-blocklisting)

If you are experiencing frequent client evictions, due to slow client hosts or an unreliable network, and you cannot fix the underlying issue, then you may want to ask the MDS to be less strict.

It is possible to respond to slow clients by simply dropping their MDS sessions, but permit them to re-open sessions and permit them to continue talking to OSDs.  To enable this mode, set `mds_session_blocklist_on_timeout` to false on your MDS nodes.

For the equivalent behaviour on manual evictions, set `mds_session_blocklist_on_evict` to false.

Note that if blocklisting is disabled, then evicting a client will only have an effect on the MDS you send the command to.  On a system with multiple active MDS daemons, you would need to send an eviction command to each active daemon.  When blocklisting is enabled (the default), sending an eviction command to just a single MDS is sufficient, because the blocklist propagates it to the others.



## Background: Blocklisting and OSD epoch barrier[](https://docs.ceph.com/en/latest/cephfs/eviction/#background-blocklisting-and-osd-epoch-barrier)

After a client is blocklisted, it is necessary to make sure that other clients and MDS daemons have the latest OSDMap (including the blocklist entry) before they try to access any data objects that the blocklisted client might have been accessing.

This is ensured using an internal “osdmap epoch barrier” mechanism.

The purpose of the barrier is to ensure that when we hand out any capabilities which might allow touching the same RADOS objects, the clients we hand out the capabilities to must have a sufficiently recent OSD map to not race with cancelled operations (from ENOSPC) or blocklisted clients (from evictions).

More specifically, the cases where an epoch barrier is set are:

> - Client eviction (where the client is blocklisted and other clients must wait for a post-blocklist epoch to touch the same objects).
> - OSD map full flag handling in the client (where the client may cancel some OSD ops from a pre-full epoch, so other clients must wait until the full epoch or later before touching the same objects).
> - MDS startup, because we don’t persist the barrier epoch, so must assume that latest OSD map is always required after a restart.

Note that this is a global value for simplicity. We could maintain this on a per-inode basis. But we don’t, because:

> - It would be more complicated.
> - It would use an extra 4 bytes of memory for every inode.
> - It would not be much more efficient as, almost always, everyone has the latest OSD map. And, in most cases everyone will breeze through this barrier rather than waiting.
> - This barrier is done in very rare cases, so any benefit from per-inode granularity would only very rarely be seen.

The epoch barrier is transmitted along with all capability messages, and instructs the receiver of the message to avoid sending any more RADOS operations to OSDs until it has seen this OSD epoch.  This mainly applies to clients (doing their data writes directly to files), but also applies to the MDS because things like file size probing and file deletion are done directly from the MDS.



# Ceph File System Scrub[](https://docs.ceph.com/en/latest/cephfs/scrub/#ceph-file-system-scrub)

CephFS provides the cluster admin (operator) to check consistency of a file system via a set of scrub commands. Scrub can be classified into two parts:

1. Forward Scrub: In which the scrub operation starts at the root of the file system (or a sub directory) and looks at everything that can be touched in the hierarchy to ensure consistency.
2. Backward Scrub: In which the scrub operation looks at every RADOS object in the file system pools and maps it back to the file system hierarchy.

This document details commands to initiate and control forward scrub (referred as scrub thereafter).

Warning

CephFS forward scrubs are started and manipulated on rank 0. All scrub commands must be directed at rank 0.

## Initiate File System Scrub[](https://docs.ceph.com/en/latest/cephfs/scrub/#initiate-file-system-scrub)

To start a scrub operation for a directory tree use the following command:

```
ceph tell mds.<fsname>:0 scrub start <path> [scrubopts] [tag]
```

where `scrubopts` is a comma delimited list of `recursive`, `force`, or `repair` and `tag` is an optional custom string tag (the default is a generated UUID). An example command is:

```
ceph tell mds.cephfs:0 scrub start / recursive
{
    "return_code": 0,
    "scrub_tag": "6f0d204c-6cfd-4300-9e02-73f382fd23c1",
    "mode": "asynchronous"
}
```

Recursive scrub is asynchronous (as hinted by mode in the output above). Asynchronous scrubs must be polled using `scrub status` to determine the status.

The scrub tag is used to differentiate scrubs and also to mark each inode’s first data object in the default data pool (where the backtrace information is stored) with a `scrub_tag` extended attribute with the value of the tag. You can verify an inode was scrubbed by looking at the extended attribute using the RADOS utilities.

Scrubs work for multiple active MDS (multiple ranks). The scrub is managed by rank 0 and distributed across MDS as appropriate.

## Monitor (ongoing) File System Scrubs[](https://docs.ceph.com/en/latest/cephfs/scrub/#monitor-ongoing-file-system-scrubs)

Status of ongoing scrubs can be monitored and polled using in scrub status command. This commands lists out ongoing scrubs (identified by the tag) along with the path and options used to initiate the scrub:

```
ceph tell mds.cephfs:0 scrub status
{
    "status": "scrub active (85 inodes in the stack)",
    "scrubs": {
        "6f0d204c-6cfd-4300-9e02-73f382fd23c1": {
            "path": "/",
            "options": "recursive"
        }
    }
}
```

status shows the number of inodes that are scheduled to be scrubbed at any point in time, hence, can change on subsequent scrub status invocations. Also, a high level summary of scrub operation (which includes the operation state and paths on which scrub is triggered) gets displayed in ceph status:

```
ceph status
[...]

task status:
  scrub status:
      mds.0: active [paths:/]

[...]
```

A scrub is complete when it no longer shows up in this list (although that may change in future releases). Any damage will be reported via cluster health warnings.

## Control (ongoing) File System Scrubs[](https://docs.ceph.com/en/latest/cephfs/scrub/#control-ongoing-file-system-scrubs)

- Pause: Pausing ongoing scrub operations results in no new or pending inodes being scrubbed after in-flight RADOS ops (for the inodes that are currently being scrubbed) finish:

  ```
  ceph tell mds.cephfs:0 scrub pause
  {
      "return_code": 0
  }
  ```

  The `scrub status` after pausing reflects the paused state. At this point, initiating new scrub operations (via `scrub start`) would just queue the inode for scrub:

  ```
  ceph tell mds.cephfs:0 scrub status
  {
      "status": "PAUSED (66 inodes in the stack)",
      "scrubs": {
          "6f0d204c-6cfd-4300-9e02-73f382fd23c1": {
              "path": "/",
              "options": "recursive"
          }
      }
  }
  ```

- Resume: Resuming kick starts a paused scrub operation:

  ```
  ceph tell mds.cephfs:0 scrub resume
  {
      "return_code": 0
  }
  ```

- Abort: Aborting ongoing scrub operations removes pending inodes from the scrub queue (thereby aborting the scrub) after in-flight RADOS ops (for the inodes that are currently being scrubbed) finish:

  ```
  ceph tell mds.cephfs:0 scrub abort
  {
      "return_code": 0
  }
  ```

## Damages[](https://docs.ceph.com/en/latest/cephfs/scrub/#damages)

The types of damage that can be reported and repaired by File System Scrub are:

- DENTRY : Inode’s dentry is missing.
- DIR_FRAG : Inode’s directory fragment(s) is missing.
- BACKTRACE : Inode’s backtrace in the data pool is corrupted.

## Evaluate strays using recursive scrub[](https://docs.ceph.com/en/latest/cephfs/scrub/#evaluate-strays-using-recursive-scrub)

- In order to evaluate strays i.e. purge stray directories in `~mdsdir` use the following command:

  ```
  ceph tell mds.<fsname>:0 scrub start ~mdsdir recursive
  ```

- `~mdsdir` is not enqueued by default when scrubbing at the CephFS root. In order to perform stray evaluation at root, run scrub with flags `scrub_mdsdir` and `recursive`:

  ```
  ceph tell mds.<fsname>:0 scrub start / recursive,scrub_mdsdir
  ```

# Handling a full Ceph file system[](https://docs.ceph.com/en/latest/cephfs/full/#handling-a-full-ceph-file-system)

When a RADOS cluster reaches its `mon_osd_full_ratio` (default 95%) capacity, it is marked with the OSD full flag.  This flag causes most normal RADOS clients to pause all operations until it is resolved (for example by adding more capacity to the cluster).

The file system has some special handling of the full flag, explained below.

## Hammer and later[](https://docs.ceph.com/en/latest/cephfs/full/#hammer-and-later)

Since the hammer release, a full file system will lead to ENOSPC results from:

> - Data writes on the client
> - Metadata operations other than deletes and truncates

Because the full condition may not be encountered until data is flushed to disk (sometime after a `write` call has already returned 0), the ENOSPC error may not be seen until the application calls `fsync` or `fclose` (or equivalent) on the file handle.

Calling `fsync` is guaranteed to reliably indicate whether the data made it to disk, and will return an error if it doesn’t.  `fclose` will only return an error if buffered data happened to be flushed since the last write -- a successful `fclose` does not guarantee that the data made it to disk, and in a full-space situation, buffered data may be discarded after an `fclose` if no space is available to persist it.

Warning

If an application appears to be misbehaving on a full file system, check that it is performing `fsync()` calls as necessary to ensure data is on disk before proceeding.

Data writes may be cancelled by the client if they are in flight at the time the OSD full flag is sent.  Clients update the `osd_epoch_barrier` when releasing capabilities on files affected by cancelled operations, in order to ensure that these cancelled operations do not interfere with subsequent access to the data objects by the MDS or other clients.  For more on the epoch barrier mechanism, see [Background: Blocklisting and OSD epoch barrier](https://docs.ceph.com/en/latest/cephfs/eviction/#background-blocklisting-and-osd-epoch-barrier).

## Legacy (pre-hammer) behavior[](https://docs.ceph.com/en/latest/cephfs/full/#legacy-pre-hammer-behavior)

In versions of Ceph earlier than hammer, the MDS would ignore the full status of the RADOS cluster, and any data writes from clients would stall until the cluster ceased to be full.

There are two dangerous conditions to watch for with this behaviour:

- If a client had pending writes to a file, then it was not possible for the client to release the file to the MDS for deletion: this could lead to difficulty clearing space on a full file system
- If clients continued to create a large number of empty files, the resulting metadata writes from the MDS could lead to total exhaustion of space on the OSDs such that no further deletions could be performed.

# Advanced: Metadata repair tools[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#advanced-metadata-repair-tools)

Warning

If you do not have expert knowledge of CephFS internals, you will need to seek assistance before using any of these tools.

The tools mentioned here can easily cause damage as well as fixing it.

It is essential to understand exactly what has gone wrong with your file system before attempting to repair it.

If you do not have access to professional support for your cluster, consult the ceph-users mailing list or the #ceph IRC channel.

## Journal export[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#journal-export)

Before attempting dangerous operations, make a copy of the journal like so:

```
cephfs-journal-tool journal export backup.bin
```

Note that this command may not always work if the journal is badly corrupted, in which case a RADOS-level copy should be made (http://tracker.ceph.com/issues/9902).

## Dentry recovery from journal[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#dentry-recovery-from-journal)

If a journal is damaged or for any reason an MDS is incapable of replaying it, attempt to recover what file metadata we can like so:

```
cephfs-journal-tool event recover_dentries summary
```

This command by default acts on MDS rank 0, pass --rank=<n> to operate on other ranks.

This command will write any inodes/dentries recoverable from the journal into the backing store, if these inodes/dentries are higher-versioned than the previous contents of the backing store.  If any regions of the journal are missing/damaged, they will be skipped.

Note that in addition to writing out dentries and inodes, this command will update the InoTables of each ‘in’ MDS rank, to indicate that any written inodes’ numbers are now in use.  In simple cases, this will result in an entirely valid backing store state.

Warning

The resulting state of the backing store is not guaranteed to be self-consistent, and an online MDS scrub will be required afterwards.  The journal contents will not be modified by this command, you should truncate the journal separately after recovering what you can.

## Journal truncation[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#journal-truncation)

If the journal is corrupt or MDSs cannot replay it for any reason, you can truncate it like so:

```
cephfs-journal-tool [--rank=N] journal reset
```

Specify the MDS rank using the `--rank` option when the file system has/had multiple active MDS.

Warning

Resetting the journal *will* lose metadata unless you have extracted it by other means such as `recover_dentries`.  It is likely to leave some orphaned objects in the data pool.  It may result in re-allocation of already-written inodes, such that permissions rules could be violated.

## MDS table wipes[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#mds-table-wipes)

After the journal has been reset, it may no longer be consistent with respect to the contents of the MDS tables (InoTable, SessionMap, SnapServer).

To reset the SessionMap (erase all sessions), use:

```
cephfs-table-tool all reset session
```

This command acts on the tables of all ‘in’ MDS ranks.  Replace ‘all’ with an MDS rank to operate on that rank only.

The session table is the table most likely to need resetting, but if you know you also need to reset the other tables then replace ‘session’ with ‘snap’ or ‘inode’.

## MDS map reset[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#mds-map-reset)

Once the in-RADOS state of the file system (i.e. contents of the metadata pool) is somewhat recovered, it may be necessary to update the MDS map to reflect the contents of the metadata pool.  Use the following command to reset the MDS map to a single MDS:

```
ceph fs reset <fs name> --yes-i-really-mean-it
```

Once this is run, any in-RADOS state for MDS ranks other than 0 will be ignored: as a result it is possible for this to result in data loss.

One might wonder what the difference is between ‘fs reset’ and ‘fs remove; fs new’.  The key distinction is that doing a remove/new will leave rank 0 in ‘creating’ state, such that it would overwrite any existing root inode on disk and orphan any existing files.  In contrast, the ‘reset’ command will leave rank 0 in ‘active’ state such that the next MDS daemon to claim the rank will go ahead and use the existing in-RADOS metadata.

## Recovery from missing metadata objects[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#recovery-from-missing-metadata-objects)

Depending on what objects are missing or corrupt, you may need to run various commands to regenerate default versions of the objects.

```
# Session table
cephfs-table-tool 0 reset session
# SnapServer
cephfs-table-tool 0 reset snap
# InoTable
cephfs-table-tool 0 reset inode
# Journal
cephfs-journal-tool --rank=0 journal reset
# Root inodes ("/" and MDS directory)
cephfs-data-scan init
```

Finally, you can regenerate metadata objects for missing files and directories based on the contents of a data pool.  This is a three-phase process.  First, scanning *all* objects to calculate size and mtime metadata for inodes.  Second, scanning the first object from every file to collect this metadata and inject it into the metadata pool. Third, checking inode linkages and fixing found errors.

```
cephfs-data-scan scan_extents [<data pool> [<extra data pool> ...]]
cephfs-data-scan scan_inodes [<data pool>]
cephfs-data-scan scan_links
```

‘scan_extents’ and ‘scan_inodes’ commands may take a *very long* time if there are many files or very large files in the data pool.

To accelerate the process, run multiple instances of the tool.

Decide on a number of workers, and pass each worker a number within the range 0-(worker_m - 1).

The example below shows how to run 4 workers simultaneously:

```
# Worker 0
cephfs-data-scan scan_extents --worker_n 0 --worker_m 4
# Worker 1
cephfs-data-scan scan_extents --worker_n 1 --worker_m 4
# Worker 2
cephfs-data-scan scan_extents --worker_n 2 --worker_m 4
# Worker 3
cephfs-data-scan scan_extents --worker_n 3 --worker_m 4

# Worker 0
cephfs-data-scan scan_inodes --worker_n 0 --worker_m 4
# Worker 1
cephfs-data-scan scan_inodes --worker_n 1 --worker_m 4
# Worker 2
cephfs-data-scan scan_inodes --worker_n 2 --worker_m 4
# Worker 3
cephfs-data-scan scan_inodes --worker_n 3 --worker_m 4
```

It is **important** to ensure that all workers have completed the scan_extents phase before any workers enter the scan_inodes phase.

After completing the metadata recovery, you may want to run cleanup operation to delete ancillary data generated during recovery.

```
cephfs-data-scan cleanup [<data pool>]
```

Note, the data pool parameters for ‘scan_extents’, ‘scan_inodes’ and ‘cleanup’ commands are optional, and usually the tool will be able to detect the pools automatically. Still you may override this. The ‘scan_extents’ command needs all data pools to be specified, while ‘scan_inodes’ and ‘cleanup’ commands need only the main data pool.

## Using an alternate metadata pool for recovery[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#using-an-alternate-metadata-pool-for-recovery)

Warning

There has not been extensive testing of this procedure. It should be undertaken with great care.

If an existing file system is damaged and inoperative, it is possible to create a fresh metadata pool and attempt to reconstruct the file system metadata into this new pool, leaving the old metadata in place. This could be used to make a safer attempt at recovery since the existing metadata pool would not be modified.

Caution

During this process, multiple metadata pools will contain data referring to the same data pool. Extreme caution must be exercised to avoid changing the data pool contents while this is the case. Once recovery is complete, the damaged metadata pool should be archived or deleted.

To begin, the existing file system should be taken down, if not done already, to prevent further modification of the data pool. Unmount all clients and then mark the file system failed:

```
ceph fs fail <fs_name>
```

Note

<fs_name> here and below indicates the original, damaged file system.

Next, create a recovery file system in which we will populate a new metadata pool backed by the original data pool.

```
ceph osd pool create cephfs_recovery_meta
ceph fs new cephfs_recovery cephfs_recovery_meta <data_pool> --recover --allow-dangerous-metadata-overlay
```

Note

You may rename the recovery metadata pool and file system at a future time. The `--recover` flag prevents any MDS from joining the new file system.

Next, we will create the intial metadata for the fs:

```
cephfs-table-tool cephfs_recovery:0 reset session
cephfs-table-tool cephfs_recovery:0 reset snap
cephfs-table-tool cephfs_recovery:0 reset inode
cephfs-journal-tool --rank cephfs_recovery:0 journal reset --force
```

Now perform the recovery of the metadata pool from the data pool:

```
cephfs-data-scan init --force-init --filesystem cephfs_recovery --alternate-pool cephfs_recovery_meta
cephfs-data-scan scan_extents --alternate-pool cephfs_recovery_meta --filesystem <fs_name>
cephfs-data-scan scan_inodes --alternate-pool cephfs_recovery_meta --filesystem <fs_name> --force-corrupt
cephfs-data-scan scan_links --filesystem cephfs_recovery
```

Note

Each scan procedure above goes through the entire data pool. This may take a significant amount of time. See the previous section on how to distribute this task among workers.

If the damaged file system contains dirty journal data, it may be recovered next with:

```
cephfs-journal-tool --rank=<fs_name>:0 event recover_dentries list --alternate-pool cephfs_recovery_meta
```

After recovery, some recovered directories will have incorrect statistics. Ensure the parameters `mds_verify_scatter` and `mds_debug_scatterstat` are set to false (the default) to prevent the MDS from checking the statistics:

```
ceph config rm mds mds_verify_scatter
ceph config rm mds mds_debug_scatterstat
```

Note

Also verify the config has not been set globally or with a local ceph.conf file.

Now, allow an MDS to join the recovery file system:

```
ceph fs set cephfs_recovery joinable true
```

Finally, run a forward [scrub](https://docs.ceph.com/en/latest/cephfs/scrub/) to repair recursive statistics. Ensure you have an MDS running and issue:

```
ceph tell mds.cephfs_recovery:0 scrub start / recursive,repair,force
```

Note

The [Symbolic link recovery](https://tracker.ceph.com/issues/46166) is supported from Quincy. Symbolic links were recovered as empty regular files before.

It is recommended to migrate any data from the recovery file system as soon as possible. Do not restore the old file system while the recovery file system is operational.

Note

If the data pool is also corrupt, some files may not be restored because backtrace information is lost. If any data objects are missing (due to issues like lost Placement Groups on the data pool), the recovered files will contain holes in place of the missing data.

# Troubleshooting[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#troubleshooting)

## Slow/stuck operations[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#slow-stuck-operations)

If you are experiencing apparent hung operations, the first task is to identify where the problem is occurring: in the client, the MDS, or the network connecting them. Start by looking to see if either side has stuck operations ([Slow requests (MDS)](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#slow-requests), below), and narrow it down from there.

We can get hints about what’s going on by dumping the MDS cache

```
ceph daemon mds.<name> dump cache /tmp/dump.txt
```

Note

The file dump.txt is on the machine executing the MDS and for systemd controlled MDS services, this is in a tmpfs in the MDS container. Use nsenter(1) to locate dump.txt or specify another system-wide path.

If high logging levels are set on the MDS, that will almost certainly hold the information we need to diagnose and solve the issue.

## Stuck during recovery[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#stuck-during-recovery)

### Stuck in up:replay[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#stuck-in-up-replay)

If your MDS is stuck in `up:replay` then it is likely that the journal is very long. Did you see `MDS_HEALTH_TRIM` cluster warnings saying the MDS is behind on trimming its journal? If the journal has grown very large, it can take hours to read the journal. There is no working around this but there are things you can do to speed things along:

Reduce MDS debugging to 0. Even at the default settings, the MDS logs some messages to memory for dumping if a fatal error is encountered. You can avoid this:

```
ceph config set mds debug_mds 0
ceph config set mds debug_ms 0
ceph config set mds debug_monc 0
```

Note if the MDS fails then there will be virtually no information to determine why. If you can calculate when `up:replay` will complete, you should restore these configs just prior to entering the next state:

```
ceph config rm mds debug_mds
ceph config rm mds debug_ms
ceph config rm mds debug_monc
```

Once you’ve got replay moving along faster, you can calculate when the MDS will complete. This is done by examining the journal replay status:

```
$ ceph tell mds.<fs_name>:0 status | jq .replay_status
{
  "journal_read_pos": 4195244,
  "journal_write_pos": 4195244,
  "journal_expire_pos": 4194304,
  "num_events": 2,
  "num_segments": 2
}
```

Replay completes when the `journal_read_pos` reaches the `journal_write_pos`. The write position will not change during replay. Track the progression of the read position to compute the expected time to complete.

### Avoiding recovery roadblocks[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#avoiding-recovery-roadblocks)

When trying to urgently restore your file system during an outage, here are some things to do:

- **Deny all reconnect to clients.** This effectively blocklists all existing CephFS sessions so all mounts will hang or become unavailable.

```
 ceph config set mds mds_deny_all_reconnect true

Remember to undo this after the MDS becomes active.
```

Note

This does not prevent new sessions from connecting. For that, see the `refuse_client_session` file system setting.

- **Extend the MDS heartbeat grace period**. This avoids replacing an MDS that appears “stuck” doing some operation. Sometimes recovery of an MDS may involve an operation that may take longer than expected (from the programmer’s perspective). This is more likely when recovery is already taking a longer than normal amount of time to complete (indicated by your reading this document). Avoid unnecessary replacement loops by extending the heartbeat graceperiod:

```
ceph config set mds mds_heartbeat_grace 3600
```

Note

This has the effect of having the MDS continue to send beacons to the monitors even when its internal “heartbeat” mechanism has not been reset (beat) in one hour. The previous mechanism for achieving this was via the mds_beacon_grace monitor setting.

- **Disable open file table prefetch.** Normally, the MDS will prefetch directory contents during recovery to heat up its cache. During long recovery, the cache is probably already hot **and large**. So this behavior can be undesirable. Disable using:

```
ceph config set mds mds_oft_prefetch_dirfrags false
```

- **Turn off clients.** Clients reconnecting to the newly `up:active` MDS may cause new load on the file system when it’s just getting back on its feet. There will likely be some general maintenance to do before workloads should be resumed. For example, expediting journal trim may be advisable if the recovery took a long time because replay was reading a overly large journal.

  You can do this manually or use the new file system tunable:

```
 ceph fs set <fs_name> refuse_client_session true

That prevents any clients from establishing new sessions with the MDS.
```

## Expediting MDS journal trim[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#expediting-mds-journal-trim)

If your MDS journal grew too large (maybe your MDS was stuck in up:replay for a long time!), you will want to have the MDS trim its journal more frequently. You will know the journal is too large because of `MDS_HEALTH_TRIM` warnings.

The main tunable available to do this is to modify the MDS tick interval. The “tick” interval drives several upkeep activities in the MDS. It is strongly recommended no significant file system load be present when modifying this tick interval. This setting only affects an MDS in `up:active`. The MDS does not trim its journal during recovery.

```
ceph config set mds mds_tick_interval 2
```

## RADOS Health[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#rados-health)

If part of the CephFS metadata or data pools is unavailable and CephFS is not responding, it is probably because RADOS itself is unhealthy. Resolve those problems first ([Troubleshooting](https://docs.ceph.com/en/latest/rados/troubleshooting/)).

## The MDS[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#the-mds)

If an operation is hung inside the MDS, it will eventually show up in `ceph health`, identifying “slow requests are blocked”. It may also identify clients as “failing to respond” or misbehaving in other ways. If the MDS identifies specific clients as misbehaving, you should investigate why they are doing so.

Generally it will be the result of

1. Overloading the system (if you have extra RAM, increase the “mds cache memory limit” config from its default 1GiB; having a larger active file set than your MDS cache is the #1 cause of this!).
2. Running an older (misbehaving) client.
3. Underlying RADOS issues.

Otherwise, you have probably discovered a new bug and should report it to the developers!



### Slow requests (MDS)[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#slow-requests-mds)

You can list current operations via the admin socket by running:

```
ceph daemon mds.<name> dump_ops_in_flight
```

from the MDS host. Identify the stuck commands and examine why they are stuck. Usually the last “event” will have been an attempt to gather locks, or sending the operation off to the MDS log. If it is waiting on the OSDs, fix them. If operations are stuck on a specific inode, you probably have a client holding caps which prevent others from using it, either because the client is trying to flush out dirty data or because you have encountered a bug in CephFS’ distributed file lock code (the file “capabilities” [“caps”] system).

If it’s a result of a bug in the capabilities code, restarting the MDS is likely to resolve the problem.

If there are no slow requests reported on the MDS, and it is not reporting that clients are misbehaving, either the client has a problem or its requests are not reaching the MDS.



## ceph-fuse debugging[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#ceph-fuse-debugging)

ceph-fuse also supports `dump_ops_in_flight`. See if it has any and where they are stuck.

### Debug output[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#debug-output)

To get more debugging information from ceph-fuse, try running in the foreground with logging to the console (`-d`) and enabling client debug (`--debug-client=20`), enabling prints for each message sent (`--debug-ms=1`).

If you suspect a potential monitor issue, enable monitor debugging as well (`--debug-monc=20`).



## Kernel mount debugging[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#kernel-mount-debugging)

If there is an issue with the kernel client, the most important thing is figuring out whether the problem is with the kernel client or the MDS. Generally, this is easy to work out. If the kernel client broke directly, there will be output in `dmesg`. Collect it and any inappropriate kernel state.

### Slow requests[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#id3)

Unfortunately the kernel client does not support the admin socket, but it has similar (if limited) interfaces if your kernel has debugfs enabled. There will be a folder in `sys/kernel/debug/ceph/`, and that folder (whose name will look something like `28f7427e-5558-4ffd-ae1a-51ec3042759a.client25386880`) will contain a variety of files that output interesting output when you `cat` them. These files are described below; the most interesting when debugging slow requests are probably the `mdsc` and `osdc` files.

- bdi: BDI info about the Ceph system (blocks dirtied, written, etc)
- caps: counts of file “caps” structures in-memory and used
- client_options: dumps the options provided to the CephFS mount
- dentry_lru: Dumps the CephFS dentries currently in-memory
- mdsc: Dumps current requests to the MDS
- mdsmap: Dumps the current MDSMap epoch and MDSes
- mds_sessions: Dumps the current sessions to MDSes
- monc: Dumps the current maps from the monitor, and any “subscriptions” held
- monmap: Dumps the current monitor map epoch and monitors
- osdc: Dumps the current ops in-flight to OSDs (ie, file data IO)
- osdmap: Dumps the current OSDMap epoch, pools, and OSDs

If the data pool is in a NEARFULL condition, then the kernel cephfs client will switch to doing writes synchronously, which is quite slow.

## Disconnected+Remounted FS[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#disconnected-remounted-fs)

Because CephFS has a “consistent cache”, if your network connection is disrupted for a long enough time, the client will be forcibly disconnected from the system. At this point, the kernel client is in a bind: it cannot safely write back dirty data, and many applications do not handle IO errors correctly on close(). At the moment, the kernel client will remount the FS, but outstanding file system IO may or may not be satisfied. In these cases, you may need to reboot your client system.

You can identify you are in this situation if dmesg/kern.log report something like:

```
Jul 20 08:14:38 teuthology kernel: [3677601.123718] ceph: mds0 closed our session
Jul 20 08:14:38 teuthology kernel: [3677601.128019] ceph: mds0 reconnect start
Jul 20 08:14:39 teuthology kernel: [3677602.093378] ceph: mds0 reconnect denied
Jul 20 08:14:39 teuthology kernel: [3677602.098525] ceph:  dropping dirty+flushing Fw state for ffff8802dc150518 1099935956631
Jul 20 08:14:39 teuthology kernel: [3677602.107145] ceph:  dropping dirty+flushing Fw state for ffff8801008e8518 1099935946707
Jul 20 08:14:39 teuthology kernel: [3677602.196747] libceph: mds0 172.21.5.114:6812 socket closed (con state OPEN)
Jul 20 08:14:40 teuthology kernel: [3677603.126214] libceph: mds0 172.21.5.114:6812 connection reset
Jul 20 08:14:40 teuthology kernel: [3677603.132176] libceph: reset on mds0
```

This is an area of ongoing work to improve the behavior. Kernels will soon be reliably issuing error codes to in-progress IO, although your application(s) may not deal with them well. In the longer-term, we hope to allow reconnect and reclaim of data in cases where it won’t violate POSIX semantics (generally, data which hasn’t been accessed or modified by other clients).

## Mounting[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#mounting)

### Mount 5 Error[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#mount-5-error)

A mount 5 error typically occurs if a MDS server is laggy or if it crashed. Ensure at least one MDS is up and running, and the cluster is `active + healthy`.

### Mount 12 Error[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#mount-12-error)

A mount 12 error with `cannot allocate memory` usually occurs if you  have a version mismatch between the [Ceph Client](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Client) version and the [Ceph Storage Cluster](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Storage-Cluster) version. Check the versions using:

```
ceph -v
```

If the Ceph Client is behind the Ceph cluster, try to upgrade it:

```
sudo apt-get update && sudo apt-get install ceph-common
```

You may need to uninstall, autoclean and autoremove `ceph-common` and then reinstall it so that you have the latest version.

## Dynamic Debugging[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#dynamic-debugging)

You can enable dynamic debug against the CephFS module.

Please see: https://github.com/ceph/ceph/blob/master/src/script/kcon_all.sh

## In-memory Log Dump[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#in-memory-log-dump)

In-memory logs can be dumped by setting `mds_extraordinary_events_dump_interval` during a lower level debugging (log level < 10). `mds_extraordinary_events_dump_interval` is the interval in seconds for dumping the recent in-memory logs when there is an Extra-Ordinary event.

The Extra-Ordinary events are classified as:

- Client Eviction
- Missed Beacon ACK from the monitors
- Missed Internal Heartbeats

In-memory Log Dump is disabled by default to prevent log file bloat in a production environment. The below commands consecutively enables it:

```
$ ceph config set mds debug_mds <log_level>/<gather_level>
$ ceph config set mds mds_extraordinary_events_dump_interval <seconds>
```

The `log_level` should be < 10 and `gather_level` should be >= 10 to enable in-memory log dump. When it is enabled, the MDS checks for the extra-ordinary events every `mds_extraordinary_events_dump_interval` seconds and if any of them occurs, MDS dumps the in-memory logs containing the relevant event details in ceph-mds log.

Note

For higher log levels (log_level >= 10) there is no reason to dump the In-memory Logs and a lower gather level (gather_level < 10) is insufficient to gather In-memory Logs. Thus a log level >=10 or a gather level < 10 in debug_mds would prevent enabling the In-memory Log Dump. In such cases, when there is a failure it’s required to reset the value of mds_extraordinary_events_dump_interval to 0 before enabling using the above commands.

The In-memory Log Dump can be disabled using:

```
$ ceph config set mds mds_extraordinary_events_dump_interval 0
```

## Filesystems Become Inaccessible After an Upgrade[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#filesystems-become-inaccessible-after-an-upgrade)

Note

You can avoid `operation not permitted` errors by running this procedure before an upgrade. As of May 2023, it seems that `operation not permitted` errors of the kind discussed here occur after upgrades after Nautilus (inclusive).

IF

you have CephFS file systems that have data and metadata pools that were created by a `ceph fs new` command (meaning that they were not created with the defaults)

OR

you have an existing CephFS file system and are upgrading to a new post-Nautilus major version of Ceph

THEN

in order for the documented `ceph fs authorize...` commands to function as documented (and to avoid ‘operation not permitted’ errors when doing file I/O or similar security-related problems for all users except the `client.admin` user), you must first run:

```
ceph osd pool application set <your metadata pool name> cephfs metadata <your ceph fs filesystem name>
```

and

```
ceph osd pool application set <your data pool name> cephfs data <your ceph fs filesystem name>
```

Otherwise, when the OSDs receive a request to read or write data (not the directory info, but file data) they will not know which Ceph file system name to look up. This is true also of pool names, because the ‘defaults’ themselves changed in the major releases, from:

```
data pool=fsname
metadata pool=fsname_metadata
```

to:

```
data pool=fsname.data and
metadata pool=fsname.meta
```

Any setup that used `client.admin` for all mounts did not run into this problem, because the admin key gave blanket permissions.

A temporary fix involves changing mount requests to the ‘client.admin’ user and its associated key. A less drastic but half-fix is to change the osd cap for your user to just `caps osd = "allow rw"`  and delete `tag cephfs data=....`

## Reporting Issues[](https://docs.ceph.com/en/latest/cephfs/troubleshooting/#reporting-issues)

If you have identified a specific issue, please report it with as much information as possible. Especially important information:

- Ceph versions installed on client and server
- Whether you are using the kernel or fuse client
- If you are using the kernel client, what kernel version?
- How many clients are in play, doing what kind of workload?
- If a system is ‘stuck’, is that affecting all clients or just one?
- Any ceph health messages
- Any backtraces in the ceph logs from crashes

If you are satisfied that you have found a bug, please file it on the bug tracker. For more general queries, please write to the ceph-users mailing list.

# Disaster recovery[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery/#disaster-recovery)

## Metadata damage and repair[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery/#metadata-damage-and-repair)

If a file system has inconsistent or missing metadata, it is considered *damaged*.  You may find out about damage from a health message, or in some unfortunate cases from an assertion in a running MDS daemon.

Metadata damage can result either from data loss in the underlying RADOS layer (e.g. multiple disk failures that lose all copies of a PG), or from software bugs.

CephFS includes some tools that may be able to recover a damaged file system, but to use them safely requires a solid understanding of CephFS internals. The documentation for these potentially dangerous operations is on a separate page: [Advanced: Metadata repair tools](https://docs.ceph.com/en/latest/cephfs/disaster-recovery-experts/#disaster-recovery-experts).

## Data pool damage (files affected by lost data PGs)[](https://docs.ceph.com/en/latest/cephfs/disaster-recovery/#data-pool-damage-files-affected-by-lost-data-pgs)

If a PG is lost in a *data* pool, then the file system will continue to operate normally, but some parts of some files will simply be missing (reads will return zeros).

Losing a data PG may affect many files.  Files are split into many objects, so identifying which files are affected by loss of particular PGs requires a full scan over all object IDs that may exist within the size of a file. This type of scan may be useful for identifying which files require restoring from a backup.

Danger

This command does not repair any metadata, so when restoring files in this case you must *remove* the damaged file, and replace it in order to have a fresh inode.  Do not overwrite damaged files in place.

If you know that objects have been lost from PGs, use the `pg_files` subcommand to scan for files that may have been damaged as a result:

```
cephfs-data-scan pg_files <path> <pg id> [<pg id>...]
```

For example, if you have lost data from PGs 1.4 and 4.5, and you would like to know which files under /home/bob might have been damaged:

```
cephfs-data-scan pg_files /home/bob 1.4 4.5
```

The output will be a list of paths to potentially damaged files, one per line.

Note that this command acts as a normal CephFS client to find all the files in the file system and read their layouts, so the MDS must be up and running.

# cephfs-journal-tool[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#cephfs-journal-tool)

## Purpose[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#purpose)

If a CephFS journal has become damaged, expert intervention may be required to restore the file system to a working state.

The `cephfs-journal-tool` utility provides functionality to aid experts in examining, modifying, and extracting data from journals.

Warning

This tool is **dangerous** because it directly modifies internal data structures of the file system.  Make backups, be careful, and seek expert advice.  If you are unsure, do not run this tool.

## Syntax[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#syntax)

```
cephfs-journal-tool journal <inspect|import|export|reset>
cephfs-journal-tool header <get|set>
cephfs-journal-tool event <get|splice|apply> [filter] <list|json|summary|binary>
```

The tool operates in three modes: `journal`, `header` and `event`, meaning the whole journal, the header, and the events within the journal respectively.

## Journal mode[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#journal-mode)

This should be your starting point to assess the state of a journal.

- `inspect` reports on the health of the journal.  This will identify any missing objects or corruption in the stored journal.  Note that this does not identify inconsistencies in the events themselves, just that events are present and can be decoded.
- `import` and `export` read and write binary dumps of the journal in a sparse file format.  Pass the filename as the last argument.  The export operation may not work reliably for journals which are damaged (missing objects).
- `reset` truncates a journal, discarding any information within it.

### Example: journal inspect[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#example-journal-inspect)

```
# cephfs-journal-tool journal inspect
Overall journal integrity: DAMAGED
Objects missing:
  0x1
Corrupt regions:
  0x400000-ffffffffffffffff
```

### Example: Journal import/export[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#example-journal-import-export)

```
# cephfs-journal-tool journal export myjournal.bin
journal is 4194304~80643
read 80643 bytes at offset 4194304
wrote 80643 bytes at offset 4194304 to myjournal.bin
NOTE: this is a _sparse_ file; you can
    $ tar cSzf myjournal.bin.tgz myjournal.bin
      to efficiently compress it while preserving sparseness.

# cephfs-journal-tool journal import myjournal.bin
undump myjournal.bin
start 4194304 len 80643
writing header 200.00000000
 writing 4194304~80643
done.
```

Note

It is wise to use the `journal export <backup file>` command to make a journal backup before any further manipulation.

## Header mode[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#header-mode)

- `get` outputs the current content of the journal header
- `set` modifies an attribute of the header.  Allowed attributes are `trimmed_pos`, `expire_pos` and `write_pos`.

### Example: header get/set[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#example-header-get-set)

```
# cephfs-journal-tool header get
{ "magic": "ceph fs volume v011",
  "write_pos": 4274947,
  "expire_pos": 4194304,
  "trimmed_pos": 4194303,
  "layout": { "stripe_unit": 4194304,
      "stripe_count": 4194304,
      "object_size": 4194304,
      "cas_hash": 4194304,
      "object_stripe_unit": 4194304,
      "pg_pool": 4194304}}

# cephfs-journal-tool header set trimmed_pos 4194303
Updating trimmed_pos 0x400000 -> 0x3fffff
Successfully updated header.
```

## Event mode[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#event-mode)

Event mode allows detailed examination and manipulation of the contents of the journal.  Event mode can operate on all events in the journal, or filters may be applied.

The arguments following `cephfs-journal-tool event` consist of an action, optional filter parameters, and an output mode:

```
cephfs-journal-tool event <action> [filter] <output>
```

Actions:

- `get` read the events from the log
- `splice` erase events or regions in the journal
- `apply` extract file system metadata from events and attempt to apply it to the metadata store.

Filtering:

- `--range <int begin>..[int end]` only include events within the range begin (inclusive) to end (exclusive)
- `--path <path substring>` only include events referring to metadata containing the specified string
- `--inode <int>` only include events referring to metadata containing the specified inode
- `--type <type string>` only include events of this type
- `--frag <ino>[.frag id]` only include events referring to this directory fragment
- `--dname <string>` only include events referring to this named dentry within a directory fragment (may only be used in conjunction with `--frag`
- `--client <int>` only include events from this client session ID

Filters may be combined on an AND basis (i.e. only the intersection of events from each filter).

Output modes:

- `binary`: write each event as a binary file, within a folder whose name is controlled by `--path`
- `json`: write all events to a single file, as a JSON serialized list of objects
- `summary`: write a human readable summary of the events read to standard out
- `list`: write a human readable terse listing of the type of each event, and which file paths the event affects.

### Example: event mode[](https://docs.ceph.com/en/latest/cephfs/cephfs-journal-tool/#example-event-mode)

```
# cephfs-journal-tool event get json --path output.json
Wrote output to JSON file 'output.json'

# cephfs-journal-tool event get summary
Events by type:
  NOOP: 2
  OPEN: 2
  SESSION: 2
  SUBTREEMAP: 1
  UPDATE: 43

# cephfs-journal-tool event get list
0x400000 SUBTREEMAP:  ()
0x400308 SESSION:  ()
0x4003de UPDATE:  (setattr)
  /
0x40068b UPDATE:  (mkdir)
  diralpha
0x400d1b UPDATE:  (mkdir)
  diralpha/filealpha1
0x401666 UPDATE:  (unlink_local)
  stray0/10000000001
  diralpha/filealpha1
0x40228d UPDATE:  (unlink_local)
  diralpha
  stray0/10000000000
0x402bf9 UPDATE:  (scatter_writebehind)
  stray0
0x403150 UPDATE:  (mkdir)
  dirbravo
0x4037e0 UPDATE:  (openc)
  dirbravo/.filebravo1.swp
0x404032 UPDATE:  (openc)
  dirbravo/.filebravo1.swpx

# cephfs-journal-tool event get --path filebravo1 list
0x40785a UPDATE:  (openc)
  dirbravo/filebravo1
0x4103ee UPDATE:  (cap update)
  dirbravo/filebravo1

# cephfs-journal-tool event splice --range 0x40f754..0x410bf1 summary
Events by type:
  OPEN: 1
  UPDATE: 2

# cephfs-journal-tool event apply --range 0x410bf1.. summary
Events by type:
  NOOP: 1
  SESSION: 1
  UPDATE: 9

# cephfs-journal-tool event get --inode=1099511627776 list
0x40068b UPDATE:  (mkdir)
  diralpha
0x400d1b UPDATE:  (mkdir)
  diralpha/filealpha1
0x401666 UPDATE:  (unlink_local)
  stray0/10000000001
  diralpha/filealpha1
0x40228d UPDATE:  (unlink_local)
  diralpha
  stray0/10000000000

# cephfs-journal-tool event get --frag=1099511627776 --dname=filealpha1 list
0x400d1b UPDATE:  (mkdir)
  diralpha/filealpha1
0x401666 UPDATE:  (unlink_local)
  stray0/10000000001
  diralpha/filealpha1

# cephfs-journal-tool event get binary --path bin_events
Wrote output to binary files in directory 'bin_events'
```

# Recovering the file system after catastrophic Monitor store loss[](https://docs.ceph.com/en/latest/cephfs/recover-fs-after-mon-store-loss/#recovering-the-file-system-after-catastrophic-monitor-store-loss)

During rare occasions, all the monitor stores of a cluster may get corrupted or lost. To recover the cluster in such a scenario, you need to rebuild the monitor stores using the OSDs (see [Recovery using OSDs](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-mon/#mon-store-recovery-using-osds)), and get back the pools intact (active+clean state). However, the rebuilt monitor stores don’t restore the file system maps (“FSMap”). Additional steps are required to bring back the file system. The steps to recover a multiple active MDS file system or multiple file systems are yet to be identified. Currently, only the steps to recover a **single active MDS** file system with no additional file systems in the cluster have been identified and tested. Briefly the steps are: recreate the FSMap with basic defaults; and allow MDSs to recover from the journal/metadata stored in the filesystem’s pools. The steps are described in more detail below.

First up, recreate the file system using the recovered file system pools. The new FSMap will have the filesystem’s default settings. However, the user defined file system settings such as `standby_count_wanted`, `required_client_features`, extra data pools, etc., are lost and need to be reapplied later.

```
ceph fs new <fs_name> <metadata_pool> <data_pool> --force --recover
```

The `recover` flag sets the state of file system’s rank 0 to existing but failed. So when a MDS daemon eventually picks up rank 0, the daemon reads the existing in-RADOS metadata and doesn’t overwrite it. The flag also prevents the standby MDS daemons to activate the file system.

The file system cluster ID, fscid, of the file system will not be preserved. This behaviour may not be desirable for certain applications (e.g., Ceph CSI) that expect the file system to be unchanged across recovery. To fix this, you can optionally set the `fscid` option in the above command (see [Advanced](https://docs.ceph.com/en/latest/cephfs/administration/#advanced-cephfs-admin-settings)).

Allow standby MDS daemons to join the file system.

```
ceph fs set <fs_name> joinable true
```

Check that the file system is no longer in degraded state and has an active MDS.

```
ceph fs status
```

Reapply any other custom file system settings.