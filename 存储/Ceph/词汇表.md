# 词汇表

- [BlueStore](https://docs.ceph.com/en/latest/rados/configuration/storage-devices/#rados-config-storage-devices-bluestore)[](https://docs.ceph.com/en/latest/glossary/#term-BlueStore)

  OSD BlueStore is a storage back end used by OSD daemons, and was designed specifically for use with Ceph. BlueStore was introduced in the Ceph Kraken release. In the Ceph Luminous release, BlueStore became Ceph’s default storage back end, supplanting FileStore. Unlike [filestore](https://docs.ceph.com/en/latest/glossary/#term-filestore), BlueStore stores objects directly on Ceph block devices without any file system interface. Since Luminous (12.2), BlueStore has been Ceph’s default and recommended storage back end.

  OSD Blue Store是OSD守护程序使用的存储后端，专门设计用于Ceph。Blue Store在Ceph Kraken发行版中推出。在Ceph Luminous版本中，Blue Store成为Ceph的默认存储后端，取代了File  Store。与文件存储不同，BlueStore将对象直接存储在Ceph块设备上，而无需任何文件系统接口。自Luminous（12.2）以来，Blue Store一直是Ceph的默认和推荐存储后端。

- Ceph[](https://docs.ceph.com/en/latest/glossary/#term-Ceph)

  Ceph is a distributed network storage and file system with distributed metadata management and POSIX semantics.Ceph是一个具有分布式元数据管理和POSIX语义的分布式网络存储和文件系统。

- Ceph Block Device[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Block-Device)

  Also called “RADOS Block Device” and [RBD](https://docs.ceph.com/en/latest/glossary/#term-RBD). A software instrument that orchestrates the storage of block-based data in Ceph. Ceph Block Device splits block-based application data into “chunks”. RADOS stores these chunks as objects. Ceph Block Device orchestrates the storage of those objects across the storage cluster.也称为“RADOS块设备”和RBD。一种协调Ceph中基于块的数据存储的软件工具。Ceph块设备将基于块的应用程序数据拆分为“块”。RADOS将这些块存储为对象。Ceph Block Device协调存储集群中这些对象的存储。

- Ceph Block Storage[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Block-Storage)

  One of the three kinds of storage supported by Ceph (the other two are object storage and file storage). Ceph Block Storage is the block storage “product”, which refers to block-storage related services and capabilities when used in conjunction with the collection of (1) `librbd` (a python module that provides file-like access to [RBD](https://docs.ceph.com/en/latest/glossary/#term-RBD) images), (2) a hypervisor such as QEMU or Xen, and (3) a hypervisor abstraction layer such as `libvirt`.

  Ceph支持的三种存储之一（其他两种是对象存储和文件存储）。Ceph Block  Storage是块存储“产品”，指与以下集合结合使用时与块存储相关的服务和功能：（1）librbd（一个提供对RBD映像的类似文件访问的python模块），（2）管理程序（如QEMU或Xen），以及（3）管理程序抽象层（如libvirt）。

- Ceph Client[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Client)

  Any of the Ceph components that can access a Ceph Storage Cluster. This includes the Ceph Object Gateway, the Ceph Block Device, the Ceph File System, and their corresponding libraries. It also includes kernel modules, and FUSEs (Filesystems in USERspace).可以访问Ceph存储群集的任何Ceph组件。这包括Ceph对象网关、Ceph块设备、Ceph文件系统及其相应的库。它还包括内核模块和FUSE（USERspace中的文件系统）。

- Ceph Client Libraries[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Client-Libraries)

  The collection of libraries that can be used to interact with components of the Ceph Cluster.可用于与Ceph集群组件交互的库集合。

- Ceph Cluster Map[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Cluster-Map)

  See [Cluster Map](https://docs.ceph.com/en/latest/glossary/#term-Cluster-Map)请参见群集映射

- Ceph Dashboard[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Dashboard)

  [The Ceph Dashboard](https://docs.ceph.com/en/latest/mgr/dashboard/#mgr-dashboard) is a built-in web-based Ceph management and monitoring application through which you can inspect and administer various resources within the cluster. It is implemented as a [Ceph Manager Daemon](https://docs.ceph.com/en/latest/mgr/#ceph-manager-daemon) module.Ceph Dashboard是一个内置的基于web的Ceph管理和监控应用程序，您可以通过它检查和管理集群中的各种资源。它被实现为Ceph Manager守护程序模块。

- Ceph File System[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-File-System)

  See [CephFS](https://docs.ceph.com/en/latest/glossary/#term-CephFS)

- [CephFS](https://docs.ceph.com/en/latest/cephfs/#ceph-file-system)[](https://docs.ceph.com/en/latest/glossary/#term-CephFS)

  The **Ceph F**ile **S**ystem, or CephFS, is a POSIX-compliant file system built on top of Ceph’s distributed object store, RADOS.  See [CephFS Architecture](https://docs.ceph.com/en/latest/architecture/#arch-cephfs) for more details.Ceph文件系统（Ceph FS）是一个基于Ceph的分布式对象存储RADOS构建的符合POSIX的文件系统。有关详细信息，请参阅Ceph FS架构。

- Ceph Interim Release[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Interim-Release)

  See [Releases](https://docs.ceph.com/en/latest/glossary/#term-Releases).

- Ceph Kernel Modules[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Kernel-Modules)

  The collection of kernel modules that can be used to interact with the Ceph Cluster (for example: `ceph.ko`, `rbd.ko`).用于与Ceph集群交互的内核模块集合（例如：Ceph.ko、rbd.ko）。

- [Ceph Manager](https://docs.ceph.com/en/latest/mgr/#ceph-manager-daemon)[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager)

  The Ceph manager daemon (ceph-mgr) is a daemon that runs alongside monitor daemons to provide monitoring and interfacing to external monitoring and management systems. Since the Luminous release (12.x), no Ceph cluster functions properly unless it contains a running ceph-mgr daemon.Ceph管理器守护程序（Ceph-mgr）是一个与监控器守护程序一起运行的守护程序，用于提供监控和与外部监控和管理系统的接口。自Luminous发行版（12.x）以来，除非Ceph集群包含一个正在运行的Ceph-mgr守护程序，否则它将无法正常运行。

- Ceph Manager Dashboard[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager-Dashboard)

  See [Ceph Dashboard](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Dashboard).

- Ceph Metadata Server[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Metadata-Server)

  See [MDS](https://docs.ceph.com/en/latest/glossary/#term-MDS).

- Ceph Monitor[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Monitor)

  A daemon that maintains a map of the state of the cluster. This “cluster state” includes the monitor map, the manager map, the OSD map, and the CRUSH map. A Ceph cluster must contain a minimum of three running monitors in order to be both redundant and highly-available. Ceph monitors and the nodes on which they run are often referred to as “mon”s. See [Monitor Config Reference](https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref/#monitor-config-reference).维护集群状态映射的守护程序。该“集群状态”包括监视器映射、管理器映射、OSD映射和CRUSH映射。Ceph集群必须至少包含三个正在运行的监视器，才能既冗余又高可用。Ceph监视器及其运行的节点通常被称为“mon”。请参阅监视器配置参考。

- Ceph Node[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Node)

  A Ceph node is a unit of the Ceph Cluster that communicates with other nodes in the Ceph Cluster in order to replicate and redistribute data. All of the nodes together are called the [Ceph Storage Cluster](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Storage-Cluster). Ceph nodes include [OSD](https://docs.ceph.com/en/latest/glossary/#term-OSD)s, [Ceph Monitor](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Monitor)s, [Ceph Manager](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager)s, and [MDS](https://docs.ceph.com/en/latest/glossary/#term-MDS)es. The term “node” is usually equivalent to “host” in the Ceph documentation. If you have a running Ceph Cluster, you can list all of the nodes in it by running the command `ceph node ls all`.Ceph节点是Ceph集群的一个单元，它与Ceph集群中的其他节点进行通信，以便复制和重新分发数据。所有节点一起称为Ceph存储集群。Ceph节点包括OSD、Ceph监视器、Ceph管理器和MDS。术语“节点”通常等同于Ceph文档中的“主机”。如果您有一个正在运行的Ceph集群，您可以通过运行命令Ceph node ls all列出其中的所有节点。

- [Ceph Object Gateway](https://docs.ceph.com/en/latest/radosgw/#object-gateway)[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Object-Gateway)

  An object storage interface built on top of librados. Ceph Object Gateway provides a RESTful gateway between applications and Ceph storage clusters.建立在librados之上的对象存储接口。Ceph对象网关提供了应用程序和Ceph存储集群之间的RESTful网关。

- Ceph Object Storage[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Object-Storage)

  See [Ceph Object Store](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Object-Store).

- Ceph Object Store[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Object-Store)

  A Ceph Object Store consists of a [Ceph Storage Cluster](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Storage-Cluster) and a [Ceph Object Gateway](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Object-Gateway) (RGW).Ceph对象存储由Ceph存储集群和Ceph对象网关（RGW）组成。

- [Ceph OSD](https://docs.ceph.com/en/latest/rados/configuration/storage-devices/#rados-configuration-storage-devices-ceph-osd)[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD)

  Ceph **O**bject **S**torage **D**aemon. The Ceph OSD software, which interacts with logical disks ([OSD](https://docs.ceph.com/en/latest/glossary/#term-OSD)). Around 2013, there was an attempt by “research and industry” (Sage’s own words) to insist on using the term “OSD” to mean only “Object Storage Device”, but the Ceph community has always persisted in using the term to mean “Object Storage Daemon” and no less an authority than Sage Weil himself confirms in November of 2022 that “Daemon is more accurate for how Ceph is built” (private correspondence between Zac Dover and Sage Weil, 07 Nov 2022).Ceph对象存储守护程序。Ceph  OSD软件，与逻辑磁盘（OSD）交互。2013年左右，“研究和工业”（Sage自己的话）试图坚持使用“OSD”一词仅表示“对象存储设备”，但Ceph社区一直坚持使用这个词来表示“对象存储守护程序”，其权威程度不亚于Sage Weil本人在2022年11月证实的“守护程序对于Ceph的构建更为准确”（Zac Dover和Sage  Wei之间的私人通信，2022年12月7日）。

- Ceph OSD Daemon[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD-Daemon)

  See [Ceph OSD](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD).

- Ceph OSD Daemons[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD-Daemons)

  See [Ceph OSD](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD).

- Ceph Platform[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Platform)

  All Ceph software, which includes any piece of code hosted at https://github.com/ceph.所有Ceph软件，包括托管在https://github.com/ceph.

- Ceph Point Release[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Point-Release)

  See [Releases](https://docs.ceph.com/en/latest/glossary/#term-Releases).

- Ceph Project[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Project)

  The aggregate term for the people, software, mission and infrastructure of Ceph.

- Ceph Release[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Release)

  See [Releases](https://docs.ceph.com/en/latest/glossary/#term-Releases).

- Ceph Release Candidate[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Release-Candidate)

  See [Releases](https://docs.ceph.com/en/latest/glossary/#term-Releases).

- Ceph Stable Release[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Stable-Release)

  See [Releases](https://docs.ceph.com/en/latest/glossary/#term-Releases).

- Ceph Stack[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Stack)

  A collection of two or more components of Ceph.

- [Ceph Storage Cluster](https://docs.ceph.com/en/latest/architecture/#arch-ceph-storage-cluster)[](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Storage-Cluster)

  The collection of [Ceph Monitor](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Monitor)s, [Ceph Manager](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager)s, [Ceph Metadata Server](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Metadata-Server)s, and [OSD](https://docs.ceph.com/en/latest/glossary/#term-OSD)s that work together to store and replicate data for use by applications, Ceph Users, and [Ceph Client](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Client)s. Ceph Storage Clusters receive data from [Ceph Client](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Client)s.

- CephX[](https://docs.ceph.com/en/latest/glossary/#term-CephX)

  The Ceph authentication protocol. CephX operates like Kerberos, but it has no single point of failure. See the [CephX Configuration Reference](https://docs.ceph.com/en/latest/rados/configuration/auth-config-ref/#rados-cephx-config-ref).

- Cloud Platforms[](https://docs.ceph.com/en/latest/glossary/#term-Cloud-Platforms)

- Cloud Stacks[](https://docs.ceph.com/en/latest/glossary/#term-Cloud-Stacks)

  Third party cloud provisioning platforms such as OpenStack, CloudStack, OpenNebula, and Proxmox VE.

- Cluster Map[](https://docs.ceph.com/en/latest/glossary/#term-Cluster-Map)

  The set of maps consisting of the monitor map, OSD map, PG map, MDS map, and CRUSH map, which together report the state of the Ceph cluster. See [the “Cluster Map” section of the Architecture document](https://docs.ceph.com/en/latest/architecture/#architecture-cluster-map) for details.

- CRUSH[](https://docs.ceph.com/en/latest/glossary/#term-CRUSH)

  **C**ontrolled **R**eplication **U**nder **S**calable **H**ashing. The algorithm that Ceph uses to compute object storage locations.

- CRUSH rule[](https://docs.ceph.com/en/latest/glossary/#term-CRUSH-rule)

  The CRUSH data placement rule that applies to a particular pool or pools.

Ceph项目

Ceph的人员、软件、任务和基础设施的统称。

Ceph释放

请参见版本。

Ceph候选发布

请参见版本。

Ceph稳定释放

请参见版本。

Ceph堆栈

Ceph的两种或多种成分的集合。

Ceph存储群集

Ceph监视器、Ceph管理器、Ceph元数据服务器和OSD的集合，它们一起工作以存储和复制供应用程序、Ceph用户和Ceph客户端使用的数据。Ceph存储集群从Ceph客户端接收数据。

头孢X

Ceph认证协议。Ceph X像Kerberos一样运行，但它没有单点故障。请参阅Ceph X配置参考。

云平台

云堆栈

第三方云供应平台，如Open Stack、cloud Stack、Open Nebula和Proxmox VE。

群集映射

这组地图由监视器地图、OSD地图、PG地图、MDS地图和CRUSH地图组成，它们一起报告Ceph集群的状态。有关详细信息，请参阅体系结构文档的“集群图”部分。

压碎，压碎

可扩展哈希下的受控复制。Ceph用于计算对象存储位置的算法。

CRUSH规则

适用于特定池的CRUSH数据放置规则。

数据采集系统

直接连接存储。不通过网络直接连接到访问计算机的存储器。与NAS和SAN形成对比。

仪表板

一个内置的基于web的Ceph管理和监控应用程序，用于管理集群的各个方面和对象。仪表板作为Ceph Manager模块实现。有关更多详细信息，请参阅Ceph Dashboard。

仪表板模块

仪表板的另一个名称。

仪表板插件

文件存储器

OSD后台进程的后端，需要日志，文件被写入文件系统。

FQDN公司

完全限定域名。应用于网络中节点的域名，用于指定该节点在DNS树层次结构中的确切位置。

在Ceph集群管理环境中，FQDN通常应用于主机。在本文档中，术语“FQDN”主要用于区分FQDN和相对较简单的主机名，它们不指定主机在DNS树层次结构中的确切位置，而只是命名主机。

主办

Ceph集群中的任何单机或服务器。请参见Ceph节点。

LVM标签

逻辑卷管理器标记。LVM卷和组的可扩展元数据。它们用于存储有关设备及其与OSD的关系的Ceph特定信息。

MDS公司

Ceph元数据服务器守护程序。也称为“ceph-mds”。Ceph元数据服务器守护程序必须在运行Ceph FS文件系统的任何Ceph集群中运行。MDS存储所有文件系统元数据。

- DAS[](https://docs.ceph.com/en/latest/glossary/#term-DAS)

  **D**irect-**A**ttached **S**torage. Storage that is attached directly to the computer accessing it, without passing through a network.  Contrast with NAS and SAN.

- [Dashboard](https://docs.ceph.com/en/latest/mgr/dashboard/#mgr-dashboard)[](https://docs.ceph.com/en/latest/glossary/#term-Dashboard)

  A built-in web-based Ceph management and monitoring application to administer various aspects and objects of the cluster. The dashboard is implemented as a Ceph Manager module. See [Ceph Dashboard](https://docs.ceph.com/en/latest/mgr/dashboard/#mgr-dashboard) for more details.

- Dashboard Module[](https://docs.ceph.com/en/latest/glossary/#term-Dashboard-Module)

  Another name for [Dashboard](https://docs.ceph.com/en/latest/glossary/#term-Dashboard).

- Dashboard Plugin[](https://docs.ceph.com/en/latest/glossary/#term-Dashboard-Plugin)

- filestore[](https://docs.ceph.com/en/latest/glossary/#term-filestore)

  A back end for OSD daemons, where a Journal is needed and files are written to the filesystem.

- FQDN[](https://docs.ceph.com/en/latest/glossary/#term-FQDN)

  **F**ully **Q**ualified **D**omain **N**ame. A domain name that is applied to a node in a network and that specifies the node’s exact location in the tree hierarchy of the DNS. In the context of Ceph cluster administration, FQDNs are often applied to hosts. In this documentation, the term “FQDN” is used mostly to distinguish between FQDNs and relatively simpler hostnames, which do not specify the exact location of the host in the tree hierarchy of the DNS but merely name the host.

- Host[](https://docs.ceph.com/en/latest/glossary/#term-Host)

  Any single machine or server in a Ceph Cluster. See [Ceph Node](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Node).

- LVM tags[](https://docs.ceph.com/en/latest/glossary/#term-LVM-tags)

  **L**ogical **V**olume **M**anager tags. Extensible metadata for LVM volumes and groups. They are used to store Ceph-specific information about devices and its relationship with OSDs.

- [MDS](https://docs.ceph.com/en/latest/cephfs/add-remove-mds/#cephfs-add-remote-mds)[](https://docs.ceph.com/en/latest/glossary/#term-MDS)

  The Ceph **M**eta**D**ata **S**erver daemon. Also referred to as “ceph-mds”. The Ceph metadata server daemon must be running in any Ceph cluster that runs the CephFS file system. The MDS stores all filesystem metadata.

- MGR[](https://docs.ceph.com/en/latest/glossary/#term-MGR)

  The Ceph manager software, which collects all the state from the whole cluster in one place.

- MON[](https://docs.ceph.com/en/latest/glossary/#term-MON)

  The Ceph monitor software.

- Node[](https://docs.ceph.com/en/latest/glossary/#term-Node)

  See [Ceph Node](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Node).

- Object Storage Device[](https://docs.ceph.com/en/latest/glossary/#term-Object-Storage-Device)

  See [OSD](https://docs.ceph.com/en/latest/glossary/#term-OSD).

- OSD[](https://docs.ceph.com/en/latest/glossary/#term-OSD)

  Probably [Ceph OSD](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD), but not necessarily. Sometimes (especially in older correspondence, and especially in documentation that is not written specifically for Ceph), “OSD” means “**O**bject **S**torage **D**evice”, which refers to a physical or logical storage unit (for example: LUN). The Ceph community has always used the term “OSD” to refer to [Ceph OSD Daemon](https://docs.ceph.com/en/latest/glossary/#term-Ceph-OSD-Daemon) despite an industry push in the mid-2010s to insist that “OSD” should refer to “Object Storage Device”, so it is important to know which meaning is intended.

- OSD fsid[](https://docs.ceph.com/en/latest/glossary/#term-OSD-fsid)

  This is a unique identifier used to identify an OSD. It is found in the OSD path in a file called `osd_fsid`. The term `fsid` is used interchangeably with `uuid`

- OSD id[](https://docs.ceph.com/en/latest/glossary/#term-OSD-id)

  The integer that defines an OSD. It is generated by the monitors during the creation of each OSD.

- OSD uuid[](https://docs.ceph.com/en/latest/glossary/#term-OSD-uuid)

  This is the unique identifier of an OSD. This term is used interchangeably with `fsid`

- [Pool](https://docs.ceph.com/en/latest/rados/operations/pools/#rados-pools)[](https://docs.ceph.com/en/latest/glossary/#term-Pool)

  A pool is a logical partition used to store objects.

- Pools[](https://docs.ceph.com/en/latest/glossary/#term-Pools)

  See [pool](https://docs.ceph.com/en/latest/glossary/#term-Pool).

- RADOS[](https://docs.ceph.com/en/latest/glossary/#term-RADOS)

  **R**eliable **A**utonomic **D**istributed **O**bject **S**tore. RADOS is the object store that provides a scalable service for variably-sized objects. The RADOS object store is the core component of a Ceph cluster.  [This blog post from 2009](https://ceph.io/en/news/blog/2009/the-rados-distributed-object-store/) provides a beginner’s introduction to RADOS. Readers interested in a deeper understanding of RADOS are directed to [RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters](https://ceph.io/assets/pdfs/weil-rados-pdsw07.pdf).

- RADOS Cluster[](https://docs.ceph.com/en/latest/glossary/#term-RADOS-Cluster)

  A proper subset of the Ceph Cluster consisting of [OSD](https://docs.ceph.com/en/latest/glossary/#term-OSD)s, [Ceph Monitor](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Monitor)s, and [Ceph Manager](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager)s.

- RADOS Gateway[](https://docs.ceph.com/en/latest/glossary/#term-RADOS-Gateway)

  See [RGW](https://docs.ceph.com/en/latest/glossary/#term-RGW).

- RBD[](https://docs.ceph.com/en/latest/glossary/#term-RBD)

  **R**ADOS **B**lock **D**evice. See [Ceph Block Device](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Block-Device).

- Releases[](https://docs.ceph.com/en/latest/glossary/#term-Releases)

  Ceph Interim ReleaseA version of Ceph that has not yet been put through quality assurance testing. May contain new features. Ceph Point ReleaseAny ad hoc release that includes only bug fixes and security fixes. Ceph ReleaseAny distinct numbered version of Ceph. Ceph Release CandidateA major version of Ceph that has undergone initial quality assurance testing and is ready for beta testers. Ceph Stable ReleaseA major version of Ceph where all features from the preceding interim releases have been put through quality assurance testing successfully.

- Reliable Autonomic Distributed Object Store[](https://docs.ceph.com/en/latest/glossary/#term-Reliable-Autonomic-Distributed-Object-Store)

  The core set of storage software which stores the user’s data (MON+OSD). See also [RADOS](https://docs.ceph.com/en/latest/glossary/#term-RADOS).

- [RGW](https://docs.ceph.com/en/latest/radosgw/#object-gateway)[](https://docs.ceph.com/en/latest/glossary/#term-RGW)

  **R**ADOS **G**ate**w**ay. Also called “Ceph Object Gateway”. The component of Ceph that provides a gateway to both the Amazon S3 RESTful API and the OpenStack Swift API.

- secrets[](https://docs.ceph.com/en/latest/glossary/#term-secrets)

  Secrets are credentials used to perform digital authentication whenever privileged users must access systems that require authentication. Secrets can be passwords, API keys, tokens, SSH keys, private certificates, or encryption keys.

- SDS[](https://docs.ceph.com/en/latest/glossary/#term-SDS)

  **S**oftware-**d**efined **S**torage.

- systemd oneshot[](https://docs.ceph.com/en/latest/glossary/#term-systemd-oneshot)

  A systemd `type` where a command is defined in `ExecStart` which will exit upon completion (it is not intended to daemonize)

- Teuthology[](https://docs.ceph.com/en/latest/glossary/#term-Teuthology)

  The collection of software that performs scripted tests on Ceph.