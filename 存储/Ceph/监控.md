# 监控

[TOC]

## 概述

作为存储管理员，您可以使用后端中带有 Cephadm 的 Ceph 编排器来部署监控和警报堆栈。监控堆栈由  Prometheus、Prometheus、Prometheus Alertmanager 和 Grafana 组成。用户需要在 YAML  配置文件中使用 Cephadm  定义这些服务，或者使用命令行界面来部署这些服务。部署多个相同类型的服务时，会部署一个高可用性的设置。节点导出器对此规则的一个例外。 	

注意

Red Hat Ceph Storage 5.0 不支持用于部署监控服务的自定义镜像，如 Prometheus、Grafana、Alertmanager 和 node-exporter。 		

以下监控服务可使用 Cephadm 部署： 	

- Prometheus 是监控和警报工具包。它收集 Prometheus 地区提供的数据，并在达到预定义阈值时触发预配置警报。Prometheus manager 模块提供了一个 Prometheus 导出器，用于从 `ceph-mgr` 中的收集点传递 Ceph 性能计数器。 			

  Prometheus 配置（包括提取目标）由 Cephadm 自动设置，如提供守护进程的指标。Cephadm 也会部署一组默认警报，如健康错误、10％ OSD 停机或 pgs inactive。 			

- Alertmanager 处理 Prometheus  服务器发送的警报。它重复数据删除、分组和将警报路由到正确的接收方。默认情况下，Ceph 控制面板自动配置为接收器。Alertmanager 处理 Prometheus 服务器发送的警报。可以使用 Alertmanager 来静默警报，但静默也可使用 Ceph 控制面板进行管理。 			

- Grafana 是视觉化和警报软件。此监控堆栈不使用 Grafana 的警报功能。用于警报，使用 Alertmanager。 			

  默认情况下，到 Grafana 的流量使用 TLS 加密。您可以提供自己的 TLS 证书，也可以使用自签名证书。如果在部署  Grafana 之前没有配置自定义证书，则会自动为 Grafana 创建和配置自签名证书。可以使用以下命令配置 Grafana 的自定义证书： 			

  ```none
   ceph config-key set mgr/cephadm/grafana_key -i PRESENT_WORKING_DIRECTORY/key.pem
   ceph config-key set mgr/cephadm/grafana_crt -i PRESENT_WORKING_DIRECTORY/certificate.pem
  ```

节点导出器是 Prometheus 的导出器，提供有关安装它的节点的数据。建议在所有节点上安装节点导出器。这可以通过将 'monitoring.yml 文件与 node-exporter 服务类型一起使用来实现。 	

## 部署监控堆栈

监控堆栈由 Prometheus、Prometheus、Prometheus Alertmanager 和 Grafana 组成。Ceph 控制面板利用这些组件存储和视觉化关于集群使用情况和性能的详细指标。 		

您可以使用 YAML 文件格式的服务规格部署监控堆栈。所有监控服务都可以在 `yml` 文件中具有它们绑定到的网络和端口。 				

**流程**

1. 在 Ceph 管理器守护进程中启用 prometheus 模块。这会公开内部 Ceph 指标，以便 Prometheus 可以读取它们：							

   ```none
   [ceph: root@host01 /]# ceph mgr module enable prometheus
   ```

   重要

   确保此命令在部署 Prometheus 之前运行。如果在部署前没有运行该命令，您必须重新部署 Prometheus 以更新配置： 					

   ```none
   ceph orch redeploy prometheus
   ```

2. 进入以下目录： 

   ```none
   cd /var/lib/ceph/DAEMON_PATH/		
   ```

   ```none
   [ceph: root@host01 mds/]# cd /var/lib/ceph/monitoring/
   ```

   注意如果目录 `监控` 不存在，请进行创建。 					

3. 创建 `monitoring.yml` 文件：

   ```none
   [ceph: root@host01 monitoring]# touch monitoring.yml
   ```

4. 使用类似以下示例的内容编辑规格文件： 

   ```yaml
   service_type: prometheus
   service_name: prometheus
   placement:
     hosts:
     - host01
   networks:
   - 192.169.142.0/24
   ---
   service_type: node-exporter
   ---
   service_type: alertmanager
   service_name: alertmanager
   placement:
     hosts:
     - host03
   networks:
   - 192.169.142.03/24
   ---
   service_type: grafana
   service_name: grafana
   placement:
     hosts:
     - host02
   networks:
   - 192.169.142.02/24
   ```

5. 应用监控服务：

   ```none
   ceph orch apply -i monitoring.yml
   ```

**验证**

- 列出服务：

  ```none
  [ceph: root@host01 /]# ceph orch ls
  ```

- 列出主机、守护进程和进程：		

  ```none
  ceph orch ps --service_name=SERVICE_NAME
  ```

  ```none
  [ceph: root@host01 /]# ceph orch ps --service_name=prometheus
  ```

重要

Prometheus、Grafana 和 Ceph 仪表板都会自动配置为相互通信，从而在 Ceph 控制面板中实现功能齐全的 Grafana 集成。 			

## 移除监控堆栈

您可以使用 `ceph orch rm` 命令删除监控堆栈。 				

**流程**

1. 使用 `ceph 或ch rm` 命令删除监控堆栈：

   ```none
   ceph orch rm SERVICE_NAME --force
   
   ceph orch rm grafana
   ceph orch rm prometheus
   ceph orch rm node-exporter
   ceph orch rm alertmanager
   ceph mgr module disable prometheus
   ```

2. 检查进程的状态：

   ```none
   ceph orch status
   ```

**验证**

- 列出服务： 

  ```none
  ceph orch ls
  ```

- 列出主机、守护进程和进程：

  ```none
  ceph orch ps
  ```





Ceph Dashboard uses [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/), and related tools to store and visualize detailed metrics on cluster utilization and performance.  Ceph users have three options:

1. Have cephadm deploy and configure these services.  This is the default when bootstrapping a new cluster unless the `--skip-monitoring-stack` option is used.
2. Deploy and configure these services manually.  This is recommended for users with existing prometheus services in their environment (and in cases where Ceph is running in Kubernetes with Rook).
3. Skip the monitoring stack completely.  Some Ceph dashboard graphs will not be available.

The monitoring stack consists of [Prometheus](https://prometheus.io/), Prometheus exporters ([Prometheus Module](https://docs.ceph.com/en/latest/mgr/prometheus/#mgr-prometheus), [Node exporter](https://prometheus.io/docs/guides/node-exporter/)), [Prometheus Alert Manager](https://prometheus.io/docs/alerting/alertmanager/) and [Grafana](https://grafana.com/).

Note

Prometheus’ security model presumes that untrusted users have access to the Prometheus HTTP endpoint and logs. Untrusted users have access to all the (meta)data Prometheus collects that is contained in the database, plus a variety of operational and debugging information.

However, Prometheus’ HTTP API is limited to read-only operations. Configurations can *not* be changed using the API and secrets are not exposed. Moreover, Prometheus has some built-in measures to mitigate the impact of denial of service attacks.

Please see Prometheus’ Security model <https://prometheus.io/docs/operating/security/> for more detailed information.

## Deploying monitoring with cephadm

By default, bootstrap will deploy a basic monitoring stack.  If you did not do this (by passing `--skip-monitoring-stack`, or if you converted an existing cluster to cephadm management, you can set up monitoring by following the steps below.

1. Enable the prometheus module in the ceph-mgr daemon.  This exposes the internal Ceph metrics so that prometheus can scrape them.

   ```
   ceph mgr module enable prometheus
   ```

2. Deploy a node-exporter service on every node of the cluster.  The node-exporter provides host-level metrics like CPU and memory  utilization.

   ```
   ceph orch apply node-exporter '*'
   ```

3. Deploy alertmanager

   ```
   ceph orch apply alertmanager 1
   ```

4. Deploy prometheus.  A single prometheus instance is sufficient, but for HA you may want to deploy two.

   ```
   ceph orch apply prometheus 1    # or 2
   ```

5. Deploy grafana

   ```
   ceph orch apply grafana 1
   ```

Cephadm takes care of the configuration of Prometheus, Grafana, and Alertmanager automatically.

However, there is one exception to this rule. In a some setups, the Dashboard user’s browser might not be able to access the Grafana URL configured in Ceph Dashboard. One such scenario is when the cluster and the accessing user are each in a different DNS zone.

For this case, there is an extra configuration option for Ceph Dashboard, which can be used to configure the URL for accessing Grafana by the user’s browser. This value will never be altered by cephadm. To set this configuration option, issue the following command:

```
$ ceph dashboard set-grafana-frontend-api-url <grafana-server-api>
```

It may take a minute or two for services to be deployed.  Once completed, you should see something like this from `ceph orch ls`

```
$ ceph orch ls
NAME           RUNNING  REFRESHED  IMAGE NAME                                      IMAGE ID        SPEC
alertmanager       1/1  6s ago     docker.io/prom/alertmanager:latest              0881eb8f169f  present
crash              2/2  6s ago     docker.io/ceph/daemon-base:latest-master-devel  mix           present
grafana            1/1  0s ago     docker.io/pcuzner/ceph-grafana-el8:latest       f77afcf0bcf6   absent
node-exporter      2/2  6s ago     docker.io/prom/node-exporter:latest             e5a616e4b9cf  present
prometheus         1/1  6s ago     docker.io/prom/prometheus:latest                e935122ab143  present
```

### Configuring SSL/TLS for Grafana

`cephadm` will deploy Grafana using the certificate defined in the ceph key/value store. If a certificate is not specified, `cephadm` will generate a self-signed certificate during deployment of the Grafana service.

A custom certificate can be configured using the following commands.

```
ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
```

If you already deployed Grafana, you need to `reconfig` the service for the configuration to be updated.

```
ceph orch reconfig grafana
```

The `reconfig` command also takes care of setting the right URL for Ceph Dashboard.

### Using custom images

It is possible to install or upgrade monitoring components based on other images.  To do so, the name of the image to be used needs to be stored in the configuration first.  The following configuration options are available.

- `container_image_prometheus`
- `container_image_grafana`
- `container_image_alertmanager`
- `container_image_node_exporter`

Custom images can be set with the `ceph config` command

```
ceph config set mgr mgr/cephadm/<option_name> <value>
```

For example

```
ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1
```

Note

By setting a custom image, the default value will be overridden (but not overwritten).  The default value changes when updates become available. By setting a custom image, you will not be able to update the component you have set the custom image for automatically.  You will need to manually update the configuration (image name and tag) to be able to install updates.

If you choose to go with the recommendations instead, you can reset the custom image you have set before.  After that, the default value will be used again.  Use `ceph config rm` to reset the configuration option

```
ceph config rm mgr mgr/cephadm/<option_name>
```

For example

```
ceph config rm mgr mgr/cephadm/container_image_prometheus
```

### Using custom configuration files

By overriding cephadm templates, it is possible to completely customize the configuration files for monitoring services.

Internally, cephadm already uses [Jinja2](https://jinja.palletsprojects.com/en/2.11.x/) templates to generate the configuration files for all monitoring components. To be able to customize the configuration of Prometheus, Grafana or the Alertmanager it is possible to store a Jinja2 template for each service that will be used for configuration generation instead. This template will be evaluated every time a service of that kind is deployed or reconfigured. That way, the custom configuration is preserved and automatically applied on future deployments of these services.

Note

The configuration of the custom template is also preserved when the default configuration of cephadm changes. If the updated configuration is to be used, the custom template needs to be migrated *manually*.

#### Option names

The following templates for files that will be generated by cephadm can be overridden. These are the names to be used when storing with `ceph config-key set`:

- `services/alertmanager/alertmanager.yml`
- `services/grafana/ceph-dashboard.yml`
- `services/grafana/grafana.ini`
- `services/prometheus/prometheus.yml`

You can look up the file templates that are currently used by cephadm in `src/pybind/mgr/cephadm/templates`:

- `services/alertmanager/alertmanager.yml.j2`
- `services/grafana/ceph-dashboard.yml.j2`
- `services/grafana/grafana.ini.j2`
- `services/prometheus/prometheus.yml.j2`

#### Usage

The following command applies a single line value:

```
ceph config-key set mgr/cephadm/<option_name> <value>
```

To set contents of files as template use the `-i` argument:

```
ceph config-key set mgr/cephadm/<option_name> -i $PWD/<filename>
```

Note

When using files as input to `config-key` an absolute path to the file must be used.

Then the configuration file for the service needs to be recreated. This is done using reconfig. For more details see the following example.

#### Example

```
# set the contents of ./prometheus.yml.j2 as template
ceph config-key set mgr/cephadm/services/prometheus/prometheus.yml \
  -i $PWD/prometheus.yml.j2

# reconfig the prometheus service
ceph orch reconfig prometheus
```

## Disabling monitoring

If you have deployed monitoring and would like to remove it, you can do so with

```
ceph orch rm grafana
ceph orch rm prometheus --force   # this will delete metrics data collected so far
ceph orch rm node-exporter
ceph orch rm alertmanager
ceph mgr module disable prometheus
```

## Deploying monitoring manually

If you have an existing prometheus monitoring infrastructure, or would like to manage it yourself, you need to configure it to integrate with your Ceph cluster.

- Enable the prometheus module in the ceph-mgr daemon

  ```
  ceph mgr module enable prometheus
  ```

  By default, ceph-mgr presents prometheus metrics on port 9283 on each host running a ceph-mgr daemon.  Configure prometheus to scrape these.

- To enable the dashboard’s prometheus-based alerting, see [Enabling Prometheus Alerting](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-alerting).

- To enable dashboard integration with Grafana, see [Enabling the Embedding of Grafana Dashboards](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-grafana).

## Enabling RBD-Image monitoring

Due to performance reasons, monitoring of RBD images is disabled by default. For more information please see [RBD IO statistics](https://docs.ceph.com/en/latest/mgr/prometheus/#prometheus-rbd-io-statistics). If disabled, the overview and details dashboards will stay empty in Grafana and the metrics will not be visible in Prometheus.

## Other

开源的ceph-web项目，是非常简单的Web前端，通过ceph-rest-api获得数据并展示。

## Ceph-web

开源地址 https://github.com/tobegit3hub/ceph-web 。

目前ceph-web已经支持通过容器运行，执行下述命令即可一键启动Ceph监控工具。

```bash
docker run -d --net=host tobegit3hub/ceph-web
```

这样通过浏览器打开 http://127.0.0.1:8080 就可以看到以下管理界面。

![](../../Image/c/ceph_web.png)

![](../../Image/c/ceph_inkscope.jpg)