# Cluster Operations[](https://docs.ceph.com/en/latest/rados/operations/#cluster-operations)

| High-level OperationsHigh-level cluster operations consist primarily of starting, stopping, and restarting a cluster with the `ceph` service;  checking the cluster’s health; and, monitoring an operating cluster.  [Operating a Cluster](https://docs.ceph.com/en/latest/rados/operations/operating/) [Health checks](https://docs.ceph.com/en/latest/rados/operations/health-checks/) [Monitoring a Cluster](https://docs.ceph.com/en/latest/rados/operations/monitoring/) [Monitoring OSDs and PGs](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/) [User Management](https://docs.ceph.com/en/latest/rados/operations/user-management/) [Repairing PG inconsistencies](https://docs.ceph.com/en/latest/rados/operations/pg-repair/) | Data PlacementOnce you have your cluster up and running, you may begin working with data placement. Ceph supports petabyte-scale data storage clusters, with storage pools and placement groups that distribute data across the cluster using Ceph’s CRUSH algorithm.  [Data Placement Overview](https://docs.ceph.com/en/latest/rados/operations/data-placement/) [Pools](https://docs.ceph.com/en/latest/rados/operations/pools/) [Erasure code](https://docs.ceph.com/en/latest/rados/operations/erasure-code/) [Cache Tiering](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/) [Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/) [Balancer](https://docs.ceph.com/en/latest/rados/operations/balancer/) [Using the pg-upmap](https://docs.ceph.com/en/latest/rados/operations/upmap/) [CRUSH Maps](https://docs.ceph.com/en/latest/rados/operations/crush-map/) [Manually editing a CRUSH Map](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/) [Stretch Clusters](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/) [Configure Monitor Election Strategies](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections/) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Low-level OperationsLow-level cluster operations consist of starting, stopping, and restarting a particular daemon within a cluster; changing the settings of a particular daemon or subsystem; and, adding a daemon to the cluster or removing a  daemon from the cluster. The most common use cases for low-level operations include growing or shrinking the Ceph cluster and replacing legacy or failed hardware with new hardware.  [Adding/Removing OSDs](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/) [Adding/Removing Monitors](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/) [Device Management](https://docs.ceph.com/en/latest/rados/operations/devices/) [BlueStore Migration](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/) [Command Reference](https://docs.ceph.com/en/latest/rados/operations/control/) | TroubleshootingCeph is still on the leading edge, so you may encounter situations that require you to evaluate your Ceph configuration and modify your logging and debugging settings to identify and remedy issues you are encountering with your cluster.  [The Ceph Community](https://docs.ceph.com/en/latest/rados/troubleshooting/community/) [Troubleshooting Monitors](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-mon/) [Troubleshooting OSDs](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/) [Troubleshooting PGs](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-pg/) [Logging and Debugging](https://docs.ceph.com/en/latest/rados/troubleshooting/log-and-debug/) [CPU Profiling](https://docs.ceph.com/en/latest/rados/troubleshooting/cpu-profiling/) [Memory Profiling](https://docs.ceph.com/en/latest/rados/troubleshooting/memory-profiling/) |

# Operating a Cluster[](https://docs.ceph.com/en/latest/rados/operations/operating/#operating-a-cluster)



## Running Ceph with systemd[](https://docs.ceph.com/en/latest/rados/operations/operating/#running-ceph-with-systemd)

For all distributions that support systemd (CentOS 7, Fedora, Debian Jessie 8 and later, SUSE), ceph daemons are now managed using native systemd files instead of the legacy sysvinit scripts.  For example:

```
sudo systemctl start ceph.target       # start all daemons
sudo systemctl status ceph-osd@12      # check status of osd.12
```

To list the Ceph systemd units on a node, execute:

```
sudo systemctl status ceph\*.service ceph\*.target
```

### Starting all Daemons[](https://docs.ceph.com/en/latest/rados/operations/operating/#starting-all-daemons)

To start all daemons on a Ceph Node (irrespective of type), execute the following:

```
sudo systemctl start ceph.target
```

### Stopping all Daemons[](https://docs.ceph.com/en/latest/rados/operations/operating/#stopping-all-daemons)

To stop all daemons on a Ceph Node (irrespective of type), execute the following:

```
sudo systemctl stop ceph\*.service ceph\*.target
```

### Starting all Daemons by Type[](https://docs.ceph.com/en/latest/rados/operations/operating/#starting-all-daemons-by-type)

To start all daemons of a particular type on a Ceph Node, execute one of the following:

```
sudo systemctl start ceph-osd.target
sudo systemctl start ceph-mon.target
sudo systemctl start ceph-mds.target
```

### Stopping all Daemons by Type[](https://docs.ceph.com/en/latest/rados/operations/operating/#stopping-all-daemons-by-type)

To stop all daemons of a particular type on a Ceph Node, execute one of the following:

```
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target
```

### Starting a Daemon[](https://docs.ceph.com/en/latest/rados/operations/operating/#starting-a-daemon)

To start a specific daemon instance on a Ceph Node, execute one of the following:

```
sudo systemctl start ceph-osd@{id}
sudo systemctl start ceph-mon@{hostname}
sudo systemctl start ceph-mds@{hostname}
```

For example:

```
sudo systemctl start ceph-osd@1
sudo systemctl start ceph-mon@ceph-server
sudo systemctl start ceph-mds@ceph-server
```

### Stopping a Daemon[](https://docs.ceph.com/en/latest/rados/operations/operating/#stopping-a-daemon)

To stop a specific daemon instance on a Ceph Node, execute one of the following:

```
sudo systemctl stop ceph-osd@{id}
sudo systemctl stop ceph-mon@{hostname}
sudo systemctl stop ceph-mds@{hostname}
```

For example:

```
sudo systemctl stop ceph-osd@1
sudo systemctl stop ceph-mon@ceph-server
sudo systemctl stop ceph-mds@ceph-server
```



## Running Ceph with sysvinit[](https://docs.ceph.com/en/latest/rados/operations/operating/#running-ceph-with-sysvinit)

Each time you to **start**, **restart**, and  **stop** Ceph daemons (or your entire cluster) you must specify at least one option and one command. You may also specify a daemon type or a daemon instance.

```
{commandline} [options] [commands] [daemons]
```

The `ceph` options include:

| Option        | Shortcut | Description                                                  |
| ------------- | -------- | ------------------------------------------------------------ |
| `--verbose`   | `-v`     | Use verbose logging.                                         |
| `--valgrind`  | `N/A`    | (Dev and QA only) Use [Valgrind](http://www.valgrind.org/) debugging. |
| `--allhosts`  | `-a`     | Execute on all nodes in `ceph.conf.` Otherwise, it only executes on `localhost`. |
| `--restart`   | `N/A`    | Automatically restart daemon if it core dumps.               |
| `--norestart` | `N/A`    | Don’t restart a daemon if it core dumps.                     |
| `--conf`      | `-c`     | Use an alternate configuration file.                         |

The `ceph` commands include:

| Command        | Description                                     |
| -------------- | ----------------------------------------------- |
| `start`        | Start the daemon(s).                            |
| `stop`         | Stop the daemon(s).                             |
| `forcestop`    | Force the daemon(s) to stop. Same as `kill -9`  |
| `killall`      | Kill all daemons of a particular type.          |
| `cleanlogs`    | Cleans out the log directory.                   |
| `cleanalllogs` | Cleans out **everything** in the log directory. |

For subsystem operations, the `ceph` service can target specific daemon types by adding a particular daemon type for the `[daemons]` option. Daemon types include:

- `mon`
- `osd`
- `mds`

# Health checks[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#health-checks)

## Overview[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#overview)

There is a finite set of possible health messages that a Ceph cluster can raise – these are defined as *health checks* which have unique identifiers.

The identifier is a terse pseudo-human-readable (i.e. like a variable name) string.  It is intended to enable tools (such as UIs) to make sense of health checks, and present them in a way that reflects their meaning.

This page lists the health checks that are raised by the monitor and manager daemons.  In addition to these, you may also see health checks that originate from MDS daemons (see [CephFS health messages](https://docs.ceph.com/en/latest/cephfs/health-messages/#cephfs-health-messages)), and health checks that are defined by ceph-mgr python modules.

## Definitions[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#definitions)

### Monitor[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#monitor)

#### DAEMON_OLD_VERSION[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#daemon-old-version)

Warn if old version(s) of Ceph are running on any daemons. It will generate a health error if multiple versions are detected. This condition must exist for over mon_warn_older_version_delay (set to 1 week by default) in order for the health condition to be triggered.  This allows most upgrades to proceed without falsely seeing the warning.  If upgrade is paused for an extended time period, health mute can be used like this “ceph health mute DAEMON_OLD_VERSION –sticky”.  In this case after upgrade has finished use “ceph health unmute DAEMON_OLD_VERSION”.

#### MON_DOWN[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mon-down)

One or more monitor daemons is currently down.  The cluster requires a majority (more than 1/2) of the monitors in order to function.  When one or more monitors are down, clients may have a harder time forming their initial connection to the cluster as they may need to try more addresses before they reach an operating monitor.

The down monitor daemon should generally be restarted as soon as possible to reduce the risk of a subsequent monitor failure leading to a service outage.

#### MON_CLOCK_SKEW[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mon-clock-skew)

The clocks on the hosts running the ceph-mon monitor daemons are not sufficiently well synchronized.  This health alert is raised if the cluster detects a clock skew greater than `mon_clock_drift_allowed`.

This is best resolved by synchronizing the clocks using a tool like `ntpd` or `chrony`.

If it is impractical to keep the clocks closely synchronized, the `mon_clock_drift_allowed` threshold can also be increased, but this value must stay significantly below the `mon_lease` interval in order for monitor cluster to function properly.

#### MON_MSGR2_NOT_ENABLED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mon-msgr2-not-enabled)

The `ms_bind_msgr2` option is enabled but one or more monitors is not configured to bind to a v2 port in the cluster’s monmap.  This means that features specific to the msgr2 protocol (e.g., encryption) are not available on some or all connections.

In most cases this can be corrected by issuing the command:

```
ceph mon enable-msgr2
```

That command will change any monitor configured for the old default port 6789 to continue to listen for v1 connections on 6789 and also listen for v2 connections on the new default 3300 port.

If a monitor is configured to listen for v1 connections on a  non-standard port (not 6789), then the monmap will need to be modified  manually.

#### MON_DISK_LOW[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mon-disk-low)

One or more monitors is low on disk space.  This alert triggers if the available space on the file system storing the monitor database (normally `/var/lib/ceph/mon`), as a percentage, drops below `mon_data_avail_warn` (default: 30%).

This may indicate that some other process or user on the system is filling up the same file system used by the monitor.  It may also indicate that the monitors database is large (see `MON_DISK_BIG` below).

If space cannot be freed, the monitor’s data directory may need to be moved to another storage device or file system (while the monitor daemon is not running, of course).

#### MON_DISK_CRIT[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mon-disk-crit)

One or more monitors is critically low on disk space.  This alert triggers if the available space on the file system storing the monitor database (normally `/var/lib/ceph/mon`), as a percentage, drops below `mon_data_avail_crit` (default: 5%).  See `MON_DISK_LOW`, above.

#### MON_DISK_BIG[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mon-disk-big)

The database size for one or more monitors is very large.  This alert triggers if the size of the monitor’s database is larger than `mon_data_size_warn` (default: 15 GiB).

A large database is unusual, but may not necessarily indicate a problem.  Monitor databases may grow in size when there are placement groups that have not reached an `active+clean` state in a long time.

This may also indicate that the monitor’s database is not properly compacting, which has been observed with some older versions of leveldb and rocksdb.  Forcing a compaction with `ceph daemon mon.<id> compact` may shrink the on-disk size.

This warning may also indicate that the monitor has a bug that is preventing it from pruning the cluster metadata it stores.  If the problem persists, please report a bug.

The warning threshold may be adjusted with:

```
ceph config set global mon_data_size_warn <size>
```

#### AUTH_INSECURE_GLOBAL_ID_RECLAIM[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#auth-insecure-global-id-reclaim)

One or more clients or daemons are connected to the cluster that are not securely reclaiming their global_id (a unique number identifying each entity in the cluster) when reconnecting to a monitor.  The client is being permitted to connect anyway because the `auth_allow_insecure_global_id_reclaim` option is set to true (which may be necessary until all ceph clients have been upgraded), and the `auth_expose_insecure_global_id_reclaim` option set to `true` (which allows monitors to detect clients with insecure reclaim early by forcing them to reconnect right after they first authenticate).

You can identify which client(s) are using unpatched ceph client code with:

```
ceph health detail
```

Clients global_id reclaim rehavior can also seen in the `global_id_status` field in the dump of clients connected to an individual monitor (`reclaim_insecure` means the client is unpatched and is contributing to this health alert):

```
ceph tell mon.\* sessions
```

We strongly recommend that all clients in the system are upgraded to a newer version of Ceph that correctly reclaims global_id values.  Once all clients have been updated, you can stop allowing insecure reconnections with:

```
ceph config set mon auth_allow_insecure_global_id_reclaim false
```

If it is impractical to upgrade all clients immediately, you can silence this warning temporarily with:

```
ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1w   # 1 week
```

Although we do NOT recommend doing so, you can also disable this warning indefinitely with:

```
ceph config set mon mon_warn_on_insecure_global_id_reclaim false
```

#### AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#auth-insecure-global-id-reclaim-allowed)

Ceph is currently configured to allow clients to reconnect to monitors using an insecure process to reclaim their previous global_id because the setting `auth_allow_insecure_global_id_reclaim` is set to `true`.  It may be necessary to leave this setting enabled while existing Ceph clients are upgraded to newer versions of Ceph that correctly and securely reclaim their global_id.

If the `AUTH_INSECURE_GLOBAL_ID_RECLAIM` health alert has not also been raised and the `auth_expose_insecure_global_id_reclaim` setting has not been disabled (it is on by default), then there are currently no clients connected that need to be upgraded, and it is safe to disallow insecure global_id reclaim with:

```
ceph config set mon auth_allow_insecure_global_id_reclaim false
```

If there are still clients that need to be upgraded, then this alert can be silenced temporarily with:

```
ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1w   # 1 week
```

Although we do NOT recommend doing so, you can also disable this warning indefinitely with:

```
ceph config set mon mon_warn_on_insecure_global_id_reclaim_allowed false
```

### Manager[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#manager)

#### MGR_DOWN[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mgr-down)

All manager daemons are currently down.  The cluster should normally have at least one running manager (`ceph-mgr`) daemon.  If no manager daemon is running, the cluster’s ability to monitor itself will be compromised, and parts of the management API will become unavailable (for example, the dashboard will not work, and most CLI commands that report metrics or runtime state will block).  However, the cluster will still be able to perform all IO operations and recover from failures.

The down manager daemon should generally be restarted as soon as possible to ensure that the cluster can be monitored (e.g., so that the `ceph -s` information is up to date, and/or metrics can be scraped by Prometheus).

#### MGR_MODULE_DEPENDENCY[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mgr-module-dependency)

An enabled manager module is failing its dependency check.  This health check should come with an explanatory message from the module about the problem.

For example, a module might report that a required package is not installed: install the required package and restart your manager daemons.

This health check is only applied to enabled modules.  If a module is not enabled, you can see whether it is reporting dependency issues in the output of ceph module ls.

#### MGR_MODULE_ERROR[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#mgr-module-error)

A manager module has experienced an unexpected error.  Typically, this means an unhandled exception was raised from the module’s serve function.  The human readable description of the error may be obscurely worded if the exception did not provide a useful description of itself.

This health check may indicate a bug: please open a Ceph bug report if you think you have encountered a bug.

If you believe the error is transient, you may restart your manager daemon(s), or use ceph mgr fail on the active daemon to prompt a failover to another daemon.

### OSDs[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osds)

#### OSD_DOWN[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-down)

One or more OSDs are marked down.  The ceph-osd daemon may have been stopped, or peer OSDs may be unable to reach the OSD over the network. Common causes include a stopped or crashed daemon, a down host, or a network outage.

Verify the host is healthy, the daemon is started, and network is functioning.  If the daemon has crashed, the daemon log file (`/var/log/ceph/ceph-osd.*`) may contain debugging information.

#### OSD_<crush type>_DOWN[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-crush-type-down)

(e.g. OSD_HOST_DOWN, OSD_ROOT_DOWN)

All the OSDs within a particular CRUSH subtree are marked down, for example all OSDs on a host.

#### OSD_ORPHAN[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-orphan)

An OSD is referenced in the CRUSH map hierarchy but does not exist.

The OSD can be removed from the CRUSH hierarchy with:

```
ceph osd crush rm osd.<id>
```

#### OSD_OUT_OF_ORDER_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-out-of-order-full)

The utilization thresholds for nearfull, backfillfull, full, and/or failsafe_full are not ascending.  In particular, we expect nearfull < backfillfull, backfillfull < full, and full < failsafe_full.

The thresholds can be adjusted with:

```
ceph osd set-nearfull-ratio <ratio>
ceph osd set-backfillfull-ratio <ratio>
ceph osd set-full-ratio <ratio>
```

#### OSD_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-full)

One or more OSDs has exceeded the full threshold and is preventing the cluster from servicing writes.

Utilization by pool can be checked with:

```
ceph df
```

The currently defined full ratio can be seen with:

```
ceph osd dump | grep full_ratio
```

A short-term workaround to restore write availability is to raise the full threshold by a small amount:

```
ceph osd set-full-ratio <ratio>
```

New storage should be added to the cluster by deploying more OSDs or existing data should be deleted in order to free up space.

#### OSD_BACKFILLFULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-backfillfull)

One or more OSDs has exceeded the backfillfull threshold, which will prevent data from being allowed to rebalance to this device.  This is an early warning that rebalancing may not be able to complete and that the cluster is approaching full.

Utilization by pool can be checked with:

```
ceph df
```

#### OSD_NEARFULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-nearfull)

One or more OSDs has exceeded the nearfull threshold.  This is an early warning that the cluster is approaching full.

Utilization by pool can be checked with:

```
ceph df
```

#### OSDMAP_FLAGS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osdmap-flags)

One or more cluster flags of interest has been set.  These flags include:

- *full* - the cluster is flagged as full and cannot serve writes
- *pauserd*, *pausewr* - paused reads or writes
- *noup* - OSDs are not allowed to start
- *nodown* - OSD failure reports are being ignored, such that the monitors will not mark OSDs down
- *noin* - OSDs that were previously marked out will not be marked back in when they start
- *noout* - down OSDs will not automatically be marked out after the configured interval
- *nobackfill*, *norecover*, *norebalance* - recovery or data rebalancing is suspended
- *noscrub*, *nodeep_scrub* - scrubbing is disabled
- *notieragent* - cache tiering activity is suspended

With the exception of *full*, these flags can be set or cleared with:

```
ceph osd set <flag>
ceph osd unset <flag>
```

#### OSD_FLAGS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-flags)

One or more OSDs or CRUSH {nodes,device classes} has a flag of interest set. These flags include:

- *noup*: these OSDs are not allowed to start
- *nodown*: failure reports for these OSDs will be ignored
- *noin*: if these OSDs were previously marked out automatically after a failure, they will not be marked in when they start
- *noout*: if these OSDs are down they will not automatically be marked out after the configured interval

These flags can be set and cleared in batch with:

```
ceph osd set-group <flags> <who>
ceph osd unset-group <flags> <who>
```

For example,

```
ceph osd set-group noup,noout osd.0 osd.1
ceph osd unset-group noup,noout osd.0 osd.1
ceph osd set-group noup,noout host-foo
ceph osd unset-group noup,noout host-foo
ceph osd set-group noup,noout class-hdd
ceph osd unset-group noup,noout class-hdd
```

#### OLD_CRUSH_TUNABLES[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#old-crush-tunables)

The CRUSH map is using very old settings and should be updated.  The oldest tunables that can be used (i.e., the oldest client version that can connect to the cluster) without triggering this health warning is determined by the `mon_crush_min_required_version` config option. See [Tunables](https://docs.ceph.com/en/latest/rados/operations/crush-map/#crush-map-tunables) for more information.

#### OLD_CRUSH_STRAW_CALC_VERSION[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#old-crush-straw-calc-version)

The CRUSH map is using an older, non-optimal method for calculating intermediate weight values for `straw` buckets.

The CRUSH map should be updated to use the newer method (`straw_calc_version=1`).  See [Tunables](https://docs.ceph.com/en/latest/rados/operations/crush-map/#crush-map-tunables) for more information.

#### CACHE_POOL_NO_HIT_SET[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#cache-pool-no-hit-set)

One or more cache pools is not configured with a *hit set* to track utilization, which will prevent the tiering agent from identifying cold objects to flush and evict from the cache.

Hit sets can be configured on the cache pool with:

```
ceph osd pool set <poolname> hit_set_type <type>
ceph osd pool set <poolname> hit_set_period <period-in-seconds>
ceph osd pool set <poolname> hit_set_count <number-of-hitsets>
ceph osd pool set <poolname> hit_set_fpp <target-false-positive-rate>
```

#### OSD_NO_SORTBITWISE[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-no-sortbitwise)

No pre-luminous v12.y.z OSDs are running but the `sortbitwise` flag has not been set.

The `sortbitwise` flag must be set before luminous v12.y.z or newer OSDs can start.  You can safely set the flag with:

```
ceph osd set sortbitwise
```

#### OSD_FILESTORE[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-filestore)

Filestore has been deprecated, considering that Bluestore has been the default objectstore for quite some time. Warn if OSDs are running Filestore.

The ‘mclock_scheduler’ is not supported for filestore OSDs. Therefore, the default ‘osd_op_queue’ is set to ‘wpq’ for filestore OSDs and is enforced even if the user attempts to change it.

Filestore OSDs can be listed with:

```
ceph report | jq -c '."osd_metadata" | .[] | select(.osd_objectstore | contains("filestore")) | {id, osd_objectstore}'
```

If it is not feasible to migrate Filestore OSDs to Bluestore immediately, you can silence this warning temporarily with:

```
ceph health mute OSD_FILESTORE
```

#### POOL_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-full)

One or more pools has reached its quota and is no longer allowing writes.

Pool quotas and utilization can be seen with:

```
ceph df detail
```

You can either raise the pool quota with:

```
ceph osd pool set-quota <poolname> max_objects <num-objects>
ceph osd pool set-quota <poolname> max_bytes <num-bytes>
```

or delete some existing data to reduce utilization.

#### BLUEFS_SPILLOVER[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluefs-spillover)

One or more OSDs that use the BlueStore backend have been allocated db partitions (storage space for metadata, normally on a faster device) but that space has filled, such that metadata has “spilled over” onto the normal slow device.  This isn’t necessarily an error condition or even unexpected, but if the administrator’s expectation was that all metadata would fit on the faster device, it indicates that not enough space was provided.

This warning can be disabled on all OSDs with:

```
ceph config set osd bluestore_warn_on_bluefs_spillover false
```

Alternatively, it can be disabled on a specific OSD with:

```
ceph config set osd.123 bluestore_warn_on_bluefs_spillover false
```

To provide more metadata space, the OSD in question could be destroyed and reprovisioned.  This will involve data migration and recovery.

It may also be possible to expand the LVM logical volume backing the db storage.  If the underlying LV has been expanded, the OSD daemon needs to be stopped and BlueFS informed of the device size change with:

```
ceph-bluestore-tool bluefs-bdev-expand --path /var/lib/ceph/osd/ceph-$ID
```

#### BLUEFS_AVAILABLE_SPACE[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluefs-available-space)

To check how much space is free for BlueFS do:

```
ceph daemon osd.123 bluestore bluefs available
```

This will output up to 3 values: BDEV_DB free, BDEV_SLOW free and available_from_bluestore. BDEV_DB and BDEV_SLOW report amount of space that has been acquired by BlueFS and is considered free. Value available_from_bluestore denotes ability of BlueStore to relinquish more space to BlueFS. It is normal that this value is different from amount of BlueStore free space, as BlueFS allocation unit is typically larger than BlueStore allocation unit. This means that only part of BlueStore free space will be acceptable for BlueFS.

#### BLUEFS_LOW_SPACE[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluefs-low-space)

If BlueFS is running low on available free space and there is little available_from_bluestore one can consider reducing BlueFS allocation unit size. To simulate available space when allocation unit is different do:

```
ceph daemon osd.123 bluestore bluefs available <alloc-unit-size>
```

#### BLUESTORE_FRAGMENTATION[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-fragmentation)

As BlueStore works free space on underlying storage will get fragmented. This is normal and unavoidable but excessive fragmentation will cause slowdown. To inspect BlueStore fragmentation one can do:

```
ceph daemon osd.123 bluestore allocator score block
```

Score is given in [0-1] range. [0.0 .. 0.4] tiny fragmentation [0.4 .. 0.7] small, acceptable fragmentation [0.7 .. 0.9] considerable, but safe fragmentation [0.9 .. 1.0] severe fragmentation, may impact BlueFS ability to get space from BlueStore

If detailed report of free fragments is required do:

```
ceph daemon osd.123 bluestore allocator dump block
```

In case when handling OSD process that is not running fragmentation can be inspected with ceph-bluestore-tool. Get fragmentation score:

```
ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-123 --allocator block free-score
```

And dump detailed free chunks:

```
ceph-bluestore-tool --path /var/lib/ceph/osd/ceph-123 --allocator block free-dump
```

#### BLUESTORE_LEGACY_STATFS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-legacy-statfs)

In the Nautilus release, BlueStore tracks its internal usage statistics on a per-pool granular basis, and one or more OSDs have BlueStore volumes that were created prior to Nautilus.  If *all* OSDs are older than Nautilus, this just means that the per-pool metrics are not available.  However, if there is a mix of pre-Nautilus and post-Nautilus OSDs, the cluster usage statistics reported by `ceph df` will not be accurate.

The old OSDs can be updated to use the new usage tracking scheme by  stopping each OSD, running a repair operation, and the restarting it.   For example, if `osd.123` needed to be updated,:

```
systemctl stop ceph-osd@123
ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-123
systemctl start ceph-osd@123
```

This warning can be disabled with:

```
ceph config set global bluestore_warn_on_legacy_statfs false
```

#### BLUESTORE_NO_PER_POOL_OMAP[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-no-per-pool-omap)

Starting with the Octopus release, BlueStore tracks omap space utilization by pool, and one or more OSDs have volumes that were created prior to Octopus.  If all OSDs are not running BlueStore with the new tracking enabled, the cluster will report and approximate value for per-pool omap usage based on the most recent deep-scrub.

The old OSDs can be updated to track by pool by stopping each OSD, running a repair operation, and the restarting it.  For example, if `osd.123` needed to be updated,:

```
systemctl stop ceph-osd@123
ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-123
systemctl start ceph-osd@123
```

This warning can be disabled with:

```
ceph config set global bluestore_warn_on_no_per_pool_omap false
```

#### BLUESTORE_NO_PER_PG_OMAP[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-no-per-pg-omap)

Starting with the Pacific release, BlueStore tracks omap space utilization by PG, and one or more OSDs have volumes that were created prior to Pacific.  Per-PG omap enables faster PG removal when PGs migrate.

The older OSDs can be updated to track by PG by stopping each OSD, running a repair operation, and the restarting it.  For example, if `osd.123` needed to be updated,:

```
systemctl stop ceph-osd@123
ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-123
systemctl start ceph-osd@123
```

This warning can be disabled with:

```
ceph config set global bluestore_warn_on_no_per_pg_omap false
```

#### BLUESTORE_DISK_SIZE_MISMATCH[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-disk-size-mismatch)

One or more OSDs using BlueStore has an internal inconsistency between the size of the physical device and the metadata tracking its size.  This can lead to the OSD crashing in the future.

The OSDs in question should be destroyed and reprovisioned.  Care should be taken to do this one OSD at a time, and in a way that doesn’t put any data at risk.  For example, if osd `$N` has the error,:

```
ceph osd out osd.$N
while ! ceph osd safe-to-destroy osd.$N ; do sleep 1m ; done
ceph osd destroy osd.$N
ceph-volume lvm zap /path/to/device
ceph-volume lvm create --osd-id $N --data /path/to/device
```

#### BLUESTORE_NO_COMPRESSION[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-no-compression)

One or more OSDs is unable to load a BlueStore compression plugin. This can be caused by a broken installation, in which the `ceph-osd` binary does not match the compression plugins, or a recent upgrade that did not include a restart of the `ceph-osd` daemon.

Verify that the package(s) on the host running the OSD(s) in question are correctly installed and that the OSD daemon(s) have been restarted.  If the problem persists, check the OSD log for any clues as to the source of the problem.

#### BLUESTORE_SPURIOUS_READ_ERRORS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#bluestore-spurious-read-errors)

One or more OSDs using BlueStore detects spurious read errors at main device. BlueStore has recovered from these errors by retrying disk reads. Though this might show some issues with underlying hardware, I/O subsystem, etc. Which theoretically might cause permanent data corruption. Some observations on the root cause can be found at https://tracker.ceph.com/issues/22464

This alert doesn’t require immediate response but corresponding host might need additional attention, e.g. upgrading to the latest OS/kernel versions and H/W resource utilization monitoring.

This warning can be disabled on all OSDs with:

```
ceph config set osd bluestore_warn_on_spurious_read_errors false
```

Alternatively, it can be disabled on a specific OSD with:

```
ceph config set osd.123 bluestore_warn_on_spurious_read_errors false
```

### Device health[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#device-health)

#### DEVICE_HEALTH[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#id2)

One or more devices is expected to fail soon, where the warning threshold is controlled by the `mgr/devicehealth/warn_threshold` config option.

This warning only applies to OSDs that are currently marked “in”, so the expected response to this failure is to mark the device “out” so that data is migrated off of the device, and then to remove the hardware from the system.  Note that the marking out is normally done automatically if `mgr/devicehealth/self_heal` is enabled based on the `mgr/devicehealth/mark_out_threshold`.

Device health can be checked with:

```
ceph device info <device-id>
```

Device life expectancy is set by a prediction model run by the mgr or an by external tool via the command:

```
ceph device set-life-expectancy <device-id> <from> <to>
```

You can change the stored life expectancy manually, but that usually doesn’t accomplish anything as whatever tool originally set it will probably set it again, and changing the stored value does not affect the actual health of the hardware device.

#### DEVICE_HEALTH_IN_USE[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#device-health-in-use)

One or more devices is expected to fail soon and has been marked “out” of the cluster based on `mgr/devicehealth/mark_out_threshold`, but it is still participating in one more PGs.  This may be because it was only recently marked “out” and data is still migrating, or because data cannot be migrated off for some reason (e.g., the cluster is nearly full, or the CRUSH hierarchy is such that there isn’t another suitable OSD to migrate the data too).

This message can be silenced by disabling the self heal behavior (setting `mgr/devicehealth/self_heal` to false), by adjusting the `mgr/devicehealth/mark_out_threshold`, or by addressing what is preventing data from being migrated off of the ailing device.

#### DEVICE_HEALTH_TOOMANY[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#device-health-toomany)

Too many devices is expected to fail soon and the `mgr/devicehealth/self_heal` behavior is enabled, such that marking out all of the ailing devices would exceed the clusters `mon_osd_min_in_ratio` ratio that prevents too many OSDs from being automatically marked “out”.

This generally indicates that too many devices in your cluster are expected to fail soon and you should take action to add newer (healthier) devices before too many devices fail and data is lost.

The health message can also be silenced by adjusting parameters like `mon_osd_min_in_ratio` or `mgr/devicehealth/mark_out_threshold`, but be warned that this will increase the likelihood of unrecoverable data loss in the cluster.

### Data health (pools & placement groups)[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#data-health-pools-placement-groups)

#### PG_AVAILABILITY[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-availability)

Data availability is reduced, meaning that the cluster is unable to service potential read or write requests for some data in the cluster. Specifically, one or more PGs is in a state that does not allow IO requests to be serviced.  Problematic PG states include *peering*, *stale*, *incomplete*, and the lack of *active* (if those conditions do not clear quickly).

Detailed information about which PGs are affected is available from:

```
ceph health detail
```

In most cases the root cause is that one or more OSDs is currently down; see the discussion for `OSD_DOWN` above.

The state of specific problematic PGs can be queried with:

```
ceph tell <pgid> query
```

#### PG_DEGRADED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-degraded)

Data redundancy is reduced for some data, meaning the cluster does not have the desired number of replicas for all data (for replicated pools) or erasure code fragments (for erasure coded pools). Specifically, one or more PGs:

- has the *degraded* or *undersized* flag set, meaning there are not enough instances of that placement group in the cluster;
- has not had the *clean* flag set for some time.

Detailed information about which PGs are affected is available from:

```
ceph health detail
```

In most cases the root cause is that one or more OSDs is currently down; see the discussion for `OSD_DOWN` above.

The state of specific problematic PGs can be queried with:

```
ceph tell <pgid> query
```

#### PG_RECOVERY_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-recovery-full)

Data redundancy may be reduced or at risk for some data due to a lack of free space in the cluster.  Specifically, one or more PGs has the *recovery_toofull* flag set, meaning that the cluster is unable to migrate or recover data because one or more OSDs is above the *full* threshold.

See the discussion for *OSD_FULL* above for steps to resolve this condition.

#### PG_BACKFILL_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-backfill-full)

Data redundancy may be reduced or at risk for some data due to a lack of free space in the cluster.  Specifically, one or more PGs has the *backfill_toofull* flag set, meaning that the cluster is unable to migrate or recover data because one or more OSDs is above the *backfillfull* threshold.

See the discussion for *OSD_BACKFILLFULL* above for steps to resolve this condition.

#### PG_DAMAGED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-damaged)

Data scrubbing has discovered some problems with data consistency in the cluster.  Specifically, one or more PGs has the *inconsistent* or *snaptrim_error* flag is set, indicating an earlier scrub operation found a problem, or that the *repair* flag is set, meaning a repair for such an inconsistency is currently in progress.

See [Repairing PG inconsistencies](https://docs.ceph.com/en/latest/rados/operations/pg-repair/) for more information.

#### OSD_SCRUB_ERRORS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-scrub-errors)

Recent OSD scrubs have uncovered inconsistencies. This error is generally paired with *PG_DAMAGED* (see above).

See [Repairing PG inconsistencies](https://docs.ceph.com/en/latest/rados/operations/pg-repair/) for more information.

#### OSD_TOO_MANY_REPAIRS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-too-many-repairs)

When a read error occurs and another replica is available it is used to repair the error immediately, so that the client can get the object data.  Scrub handles errors for data at rest.  In order to identify possible failing disks that aren’t seeing scrub errors, a count of read repairs is maintained.  If it exceeds a config value threshold *mon_osd_warn_num_repaired* default 10, this health warning is generated.

#### LARGE_OMAP_OBJECTS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#large-omap-objects)

One or more pools contain large omap objects as determined by `osd_deep_scrub_large_omap_object_key_threshold` (threshold for number of keys to determine a large omap object) or `osd_deep_scrub_large_omap_object_value_sum_threshold` (the threshold for summed size (bytes) of all key values to determine a large omap object) or both. More information on the object name, key count, and size in bytes can be found by searching the cluster log for ‘Large omap object found’. Large omap objects can be caused by RGW bucket index objects that do not have automatic resharding enabled. Please see [RGW Dynamic Bucket Index Resharding](https://docs.ceph.com/en/latest/radosgw/dynamicresharding/#rgw-dynamic-bucket-index-resharding) for more information on resharding.

The thresholds can be adjusted with:

```
ceph config set osd osd_deep_scrub_large_omap_object_key_threshold <keys>
ceph config set osd osd_deep_scrub_large_omap_object_value_sum_threshold <bytes>
```

#### CACHE_POOL_NEAR_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#cache-pool-near-full)

A cache tier pool is nearly full.  Full in this context is determined by the `target_max_bytes` and `target_max_objects` properties on the cache pool.  Once the pool reaches the target threshold, write requests to the pool may block while data is flushed and evicted from the cache, a state that normally leads to very high latencies and poor performance.

The cache pool target size can be adjusted with:

```
ceph osd pool set <cache-pool-name> target_max_bytes <bytes>
ceph osd pool set <cache-pool-name> target_max_objects <objects>
```

Normal cache flush and evict activity may also be throttled due to reduced availability or performance of the base tier, or overall cluster load.

#### TOO_FEW_PGS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#too-few-pgs)

The number of PGs in use in the cluster is below the configurable threshold of `mon_pg_warn_min_per_osd` PGs per OSD.  This can lead to suboptimal distribution and balance of data across the OSDs in the cluster, and similarly reduce overall performance.

This may be an expected condition if data pools have not yet been created.

The PG count for existing pools can be increased or new pools can be created. Please refer to [Choosing the number of Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-number-of-placement-groups) for more information.

#### POOL_PG_NUM_NOT_POWER_OF_TWO[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-pg-num-not-power-of-two)

One or more pools has a `pg_num` value that is not a power of two. Although this is not strictly incorrect, it does lead to a less balanced distribution of data because some PGs have roughly twice as much data as others.

This is easily corrected by setting the `pg_num` value for the affected pool(s) to a nearby power of two:

```
ceph osd pool set <pool-name> pg_num <value>
```

This health warning can be disabled with:

```
ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false
```

#### POOL_TOO_FEW_PGS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-too-few-pgs)

One or more pools should probably have more PGs, based on the amount of data that is currently stored in the pool.  This can lead to suboptimal distribution and balance of data across the OSDs in the cluster, and similarly reduce overall performance.  This warning is generated if the `pg_autoscale_mode` property on the pool is set to `warn`.

To disable the warning, you can disable auto-scaling of PGs for the pool entirely with:

```
ceph osd pool set <pool-name> pg_autoscale_mode off
```

To allow the cluster to automatically adjust the number of PGs,:

```
ceph osd pool set <pool-name> pg_autoscale_mode on
```

You can also manually set the number of PGs for the pool to the recommended amount with:

```
ceph osd pool set <pool-name> pg_num <new-pg-num>
```

Please refer to [Choosing the number of Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-number-of-placement-groups) and [Autoscaling placement groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#pg-autoscaler) for more information.

#### TOO_MANY_PGS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#too-many-pgs)

The number of PGs in use in the cluster is above the configurable threshold of `mon_max_pg_per_osd` PGs per OSD.  If this threshold is exceed the cluster will not allow new pools to be created, pool pg_num to be increased, or pool replication to be increased (any of which would lead to more PGs in the cluster).  A large number of PGs can lead to higher memory utilization for OSD daemons, slower peering after cluster state changes (like OSD restarts, additions, or removals), and higher load on the Manager and Monitor daemons.

The simplest way to mitigate the problem is to increase the number of OSDs in the cluster by adding more hardware.  Note that the OSD count used for the purposes of this health check is the number of “in” OSDs, so marking “out” OSDs “in” (if there are any) can also help:

```
ceph osd in <osd id(s)>
```

Please refer to [Choosing the number of Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-number-of-placement-groups) for more information.

#### POOL_TOO_MANY_PGS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-too-many-pgs)

One or more pools should probably have more PGs, based on the amount of data that is currently stored in the pool.  This can lead to higher memory utilization for OSD daemons, slower peering after cluster state changes (like OSD restarts, additions, or removals), and higher load on the Manager and Monitor daemons.  This warning is generated if the `pg_autoscale_mode` property on the pool is set to `warn`.

To disable the warning, you can disable auto-scaling of PGs for the pool entirely with:

```
ceph osd pool set <pool-name> pg_autoscale_mode off
```

To allow the cluster to automatically adjust the number of PGs,:

```
ceph osd pool set <pool-name> pg_autoscale_mode on
```

You can also manually set the number of PGs for the pool to the recommended amount with:

```
ceph osd pool set <pool-name> pg_num <new-pg-num>
```

Please refer to [Choosing the number of Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-number-of-placement-groups) and [Autoscaling placement groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#pg-autoscaler) for more information.

#### POOL_TARGET_SIZE_BYTES_OVERCOMMITTED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-target-size-bytes-overcommitted)

One or more pools have a `target_size_bytes` property set to estimate the expected size of the pool, but the value(s) exceed the total available storage (either by themselves or in combination with other pools’ actual usage).

This is usually an indication that the `target_size_bytes` value for the pool is too large and should be reduced or set to zero with:

```
ceph osd pool set <pool-name> target_size_bytes 0
```

For more information, see [Specifying expected pool size](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#specifying-pool-target-size).

#### POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-has-target-size-bytes-and-ratio)

One or more pools have both `target_size_bytes` and `target_size_ratio` set to estimate the expected size of the pool. Only one of these properties should be non-zero. If both are set, `target_size_ratio` takes precedence and `target_size_bytes` is ignored.

To reset `target_size_bytes` to zero:

```
ceph osd pool set <pool-name> target_size_bytes 0
```

For more information, see [Specifying expected pool size](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#specifying-pool-target-size).

#### TOO_FEW_OSDS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#too-few-osds)

The number of OSDs in the cluster is below the configurable threshold of `osd_pool_default_size`.

#### SMALLER_PGP_NUM[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#smaller-pgp-num)

One or more pools has a `pgp_num` value less than `pg_num`.  This is normally an indication that the PG count was increased without also increasing the placement behavior.

This is sometimes done deliberately to separate out the split step when the PG count is adjusted from the data migration that is needed when `pgp_num` is changed.

This is normally resolved by setting `pgp_num` to match `pg_num`, triggering the data migration, with:

```
ceph osd pool set <pool> pgp_num <pg-num-value>
```

#### MANY_OBJECTS_PER_PG[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#many-objects-per-pg)

One or more pools has an average number of objects per PG that is significantly higher than the overall cluster average.  The specific threshold is controlled by the `mon_pg_warn_max_object_skew` configuration value.

This is usually an indication that the pool(s) containing most of the data in the cluster have too few PGs, and/or that other pools that do not contain as much data have too many PGs.  See the discussion of *TOO_MANY_PGS* above.

The threshold can be raised to silence the health warning by adjusting the `mon_pg_warn_max_object_skew` config option on the managers.

The health warning will be silenced for a particular pool if `pg_autoscale_mode` is set to `on`.

#### POOL_APP_NOT_ENABLED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-app-not-enabled)

A pool exists that contains one or more objects but has not been tagged for use by a particular application.

Resolve this warning by labeling the pool for use by an application.  For example, if the pool is used by RBD,:

```
rbd pool init <poolname>
```

If the pool is being used by a custom application ‘foo’, you can also label via the low-level command:

```
ceph osd pool application enable foo
```

For more information, see [Associate Pool to Application](https://docs.ceph.com/en/latest/rados/operations/pools/#associate-pool-to-application).

#### POOL_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#id3)

One or more pools has reached (or is very close to reaching) its quota.  The threshold to trigger this error condition is controlled by the `mon_pool_quota_crit_threshold` configuration option.

Pool quotas can be adjusted up or down (or removed) with:

```
ceph osd pool set-quota <pool> max_bytes <bytes>
ceph osd pool set-quota <pool> max_objects <objects>
```

Setting the quota value to 0 will disable the quota.

#### POOL_NEAR_FULL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-near-full)

One or more pools is approaching a configured fullness threshold.

One threshold that can trigger this warning condition is the `mon_pool_quota_warn_threshold` configuration option.

Pool quotas can be adjusted up or down (or removed) with:

```
ceph osd pool set-quota <pool> max_bytes <bytes>
ceph osd pool set-quota <pool> max_objects <objects>
```

Setting the quota value to 0 will disable the quota.

Other thresholds that can trigger the above two warning conditions are `mon_osd_nearfull_ratio` and `mon_osd_full_ratio`.  Visit the [Storage Capacity](https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref/#storage-capacity) and [No Free Drive Space](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd/#no-free-drive-space) documents for details and resolution.

#### OBJECT_MISPLACED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#object-misplaced)

One or more objects in the cluster is not stored on the node the cluster would like it to be stored on.  This is an indication that data migration due to some recent cluster change has not yet completed.

Misplaced data is not a dangerous condition in and of itself; data consistency is never at risk, and old copies of objects are never removed until the desired number of new copies (in the desired locations) are present.

#### OBJECT_UNFOUND[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#object-unfound)

One or more objects in the cluster cannot be found.  Specifically, the OSDs know that a new or updated copy of an object should exist, but a copy of that version of the object has not been found on OSDs that are currently online.

Read or write requests to unfound objects will block.

Ideally, a down OSD can be brought back online that has the more recent copy of the unfound object.  Candidate OSDs can be identified from the peering state for the PG(s) responsible for the unfound object:

```
ceph tell <pgid> query
```

If the latest copy of the object is not available, the cluster can be told to roll back to a previous version of the object. See [Unfound Objects](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-pg/#failures-osd-unfound) for more information.

#### SLOW_OPS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#slow-ops)

One or more OSD or monitor requests is taking a long time to process.  This can be an indication of extreme load, a slow storage device, or a software bug.

The request queue for the daemon in question can be queried with the following command, executed from the daemon’s host:

```
ceph daemon osd.<id> ops
```

A summary of the slowest recent requests can be seen with:

```
ceph daemon osd.<id> dump_historic_ops
```

The location of an OSD can be found with:

```
ceph osd find osd.<id>
```

#### PG_NOT_SCRUBBED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-not-scrubbed)

One or more PGs has not been scrubbed recently.  PGs are normally scrubbed within every configured interval specified by [`osd_scrub_max_interval`](https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/#confval-osd_scrub_max_interval) globally. This interval can be overridden on per-pool basis with `scrub_max_interval`. The warning triggers when `mon_warn_pg_not_scrubbed_ratio` percentage of interval has elapsed without a scrub since it was due.

PGs will not scrub if they are not flagged as *clean*, which may happen if they are misplaced or degraded (see *PG_AVAILABILITY* and *PG_DEGRADED* above).

You can manually initiate a scrub of a clean PG with:

```
ceph pg scrub <pgid>
```

#### PG_NOT_DEEP_SCRUBBED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-not-deep-scrubbed)

One or more PGs has not been deep scrubbed recently.  PGs are normally scrubbed every [`osd_deep_scrub_interval`](https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/#confval-osd_deep_scrub_interval) seconds, and this warning triggers when `mon_warn_pg_not_deep_scrubbed_ratio` percentage of interval has elapsed without a scrub since it was due.

PGs will not (deep) scrub if they are not flagged as *clean*, which may happen if they are misplaced or degraded (see *PG_AVAILABILITY* and *PG_DEGRADED* above).

You can manually initiate a scrub of a clean PG with:

```
ceph pg deep-scrub <pgid>
```

#### PG_SLOW_SNAP_TRIMMING[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#pg-slow-snap-trimming)

The snapshot trim queue for one or more PGs has exceeded the configured warning threshold.  This indicates that either an extremely large number of snapshots were recently deleted, or that the OSDs are unable to trim snapshots quickly enough to keep up with the rate of new snapshot deletions.

The warning threshold is controlled by the `mon_osd_snap_trim_queue_warn_on` option (default: 32768).

This warning may trigger if OSDs are under excessive load and unable to keep up with their background work, or if the OSDs’ internal metadata database is heavily fragmented and unable to perform.  It may also indicate some other performance issue with the OSDs.

The exact size of the snapshot trim queue is reported by the `snaptrimq_len` field of `ceph pg ls -f json-detail`.

### Miscellaneous[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#miscellaneous)

#### RECENT_CRASH[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#recent-crash)

One or more Ceph daemons has crashed recently, and the crash has not yet been archived (acknowledged) by the administrator.  This may indicate a software bug, a hardware problem (e.g., a failing disk), or some other problem.

New crashes can be listed with:

```
ceph crash ls-new
```

Information about a specific crash can be examined with:

```
ceph crash info <crash-id>
```

This warning can be silenced by “archiving” the crash (perhaps after being examined by an administrator) so that it does not generate this warning:

```
ceph crash archive <crash-id>
```

Similarly, all new crashes can be archived with:

```
ceph crash archive-all
```

Archived crashes will still be visible via `ceph crash ls` but not `ceph crash ls-new`.

The time period for what “recent” means is controlled by the option `mgr/crash/warn_recent_interval` (default: two weeks).

These warnings can be disabled entirely with:

```
ceph config set mgr/crash/warn_recent_interval 0
```

#### RECENT_MGR_MODULE_CRASH[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#recent-mgr-module-crash)

One or more ceph-mgr modules has crashed recently, and the crash as not yet been archived (acknowledged) by the administrator.  This generally indicates a software bug in one of the software modules run inside the ceph-mgr daemon.  Although the module that experienced the problem maybe be disabled as a result, the function of other modules is normally unaffected.

As with the *RECENT_CRASH* health alert, the crash can be inspected with:

```
ceph crash info <crash-id>
```

This warning can be silenced by “archiving” the crash (perhaps after being examined by an administrator) so that it does not generate this warning:

```
ceph crash archive <crash-id>
```

Similarly, all new crashes can be archived with:

```
ceph crash archive-all
```

Archived crashes will still be visible via `ceph crash ls` but not `ceph crash ls-new`.

The time period for what “recent” means is controlled by the option `mgr/crash/warn_recent_interval` (default: two weeks).

These warnings can be disabled entirely with:

```
ceph config set mgr/crash/warn_recent_interval 0
```

#### TELEMETRY_CHANGED[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#telemetry-changed)

Telemetry has been enabled, but the contents of the telemetry report have changed since that time, so telemetry reports will not be sent.

The Ceph developers periodically revise the telemetry feature to include new and useful information, or to remove information found to be useless or sensitive.  If any new information is included in the report, Ceph will require the administrator to re-enable telemetry to ensure they have an opportunity to (re)review what information will be shared.

To review the contents of the telemetry report,:

```
ceph telemetry show
```

Note that the telemetry report consists of several optional channels that may be independently enabled or disabled.  For more information, see [Telemetry Module](https://docs.ceph.com/en/latest/mgr/telemetry/#telemetry).

To re-enable telemetry (and make this warning go away),:

```
ceph telemetry on
```

To disable telemetry (and make this warning go away),:

```
ceph telemetry off
```

#### AUTH_BAD_CAPS[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#auth-bad-caps)

One or more auth users has capabilities that cannot be parsed by the monitor.  This generally indicates that the user will not be authorized to perform any action with one or more daemon types.

This error is mostly likely to occur after an upgrade if the capabilities were set with an older version of Ceph that did not properly validate their syntax, or if the syntax of the capabilities has changed.

The user in question can be removed with:

```
ceph auth rm <entity-name>
```

(This will resolve the health alert, but obviously clients will not be able to authenticate as that user.)

Alternatively, the capabilities for the user can be updated with:

```
ceph auth <entity-name> <daemon-type> <caps> [<daemon-type> <caps> ...]
```

For more information about auth capabilities, see [User Management](https://docs.ceph.com/en/latest/rados/operations/user-management/#user-management).

#### OSD_NO_DOWN_OUT_INTERVAL[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#osd-no-down-out-interval)

The `mon_osd_down_out_interval` option is set to zero, which means that the system will not automatically perform any repair or healing operations after an OSD fails.  Instead, an administrator (or some other external entity) will need to manually mark down OSDs as ‘out’ (i.e., via `ceph osd out <osd-id>`) in order to trigger recovery.

This option is normally set to five or ten minutes–enough time for a host to power-cycle or reboot.

This warning can silenced by setting the `mon_warn_on_osd_down_out_interval_zero` to false:

```
ceph config global mon mon_warn_on_osd_down_out_interval_zero false
```

#### DASHBOARD_DEBUG[](https://docs.ceph.com/en/latest/rados/operations/health-checks/#dashboard-debug)

The Dashboard debug mode is enabled. This means, if there is an error while processing a REST API request, the HTTP error response contains a Python traceback. This behaviour should be disabled in production environments because such a traceback might contain and expose sensible information.

The debug mode can be disabled with:

```
ceph dashboard debug disable
```

# Monitoring a Cluster[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#monitoring-a-cluster)

Once you have a running cluster, you may use the `ceph` tool to monitor your cluster. Monitoring a cluster typically involves checking OSD status, monitor status, placement group status and metadata server status.

## Using the command line[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#using-the-command-line)

### Interactive mode[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#interactive-mode)

To run the `ceph` tool in interactive mode, type `ceph` at the command line with no arguments.  For example:

```
ceph
ceph> health
ceph> status
ceph> quorum_status
ceph> mon stat
```

### Non-default paths[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#non-default-paths)

If you specified non-default locations for your configuration or keyring, you may specify their locations:

```
ceph -c /path/to/conf -k /path/to/keyring health
```

## Checking a Cluster’s Status[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#checking-a-cluster-s-status)

After you start your cluster, and before you start reading and/or writing data, check your cluster’s status first.

To check a cluster’s status, execute the following:

```
ceph status
```

Or:

```
ceph -s
```

In interactive mode, type `status` and press **Enter**.

```
ceph> status
```

Ceph will print the cluster status. For example, a tiny Ceph demonstration cluster with one of each service may print the following:

```
cluster:
  id:     477e46f1-ae41-4e43-9c8f-72c918ab0a20
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum a,b,c
  mgr: x(active)
  mds: cephfs_a-1/1/1 up  {0=a=up:active}, 2 up:standby
  osd: 3 osds: 3 up, 3 in

data:
  pools:   2 pools, 16 pgs
  objects: 21 objects, 2.19K
  usage:   546 GB used, 384 GB / 931 GB avail
  pgs:     16 active+clean
```

How Ceph Calculates Data Usage

The `usage` value reflects the *actual* amount of raw storage used. The `xxx GB / xxx GB` value means the amount available (the lesser number) of the overall storage capacity of the cluster. The notional number reflects the size of the stored data before it is replicated, cloned or snapshotted. Therefore, the amount of data actually stored typically exceeds the notional amount stored, because Ceph creates replicas of the data and may also use storage capacity for cloning and snapshotting.

## Watching a Cluster[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#watching-a-cluster)

In addition to local logging by each daemon, Ceph clusters maintain a *cluster log* that records high level events about the whole system. This is logged to disk on monitor servers (as `/var/log/ceph/ceph.log` by default), but can also be monitored via the command line.

To follow the cluster log, use the following command

```
ceph -w
```

Ceph will print the status of the system, followed by each log message as it is emitted.  For example:

```
cluster:
  id:     477e46f1-ae41-4e43-9c8f-72c918ab0a20
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum a,b,c
  mgr: x(active)
  mds: cephfs_a-1/1/1 up  {0=a=up:active}, 2 up:standby
  osd: 3 osds: 3 up, 3 in

data:
  pools:   2 pools, 16 pgs
  objects: 21 objects, 2.19K
  usage:   546 GB used, 384 GB / 931 GB avail
  pgs:     16 active+clean


2017-07-24 08:15:11.329298 mon.a mon.0 172.21.9.34:6789/0 23 : cluster [INF] osd.0 172.21.9.34:6806/20527 boot
2017-07-24 08:15:14.258143 mon.a mon.0 172.21.9.34:6789/0 39 : cluster [INF] Activating manager daemon x
2017-07-24 08:15:15.446025 mon.a mon.0 172.21.9.34:6789/0 47 : cluster [INF] Manager daemon x is now available
```

In addition to using `ceph -w` to print log lines as they are emitted, use `ceph log last [n]` to see the most recent `n` lines from the cluster log.

## Monitoring Health Checks[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#monitoring-health-checks)

Ceph continuously runs various *health checks* against its own status.  When a health check fails, this is reflected in the output of `ceph status` (or `ceph health`).  In addition, messages are sent to the cluster log to indicate when a check fails, and when the cluster recovers.

For example, when an OSD goes down, the `health` section of the status output may be updated as follows:

```
health: HEALTH_WARN
        1 osds down
        Degraded data redundancy: 21/63 objects degraded (33.333%), 16 pgs unclean, 16 pgs degraded
```

At this time, cluster log messages are also emitted to record the failure of the health checks:

```
2017-07-25 10:08:58.265945 mon.a mon.0 172.21.9.34:6789/0 91 : cluster [WRN] Health check failed: 1 osds down (OSD_DOWN)
2017-07-25 10:09:01.302624 mon.a mon.0 172.21.9.34:6789/0 94 : cluster [WRN] Health check failed: Degraded data redundancy: 21/63 objects degraded (33.333%), 16 pgs unclean, 16 pgs degraded (PG_DEGRADED)
```

When the OSD comes back online, the cluster log records the cluster’s return to a health state:

```
2017-07-25 10:11:11.526841 mon.a mon.0 172.21.9.34:6789/0 109 : cluster [WRN] Health check update: Degraded data redundancy: 2 pgs unclean, 2 pgs degraded, 2 pgs undersized (PG_DEGRADED)
2017-07-25 10:11:13.535493 mon.a mon.0 172.21.9.34:6789/0 110 : cluster [INF] Health check cleared: PG_DEGRADED (was: Degraded data redundancy: 2 pgs unclean, 2 pgs degraded, 2 pgs undersized)
2017-07-25 10:11:13.535577 mon.a mon.0 172.21.9.34:6789/0 111 : cluster [INF] Cluster is now healthy
```

### Network Performance Checks[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#network-performance-checks)

Ceph OSDs send heartbeat ping messages amongst themselves to monitor daemon availability.  We also use the response times to monitor network performance. While it is possible that a busy OSD could delay a ping response, we can assume that if a network switch fails multiple delays will be detected between distinct pairs of OSDs.

By default we will warn about ping times which exceed 1 second (1000 milliseconds).

```
HEALTH_WARN Slow OSD heartbeats on back (longest 1118.001ms)
```

The health detail will add the combination of OSDs are seeing the delays and by how much.  There is a limit of 10 detail line items.

```
[WRN] OSD_SLOW_PING_TIME_BACK: Slow OSD heartbeats on back (longest 1118.001ms)
    Slow OSD heartbeats on back from osd.0 [dc1,rack1] to osd.1 [dc1,rack1] 1118.001 msec possibly improving
    Slow OSD heartbeats on back from osd.0 [dc1,rack1] to osd.2 [dc1,rack2] 1030.123 msec
    Slow OSD heartbeats on back from osd.2 [dc1,rack2] to osd.1 [dc1,rack1] 1015.321 msec
    Slow OSD heartbeats on back from osd.1 [dc1,rack1] to osd.0 [dc1,rack1] 1010.456 msec
```

To see even more detail and a complete dump of network performance information the `dump_osd_network` command can be used.  Typically, this would be sent to a mgr, but it can be limited to a particular OSD’s interactions  by issuing it to any OSD.  The current threshold which defaults to 1  second (1000 milliseconds) can be overridden as an argument in milliseconds.

The following command will show all gathered network performance data by specifying a threshold of 0 and sending to the mgr.

```
$ ceph daemon /var/run/ceph/ceph-mgr.x.asok dump_osd_network 0
{
    "threshold": 0,
    "entries": [
        {
            "last update": "Wed Sep  4 17:04:49 2019",
            "stale": false,
            "from osd": 2,
            "to osd": 0,
            "interface": "front",
            "average": {
                "1min": 1.023,
                "5min": 0.860,
                "15min": 0.883
            },
            "min": {
                "1min": 0.818,
                "5min": 0.607,
                "15min": 0.607
            },
            "max": {
                "1min": 1.164,
                "5min": 1.173,
                "15min": 1.544
            },
            "last": 0.924
        },
        {
            "last update": "Wed Sep  4 17:04:49 2019",
            "stale": false,
            "from osd": 2,
            "to osd": 0,
            "interface": "back",
            "average": {
                "1min": 0.968,
                "5min": 0.897,
                "15min": 0.830
            },
            "min": {
                "1min": 0.860,
                "5min": 0.563,
                "15min": 0.502
            },
            "max": {
                "1min": 1.171,
                "5min": 1.216,
                "15min": 1.456
            },
            "last": 0.845
        },
        {
            "last update": "Wed Sep  4 17:04:48 2019",
            "stale": false,
            "from osd": 0,
            "to osd": 1,
            "interface": "front",
            "average": {
                "1min": 0.965,
                "5min": 0.811,
                "15min": 0.850
            },
            "min": {
                "1min": 0.650,
                "5min": 0.488,
                "15min": 0.466
            },
            "max": {
                "1min": 1.252,
                "5min": 1.252,
                "15min": 1.362
            },
        "last": 0.791
    },
    ...
```

### Muting health checks[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#muting-health-checks)

Health checks can be muted so that they do not affect the overall reported status of the cluster.  Alerts are specified using the health check code (see [Health checks](https://docs.ceph.com/en/latest/rados/operations/health-checks/#health-checks)):

```
ceph health mute <code>
```

For example, if there is a health warning, muting it will make the cluster report an overall status of `HEALTH_OK`.  For example, to mute an `OSD_DOWN` alert,:

```
ceph health mute OSD_DOWN
```

Mutes are reported as part of the short and long form of the `ceph health` command. For example, in the above scenario, the cluster would report:

```
$ ceph health
HEALTH_OK (muted: OSD_DOWN)
$ ceph health detail
HEALTH_OK (muted: OSD_DOWN)
(MUTED) OSD_DOWN 1 osds down
    osd.1 is down
```

A mute can be explicitly removed with:

```
ceph health unmute <code>
```

For example,:

```
ceph health unmute OSD_DOWN
```

A health check mute may optionally have a TTL (time to live) associated with it, such that the mute will automatically expire after the specified period of time has elapsed.  The TTL is specified as an optional duration argument, e.g.:

```
ceph health mute OSD_DOWN 4h    # mute for 4 hours
ceph health mute MON_DOWN 15m   # mute for 15  minutes
```

Normally, if a muted health alert is resolved (e.g., in the example above, the OSD comes back up), the mute goes away.  If the alert comes back later, it will be reported in the usual way.

It is possible to make a mute “sticky” such that the mute will remain even if the alert clears.  For example,:

```
ceph health mute OSD_DOWN 1h --sticky   # ignore any/all down OSDs for next hour
```

Most health mutes also disappear if the extent of an alert gets worse.  For example, if there is one OSD down, and the alert is muted, the mute will disappear if one or more additional OSDs go down.  This is true for any health alert that involves a count indicating how much or how many of something is triggering the warning or error.

## Detecting configuration issues[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#detecting-configuration-issues)

In addition to the health checks that Ceph continuously runs on its own status, there are some configuration issues that may only be detected by an external tool.

Use the [ceph-medic](http://docs.ceph.com/ceph-medic/master/) tool to run these additional checks on your Ceph cluster’s configuration.

## Checking a Cluster’s Usage Stats[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#checking-a-cluster-s-usage-stats)

To check a cluster’s data usage and data distribution among pools, you can use the `df` option. It is similar to Linux `df`. Execute the following:

```
ceph df
```

The output of `ceph df` looks like this:

```
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
ssd    202 GiB  200 GiB  2.0 GiB   2.0 GiB       1.00
TOTAL  202 GiB  200 GiB  2.0 GiB   2.0 GiB       1.00

--- POOLS ---
POOL                   ID  PGS   STORED   (DATA)   (OMAP)   OBJECTS     USED  (DATA)   (OMAP)   %USED  MAX AVAIL  QUOTA OBJECTS  QUOTA BYTES  DIRTY  USED COMPR  UNDER COMPR
device_health_metrics   1    1  242 KiB   15 KiB  227 KiB         4  251 KiB  24 KiB  227 KiB       0    297 GiB            N/A          N/A      4         0 B          0 B
cephfs.a.meta           2   32  6.8 KiB  6.8 KiB      0 B        22   96 KiB  96 KiB      0 B       0    297 GiB            N/A          N/A     22         0 B          0 B
cephfs.a.data           3   32      0 B      0 B      0 B         0      0 B     0 B      0 B       0     99 GiB            N/A          N/A      0         0 B          0 B
test                    4   32   22 MiB   22 MiB   50 KiB       248   19 MiB  19 MiB   50 KiB       0    297 GiB            N/A          N/A    248         0 B          0 B
```

- **CLASS:** for example, “ssd” or “hdd”
- **SIZE:** The amount of storage capacity managed by the cluster.
- **AVAIL:** The amount of free space available in the cluster.
- **USED:** The amount of raw storage consumed by user data (excluding BlueStore’s database)
- **RAW USED:** The amount of raw storage consumed by user data, internal overhead, or reserved capacity.
- **%RAW USED:** The percentage of raw storage used. Use this number in conjunction with the `full ratio` and `near full ratio` to ensure that you are not reaching your cluster’s capacity. See [Storage Capacity](https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref#storage-capacity) for additional details.

**POOLS:**

The **POOLS** section of the output provides a list of pools and the notional usage of each pool. The output from this section **DOES NOT** reflect replicas, clones or snapshots. For example, if you store an object with 1MB of data, the notional usage will be 1MB, but the actual usage may be 2MB or more depending on the number of replicas, clones and snapshots.

- **ID:** The number of the node within the pool.
- **STORED:** actual amount of data user/Ceph has stored in a pool. This is similar to the USED column in earlier versions of Ceph but the calculations (for BlueStore!) are more precise (gaps are properly handled).
  - **(DATA):** usage for RBD (RADOS Block Device), CephFS file data, and RGW (RADOS Gateway) object data.
  - **(OMAP):** key-value pairs. Used primarily by CephFS and RGW (RADOS Gateway) for metadata storage.
- **OBJECTS:** The notional number of objects stored per pool. “Notional” is defined above in the paragraph immediately under “POOLS”.
- **USED:** The space allocated for a pool over all OSDs. This includes replication, allocation granularity, and erasure-coding overhead. Compression savings and object content gaps are also taken into account. BlueStore’s database is not included in this amount.
  - **(DATA):** object usage for RBD (RADOS Block Device), CephFS file data, and RGW (RADOS Gateway) object data.
  - **(OMAP):** object key-value pairs. Used primarily by CephFS and RGW (RADOS Gateway) for metadata storage.
- **%USED:** The notional percentage of storage used per pool.
- **MAX AVAIL:** An estimate of the notional amount of data that can be written to this pool.
- **QUOTA OBJECTS:** The number of quota objects.
- **QUOTA BYTES:** The number of bytes in the quota objects.
- **DIRTY:** The number of objects in the cache pool that have been written to the cache pool but have not been flushed yet to the base pool. This field is only available when cache tiering is in use.
- **USED COMPR:** amount of space allocated for compressed data (i.e. this includes compressed data plus all the allocation, replication and erasure coding overhead).
- **UNDER COMPR:** amount of data passed through compression (summed over all replicas) and beneficial enough to be stored in a compressed form.

Note

The numbers in the POOLS section are notional. They are not inclusive of the number of replicas, snapshots or clones. As a result, the sum of the USED and %USED amounts will not add up to the USED and %USED amounts in the RAW section of the output.

Note

The MAX AVAIL value is a complicated function of the replication or erasure code used, the CRUSH rule that maps storage to devices, the utilization of those devices, and the configured `mon_osd_full_ratio`.

## Checking OSD Status[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#checking-osd-status)

You can check OSDs to ensure they are `up` and `in` by executing the following command:

```
ceph osd stat
```

Or:

```
ceph osd dump
```

You can also check view OSDs according to their position in the CRUSH map by using the following command:

```
ceph osd tree
```

Ceph will print out a CRUSH tree with a host, its OSDs, whether they are up and their weight:

```
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
 -1       3.00000 pool default
 -3       3.00000 rack mainrack
 -2       3.00000 host osd-host
  0   ssd 1.00000         osd.0             up  1.00000 1.00000
  1   ssd 1.00000         osd.1             up  1.00000 1.00000
  2   ssd 1.00000         osd.2             up  1.00000 1.00000
```

For a detailed discussion, refer to [Monitoring OSDs and Placement Groups](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg).

## Checking Monitor Status[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#checking-monitor-status)

If your cluster has multiple monitors (likely), you should check the monitor quorum status after you start the cluster and before reading and/or writing data. A quorum must be present when multiple monitors are running. You should also check monitor status periodically to ensure that they are running.

To see display the monitor map, execute the following:

```
ceph mon stat
```

Or:

```
ceph mon dump
```

To check the quorum status for the monitor cluster, execute the following:

```
ceph quorum_status
```

Ceph will return the quorum status. For example, a Ceph  cluster consisting of three monitors may return the following:

```
{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "quorum_names": [
        "a",
        "b",
        "c"],
  "quorum_leader_name": "a",
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "features": {"persistent": [
                        "kraken",
                        "luminous",
                        "mimic"],
        "optional": []
      },
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789/0",
              "public_addr": "127.0.0.1:6789/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790/0",
              "public_addr": "127.0.0.1:6790/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791/0",
              "public_addr": "127.0.0.1:6791/0"}
           ]
  }
}
```

## Checking MDS Status[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#checking-mds-status)

Metadata servers provide metadata services for  CephFS. Metadata servers have two sets of states: `up | down` and `active | inactive`. To ensure your metadata servers are `up` and `active`,  execute the following:

```
ceph mds stat
```

To display details of the metadata cluster, execute the following:

```
ceph fs dump
```

## Checking Placement Group States[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#checking-placement-group-states)

Placement groups map objects to OSDs. When you monitor your placement groups,  you will want them to be `active` and `clean`. For a detailed discussion, refer to [Monitoring OSDs and Placement Groups](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg).



## Using the Admin Socket[](https://docs.ceph.com/en/latest/rados/operations/monitoring/#using-the-admin-socket)

The Ceph admin socket allows you to query a daemon via a socket interface. By default, Ceph sockets reside under `/var/run/ceph`. To access a daemon via the admin socket, login to the host running the daemon and use the following command:

```
ceph daemon {daemon-name}
ceph daemon {path-to-socket-file}
```

For example, the following are equivalent:

```
ceph daemon osd.0 foo
ceph daemon /var/run/ceph/ceph-osd.0.asok foo
```

To view the available admin socket commands, execute the following command:

```
ceph daemon {daemon-name} help
```

The admin socket command enables you to show and set your configuration at runtime. See [Viewing a Configuration at Runtime](https://docs.ceph.com/en/latest/rados/configuration/ceph-conf#viewing-a-configuration-at-runtime) for details.

Additionally, you can set configuration values at runtime directly (i.e., the admin socket bypasses the monitor, unlike `ceph tell {daemon-type}.{id} config set`, which relies on the monitor but doesn’t require you to login directly to the host in question ).

 

# Monitoring OSDs and PGs[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#monitoring-osds-and-pgs)

High availability and high reliability require a fault-tolerant approach to managing hardware and software issues. Ceph has no single point-of-failure, and can service requests for data in a “degraded” mode. Ceph’s [data placement](https://docs.ceph.com/en/latest/rados/operations/data-placement) introduces a layer of indirection to ensure that data doesn’t bind directly to particular OSD addresses. This means that tracking down system faults requires finding the [placement group](https://docs.ceph.com/en/latest/rados/operations/placement-groups) and the underlying OSDs at root of the problem.

Tip

A fault in one part of the cluster may prevent you from accessing a particular object, but that doesn’t mean that you cannot access other objects. When you run into a fault, don’t panic. Just follow the steps for monitoring your OSDs and placement groups. Then, begin troubleshooting.

Ceph is generally self-repairing. However, when problems persist, monitoring OSDs and placement groups will help you identify the problem.

## Monitoring OSDs[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#monitoring-osds)

An OSD’s status is either in the cluster (`in`) or out of the cluster (`out`); and, it is either up and running (`up`), or it is down and not running (`down`). If an OSD is `up`, it may be either `in` the cluster (you can read and write data) or it is `out` of the cluster. If it was `in` the cluster and recently moved `out` of the cluster, Ceph will migrate placement groups to other OSDs. If an OSD is `out` of the cluster, CRUSH will not assign placement groups to the OSD. If an OSD is `down`, it should also be `out`.

Note

If an OSD is `down` and `in`, there is a problem and the cluster will not be in a healthy state.

![img](https://docs.ceph.com/en/latest/_images/ditaa-83a02a232b72a2f08fddcc5f7d697a751499f16c.png)

If you execute a command such as `ceph health`, `ceph -s` or `ceph -w`, you may notice that the cluster does not always echo back `HEALTH OK`. Don’t panic. With respect to OSDs, you should expect that the cluster will **NOT** echo `HEALTH OK` in a few expected circumstances:

1. You haven’t started the cluster yet (it won’t respond).
2. You have just started or restarted the cluster and it’s not ready yet, because the placement groups are getting created and the OSDs are in the process of peering.
3. You just added or removed an OSD.
4. You just have modified your cluster map.

An important aspect of monitoring OSDs is to ensure that when the cluster is up and running that all OSDs that are `in` the cluster are `up` and running, too. To see if all OSDs are running, execute:

```
ceph osd stat
```

The result should tell you the total number of OSDs (x), how many are `up` (y), how many are `in` (z) and the map epoch (eNNNN).

```
x osds: y up, z in; epoch: eNNNN
```

If the number of OSDs that are `in` the cluster is more than the number of OSDs that are `up`, execute the following command to identify the `ceph-osd` daemons that are not running:

```
ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
 -1       2.00000 pool openstack
 -3       2.00000 rack dell-2950-rack-A
 -2       2.00000 host dell-2950-A1
  0   ssd 1.00000      osd.0                up  1.00000 1.00000
  1   ssd 1.00000      osd.1              down  1.00000 1.00000
```

Tip

The ability to search through a well-designed CRUSH hierarchy may help you troubleshoot your cluster by identifying the physical locations faster.

If an OSD is `down`, start it:

```
sudo systemctl start ceph-osd@1
```

See [OSD Not Running](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-osd#osd-not-running) for problems associated with OSDs that stopped, or won’t restart.

## PG Sets[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#pg-sets)

When CRUSH assigns placement groups to OSDs, it looks at the number of replicas for the pool and assigns the placement group to OSDs such that each replica of the placement group gets assigned to a different OSD. For example, if the pool requires three replicas of a placement group, CRUSH may assign them to `osd.1`, `osd.2` and `osd.3` respectively. CRUSH actually seeks a pseudo-random placement that will take into account failure domains you set in your [CRUSH map](https://docs.ceph.com/en/latest/rados/operations/crush-map), so you will rarely see placement groups assigned to nearest neighbor OSDs in a large cluster.

Ceph processes a client request using the **Acting Set**, which is the set of OSDs that will actually handle the requests since they have a full and working version of a placement group shard. The set of OSDs that should contain a shard of a particular placement group as the **Up Set**, i.e. where data is moved/copied to (or planned to be).

In some cases, an OSD in the Acting Set is `down` or otherwise not able to service requests for objects in the placement group. When these situations arise, don’t panic. Common examples include:

- You added or removed an OSD. Then, CRUSH reassigned the placement group to other OSDs–thereby changing the composition of the Acting Set and spawning the migration of data with a “backfill” process.
- An OSD was `down`, was restarted, and is now `recovering`.
- An OSD in the Acting Set is `down` or unable to service requests, and another OSD has temporarily assumed its duties.

In most cases, the Up Set and the Acting Set are identical. When they are not, it may indicate that Ceph is migrating the PG (it’s remapped), an OSD is recovering, or that there is a problem (i.e., Ceph usually echoes a “HEALTH WARN” state with a “stuck stale” message in such scenarios).

To retrieve a list of placement groups, execute:

```
ceph pg dump
```

To view which OSDs are within the Acting Set or the Up Set for a given placement group, execute:

```
ceph pg map {pg-num}
```

The result should tell you the osdmap epoch (eNNN), the placement group number ({pg-num}), the OSDs in the Up Set (up[]), and the OSDs in the acting set (acting[]).

```
osdmap eNNN pg {raw-pg-num} ({pg-num}) -> up [0,1,2] acting [0,1,2]
```

Note

If the Up Set and Acting Set do not match, this may be an indicator that the cluster rebalancing itself or of a potential problem with the cluster.

## Peering[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#peering)

Before you can write data to a placement group, it must be in an `active` state, and it **should** be in a `clean` state. For Ceph to determine the current state of a placement group, the primary OSD of the placement group (i.e., the first OSD in the acting set), peers with the secondary and tertiary OSDs to establish agreement on the current state of the placement group (assuming a pool with 3 replicas of the PG).

![img](https://docs.ceph.com/en/latest/_images/ditaa-c534576b8b40a180628e34f649b6bfc104bc853d.png)

The OSDs also report their status to the monitor. See [Configuring Monitor/OSD Interaction](https://docs.ceph.com/en/latest/rados/configuration/mon-osd-interaction/) for details. To troubleshoot peering issues, see [Peering Failure](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-pg#failures-osd-peering).

## Monitoring Placement Group States[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#monitoring-placement-group-states)

If you execute a command such as `ceph health`, `ceph -s` or `ceph -w`, you may notice that the cluster does not always echo back `HEALTH OK`. After you check to see if the OSDs are running, you should also check placement group states. You should expect that the cluster will **NOT** echo `HEALTH OK` in a number of placement group peering-related circumstances:

1. You have just created a pool and placement groups haven’t peered yet.
2. The placement groups are recovering.
3. You have just added an OSD to or removed an OSD from the cluster.
4. You have just modified your CRUSH map and your placement groups are migrating.
5. There is inconsistent data in different replicas of a placement group.
6. Ceph is scrubbing a placement group’s replicas.
7. Ceph doesn’t have enough storage capacity to complete backfilling operations.

If one of the foregoing circumstances causes Ceph to echo `HEALTH WARN`, don’t panic. In many cases, the cluster will recover on its own. In some cases, you may need to take action. An important aspect of monitoring placement groups is to ensure that when the cluster is up and running that all placement groups are `active`, and preferably in the `clean` state. To see the status of all placement groups, execute:

```
ceph pg stat
```

The result should tell you the total number of placement groups (x), how many placement groups are in a particular state such as `active+clean` (y) and the amount of data stored (z).

```
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
```

Note

It is common for Ceph to report multiple states for placement groups.

In addition to the placement group states, Ceph will also echo back the amount of storage capacity used (aa), the amount of storage capacity remaining (bb), and the total storage capacity for the placement group. These numbers can be important in a few cases:

- You are reaching your `near full ratio` or `full ratio`.
- Your data is not getting distributed across the cluster due to an error in your CRUSH configuration.

Placement Group IDs

Placement group IDs consist of the pool number (not pool name) followed by a period (.) and the placement group ID–a hexadecimal number. You can view pool numbers and their names from the output of `ceph osd lspools`. For example, the first pool created corresponds to pool number `1`. A fully qualified placement group ID has the following form:

```
{pool-num}.{pg-id}
```

And it typically looks like this:

```
1.1f
```

To retrieve a list of placement groups, execute the following:

```
ceph pg dump
```

You can also format the output in JSON format and save it to a file:

```
ceph pg dump -o {filename} --format=json
```

To query a particular placement group, execute the following:

```
ceph pg {poolnum}.{pg-id} query
```

Ceph will output the query in JSON format.

The following subsections describe the common pg states in detail.

### Creating[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#creating)

When you create a pool, it will create the number of placement groups you specified. Ceph will echo `creating` when it is creating one or more placement groups. Once they are created, the OSDs that are part of a placement group’s Acting Set will peer. Once peering is complete, the placement group status should be `active+clean`, which means a Ceph client can begin writing to the placement group.

![img](https://docs.ceph.com/en/latest/_images/ditaa-bfce6c25b264cd756b5f4cf49e07ce71ccd1f775.png)

### Peering[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#id1)

When Ceph is Peering a placement group, Ceph is bringing the OSDs that store the replicas of the placement group into **agreement about the state** of the objects and metadata in the placement group. When Ceph completes peering, this means that the OSDs that store the placement group agree about the current state of the placement group. However, completion of the peering process does **NOT** mean that each replica has the latest contents.

Authoritative History

Ceph will **NOT** acknowledge a write operation to a client, until all OSDs of the acting set persist the write operation. This practice ensures that at least one member of the acting set will have a record of every acknowledged write operation since the last successful peering operation.

With an accurate record of each acknowledged write operation, Ceph can construct and disseminate a new authoritative history of the placement group–a complete, and fully ordered set of operations that, if performed, would bring an OSD’s copy of a placement group up to date.

### Active[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#active)

Once Ceph completes the peering process, a placement group may become `active`. The `active` state means that the data in the placement group is generally available in the primary placement group and the replicas for read and write operations.

### Clean[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#clean)

When a placement group is in the `clean` state, the primary OSD and the replica OSDs have successfully peered and there are no stray replicas for the placement group. Ceph replicated all objects in the placement group the correct number of times.

### Degraded[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#degraded)

When a client writes an object to the primary OSD, the primary OSD is responsible for writing the replicas to the replica OSDs. After the primary OSD writes the object to storage, the placement group will remain in a `degraded` state until the primary OSD has received an acknowledgement from the replica OSDs that Ceph created the replica objects successfully.

The reason a placement group can be `active+degraded` is that an OSD may be `active` even though it doesn’t hold all of the objects yet. If an OSD goes `down`, Ceph marks each placement group assigned to the OSD as `degraded`. The OSDs must peer again when the OSD comes back online. However, a client can still write a new object to a `degraded` placement group if it is `active`.

If an OSD is `down` and the `degraded` condition persists, Ceph may mark the `down` OSD as `out` of the cluster and remap the data from the `down` OSD to another OSD. The time between being marked `down` and being marked `out` is controlled by `mon_osd_down_out_interval`, which is set to `600` seconds by default.

A placement group can also be `degraded`, because Ceph cannot find one or more objects that Ceph thinks should be in the placement group. While you cannot read or write to unfound objects, you can still access all of the other objects in the `degraded` placement group.

### Recovering[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#recovering)

Ceph was designed for fault-tolerance at a scale where hardware and software problems are ongoing. When an OSD goes `down`, its contents may fall behind the current state of other replicas in the placement groups. When the OSD is back `up`, the contents of the placement groups must be updated to reflect the current state. During that time period, the OSD may reflect a `recovering` state.

Recovery is not always trivial, because a hardware failure might cause a cascading failure of multiple OSDs. For example, a network switch for a rack or cabinet may fail, which can cause the OSDs of a number of host machines to fall behind the current state of the cluster. Each one of the OSDs must recover once the fault is resolved.

Ceph provides a number of settings to balance the resource contention between new service requests and the need to recover data objects and restore the placement groups to the current state. The `osd_recovery_delay_start` setting allows an OSD to restart, re-peer and even process some replay requests before starting the recovery process. The `osd_recovery_thread_timeout` sets a thread timeout, because multiple OSDs may fail, restart and re-peer at staggered rates. The `osd_recovery_max_active` setting limits the number of recovery requests an OSD will entertain simultaneously to prevent the OSD from failing to serve. The `osd_recovery_max_chunk` setting limits the size of the recovered data chunks to prevent network congestion.

### Back Filling[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#back-filling)

When a new OSD joins the cluster, CRUSH will reassign placement groups from OSDs in the cluster to the newly added OSD. Forcing the new OSD to accept the reassigned placement groups immediately can put excessive load on the new OSD. Back filling the OSD with the placement groups allows this process to begin in the background. Once backfilling is complete, the new OSD will begin serving requests when it is ready.

During the backfill operations, you may see one of several states: `backfill_wait` indicates that a backfill operation is pending, but is not underway yet; `backfilling` indicates that a backfill operation is underway; and, `backfill_toofull` indicates that a backfill operation was requested, but couldn’t be completed due to insufficient storage capacity. When a placement group cannot be backfilled, it may be considered `incomplete`.

The `backfill_toofull` state may be transient. It is possible that as PGs are moved around, space may become available. The `backfill_toofull` is similar to `backfill_wait` in that as soon as conditions change backfill can proceed.

Ceph provides a number of settings to manage the load spike associated with reassigning placement groups to an OSD (especially a new OSD). By default, `osd_max_backfills` sets the maximum number of concurrent backfills to and from an OSD to 1. The `backfill_full_ratio` enables an OSD to refuse a backfill request if the OSD is approaching its full ratio (90%, by default) and change with `ceph osd set-backfillfull-ratio` command. If an OSD refuses a backfill request, the `osd_backfill_retry_interval` enables an OSD to retry the request (after 30 seconds, by default). OSDs can also set `osd_backfill_scan_min` and `osd_backfill_scan_max` to manage scan intervals (64 and 512, by default).

### Remapped[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#remapped)

When the Acting Set that services a placement group changes, the data migrates from the old acting set to the new acting set. It may take some time for a new primary OSD to service requests. So it may ask the old primary to continue to service requests until the placement group migration is complete. Once data migration completes, the mapping uses the primary OSD of the new acting set.

### Stale[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#stale)

While Ceph uses heartbeats to ensure that hosts and daemons are running, the `ceph-osd` daemons may also get into a `stuck` state where they are not reporting statistics in a timely manner (e.g., a temporary network fault). By default, OSD daemons report their placement group, up through, boot and failure statistics every half second (i.e., `0.5`), which is more frequent than the heartbeat thresholds. If the **Primary OSD** of a placement group’s acting set fails to report to the monitor or if other OSDs have reported the primary OSD `down`, the monitors will mark the placement group `stale`.

When you start your cluster, it is common to see the `stale` state until the peering process completes. After your cluster has been running for awhile, seeing placement groups in the `stale` state indicates that the primary OSD for those placement groups is `down` or not reporting placement group statistics to the monitor.

## Identifying Troubled PGs[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#identifying-troubled-pgs)

As previously noted, a placement group is not necessarily problematic just because its state is not `active+clean`. Generally, Ceph’s ability to self repair may not be working when placement groups get stuck. The stuck states include:

- **Unclean**: Placement groups contain objects that are not replicated the desired number of times. They should be recovering.
- **Inactive**: Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come back `up`.
- **Stale**: Placement groups are in an unknown state, because the OSDs that host them have not reported to the monitor cluster in a while (configured by `mon_osd_report_timeout`).

To identify stuck placement groups, execute the following:

```
ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
```

See [Placement Group Subsystem](https://docs.ceph.com/en/latest/rados/operations/control#placement-group-subsystem) for additional details. To troubleshoot stuck placement groups, see [Troubleshooting PG Errors](https://docs.ceph.com/en/latest/rados/troubleshooting/troubleshooting-pg#troubleshooting-pg-errors).

## Finding an Object Location[](https://docs.ceph.com/en/latest/rados/operations/monitoring-osd-pg/#finding-an-object-location)

To store object data in the Ceph Object Store, a Ceph client must:

1. Set an object name
2. Specify a [pool](https://docs.ceph.com/en/latest/rados/operations/pools)

The Ceph client retrieves the latest cluster map and the CRUSH algorithm calculates how to map the object to a [placement group](https://docs.ceph.com/en/latest/rados/operations/placement-groups), and then calculates how to assign the placement group to an OSD dynamically. To find the object location, all you need is the object name and the pool name. For example:

```
ceph osd map {poolname} {object-name} [namespace]
```

Exercise: Locate an Object

As an exercise, lets create an object. Specify an object name, a path to a test file containing some object data and a pool name using the `rados put` command on the command line. For example:

```
rados put {object-name} {file-path} --pool=data
rados put test-object-1 testfile.txt --pool=data
```

To verify that the Ceph Object Store stored the object, execute the following:

```
rados -p data ls
```

Now, identify the object location:

```
ceph osd map {pool-name} {object-name}
ceph osd map data test-object-1
```

Ceph should output the object’s location. For example:

```
osdmap e537 pool 'data' (1) object 'test-object-1' -> pg 1.d1743484 (1.4) -> up ([0,1], p0) acting ([0,1], p0)
```

To remove the test object, simply delete it using the `rados rm` command. For example:

```
rados rm test-object-1 --pool=data
```

As the cluster evolves, the object location may change dynamically. One benefit of Ceph’s dynamic rebalancing is that Ceph relieves you from having to perform the migration manually. See the [Architecture](https://docs.ceph.com/en/latest/architecture) section for details.

# User Management[](https://docs.ceph.com/en/latest/rados/operations/user-management/#user-management)

This document describes [Ceph Client](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Client) users, and their authentication and authorization with the [Ceph Storage Cluster](https://docs.ceph.com/en/latest/glossary/#term-Ceph-Storage-Cluster). Users are either individuals or system actors such as applications, which use Ceph clients to interact with the Ceph Storage Cluster daemons.

![img](https://docs.ceph.com/en/latest/_images/ditaa-e4363571a389f38b579ed95f21e5b3b8e719866e.png)

When Ceph runs with authentication and authorization enabled (enabled by default), you must specify a user name and a keyring containing the secret key of the specified user (usually via the command line). If you do not specify a user name, Ceph will use `client.admin` as the default user name. If you do not specify a keyring, Ceph will look for a keyring via the `keyring` setting in the Ceph configuration. For example, if you execute the `ceph health` command without specifying a user or keyring:

```
ceph health
```

Ceph interprets the command like this:

```
ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health
```

Alternatively, you may use the `CEPH_ARGS` environment variable to avoid re-entry of the user name and secret.

For details on configuring the Ceph Storage Cluster to use authentication, see [Cephx Config Reference](https://docs.ceph.com/en/latest/rados/configuration/auth-config-ref). For details on the architecture of Cephx, see [Architecture - High Availability Authentication](https://docs.ceph.com/en/latest/architecture#high-availability-authentication).

## Background[](https://docs.ceph.com/en/latest/rados/operations/user-management/#background)

Irrespective of the type of Ceph client (e.g., Block Device, Object Storage, Filesystem, native API, etc.), Ceph stores all data as objects within [pools](https://docs.ceph.com/en/latest/rados/operations/pools). Ceph users must have access to pools in order to read and write data. Additionally, Ceph users must have execute permissions to use Ceph’s administrative commands. The following concepts will help you understand Ceph user management.

### User[](https://docs.ceph.com/en/latest/rados/operations/user-management/#user)

A user is either an individual or a system actor such as an application. Creating users allows you to control who (or what) can access your Ceph Storage Cluster, its pools, and the data within pools.

Ceph has the notion of a `type` of user. For the purposes of user management, the type will always be `client`. Ceph identifies users in period (.) delimited form consisting of the user type and the user ID: for example, `TYPE.ID`, `client.admin`, or `client.user1`. The reason for user typing is that Ceph Monitors, OSDs, and Metadata Servers also use the Cephx protocol, but they are not clients. Distinguishing the user type helps to distinguish between client users and other users–streamlining access control, user monitoring and traceability.

Sometimes Ceph’s user type may seem confusing, because the Ceph command line allows you to specify a user with or without the type, depending upon your command line usage. If you specify `--user` or `--id`, you can omit the type. So `client.user1` can be entered simply as `user1`. If you specify `--name` or `-n`, you must specify the type and name, such as `client.user1`. We recommend using the type and name as a best practice wherever possible.

Note

A Ceph Storage Cluster user is not the same as a Ceph Object Storage user or a Ceph File System user. The Ceph Object Gateway uses a Ceph Storage Cluster user to communicate between the gateway daemon and the storage cluster, but the gateway has its own user management functionality for end users. The Ceph File System uses POSIX semantics. The user space associated with the Ceph File System is not the same as a Ceph Storage Cluster user.

### Authorization (Capabilities)[](https://docs.ceph.com/en/latest/rados/operations/user-management/#authorization-capabilities)

Ceph uses the term “capabilities” (caps) to describe authorizing an authenticated user to exercise the functionality of the monitors, OSDs and metadata servers. Capabilities can also restrict access to data within a pool, a namespace within a pool, or a set of pools based on their application tags. A Ceph administrative user sets a user’s capabilities when creating or updating a user.

Capability syntax follows the form:

```
{daemon-type} '{cap-spec}[, {cap-spec} ...]'
```

- **Monitor Caps:** Monitor capabilities include `r`, `w`, `x` access settings or `profile {name}`. For example:

  ```
  mon 'allow {access-spec} [network {network/prefix}]'
  
  mon 'profile {name}'
  ```

  The `{access-spec}` syntax is as follows:

  ```
  * | all | [r][w][x]
  ```

  The optional `{network/prefix}` is a standard network name and prefix length in CIDR notation (e.g., `10.3.0.0/16`).  If present, the use of this capability is restricted to clients connecting from this network.

- **OSD Caps:** OSD capabilities include `r`, `w`, `x`, `class-read`, `class-write` access settings or `profile {name}`. Additionally, OSD capabilities also allow for pool and namespace settings.

  ```
  osd 'allow {access-spec} [{match-spec}] [network {network/prefix}]'
  
  osd 'profile {name} [pool={pool-name} [namespace={namespace-name}]] [network {network/prefix}]'
  ```

  The `{access-spec}` syntax is either of the following:

  ```
  * | all | [r][w][x] [class-read] [class-write]
  
  class {class name} [{method name}]
  ```

  The optional `{match-spec}` syntax is either of the following:

  ```
  pool={pool-name} [namespace={namespace-name}] [object_prefix {prefix}]
  
  [namespace={namespace-name}] tag {application} {key}={value}
  ```

  The optional `{network/prefix}` is a standard network name and prefix length in CIDR notation (e.g., `10.3.0.0/16`).  If present, the use of this capability is restricted to clients connecting from this network.

- **Manager Caps:** Manager (`ceph-mgr`) capabilities include `r`, `w`, `x` access settings or `profile {name}`. For example:

  ```
  mgr 'allow {access-spec} [network {network/prefix}]'
  
  mgr 'profile {name} [{key1} {match-type} {value1} ...] [network {network/prefix}]'
  ```

  Manager capabilities can also be specified for specific commands, all commands exported by a built-in manager service, or all commands exported by a specific add-on module. For example:

  ```
  mgr 'allow command "{command-prefix}" [with {key1} {match-type} {value1} ...] [network {network/prefix}]'
  
  mgr 'allow service {service-name} {access-spec} [network {network/prefix}]'
  
  mgr 'allow module {module-name} [with {key1} {match-type} {value1} ...] {access-spec} [network {network/prefix}]'
  ```

  The `{access-spec}` syntax is as follows:

  ```
  * | all | [r][w][x]
  ```

  The `{service-name}` is one of the following:

  ```
  mgr | osd | pg | py
  ```

  The `{match-type}` is one of the following:

  ```
  = | prefix | regex
  ```

- **Metadata Server Caps:** For administrators, use `allow *`.  For all other users, such as CephFS clients, consult [CephFS Client Capabilities](https://docs.ceph.com/en/latest/cephfs/client-auth/)

Note

The Ceph Object Gateway daemon (`radosgw`) is a client of the Ceph Storage Cluster, so it is not represented as a Ceph Storage Cluster daemon type.

The following entries describe each access capability.

```
allow
```

- Description

  Precedes access settings for a daemon. Implies `rw` for MDS only.

```
r
```

- Description

  Gives the user read access. Required with monitors to retrieve the CRUSH map.

```
w
```

- Description

  Gives the user write access to objects.

```
x
```

- Description

  Gives the user the capability to call class methods (i.e., both read and write) and to conduct `auth` operations on monitors.

```
class-read
```

- Descriptions

  Gives the user the capability to call class read methods. Subset of `x`.

```
class-write
```

- Description

  Gives the user the capability to call class write methods. Subset of `x`.

```
*`, `all
```

- Description

  Gives the user read, write and execute permissions for a particular daemon/pool, and the ability to execute admin commands.

The following entries describe valid capability profiles:

`profile osd` (Monitor only)

- Description

  Gives a user permissions to connect as an OSD to other OSDs or monitors. Conferred on OSDs to enable OSDs to handle replication heartbeat traffic and status reporting.

`profile mds` (Monitor only)

- Description

  Gives a user permissions to connect as a MDS to other MDSs or monitors.

`profile bootstrap-osd` (Monitor only)

- Description

  Gives a user permissions to bootstrap an OSD. Conferred on deployment tools such as `ceph-volume`, `cephadm`, etc. so that they have permissions to add keys, etc. when bootstrapping an OSD.

`profile bootstrap-mds` (Monitor only)

- Description

  Gives a user permissions to bootstrap a metadata server. Conferred on deployment tools such as `cephadm`, etc. so they have permissions to add keys, etc. when bootstrapping a metadata server.

`profile bootstrap-rbd` (Monitor only)

- Description

  Gives a user permissions to bootstrap an RBD user. Conferred on deployment tools such as `cephadm`, etc. so they have permissions to add keys, etc. when bootstrapping an RBD user.

`profile bootstrap-rbd-mirror` (Monitor only)

- Description

  Gives a user permissions to bootstrap an `rbd-mirror` daemon user. Conferred on deployment tools such as `cephadm`, etc. so they have permissions to add keys, etc. when bootstrapping an `rbd-mirror` daemon.

`profile rbd` (Manager, Monitor, and OSD)

- Description

  Gives a user permissions to manipulate RBD images. When used as a Monitor cap, it provides the minimal privileges required by an RBD client application; this includes the ability to blocklist other client users. When used as an OSD cap, it provides read-write access to the specified pool to an RBD client application. The Manager cap supports optional `pool` and `namespace` keyword arguments.

`profile rbd-mirror` (Monitor only)

- Description

  Gives a user permissions to manipulate RBD images and retrieve RBD mirroring config-key secrets. It provides the minimal privileges required for the `rbd-mirror` daemon.

`profile rbd-read-only` (Manager and OSD)

- Description

  Gives a user read-only permissions to RBD images. The Manager cap supports optional `pool` and `namespace` keyword arguments.

`profile simple-rados-client` (Monitor only)

- Description

  Gives a user read-only permissions for monitor, OSD, and PG data. Intended for use by direct librados client applications.

`profile simple-rados-client-with-blocklist` (Monitor only)

- Description

  Gives a user read-only permissions for monitor, OSD, and PG data. Intended for use by direct librados client applications. Also includes permission to add blocklist entries to build HA applications.

`profile fs-client` (Monitor only)

- Description

  Gives a user read-only permissions for monitor, OSD, PG, and MDS data.  Intended for CephFS clients.

`profile role-definer` (Monitor and Auth)

- Description

  Gives a user **all** permissions for the auth subsystem, read-only access to monitors, and nothing else.  Useful for automation tools.  Do not assign this unless you really, **really** know what you’re doing as the security ramifications are substantial and pervasive.

`profile crash` (Monitor and MGR)

- Description

  Gives a user read-only access to monitors, used in conjunction with the manager `crash` module to upload daemon crash dumps into monitor storage for later analysis.

### Pool[](https://docs.ceph.com/en/latest/rados/operations/user-management/#pool)

A pool is a logical partition where users store data. In Ceph deployments, it is common to create a pool as a logical partition for similar types of data. For example, when deploying Ceph as a backend for OpenStack, a typical deployment would have pools for volumes, images, backups and virtual machines, and users such as `client.glance`, `client.cinder`, etc.

### Application Tags[](https://docs.ceph.com/en/latest/rados/operations/user-management/#application-tags)

Access may be restricted to specific pools as defined by their application metadata. The `*` wildcard may be used for the `key` argument, the `value` argument, or both. `all` is a synony for `*`.

### Namespace[](https://docs.ceph.com/en/latest/rados/operations/user-management/#namespace)

Objects within a pool can be associated to a namespace–a logical group of objects within the pool. A user’s access to a pool can be associated with a namespace such that reads and writes by the user take place only within the namespace. Objects written to a namespace within the pool can only be accessed by users who have access to the namespace.

Note

Namespaces are primarily useful for applications written on top of `librados` where the logical grouping can alleviate the need to create different pools. Ceph Object Gateway (from `luminous`) uses namespaces for various metadata objects.

The rationale for namespaces is that pools can be a computationally expensive method of segregating data sets for the purposes of authorizing separate sets of users. For example, a pool should have ~100 placement groups per OSD. So an exemplary cluster with 1000 OSDs would have 100,000 placement groups for one pool. Each pool would create another 100,000 placement groups in the exemplary cluster. By contrast, writing an object to a namespace simply associates the namespace to the object name with out the computational overhead of a separate pool. Rather than creating a separate pool for a user or set of users, you may use a namespace. **Note:** Only available using `librados` at this time.

Access may be restricted to specific RADOS namespaces using the `namespace` capability. Limited globbing of namespaces is supported; if the last character of the specified namespace is `*`, then access is granted to any namespace starting with the provided argument.

## Managing Users[](https://docs.ceph.com/en/latest/rados/operations/user-management/#managing-users)

User management functionality provides Ceph Storage Cluster administrators with the ability to create, update and delete users directly in the Ceph Storage Cluster.

When you create or delete users in the Ceph Storage Cluster, you may need to distribute keys to clients so that they can be added to keyrings. See [Keyring Management](https://docs.ceph.com/en/latest/rados/operations/user-management/#keyring-management) for details.

### List Users[](https://docs.ceph.com/en/latest/rados/operations/user-management/#list-users)

To list the users in your cluster, execute the following:

```
ceph auth ls
```

Ceph will list out all users in your cluster. For example, in a two-node exemplary cluster, `ceph auth ls` will output something that looks like this:

```
installed auth entries:

osd.0
        key: AQCvCbtToC6MDhAATtuT70Sl+DymPCfDSsyV4w==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQC4CbtTCFJBChAAVq5spj0ff4eHZICxIOVZeA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQBHCbtT6APDHhAA5W00cBchwkQjh3dkKsyPjw==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQBICbtTOK9uGBAAdbe5zcIGHZL3T/u2g6EBww==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQBHCbtT4GxqORAADE5u7RkpCN/oo4e5W0uBtw==
        caps: [mon] allow profile bootstrap-osd
```

Note that the `TYPE.ID` notation for users applies such that `osd.0` is a user of type `osd` and its ID is `0`, `client.admin` is a user of type `client` and its ID is `admin` (i.e., the default `client.admin` user). Note also that each entry has a `key: <value>` entry, and one or more `caps:` entries.

You may use the `-o {filename}` option with `ceph auth ls` to save the output to a file.

### Get a User[](https://docs.ceph.com/en/latest/rados/operations/user-management/#get-a-user)

To retrieve a specific user, key and capabilities, execute the following:

```
ceph auth get {TYPE.ID}
```

For example:

```
ceph auth get client.admin
```

You may also use the `-o {filename}` option with `ceph auth get` to save the output to a file. Developers may also execute the following:

```
ceph auth export {TYPE.ID}
```

The `auth export` command is identical to `auth get`.

### Add a User[](https://docs.ceph.com/en/latest/rados/operations/user-management/#add-a-user)

Adding a user creates a username (i.e., `TYPE.ID`), a secret key and any capabilities included in the command you use to create the user.

A user’s key enables the user to authenticate with the Ceph Storage Cluster. The user’s capabilities authorize the user to read, write, or execute on Ceph monitors (`mon`), Ceph OSDs (`osd`) or Ceph Metadata  Servers (`mds`).

There are a few ways to add a user:

- `ceph auth add`: This command is the canonical way to add a user. It will create the user, generate a key and add any specified capabilities.
- `ceph auth get-or-create`: This command is often the most convenient way to create a user, because it returns a keyfile format with the user name (in brackets) and the key. If the user already exists, this command simply returns the user name and key in the keyfile format. You may use the `-o {filename}` option to save the output to a file.
- `ceph auth get-or-create-key`: This command is a convenient way to create a user and return the user’s key (only). This is useful for clients that need the key only (e.g., libvirt). If the user already exists, this command simply returns the key. You may use the `-o {filename}` option to save the output to a file.

When creating client users, you may create a user with no capabilities. A user with no capabilities is useless beyond mere authentication, because the client cannot retrieve the cluster map from the monitor. However, you can create a user with no capabilities if you wish to defer adding capabilities later using the `ceph auth caps` command.

A typical user has at least read capabilities on the Ceph monitor and read and write capability on Ceph OSDs. Additionally, a user’s OSD permissions are often restricted to accessing a particular pool.

```
ceph auth add client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.paul mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth get-or-create client.george mon 'allow r' osd 'allow rw pool=liverpool' -o george.keyring
ceph auth get-or-create-key client.ringo mon 'allow r' osd 'allow rw pool=liverpool' -o ringo.key
```

Important

If you provide a user with capabilities to OSDs, but you DO NOT restrict access to particular pools, the user will have access to ALL pools in the cluster!



### Modify User Capabilities[](https://docs.ceph.com/en/latest/rados/operations/user-management/#modify-user-capabilities)

The `ceph auth caps` command allows you to specify a user and change the user’s capabilities. Setting new capabilities will overwrite current capabilities. To view current capabilities run `ceph auth get USERTYPE.USERID`.  To add capabilities, you should also specify the existing capabilities when using the form:

```
ceph auth caps USERTYPE.USERID {daemon} 'allow [r|w|x|*|...] [pool={pool-name}] [namespace={namespace-name}]' [{daemon} 'allow [r|w|x|*|...] [pool={pool-name}] [namespace={namespace-name}]']
```

For example:

```
ceph auth get client.john
ceph auth caps client.john mon 'allow r' osd 'allow rw pool=liverpool'
ceph auth caps client.paul mon 'allow rw' osd 'allow rwx pool=liverpool'
ceph auth caps client.brian-manager mon 'allow *' osd 'allow *'
```

See [Authorization (Capabilities)](https://docs.ceph.com/en/latest/rados/operations/user-management/#authorization-capabilities) for additional details on capabilities.

### Delete a User[](https://docs.ceph.com/en/latest/rados/operations/user-management/#delete-a-user)

To delete a user, use `ceph auth del`:

```
ceph auth del {TYPE}.{ID}
```

Where `{TYPE}` is one of `client`, `osd`, `mon`, or `mds`, and `{ID}` is the user name or ID of the daemon.

### Print a User’s Key[](https://docs.ceph.com/en/latest/rados/operations/user-management/#print-a-user-s-key)

To print a user’s authentication key to standard output, execute the following:

```
ceph auth print-key {TYPE}.{ID}
```

Where `{TYPE}` is one of `client`, `osd`, `mon`, or `mds`, and `{ID}` is the user name or ID of the daemon.

Printing a user’s key is useful when you need to populate client software with a user’s key  (e.g., libvirt).

```
mount -t ceph serverhost:/ mountpoint -o name=client.user,secret=`ceph auth print-key client.user`
```

### Import a User(s)[](https://docs.ceph.com/en/latest/rados/operations/user-management/#import-a-user-s)

To import one or more users, use `ceph auth import` and specify a keyring:

```
ceph auth import -i /path/to/keyring
```

For example:

```
sudo ceph auth import -i /etc/ceph/ceph.keyring
```

Note

The Ceph storage cluster will add new users, their keys and their capabilities and will update existing users, their keys and their capabilities.

## Keyring Management[](https://docs.ceph.com/en/latest/rados/operations/user-management/#keyring-management)

When you access Ceph via a Ceph client, the Ceph client will look for a local keyring. Ceph presets the `keyring` setting with the following four keyring names by default so you don’t have to set them in your Ceph configuration file unless you want to override the defaults (not recommended):

- `/etc/ceph/$cluster.$name.keyring`
- `/etc/ceph/$cluster.keyring`
- `/etc/ceph/keyring`
- `/etc/ceph/keyring.bin`

The `$cluster` metavariable is your Ceph cluster name as defined by the name of the Ceph configuration file (i.e., `ceph.conf` means the cluster name is `ceph`; thus, `ceph.keyring`). The `$name` metavariable is the user type and user ID (e.g., `client.admin`; thus, `ceph.client.admin.keyring`).

Note

When executing commands that read or write to `/etc/ceph`, you may need to use `sudo` to execute the command as `root`.

After you create a user (e.g., `client.ringo`), you must get the key and add it to a keyring on a Ceph client so that the user can access the Ceph Storage Cluster.

The [User Management](https://docs.ceph.com/en/latest/rados/operations/user-management/#id1) section details how to list, get, add, modify and delete users directly in the Ceph Storage Cluster. However, Ceph also provides the `ceph-authtool` utility to allow you to manage keyrings from a Ceph client.

### Create a Keyring[](https://docs.ceph.com/en/latest/rados/operations/user-management/#create-a-keyring)

When you use the procedures in the [Managing Users](https://docs.ceph.com/en/latest/rados/operations/user-management/#managing-users) section to create users, you need to provide user keys to the Ceph client(s) so that the Ceph client can retrieve the key for the specified user and authenticate with the Ceph Storage Cluster. Ceph Clients access keyrings to lookup a user name and retrieve the user’s key.

The `ceph-authtool` utility allows you to create a keyring. To create an empty keyring, use `--create-keyring` or `-C`. For example:

```
ceph-authtool --create-keyring /path/to/keyring
```

When creating a keyring with multiple users, we recommend using the cluster name (e.g., `$cluster.keyring`) for the keyring filename and saving it in the `/etc/ceph` directory so that the `keyring` configuration default setting will pick up the filename without requiring you to specify it in the local copy of your Ceph configuration file. For example, create `ceph.keyring` by executing the following:

```
sudo ceph-authtool -C /etc/ceph/ceph.keyring
```

When creating a keyring with a single user, we recommend using the cluster name, the user type and the user name and saving it in the `/etc/ceph` directory. For example, `ceph.client.admin.keyring` for the `client.admin` user.

To create a keyring in `/etc/ceph`, you must do so as `root`. This means the file will have `rw` permissions for the `root` user only, which is appropriate when the keyring contains administrator keys. However, if you intend to use the keyring for a particular user or group of users, ensure that you execute `chown` or `chmod` to establish appropriate keyring ownership and access.

### Add a User to a Keyring[](https://docs.ceph.com/en/latest/rados/operations/user-management/#add-a-user-to-a-keyring)

When you  [Add a User](https://docs.ceph.com/en/latest/rados/operations/user-management/#add-a-user) to the Ceph Storage Cluster, you can use the [Get a User](https://docs.ceph.com/en/latest/rados/operations/user-management/#get-a-user) procedure to retrieve a user, key and capabilities and save the user to a keyring.

When you only want to use one user per keyring, the [Get a User](https://docs.ceph.com/en/latest/rados/operations/user-management/#get-a-user) procedure with the `-o` option will save the output in the keyring file format. For example, to create a keyring for the `client.admin` user, execute the following:

```
sudo ceph auth get client.admin -o /etc/ceph/ceph.client.admin.keyring
```

Notice that we use the recommended file format for an individual user.

When you want to import users to a keyring, you can use `ceph-authtool` to specify the destination keyring and the source keyring. For example:

```
sudo ceph-authtool /etc/ceph/ceph.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
```

### Create a User[](https://docs.ceph.com/en/latest/rados/operations/user-management/#create-a-user)

Ceph provides the [Add a User](https://docs.ceph.com/en/latest/rados/operations/user-management/#add-a-user) function to create a user directly in the Ceph Storage Cluster. However, you can also create a user, keys and capabilities directly on a Ceph client keyring. Then, you can import the user to the Ceph Storage Cluster. For example:

```
sudo ceph-authtool -n client.ringo --cap osd 'allow rwx' --cap mon 'allow rwx' /etc/ceph/ceph.keyring
```

See [Authorization (Capabilities)](https://docs.ceph.com/en/latest/rados/operations/user-management/#authorization-capabilities) for additional details on capabilities.

You can also create a keyring and add a new user to the keyring simultaneously. For example:

```
sudo ceph-authtool -C /etc/ceph/ceph.keyring -n client.ringo --cap osd 'allow rwx' --cap mon 'allow rwx' --gen-key
```

In the foregoing scenarios, the new user `client.ringo` is only in the keyring. To add the new user to the Ceph Storage Cluster, you must still add the new user to the Ceph Storage Cluster.

```
sudo ceph auth add client.ringo -i /etc/ceph/ceph.keyring
```

### Modify a User[](https://docs.ceph.com/en/latest/rados/operations/user-management/#modify-a-user)

To modify the capabilities of a user record in a keyring, specify the keyring, and the user followed by the capabilities. For example:

```
sudo ceph-authtool /etc/ceph/ceph.keyring -n client.ringo --cap osd 'allow rwx' --cap mon 'allow rwx'
```

To update the user to the Ceph Storage Cluster, you must update the user in the keyring to the user entry in the Ceph Storage Cluster.

```
sudo ceph auth import -i /etc/ceph/ceph.keyring
```

See [Import a User(s)](https://docs.ceph.com/en/latest/rados/operations/user-management/#import-a-user-s) for details on updating a Ceph Storage Cluster user from a keyring.

You may also [Modify User Capabilities](https://docs.ceph.com/en/latest/rados/operations/user-management/#id2) directly in the cluster, store the results to a keyring file; then, import the keyring into your main `ceph.keyring` file.

## Command Line Usage[](https://docs.ceph.com/en/latest/rados/operations/user-management/#command-line-usage)

Ceph supports the following usage for user name and secret:

```
--id` | `--user
```

- Description

  Ceph identifies users with a type and an ID (e.g., `TYPE.ID` or `client.admin`, `client.user1`). The `id`, `name` and `-n` options enable you to specify the ID portion of the user name (e.g., `admin`, `user1`, `foo`, etc.). You can specify the user with the `--id` and omit the type. For example, to specify user `client.foo` enter the following: `ceph --id foo --keyring /path/to/keyring health ceph --user foo --keyring /path/to/keyring health `

```
--name` | `-n
```

- Description

  Ceph identifies users with a type and an ID (e.g., `TYPE.ID` or `client.admin`, `client.user1`). The `--name` and `-n` options enables you to specify the fully qualified user name. You must specify the user type (typically `client`) with the user ID. For example: `ceph --name client.foo --keyring /path/to/keyring health ceph -n client.foo --keyring /path/to/keyring health `

```
--keyring
```

- Description

  The path to the keyring containing one or more user name and secret. The `--secret` option provides the same functionality, but it does not work with Ceph RADOS Gateway, which uses `--secret` for another purpose. You may retrieve a keyring with `ceph auth get-or-create` and store it locally. This is a preferred approach, because you can switch user names without switching the keyring path. For example: `sudo rbd map --id foo --keyring /path/to/keyring mypool/myimage `

## Limitations[](https://docs.ceph.com/en/latest/rados/operations/user-management/#limitations)

The `cephx` protocol authenticates Ceph clients and servers to each other.  It is not intended to handle authentication of human users or application programs run on their behalf.  If that effect is required to handle your access control needs, you must have another mechanism, which is likely to be specific to the front end used to access the Ceph object store.  This other mechanism has the role of ensuring that only acceptable users and programs are able to run on the machine that Ceph will permit to access its object store.

The keys used to authenticate Ceph clients and servers are typically stored in a plain text file with appropriate permissions in a trusted host.

Important

Storing keys in plaintext files has security shortcomings, but they are difficult to avoid, given the basic authentication methods Ceph uses in the background. Those setting up Ceph systems should be aware of these shortcomings.

In particular, arbitrary user machines, especially portable machines, should not be configured to interact directly with Ceph, since that mode of use would require the storage of a plaintext authentication key on an insecure machine. Anyone  who stole that machine or obtained surreptitious access to it could obtain the key that will allow them to authenticate their own machines to Ceph.

Rather than permitting potentially insecure machines to access a Ceph object store directly,  users should be required to sign in to a trusted machine in your environment using a method  that provides sufficient security for your purposes.  That trusted machine will store the plaintext Ceph keys for the human users.  A future version of Ceph may address these particular authentication issues more fully.

At the moment, none of the Ceph authentication protocols provide secrecy for messages in transit. Thus, an eavesdropper on the wire can hear and understand all data sent between clients and servers in Ceph, even if it cannot create or alter them. Further, Ceph does not include options to encrypt user data in the object store. Users can hand-encrypt and store their own data in the Ceph object store, of course, but Ceph provides no features to perform object encryption itself. Those storing sensitive data in Ceph should consider encrypting their data before providing it  to the Ceph system.

# Repairing PG inconsistencies[](https://docs.ceph.com/en/latest/rados/operations/pg-repair/#repairing-pg-inconsistencies)

Sometimes a placement group might become “inconsistent”. To return the placement group to an active+clean state, you must first determine which of the placement groups has become inconsistent and then run the “pg repair” command on it. This page contains commands for diagnosing placement groups and the command for repairing placement groups that have become inconsistent.

## Commands for Diagnosing Placement-group Problems[](https://docs.ceph.com/en/latest/rados/operations/pg-repair/#commands-for-diagnosing-placement-group-problems)

The commands in this section provide various ways of diagnosing broken placement groups.

The following command provides a high-level (low detail) overview of the health of the ceph cluster:

```
ceph health detail
```

The following command provides more detail on the status of the placement groups:

```
ceph pg dump --format=json-pretty
```

The following command lists inconsistent placement groups:

```
rados list-inconsistent-pg {pool}
```

The following command lists inconsistent rados objects:

```
rados list-inconsistent-obj {pgid}
```

The following command lists inconsistent snapsets in the given placement group:

```
rados list-inconsistent-snapset {pgid}
```

## Commands for Repairing Placement Groups[](https://docs.ceph.com/en/latest/rados/operations/pg-repair/#commands-for-repairing-placement-groups)

The form of the command to repair a broken placement group is:

```
ceph pg repair {pgid}
```

Where `{pgid}` is the id of the affected placement group.

For example:

```
ceph pg repair 1.4
```

## More Information on Placement Group Repair[](https://docs.ceph.com/en/latest/rados/operations/pg-repair/#more-information-on-placement-group-repair)

Ceph stores and updates the checksums of objects stored in the  cluster. When a scrub is performed on a placement group, the OSD  attempts to choose an authoritative copy from among its replicas. Among  all of the possible cases, only one case is consistent. After a deep  scrub, Ceph calculates the checksum of an object read from the disk and  compares it to the checksum previously recorded. If the current checksum and the previously recorded checksums do not match, that is an  inconsistency. In the case of replicated pools, any mismatch between the checksum of any replica of an object and the checksum of the  authoritative copy means that there is an inconsistency.

The “pg repair” command attempts to fix inconsistencies of various  kinds. If “pg repair” finds an inconsistent placement group, it attempts to overwrite the digest of the inconsistent copy with the digest of the authoritative copy. If “pg repair” finds an inconsistent replicated  pool, it marks the inconsistent copy as missing. Recovery, in the case  of replicated pools, is beyond the scope of “pg repair”.

For erasure coded and bluestore pools, Ceph will automatically repair if osd_scrub_auto_repair (configuration default “false”) is set to true and at most osd_scrub_auto_repair_num_errors (configuration default 5)  errors are found.

“pg repair” will not solve every problem. Ceph does not automatically repair placement groups when inconsistencies are found in them.

The checksum of an object or an omap is not always available.  Checksums are calculated incrementally. If a replicated object is  updated non-sequentially, the write operation involved in the update  changes the object and invalidates its checksum. The whole object is not read while recalculating the checksum. “ceph pg repair” is able to  repair things even when checksums are not available to it, as in the  case of filestore. When replicated filestore pools are in question,  users might prefer manual repair to “ceph pg repair”.

The material in this paragraph is relevant for filestore, and  bluestore has its own internal checksums. The matched-record checksum  and the calculated checksum cannot prove that the authoritative copy is  in fact authoritative. In the case that there is no checksum available,  “pg repair” favors the data on the primary. this might or might not be  the uncorrupted replica. This is why human intervention is necessary  when an inconsistency is discovered. Human intervention sometimes means  using the “ceph-objectstore-tool”.

## External Links[](https://docs.ceph.com/en/latest/rados/operations/pg-repair/#external-links)

https://ceph.io/geen-categorie/ceph-manually-repair-object/ - This page contains a walkthrough of the repair of a placement group,  and is recommended reading if you want to repair a placement group but have never done so.

# Data Placement Overview[](https://docs.ceph.com/en/latest/rados/operations/data-placement/#data-placement-overview)

Ceph stores, replicates and rebalances data objects across a RADOS cluster dynamically.  With many different users storing objects in different pools for different purposes on countless OSDs, Ceph operations require some data placement planning.  The main data placement planning concepts in Ceph include:

- **Pools:** Ceph stores data within pools, which are logical groups for storing objects. Pools manage the number of placement groups, the number of replicas, and the CRUSH rule for the pool. To store data in a pool, you must have an authenticated user with permissions for the pool. Ceph can snapshot pools. See [Pools](https://docs.ceph.com/en/latest/rados/operations/pools) for additional details.
- **Placement Groups:** Ceph maps objects to placement groups (PGs). Placement groups (PGs) are shards or fragments of a logical object pool that place objects as a group into OSDs. Placement groups reduce the amount of per-object metadata when Ceph stores the data in OSDs. A larger number of placement groups (e.g., 100 per OSD) leads to better balancing. See [Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#placement-groups) for additional details.
- **CRUSH Maps:**  CRUSH is a big part of what allows Ceph to scale without performance bottlenecks, without limitations to scalability, and without a single point of failure. CRUSH maps provide the physical topology of the cluster to the CRUSH algorithm to determine where the data for an object and its replicas should be stored, and how to do so across failure domains for added data safety among other things. See [CRUSH Maps](https://docs.ceph.com/en/latest/rados/operations/crush-map) for additional details.
- **Balancer:** The balancer is a feature that will automatically optimize the distribution of PGs across devices to achieve a balanced data distribution, maximizing the amount of data that can be stored in the cluster and evenly distributing the workload across OSDs.

When you initially set up a test cluster, you can use the default values. Once you begin planning for a large Ceph cluster, refer to pools, placement groups and CRUSH for data placement operations.

# Pools[](https://docs.ceph.com/en/latest/rados/operations/pools/#pools)

Pools are logical partitions for storing objects.

When you first deploy a cluster without creating a pool, Ceph uses the default pools for storing data. A pool provides you with:

- **Resilience**: You can set how many OSD are allowed to fail without losing data. For replicated pools, it is the desired number of copies/replicas of an object. A typical configuration stores an object and two additional copies (i.e., `size = 3`), but you can configure the number of copies/replicas at pool granularity. For [erasure coded pools](https://docs.ceph.com/en/latest/rados/operations/erasure-code), it is the number of coding chunks (i.e. `m=2` in the **erasure code profile**)
- **Placement Groups**: You can set the number of placement groups for the pool. A typical configuration targets approximately 100 placement groups per OSD to provide optimal balancing without using up too many computing resources. When setting up multiple pools, be careful to set a reasonable number of placement groups for each pool and for the cluster as a whole.  Note that each PG belongs to a specific pool, so when multiple pools use the same OSDs, you must take care that the **sum** of PG replicas per OSD is in the desired PG per OSD target range.
- **CRUSH Rules**: When you store data in a pool, placement of the object and its replicas (or chunks for erasure coded pools) in your cluster is governed by CRUSH rules. You can create a custom CRUSH rule for your pool if the default rule is not appropriate for your use case.
- **Snapshots**: When you create snapshots with `ceph osd pool mksnap`, you effectively take a snapshot of a particular pool.

To organize data into pools, you can list, create, and remove pools. You can also view the utilization statistics for each pool.

## Pool Names[](https://docs.ceph.com/en/latest/rados/operations/pools/#pool-names)

Pool names beginning with `.` are reserved for use by Ceph’s internal operations. Please do not create or manipulate pools with these names.

## List Pools[](https://docs.ceph.com/en/latest/rados/operations/pools/#list-pools)

To list your cluster’s pools, execute:

```
ceph osd lspools
```



## Create a Pool[](https://docs.ceph.com/en/latest/rados/operations/pools/#create-a-pool)

Before creating pools, refer to the [Pool, PG and CRUSH Config Reference](https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref). Ideally, you should override the default value for the number of placement groups in your Ceph configuration file, as the default is NOT ideal. For details on placement group numbers refer to [setting the number of placement groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups#set-the-number-of-placement-groups)

Note

Starting with Luminous, all pools need to be associated to the application using the pool. See [Associate Pool to Application](https://docs.ceph.com/en/latest/rados/operations/pools/#id1) below for more information.

For example:

```
osd_pool_default_pg_num = 128
osd_pool_default_pgp_num = 128
```

To create a pool, execute:

```
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]] [replicated] \
     [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} [{pg-num} [{pgp-num}]]   erasure \
     [erasure-code-profile] [crush-rule-name] [expected_num_objects] [--autoscale-mode=<on,off,warn>]
```

Where:

- {pool-name}

  The name of the pool. It must be unique. Type String Required Yes.

- {pg-num}

  The total number of placement groups for the pool. See [Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#placement-groups) for details on calculating a suitable number. The default value `8` is NOT suitable for most systems.  Type Integer Required Yes. Default 8

- {pgp-num}

  The total number of placement groups for placement purposes. This **should be equal to the total number of placement groups**, except for placement group splitting scenarios.  Type Integer Required Yes. Picks up default or Ceph configuration value if not specified. Default 8

- {replicated|erasure}

  The pool type which may either be **replicated** to recover from lost OSDs by keeping multiple copies of the objects or **erasure** to get a kind of [generalized RAID5](https://docs.ceph.com/en/latest/rados/operations/erasure-code) capability. The **replicated** pools require more raw storage but implement all Ceph operations. The **erasure** pools require less raw storage but only implement a subset of the available operations.  Type String Required No. Default replicated

- [crush-rule-name]

  The name of a CRUSH rule to use for this pool.  The specified rule must exist. Type String Required No. Default For **replicated** pools it is the rule specified by the [`osd_pool_default_crush_rule`](https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref/#confval-osd_pool_default_crush_rule) config variable.  This rule must exist. For **erasure** pools it is `erasure-code` if the `default` [erasure code profile](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile) is used or `{pool-name}` otherwise.  This rule will be created implicitly if it doesn’t exist already.

- [erasure-code-profile=profile]

  For **erasure** pools only. Use the [erasure code profile](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile). It must be an existing profile as defined by **osd erasure-code-profile set**.  Type String Required No.

- --autoscale-mode=<on,off,warn>

  If you set the autoscale mode to `on` or `warn`, you can let the system autotune or recommend changes to the number of placement groups in your pool based on actual usage.  If you leave it off, then you should refer to [Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#placement-groups) for more information.  Type String Required No. Default The default behavior is controlled by the [`osd_pool_default_pg_autoscale_mode`](https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref/#confval-osd_pool_default_pg_autoscale_mode) option.

- [expected-num-objects]

  The expected number of objects for this pool. By setting this value ( together with a negative **filestore merge threshold**), the PG folder splitting would happen at the pool creation time, to avoid the latency impact to do a runtime folder splitting. Type Integer Required No. Default 0, no splitting at the pool creation time.



## Associate Pool to Application[](https://docs.ceph.com/en/latest/rados/operations/pools/#associate-pool-to-application)

Pools need to be associated with an application before use. Pools that will be used with CephFS or pools that are automatically created by RGW are automatically associated. Pools that are intended for use with RBD should be initialized using the `rbd` tool (see [Block Device Commands](https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-pool) for more information).

For other cases, you can manually associate a free-form application name to a pool.:

```
ceph osd pool application enable {pool-name} {application-name}
```

Note

CephFS uses the application name `cephfs`, RBD uses the application name `rbd`, and RGW uses the application name `rgw`.

## Set Pool Quotas[](https://docs.ceph.com/en/latest/rados/operations/pools/#set-pool-quotas)

You can set pool quotas for the maximum number of bytes and/or the maximum number of objects per pool.

```
ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]
```

For example:

```
ceph osd pool set-quota data max_objects 10000
```

To remove a quota, set its value to `0`.

## Delete a Pool[](https://docs.ceph.com/en/latest/rados/operations/pools/#delete-a-pool)

To delete a pool, execute:

```
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]
```

To remove a pool the mon_allow_pool_delete flag must be set to true in the Monitor’s configuration. Otherwise they will refuse to remove a pool.

See [Monitor Configuration](https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref) for more information.

If you created your own rules for a pool you created, you should consider removing them when you no longer need your pool:

```
ceph osd pool get {pool-name} crush_rule
```

If the rule was “123”, for example, you can check the other pools like so:

```
ceph osd dump | grep "^pool" | grep "crush_rule 123"
```

If no other pools use that custom rule, then it’s safe to delete that rule from the cluster.

If you created users with permissions strictly for a pool that no longer exists, you should consider deleting those users too:

```
ceph auth ls | grep -C 5 {pool-name}
ceph auth del {user}
```

## Rename a Pool[](https://docs.ceph.com/en/latest/rados/operations/pools/#rename-a-pool)

To rename a pool, execute:

```
ceph osd pool rename {current-pool-name} {new-pool-name}
```

If you rename a pool and you have per-pool capabilities for an authenticated user, you must update the user’s capabilities (i.e., caps) with the new pool name.

## Show Pool Statistics[](https://docs.ceph.com/en/latest/rados/operations/pools/#show-pool-statistics)

To show a pool’s utilization statistics, execute:

```
rados df
```

Additionally, to obtain I/O information for a specific pool or all, execute:

```
ceph osd pool stats [{pool-name}]
```

## Make a Snapshot of a Pool[](https://docs.ceph.com/en/latest/rados/operations/pools/#make-a-snapshot-of-a-pool)

To make a snapshot of a pool, execute:

```
ceph osd pool mksnap {pool-name} {snap-name}
```

## Remove a Snapshot of a Pool[](https://docs.ceph.com/en/latest/rados/operations/pools/#remove-a-snapshot-of-a-pool)

To remove a snapshot of a pool, execute:

```
ceph osd pool rmsnap {pool-name} {snap-name}
```



## Set Pool Values[](https://docs.ceph.com/en/latest/rados/operations/pools/#set-pool-values)

To set a value to a pool, execute the following:

```
ceph osd pool set {pool-name} {key} {value}
```

You may set values for the following keys:



- compression_algorithm

  Sets inline compression algorithm to use for underlying BlueStore. This setting overrides the global setting [`bluestore_compression_algorithm`](https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#confval-bluestore_compression_algorithm). Type String Valid Settings `lz4`, `snappy`, `zlib`, `zstd`

- compression_mode

  Sets the policy for the inline compression algorithm for underlying BlueStore. This setting overrides the global setting [`bluestore_compression_mode`](https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#confval-bluestore_compression_mode). Type String Valid Settings `none`, `passive`, `aggressive`, `force`

- compression_min_blob_size

  Chunks smaller than this are never compressed. This setting overrides the global settings of [`bluestore_compression_min_blob_size`](https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#confval-bluestore_compression_min_blob_size), [`bluestore_compression_min_blob_size_hdd`](https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#confval-bluestore_compression_min_blob_size_hdd) and [`bluestore_compression_min_blob_size_ssd`](https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#confval-bluestore_compression_min_blob_size_ssd) Type Unsigned Integer

- compression_max_blob_size

  Chunks larger than this are broken into smaller blobs sizing `compression_max_blob_size` before being compressed. Type Unsigned Integer



- size

  Sets the number of replicas for objects in the pool. See [Set the Number of Object Replicas](https://docs.ceph.com/en/latest/rados/operations/pools/#set-the-number-of-object-replicas) for further details. Replicated pools only. Type Integer



- min_size

  Sets the minimum number of replicas required for I/O. See [Set the Number of Object Replicas](https://docs.ceph.com/en/latest/rados/operations/pools/#set-the-number-of-object-replicas) for further details. In the case of Erasure Coded pools this should be set to a value greater than ‘k’ since if we allow IO at the value ‘k’ there is no redundancy and data will be lost in the event of a permanent OSD failure. For more information see [Erasure Code](https://docs.ceph.com/en/latest/rados/operations/erasure-code) Type Integer Version `0.54` and above



- pg_num

  The effective number of placement groups to use when calculating data placement. Type Integer Valid Range Superior to `pg_num` current value.



- pgp_num

  The effective number of placement groups for placement to use when calculating data placement. Type Integer Valid Range Equal to or less than `pg_num`.



- crush_rule

  The rule to use for mapping object placement in the cluster. Type String



- allow_ec_overwrites

  Whether writes to an erasure coded pool can update part of an object, so cephfs and rbd can use it. See [Erasure Coding with Overwrites](https://docs.ceph.com/en/latest/rados/operations/erasure-code#erasure-coding-with-overwrites) for more details. Type Boolean  New in version 12.2.0.



- hashpspool

  Set/Unset HASHPSPOOL flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag



- nodelete

  Set/Unset NODELETE flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag Version Version `FIXME`



- nopgchange

  Description Set/Unset NOPGCHANGE flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag Version Version `FIXME`



- nosizechange

  Set/Unset NOSIZECHANGE flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag Version Version `FIXME`



- bulk

  Set/Unset bulk flag on a given pool. Type Boolean Valid Range true/1 sets flag, false/0 unsets flag



- write_fadvise_dontneed

  Set/Unset WRITE_FADVISE_DONTNEED flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag



- noscrub

  Set/Unset NOSCRUB flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag



- nodeep-scrub

  Set/Unset NODEEP_SCRUB flag on a given pool. Type Integer Valid Range 1 sets flag, 0 unsets flag



- hit_set_type

  Enables hit set tracking for cache pools. See [Bloom Filter](https://en.wikipedia.org/wiki/Bloom_filter) for additional information. Type String Valid Settings `bloom`, `explicit_hash`, `explicit_object` Default `bloom`. Other values are for testing.



- hit_set_count

  The number of hit sets to store for cache pools. The higher the number, the more RAM consumed by the `ceph-osd` daemon. Type Integer Valid Range `1`. Agent doesn’t handle > 1 yet.



- hit_set_period

  The duration of a hit set period in seconds for cache pools. The higher the number, the more RAM consumed by the `ceph-osd` daemon. Type Integer Example `3600` 1hr



- hit_set_fpp

  The false positive probability for the `bloom` hit set type. See [Bloom Filter](https://en.wikipedia.org/wiki/Bloom_filter) for additional information. Type Double Valid Range 0.0 - 1.0 Default `0.05`



- cache_target_dirty_ratio

  The percentage of the cache pool containing modified (dirty) objects before the cache tiering agent will flush them to the backing storage pool. Type Double Default `.4`



- cache_target_dirty_high_ratio

  The percentage of the cache pool containing modified (dirty) objects before the cache tiering agent will flush them to the backing storage pool with a higher speed. Type Double Default `.6`



- cache_target_full_ratio

  The percentage of the cache pool containing unmodified (clean) objects before the cache tiering agent will evict them from the cache pool. Type Double Default `.8`



- target_max_bytes

  Ceph will begin flushing or evicting objects when the `max_bytes` threshold is triggered. Type Integer Example `1000000000000`  #1-TB



- target_max_objects

  Ceph will begin flushing or evicting objects when the `max_objects` threshold is triggered. Type Integer Example `1000000` #1M objects

- hit_set_grade_decay_rate

  Temperature decay rate between two successive hit_sets Type Integer Valid Range 0 - 100 Default `20`

- hit_set_search_last_n

  Count at most N appearance in hit_sets for temperature calculation Type Integer Valid Range 0 - hit_set_count Default `1`



- cache_min_flush_age

  The time (in seconds) before the cache tiering agent will flush an object from the cache pool to the storage pool. Type Integer Example `600` 10min



- cache_min_evict_age

  The time (in seconds) before the cache tiering agent will evict an object from the cache pool. Type Integer Example `1800` 30min



- fast_read

  On Erasure Coding pool, if this flag is turned on, the read request would issue sub reads to all shards, and waits until it receives enough shards to decode to serve the client. In the case of jerasure and isa erasure plugins, once the first K replies return, client’s request is served immediately using the data decoded from these replies. This helps to tradeoff some resources for better performance. Currently this flag is only supported for Erasure Coding pool. Type Boolean Defaults `0`



- scrub_min_interval

  The minimum interval in seconds for pool scrubbing when load is low. If it is 0, the value osd_scrub_min_interval from config is used. Type Double Default `0`



- scrub_max_interval

  The maximum interval in seconds for pool scrubbing irrespective of cluster load. If it is 0, the value osd_scrub_max_interval from config is used. Type Double Default `0`



- deep_scrub_interval

  The interval in seconds for pool “deep” scrubbing. If it is 0, the value osd_deep_scrub_interval from config is used. Type Double Default `0`



- recovery_priority

  When a value is set it will increase or decrease the computed reservation priority. This value must be in the range -10 to 10.  Use a negative priority for less important pools so they have lower priority than any new pools. Type Integer Default `0`



- recovery_op_priority

  Specify the recovery operation priority for this pool instead of [`osd_recovery_op_priority`](https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/#confval-osd_recovery_op_priority). Type Integer Default `0`

## Get Pool Values[](https://docs.ceph.com/en/latest/rados/operations/pools/#get-pool-values)

To get a value from a pool, execute the following:

```
ceph osd pool get {pool-name} {key}
```

You may get values for the following keys:

```
size
```

- Description

  see [size](https://docs.ceph.com/en/latest/rados/operations/pools/#size)

- Type

  Integer

```
min_size
```

- Description

  see [min_size](https://docs.ceph.com/en/latest/rados/operations/pools/#min-size)

- Type

  Integer

- Version

  `0.54` and above

```
pg_num
```

- Description

  see [pg_num](https://docs.ceph.com/en/latest/rados/operations/pools/#pg-num)

- Type

  Integer

```
pgp_num
```

- Description

  see [pgp_num](https://docs.ceph.com/en/latest/rados/operations/pools/#pgp-num)

- Type

  Integer

- Valid Range

  Equal to or less than `pg_num`.

```
crush_rule
```

- Description

  see [crush_rule](https://docs.ceph.com/en/latest/rados/operations/pools/#crush-rule)

```
hit_set_type
```

- Description

  see [hit_set_type](https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-type)

- Type

  String

- Valid Settings

  `bloom`, `explicit_hash`, `explicit_object`

```
hit_set_count
```

- Description

  see [hit_set_count](https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-count)

- Type

  Integer

```
hit_set_period
```

- Description

  see [hit_set_period](https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-period)

- Type

  Integer

```
hit_set_fpp
```

- Description

  see [hit_set_fpp](https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-fpp)

- Type

  Double

```
cache_target_dirty_ratio
```

- Description

  see [cache_target_dirty_ratio](https://docs.ceph.com/en/latest/rados/operations/pools/#cache-target-dirty-ratio)

- Type

  Double

```
cache_target_dirty_high_ratio
```

- Description

  see [cache_target_dirty_high_ratio](https://docs.ceph.com/en/latest/rados/operations/pools/#cache-target-dirty-high-ratio)

- Type

  Double

```
cache_target_full_ratio
```

- Description

  see [cache_target_full_ratio](https://docs.ceph.com/en/latest/rados/operations/pools/#cache-target-full-ratio)

- Type

  Double

```
target_max_bytes
```

- Description

  see [target_max_bytes](https://docs.ceph.com/en/latest/rados/operations/pools/#target-max-bytes)

- Type

  Integer

```
target_max_objects
```

- Description

  see [target_max_objects](https://docs.ceph.com/en/latest/rados/operations/pools/#target-max-objects)

- Type

  Integer

```
cache_min_flush_age
```

- Description

  see [cache_min_flush_age](https://docs.ceph.com/en/latest/rados/operations/pools/#cache-min-flush-age)

- Type

  Integer

```
cache_min_evict_age
```

- Description

  see [cache_min_evict_age](https://docs.ceph.com/en/latest/rados/operations/pools/#cache-min-evict-age)

- Type

  Integer

```
fast_read
```

- Description

  see [fast_read](https://docs.ceph.com/en/latest/rados/operations/pools/#fast-read)

- Type

  Boolean

```
scrub_min_interval
```

- Description

  see [scrub_min_interval](https://docs.ceph.com/en/latest/rados/operations/pools/#scrub-min-interval)

- Type

  Double

```
scrub_max_interval
```

- Description

  see [scrub_max_interval](https://docs.ceph.com/en/latest/rados/operations/pools/#scrub-max-interval)

- Type

  Double

```
deep_scrub_interval
```

- Description

  see [deep_scrub_interval](https://docs.ceph.com/en/latest/rados/operations/pools/#deep-scrub-interval)

- Type

  Double

```
allow_ec_overwrites
```

- Description

  see [allow_ec_overwrites](https://docs.ceph.com/en/latest/rados/operations/pools/#allow-ec-overwrites)

- Type

  Boolean

```
recovery_priority
```

- Description

  see [recovery_priority](https://docs.ceph.com/en/latest/rados/operations/pools/#recovery-priority)

- Type

  Integer

```
recovery_op_priority
```

- Description

  see [recovery_op_priority](https://docs.ceph.com/en/latest/rados/operations/pools/#recovery-op-priority)

- Type

  Integer

## Set the Number of Object Replicas[](https://docs.ceph.com/en/latest/rados/operations/pools/#set-the-number-of-object-replicas)

To set the number of object replicas on a replicated pool, execute the following:

```
ceph osd pool set {poolname} size {num-replicas}
```

Important

The `{num-replicas}` includes the object itself. If you want the object and two copies of the object for a total of three instances of the object, specify `3`.

For example:

```
ceph osd pool set data size 3
```

You may execute this command for each pool. **Note:** An object might accept I/Os in degraded mode with fewer than `pool size` replicas.  To set a minimum number of required replicas for I/O, you should use the `min_size` setting. For example:

```
ceph osd pool set data min_size 2
```

This ensures that no object in the data pool will receive I/O with fewer than `min_size` replicas.

## Get the Number of Object Replicas[](https://docs.ceph.com/en/latest/rados/operations/pools/#get-the-number-of-object-replicas)

To get the number of object replicas, execute the following:

```
ceph osd dump | grep 'replicated size'
```

Ceph will list the pools, with the `replicated size` attribute highlighted. By default, ceph creates two replicas of an object (a total of three copies, or a size of 3).

# Erasure code[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#erasure-code)

A Ceph pool is associated to a type to sustain the loss of an OSD (i.e. a disk since most of the time there is one OSD per disk). The default choice when [creating a pool](https://docs.ceph.com/en/latest/rados/operations/pools) is *replicated*, meaning every object is copied on multiple disks. The [Erasure Code](https://en.wikipedia.org/wiki/Erasure_code) pool type can be used instead to save space.

## Creating a sample erasure coded pool[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#creating-a-sample-erasure-coded-pool)

The simplest erasure coded pool is equivalent to [RAID5](https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_5) and requires at least three hosts:

```
$ ceph osd pool create ecpool erasure
pool 'ecpool' created
$ echo ABCDEFGHI | rados --pool ecpool put NYAN -
$ rados --pool ecpool get NYAN -
ABCDEFGHI
```

## Erasure code profiles[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#erasure-code-profiles)

The default erasure code profile sustains the loss of a two OSDs. It is equivalent to a replicated pool of size three but requires 2TB instead of 3TB to store 1TB of data. The default profile can be displayed with:

```
$ ceph osd erasure-code-profile get default
k=2
m=2
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van
```

Choosing the right profile is important because it cannot be modified after the pool is created: a new pool with a different profile needs to be created and all objects from the previous pool moved to the new.

The most important parameters of the profile are *K*, *M* and *crush-failure-domain* because they define the storage overhead and the data durability. For instance, if the desired architecture must sustain the loss of two racks with a storage overhead of 67% overhead, the following profile can be defined:

```
$ ceph osd erasure-code-profile set myprofile \
   k=3 \
   m=2 \
   crush-failure-domain=rack
$ ceph osd pool create ecpool erasure myprofile
$ echo ABCDEFGHI | rados --pool ecpool put NYAN -
$ rados --pool ecpool get NYAN -
ABCDEFGHI
```

The *NYAN* object will be divided in three (*K=3*) and two additional *chunks* will be created (*M=2*). The value of *M* defines how many OSD can be lost simultaneously without losing any data. The *crush-failure-domain=rack* will create a CRUSH rule that ensures no two *chunks* are stored in the same rack.

![img](https://docs.ceph.com/en/latest/_images/ditaa-86952ee961bd79c367ecd85023e44450c2b55b0d.png)

More information can be found in the [erasure code profiles](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile) documentation.

## Erasure Coding with Overwrites[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#erasure-coding-with-overwrites)

By default, erasure coded pools only work with uses like RGW that perform full object writes and appends.

Since Luminous, partial writes for an erasure coded pool may be enabled with a per-pool setting. This lets RBD and CephFS store their data in an erasure coded pool:

```
ceph osd pool set ec_pool allow_ec_overwrites true
```

This can only be enabled on a pool residing on bluestore OSDs, since bluestore’s checksumming is used to detect bitrot or other corruption during deep-scrub. In addition to being unsafe, using filestore with ec overwrites yields low performance compared to bluestore.

Erasure coded pools do not support omap, so to use them with RBD and CephFS you must instruct them to store their data in an ec pool, and their metadata in a replicated pool. For RBD, this means using the erasure coded pool as the `--data-pool` during image creation:

```
rbd create --size 1G --data-pool ec_pool replicated_pool/image_name
```

For CephFS, an erasure coded pool can be set as the default data pool during file system creation or via [file layouts](https://docs.ceph.com/en/latest/cephfs/file-layouts).

## Erasure coded pool and cache tiering[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#erasure-coded-pool-and-cache-tiering)

Erasure coded pools require more resources than replicated pools and lack some functionalities such as omap. To overcome these limitations, one can set up a [cache tier](https://docs.ceph.com/en/latest/rados/operations/cache-tiering) before the erasure coded pool.

For instance, if the pool *hot-storage* is made of fast storage:

```
$ ceph osd tier add ecpool hot-storage
$ ceph osd tier cache-mode hot-storage writeback
$ ceph osd tier set-overlay ecpool hot-storage
```

will place the *hot-storage* pool as tier of *ecpool* in *writeback* mode so that every write and read to the *ecpool* are actually using the *hot-storage* and benefit from its flexibility and speed.

More information can be found in the [cache tiering](https://docs.ceph.com/en/latest/rados/operations/cache-tiering) documentation.

## Erasure coded pool recovery[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#erasure-coded-pool-recovery)

If an erasure coded pool loses some shards, it must recover them from the others. This generally involves reading from the remaining shards, reconstructing the data, and writing it to the new peer. In Octopus, erasure coded pools can recover as long as there are at least *K* shards available. (With fewer than *K* shards, you have actually lost data!)

Prior to Octopus, erasure coded pools required at least *min_size* shards to be available, even if *min_size* is greater than *K*. (We generally recommend min_size be *K+2* or more to prevent loss of writes and data.) This conservative decision was made out of an abundance of caution when designing the new pool mode but also meant pools with lost OSDs but no data loss were unable to recover and go active without manual intervention to change the *min_size*.

## Glossary[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#glossary)

- *chunk*

  when the encoding function is called, it returns chunks of the same size. Data chunks which can be concatenated to reconstruct the original object and coding chunks which can be used to rebuild a lost chunk.

- *K*

  the number of data *chunks*, i.e. the number of *chunks* in which the original object is divided. For instance if *K* = 2 a 10KB object will be divided into *K* objects of 5KB each.

- *M*

  the number of coding *chunks*, i.e. the number of additional *chunks* computed by the encoding functions. If there are 2 coding *chunks*, it means 2 OSDs can be out without losing data.

## Table of content[](https://docs.ceph.com/en/latest/rados/operations/erasure-code/#table-of-content)

- [Erasure code profiles](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/)
- [Jerasure erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-jerasure/)
- [ISA erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-isa/)
- [Locally repairable erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/)
- [SHEC erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/)
- [CLAY code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/)

​        

# Erasure code profiles[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/#erasure-code-profiles)

Erasure code is defined by a **profile** and is used when creating an erasure coded pool and the associated CRUSH rule.

The **default** erasure code profile (which is created when the Ceph cluster is initialized) will split the data into 2 equal-sized chunks, and have 2 parity chunks of the same size. It will take as much space in the cluster as a 2-replica pool but can sustain the data loss of 2 chunks out of 4. It is described as a profile with **k=2** and **m=2**, meaning the information is spread over four OSD (k+m == 4) and two of them can be lost.

To improve redundancy without increasing raw storage requirements, a new profile can be created. For instance, a profile with **k=10** and **m=4** can sustain the loss of four (**m=4**) OSDs by distributing an object on fourteen (k+m=14) OSDs. The object is first divided in **10** chunks (if the object is 10MB, each chunk is 1MB) and **4** coding chunks are computed, for recovery (each coding chunk has the same size as the data chunk, i.e. 1MB). The raw space overhead is only 40% and the object will not be lost even if four OSDs break at the same time.

- [Jerasure erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-jerasure/)
- [ISA erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-isa/)
- [Locally repairable erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/)
- [SHEC erasure code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/)
- [CLAY code plugin](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/)

## osd erasure-code-profile set[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/#osd-erasure-code-profile-set)

To create a new erasure code profile:

```
ceph osd erasure-code-profile set {name} \
     [{directory=directory}] \
     [{plugin=plugin}] \
     [{stripe_unit=stripe_unit}] \
     [{key=value} ...] \
     [--force]
```

Where:

```
{directory=directory}
```

- Description

  Set the **directory** name from which the erasure code plugin is loaded.

- Type

  String

- Required

  No.

- Default

  /usr/lib/ceph/erasure-code

```
{plugin=plugin}
```

- Description

  Use the erasure code **plugin** to compute coding chunks and recover missing chunks. See the [list of available plugins](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/#list-of-available-plugins) for more information.

- Type

  String

- Required

  No.

- Default

  jerasure

```
{stripe_unit=stripe_unit}
```

- Description

  The amount of data in a data chunk, per stripe. For example, a profile with 2 data chunks and stripe_unit=4K would put the range 0-4K in chunk 0, 4K-8K in chunk 1, then 8K-12K in chunk 0 again. This should be a multiple of 4K for best performance. The default value is taken from the monitor config option `osd_pool_erasure_code_stripe_unit` when a pool is created.  The stripe_width of a pool using this profile will be the number of data chunks multiplied by this stripe_unit.

- Type

  String

- Required

  No.

```
{key=value}
```

- Description

  The semantic of the remaining key/value pairs is defined by the erasure code plugin.

- Type

  String

- Required

  No.

```
--force
```

- Description

  Override an existing profile by the same name, and allow setting a non-4K-aligned stripe_unit.

- Type

  String

- Required

  No.

## osd erasure-code-profile rm[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/#osd-erasure-code-profile-rm)

To remove an erasure code profile:

```
ceph osd erasure-code-profile rm {name}
```

If the profile is referenced by a pool, the deletion will fail.

## osd erasure-code-profile get[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/#osd-erasure-code-profile-get)

To display an erasure code profile:

```
ceph osd erasure-code-profile get {name}
```

## osd erasure-code-profile ls[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile/#osd-erasure-code-profile-ls)

To list the names of all erasure code profiles:

```
ceph osd erasure-code-profile ls
```

# Jerasure erasure code plugin[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-jerasure/#jerasure-erasure-code-plugin)

The *jerasure* plugin is the most generic and flexible plugin, it is also the default for Ceph erasure coded pools.

The *jerasure* plugin encapsulates the [Jerasure](http://jerasure.org) library. It is recommended to read the *jerasure* documentation to get a better understanding of the parameters.

## Create a jerasure profile[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-jerasure/#create-a-jerasure-profile)

To create a new *jerasure* erasure code profile:

```
ceph osd erasure-code-profile set {name} \
     plugin=jerasure \
     k={data-chunks} \
     m={coding-chunks} \
     technique={reed_sol_van|reed_sol_r6_op|cauchy_orig|cauchy_good|liberation|blaum_roth|liber8tion} \
     [crush-root={root}] \
     [crush-failure-domain={bucket-type}] \
     [crush-device-class={device-class}] \
     [directory={directory}] \
     [--force]
```

Where:

```
k={data chunks}
```

- Description

  Each object is split in **data-chunks** parts, each stored on a different OSD.

- Type

  Integer

- Required

  Yes.

- Example

  4

```
m={coding-chunks}
```

- Description

  Compute **coding chunks** for each object and store them on different OSDs. The number of coding chunks is also the number of OSDs that can be down without losing data.

- Type

  Integer

- Required

  Yes.

- Example

  2

```
technique={reed_sol_van|reed_sol_r6_op|cauchy_orig|cauchy_good|liberation|blaum_roth|liber8tion}
```

- Description

  The more flexible technique is *reed_sol_van* : it is enough to set *k* and *m*. The *cauchy_good* technique can be faster but you need to chose the *packetsize* carefully. All of *reed_sol_r6_op*, *liberation*, *blaum_roth*, *liber8tion* are *RAID6* equivalents in the sense that they can only be configured with *m=2*.

- Type

  String

- Required

  No.

- Default

  reed_sol_van

```
packetsize={bytes}
```

- Description

  The encoding will be done on packets of *bytes* size at a time. Choosing the right packet size is difficult. The *jerasure* documentation contains extensive information on this topic.

- Type

  Integer

- Required

  No.

- Default

  2048

```
crush-root={root}
```

- Description

  The name of the crush bucket used for the first step of the CRUSH rule. For instance **step take default**.

- Type

  String

- Required

  No.

- Default

  default

```
crush-failure-domain={bucket-type}
```

- Description

  Ensure that no two chunks are in a bucket with the same failure domain. For instance, if the failure domain is **host** no two chunks will be stored on the same host. It is used to create a CRUSH rule step such as **step chooseleaf host**.

- Type

  String

- Required

  No.

- Default

  host

```
crush-device-class={device-class}
```

- Description

  Restrict placement to devices of a specific class (e.g., `ssd` or `hdd`), using the crush device class names in the CRUSH map.

- Type

  String

- Required

  No.

```
directory={directory}
```

- Description

  Set the **directory** name from which the erasure code plugin is loaded.

- Type

  String

- Required

  No.

- Default

  /usr/lib/ceph/erasure-code

```
--force
```

- Description

  Override an existing profile by the same name.

- Type

  String

- Required

  No.

# ISA erasure code plugin[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-isa/#isa-erasure-code-plugin)

The *isa* plugin encapsulates the [ISA](https://01.org/intel®-storage-acceleration-library-open-source-version/) library.

## Create an isa profile[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-isa/#create-an-isa-profile)

To create a new *isa* erasure code profile:

```
ceph osd erasure-code-profile set {name} \
     plugin=isa \
     technique={reed_sol_van|cauchy} \
     [k={data-chunks}] \
     [m={coding-chunks}] \
     [crush-root={root}] \
     [crush-failure-domain={bucket-type}] \
     [crush-device-class={device-class}] \
     [directory={directory}] \
     [--force]
```

Where:

```
k={data chunks}
```

- Description

  Each object is split in **data-chunks** parts, each stored on a different OSD.

- Type

  Integer

- Required

  No.

- Default

  7

```
m={coding-chunks}
```

- Description

  Compute **coding chunks** for each object and store them on different OSDs. The number of coding chunks is also the number of OSDs that can be down without losing data.

- Type

  Integer

- Required

  No.

- Default

  3

```
technique={reed_sol_van|cauchy}
```

- Description

  The ISA plugin comes in two [Reed Solomon](https://en.wikipedia.org/wiki/Reed–Solomon_error_correction) forms. If *reed_sol_van* is set, it is [Vandermonde](https://en.wikipedia.org/wiki/Vandermonde_matrix), if *cauchy* is set, it is [Cauchy](https://en.wikipedia.org/wiki/Cauchy_matrix).

- Type

  String

- Required

  No.

- Default

  reed_sol_van

```
crush-root={root}
```

- Description

  The name of the crush bucket used for the first step of the CRUSH rule. For instance **step take default**.

- Type

  String

- Required

  No.

- Default

  default

```
crush-failure-domain={bucket-type}
```

- Description

  Ensure that no two chunks are in a bucket with the same failure domain. For instance, if the failure domain is **host** no two chunks will be stored on the same host. It is used to create a CRUSH rule step such as **step chooseleaf host**.

- Type

  String

- Required

  No.

- Default

  host

```
crush-device-class={device-class}
```

- Description

  Restrict placement to devices of a specific class (e.g., `ssd` or `hdd`), using the crush device class names in the CRUSH map.

- Type

  String

- Required

  No.

- Default

  

```
directory={directory}
```

- Description

  Set the **directory** name from which the erasure code plugin is loaded.

- Type

  String

- Required

  No.

- Default

  /usr/lib/ceph/erasure-code

```
--force
```

- Description

  Override an existing profile by the same name.

- Type

  String

- Required

  No.

# Locally repairable erasure code plugin[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#locally-repairable-erasure-code-plugin)

With the *jerasure* plugin, when an erasure coded object is stored on multiple OSDs, recovering from the loss of one OSD requires reading from *k* others. For instance if *jerasure* is configured with *k=8* and *m=4*, recovering from the loss of one OSD requires reading from eight others.

The *lrc* erasure code plugin creates local parity chunks to enable recovery using fewer surviving OSDs. For instance if *lrc* is configured with *k=8*, *m=4* and *l=4*, it will create an additional parity chunk for every four OSDs. When a single OSD is lost, it can be recovered with only four OSDs instead of eight.

## Erasure code profile examples[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#erasure-code-profile-examples)

### Reduce recovery bandwidth between hosts[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#reduce-recovery-bandwidth-between-hosts)

Although it is probably not an interesting use case when all hosts are connected to the same switch, reduced bandwidth usage can actually be observed.:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     k=4 m=2 l=3 \
     crush-failure-domain=host
$ ceph osd pool create lrcpool erasure LRCprofile
```

### Reduce recovery bandwidth between racks[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#reduce-recovery-bandwidth-between-racks)

In Firefly the bandwidth reduction will only be observed if the primary OSD is in the same rack as the lost chunk.:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     k=4 m=2 l=3 \
     crush-locality=rack \
     crush-failure-domain=host
$ ceph osd pool create lrcpool erasure LRCprofile
```

## Create an lrc profile[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#create-an-lrc-profile)

To create a new lrc erasure code profile:

```
ceph osd erasure-code-profile set {name} \
     plugin=lrc \
     k={data-chunks} \
     m={coding-chunks} \
     l={locality} \
     [crush-root={root}] \
     [crush-locality={bucket-type}] \
     [crush-failure-domain={bucket-type}] \
     [crush-device-class={device-class}] \
     [directory={directory}] \
     [--force]
```

Where:

```
k={data chunks}
```

- Description

  Each object is split in **data-chunks** parts, each stored on a different OSD.

- Type

  Integer

- Required

  Yes.

- Example

  4

```
m={coding-chunks}
```

- Description

  Compute **coding chunks** for each object and store them on different OSDs. The number of coding chunks is also the number of OSDs that can be down without losing data.

- Type

  Integer

- Required

  Yes.

- Example

  2

```
l={locality}
```

- Description

  Group the coding and data chunks into sets of size **locality**. For instance, for **k=4** and **m=2**, when **locality=3** two groups of three are created. Each set can be recovered without reading chunks from another set.

- Type

  Integer

- Required

  Yes.

- Example

  3

```
crush-root={root}
```

- Description

  The name of the crush bucket used for the first step of the CRUSH rule. For instance **step take default**.

- Type

  String

- Required

  No.

- Default

  default

```
crush-locality={bucket-type}
```

- Description

  The type of the CRUSH bucket in which each set of chunks defined by **l** will be stored. For instance, if it is set to **rack**, each group of **l** chunks will be placed in a different rack. It is used to create a CRUSH rule step such as **step choose rack**. If it is not set, no such grouping is done.

- Type

  String

- Required

  No.

```
crush-failure-domain={bucket-type}
```

- Description

  Ensure that no two chunks are in a bucket with the same failure domain. For instance, if the failure domain is **host** no two chunks will be stored on the same host. It is used to create a CRUSH rule step such as **step chooseleaf host**.

- Type

  String

- Required

  No.

- Default

  host

```
crush-device-class={device-class}
```

- Description

  Restrict placement to devices of a specific class (e.g., `ssd` or `hdd`), using the crush device class names in the CRUSH map.

- Type

  String

- Required

  No.

- Default

  

```
directory={directory}
```

- Description

  Set the **directory** name from which the erasure code plugin is loaded.

- Type

  String

- Required

  No.

- Default

  /usr/lib/ceph/erasure-code

```
--force
```

- Description

  Override an existing profile by the same name.

- Type

  String

- Required

  No.

## Low level plugin configuration[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#low-level-plugin-configuration)

The sum of **k** and **m** must be a multiple of the **l** parameter. The low level configuration parameters however do not enforce this restriction and it may be advantageous to use them for specific purposes. It is for instance possible to define two groups, one with 4 chunks and another with 3 chunks. It is also possible to recursively define locality sets, for instance datacenters and racks into datacenters. The **k/m/l** are implemented by generating a low level configuration.

The *lrc* erasure code plugin recursively applies erasure code techniques so that recovering from the loss of some chunks only requires a subset of the available chunks, most of the time.

For instance, when three coding steps are described as:

```
chunk nr    01234567
step 1      _cDD_cDD
step 2      cDDD____
step 3      ____cDDD
```

where *c* are coding chunks calculated from the data chunks *D*, the loss of chunk *7* can be recovered with the last four chunks. And the loss of chunk *2* chunk can be recovered with the first four chunks.

## Erasure code profile examples using low level configuration[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#erasure-code-profile-examples-using-low-level-configuration)

### Minimal testing[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#minimal-testing)

It is strictly equivalent to using a *K=2* *M=1* erasure code profile. The *DD* implies *K=2*, the *c* implies *M=1* and the *jerasure* plugin is used by default.:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     mapping=DD_ \
     layers='[ [ "DDc", "" ] ]'
$ ceph osd pool create lrcpool erasure LRCprofile
```

### Reduce recovery bandwidth between hosts[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#id1)

Although it is probably not an interesting use case when all hosts are connected to the same switch, reduced bandwidth usage can actually be observed. It is equivalent to **k=4**, **m=2** and **l=3** although the layout of the chunks is different:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     mapping=__DD__DD \
     layers='[
               [ "_cDD_cDD", "" ],
               [ "cDDD____", "" ],
               [ "____cDDD", "" ],
             ]'
$ ceph osd pool create lrcpool erasure LRCprofile
```

### Reduce recovery bandwidth between racks[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#id2)

In Firefly the reduced bandwidth will only be observed if the primary OSD is in the same rack as the lost chunk.:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     mapping=__DD__DD \
     layers='[
               [ "_cDD_cDD", "" ],
               [ "cDDD____", "" ],
               [ "____cDDD", "" ],
             ]' \
     crush-steps='[
                     [ "choose", "rack", 2 ],
                     [ "chooseleaf", "host", 4 ],
                    ]'
$ ceph osd pool create lrcpool erasure LRCprofile
```

### Testing with different Erasure Code backends[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#testing-with-different-erasure-code-backends)

LRC now uses jerasure as the default EC backend. It is possible to specify the EC backend/algorithm on a per layer basis using the low level configuration. The second argument in layers=’[ [ “DDc”, “” ] ]’ is actually an erasure code profile to be used for this level. The example below specifies the ISA backend with the cauchy technique to be used in the lrcpool.:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     mapping=DD_ \
     layers='[ [ "DDc", "plugin=isa technique=cauchy" ] ]'
$ ceph osd pool create lrcpool erasure LRCprofile
```

You could also use a different erasure code profile for for each layer.:

```
$ ceph osd erasure-code-profile set LRCprofile \
     plugin=lrc \
     mapping=__DD__DD \
     layers='[
               [ "_cDD_cDD", "plugin=isa technique=cauchy" ],
               [ "cDDD____", "plugin=isa" ],
               [ "____cDDD", "plugin=jerasure" ],
             ]'
$ ceph osd pool create lrcpool erasure LRCprofile
```

## Erasure coding and decoding algorithm[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#erasure-coding-and-decoding-algorithm)

The steps found in the layers description:

```
chunk nr    01234567

step 1      _cDD_cDD
step 2      cDDD____
step 3      ____cDDD
```

are applied in order. For instance, if a 4K object is encoded, it will first go through *step 1* and be divided in four 1K chunks (the four uppercase D). They are stored in the chunks 2, 3, 6 and 7, in order. From these, two coding chunks are calculated (the two lowercase c). The coding chunks are stored in the chunks 1 and 5, respectively.

The *step 2* re-uses the content created by *step 1* in a similar fashion and stores a single coding chunk *c* at position 0. The last four chunks, marked with an underscore (*_*) for readability, are ignored.

The *step 3* stores a single coding chunk *c* at position 4. The three chunks created by *step 1* are used to compute this coding chunk, i.e. the coding chunk from *step 1* becomes a data chunk in *step 3*.

If chunk *2* is lost:

```
chunk nr    01234567

step 1      _c D_cDD
step 2      cD D____
step 3      __ _cDDD
```

decoding will attempt to recover it by walking the steps in reverse order: *step 3* then *step 2* and finally *step 1*.

The *step 3* knows nothing about chunk *2* (i.e. it is an underscore) and is skipped.

The coding chunk from *step 2*, stored in chunk *0*, allows it to recover the content of chunk *2*. There are no more chunks to recover and the process stops, without considering *step 1*.

Recovering chunk *2* requires reading chunks *0, 1, 3* and writing back chunk *2*.

If chunk *2, 3, 6* are lost:

```
chunk nr    01234567

step 1      _c  _c D
step 2      cD  __ _
step 3      __  cD D
```

The *step 3* can recover the content of chunk *6*:

```
chunk nr    01234567

step 1      _c  _cDD
step 2      cD  ____
step 3      __  cDDD
```

The *step 2* fails to recover and is skipped because there are two chunks missing (*2, 3*) and it can only recover from one missing chunk.

The coding chunk from *step 1*, stored in chunk *1, 5*, allows it to recover the content of chunk *2, 3*:

```
chunk nr    01234567

step 1      _cDD_cDD
step 2      cDDD____
step 3      ____cDDD
```

## Controlling CRUSH placement[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-lrc/#controlling-crush-placement)

The default CRUSH rule provides OSDs that are on different hosts. For instance:

```
chunk nr    01234567

step 1      _cDD_cDD
step 2      cDDD____
step 3      ____cDDD
```

needs exactly *8* OSDs, one for each chunk. If the hosts are in two adjacent racks, the first four chunks can be placed in the first rack and the last four in the second rack. So that recovering from the loss of a single OSD does not require using bandwidth between the two racks.

For instance:

```
crush-steps='[ [ "choose", "rack", 2 ], [ "chooseleaf", "host", 4 ] ]'
```

will create a rule that will select two crush buckets of type *rack* and for each of them choose four OSDs, each of them located in different buckets of type *host*.

The CRUSH rule can also be manually crafted for finer control.

# SHEC erasure code plugin[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#shec-erasure-code-plugin)

The *shec* plugin encapsulates the [multiple SHEC](http://tracker.ceph.com/projects/ceph/wiki/Shingled_Erasure_Code_(SHEC)) library. It allows ceph to recover data more efficiently than Reed Solomon codes.

## Create an SHEC profile[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#create-an-shec-profile)

To create a new *shec* erasure code profile:

```
ceph osd erasure-code-profile set {name} \
     plugin=shec \
     [k={data-chunks}] \
     [m={coding-chunks}] \
     [c={durability-estimator}] \
     [crush-root={root}] \
     [crush-failure-domain={bucket-type}] \
     [crush-device-class={device-class}] \
     [directory={directory}] \
     [--force]
```

Where:

```
k={data-chunks}
```

- Description

  Each object is split in **data-chunks** parts, each stored on a different OSD.

- Type

  Integer

- Required

  No.

- Default

  4

```
m={coding-chunks}
```

- Description

  Compute **coding-chunks** for each object and store them on different OSDs. The number of **coding-chunks** does not necessarily equal the number of OSDs that can be down without losing data.

- Type

  Integer

- Required

  No.

- Default

  3

```
c={durability-estimator}
```

- Description

  The number of parity chunks each of which includes each data chunk in its calculation range. The number is used as a **durability estimator**. For instance, if c=2, 2 OSDs can be down without losing data.

- Type

  Integer

- Required

  No.

- Default

  2

```
crush-root={root}
```

- Description

  The name of the crush bucket used for the first step of the CRUSH rule. For instance **step take default**.

- Type

  String

- Required

  No.

- Default

  default

```
crush-failure-domain={bucket-type}
```

- Description

  Ensure that no two chunks are in a bucket with the same failure domain. For instance, if the failure domain is **host** no two chunks will be stored on the same host. It is used to create a CRUSH rule step such as **step chooseleaf host**.

- Type

  String

- Required

  No.

- Default

  host

```
crush-device-class={device-class}
```

- Description

  Restrict placement to devices of a specific class (e.g., `ssd` or `hdd`), using the crush device class names in the CRUSH map.

- Type

  String

- Required

  No.

- Default

  

```
directory={directory}
```

- Description

  Set the **directory** name from which the erasure code plugin is loaded.

- Type

  String

- Required

  No.

- Default

  /usr/lib/ceph/erasure-code

```
--force
```

- Description

  Override an existing profile by the same name.

- Type

  String

- Required

  No.

## Brief description of SHEC’s layouts[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#brief-description-of-shec-s-layouts)

### Space Efficiency[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#space-efficiency)

Space efficiency is a ratio of data chunks to all ones in a object and represented as k/(k+m). In order to improve space efficiency, you should increase k or decrease m:

> space efficiency of SHEC(4,3,2) = 

>  = 0.57 SHEC(5,3,2) or SHEC(4,2,2) improves SHEC(4,3,2)’s space efficiency

### Durability[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#durability)

The third parameter of SHEC (=c) is a durability estimator, which approximates the number of OSDs that can be down without losing data.

```
durability estimator of SHEC(4,3,2) = 2
```

### Recovery Efficiency[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#recovery-efficiency)

Describing calculation of recovery efficiency is beyond the scope of this document, but at least increasing m without increasing c achieves improvement of recovery efficiency. (However, we must pay attention to the sacrifice of space efficiency in this case.)

```
SHEC(4,2,2) -> SHEC(4,3,2) : achieves improvement of recovery efficiency
```

## Erasure code profile examples[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-shec/#erasure-code-profile-examples)

```
$ ceph osd erasure-code-profile set SHECprofile \
     plugin=shec \
     k=8 m=4 c=3 \
     crush-failure-domain=host
$ ceph osd pool create shecpool erasure SHECprofile
```

# CLAY code plugin[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#clay-code-plugin)

CLAY (short for coupled-layer) codes are erasure codes designed to bring about significant savings in terms of network bandwidth and disk IO when a failed node/OSD/rack is being repaired. Let:

> d = number of OSDs contacted during repair

If *jerasure* is configured with *k=8* and *m=4*, losing one OSD requires reading from the *d=8* others to repair. And recovery of say a 1GiB needs a download of 8 X 1GiB = 8GiB of information.

However, in the case of the *clay* plugin *d* is configurable within the limits:

> k+1 <= d <= k+m-1

By default, the clay code plugin picks *d=k+m-1* as it provides the greatest savings in terms of network bandwidth and disk IO. In the case of the *clay* plugin configured with *k=8*, *m=4* and *d=11* when a single OSD fails, d=11 osds are contacted and 250MiB is downloaded from each of them, resulting in a total download of 11 X 250MiB = 2.75GiB amount of information. More general parameters are provided below. The benefits are substantial when the repair is carried out for a rack that stores information on the order of Terabytes.

> | plugin       | total amount of disk IO |
> | ------------ | ----------------------- |
> | jerasure,isa |                         |

| clay |      |
| ---- | ---- |
|      |      |

> 

where *S* is the amount of data stored on a single OSD undergoing repair. In the table above, we have used the largest possible value of *d* as this will result in the smallest amount of data download needed to achieve recovery from an OSD failure.

## Erasure-code profile examples[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#erasure-code-profile-examples)

An example configuration that can be used to observe reduced bandwidth usage:

```
$ ceph osd erasure-code-profile set CLAYprofile \
     plugin=clay \
     k=4 m=2 d=5 \
     crush-failure-domain=host
$ ceph osd pool create claypool erasure CLAYprofile
```

## Creating a clay profile[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#creating-a-clay-profile)

To create a new clay code profile:

```
ceph osd erasure-code-profile set {name} \
     plugin=clay \
     k={data-chunks} \
     m={coding-chunks} \
     [d={helper-chunks}] \
     [scalar_mds={plugin-name}] \
     [technique={technique-name}] \
     [crush-failure-domain={bucket-type}] \
     [crush-device-class={device-class}] \
     [directory={directory}] \
     [--force]
```

Where:

```
k={data chunks}
```

- Description

  Each object is split into **data-chunks** parts, each of which is stored on a different OSD.

- Type

  Integer

- Required

  Yes.

- Example

  4

```
m={coding-chunks}
```

- Description

  Compute **coding chunks** for each object and store them on different OSDs. The number of coding chunks is also the number of OSDs that can be down without losing data.

- Type

  Integer

- Required

  Yes.

- Example

  2

```
d={helper-chunks}
```

- Description

  Number of OSDs requested to send data during recovery of a single chunk. *d* needs to be chosen such that k+1 <= d <= k+m-1. The larger the *d*, the better the savings.

- Type

  Integer

- Required

  No.

- Default

  k+m-1

```
scalar_mds={jerasure|isa|shec}
```

- Description

  **scalar_mds** specifies the plugin that is used as a building block in the layered construction. It can be one of *jerasure*, *isa*, *shec*

- Type

  String

- Required

  No.

- Default

  jerasure

```
technique={technique}
```

- Description

  **technique** specifies the technique that will be picked within the ‘scalar_mds’ plugin specified. Supported techniques are ‘reed_sol_van’, ‘reed_sol_r6_op’, ‘cauchy_orig’, ‘cauchy_good’, ‘liber8tion’ for jerasure, ‘reed_sol_van’, ‘cauchy’ for isa and ‘single’, ‘multiple’ for shec.

- Type

  String

- Required

  No.

- Default

  reed_sol_van (for jerasure, isa), single (for shec)

```
crush-root={root}
```

- Description

  The name of the crush bucket used for the first step of the CRUSH rule. For instance **step take default**.

- Type

  String

- Required

  No.

- Default

  default

```
crush-failure-domain={bucket-type}
```

- Description

  Ensure that no two chunks are in a bucket with the same failure domain. For instance, if the failure domain is **host** no two chunks will be stored on the same host. It is used to create a CRUSH rule step such as **step chooseleaf host**.

- Type

  String

- Required

  No.

- Default

  host

```
crush-device-class={device-class}
```

- Description

  Restrict placement to devices of a specific class (e.g., `ssd` or `hdd`), using the crush device class names in the CRUSH map.

- Type

  String

- Required

  No.

- Default

  

```
directory={directory}
```

- Description

  Set the **directory** name from which the erasure code plugin is loaded.

- Type

  String

- Required

  No.

- Default

  /usr/lib/ceph/erasure-code

```
--force
```

- Description

  Override an existing profile by the same name.

- Type

  String

- Required

  No.

## Notion of sub-chunks[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#notion-of-sub-chunks)

The Clay code is able to save in terms of disk IO, network bandwidth as it is a vector code and it is able to view and manipulate data within a chunk at a finer granularity termed as a sub-chunk. The number of sub-chunks within a chunk for a Clay code is given by:

> sub-chunk count = 

, where 

During repair of an OSD, the helper information requested from an available OSD is only a fraction of a chunk. In fact, the number of sub-chunks within a chunk that are accessed during repair is given by:

> repair sub-chunk count = 

### Examples[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#examples)

1. For a configuration with *k=4*, *m=2*, *d=5*, the sub-chunk count is 8 and  the repair sub-chunk count is 4. Therefore, only half of a chunk is read during repair.
2. When *k=8*, *m=4*, *d=11* the sub-chunk count is 64 and repair sub-chunk count is 16. A quarter of a chunk is read from an available OSD for repair of a failed chunk.

## How to choose a configuration given a workload[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#how-to-choose-a-configuration-given-a-workload)

Only a few sub-chunks are read of all the sub-chunks within a chunk. These sub-chunks are not necessarily stored consecutively within a chunk. For best disk IO performance, it is helpful to read contiguous data. For this reason, it is suggested that you choose stripe-size such that the sub-chunk size is sufficiently large.

For a given stripe-size (that’s fixed based on a workload), choose `k`, `m`, `d` such that:

> sub-chunk size = 

>  = 4KB, 8KB, 12KB …

1. For large size workloads for which the stripe size is large, it is easy to choose k, m, d. For example consider a stripe-size of size 64MB, choosing *k=16*, *m=4* and *d=19* will result in a sub-chunk count of 1024 and a sub-chunk size of 4KB.
2. For small size workloads, *k=4*, *m=2* is a good configuration that provides both network and disk IO benefits.

## Comparisons with LRC[](https://docs.ceph.com/en/latest/rados/operations/erasure-code-clay/#comparisons-with-lrc)

Locally Recoverable Codes (LRC) are also designed in order to save in terms of network bandwidth, disk IO during single OSD recovery. However, the focus in LRCs is to keep the number of OSDs contacted during repair (d) to be minimal, but this comes at the cost of storage overhead. The *clay* code has a storage overhead m/k. In the case of an *lrc*, it stores (k+m)/d parities in addition to the `m` parities resulting in a storage overhead (m+(k+m)/d)/k. Both *clay* and *lrc* can recover from the failure of any `m` OSDs.

> | Parameters  | disk IO, storage overhead (LRC) | disk IO, storage overhead (CLAY) |
> | ----------- | ------------------------------- | -------------------------------- |
> | (k=10, m=4) | 7 * S, 0.6 (d=7)                | 3.25 * S, 0.4 (d=13)             |
> | (k=16, m=4) | 4 * S, 0.5625 (d=4)             | 4.75 * S, 0.25 (d=19)            |

where `S` is the amount of data stored of single OSD being recovered.

# Cache Tiering[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#cache-tiering)

A cache tier provides Ceph Clients with better I/O performance for a subset of the data stored in a backing storage tier. Cache tiering involves creating a pool of relatively fast/expensive storage devices (e.g., solid state drives) configured to act as a cache tier, and a backing pool of either erasure-coded or relatively slower/cheaper devices configured to act as an economical storage tier. The Ceph objecter handles where to place the objects and the tiering agent determines when to flush objects from the cache to the backing storage tier. So the cache tier and the backing storage tier are completely transparent to Ceph clients.

![img](https://docs.ceph.com/en/latest/_images/ditaa-644de96be5ceeacfe47c2ad4fd6748a1bc13f928.png)

The cache tiering agent handles the migration of data between the cache tier and the backing storage tier automatically. However, admins have the ability to configure how this migration takes place by setting the `cache-mode`. There are two main scenarios:

- **writeback** mode: When admins configure tiers with `writeback` mode, Ceph clients write data to the cache tier and receive an ACK from the cache tier. In time, the data written to the cache tier migrates to the storage tier and gets flushed from the cache tier. Conceptually, the cache tier is overlaid “in front” of the backing storage tier. When a Ceph client needs data that resides in the storage tier, the cache tiering agent migrates the data to the cache tier on read, then it is sent to the Ceph client. Thereafter, the Ceph client can perform I/O using the cache tier, until the data becomes inactive. This is ideal for mutable data (e.g., photo/video editing, transactional data, etc.).
- **readproxy** mode: This mode will use any objects that already exist in the cache tier, but if an object is not present in the cache the request will be proxied to the base tier.  This is useful for transitioning from `writeback` mode to a disabled cache as it allows the workload to function properly while the cache is drained, without adding any new objects to the cache.

Other cache modes are:

- **readonly** promotes objects to the cache on read operations only; write operations are forwarded to the base tier. This mode is intended for read-only workloads that do not require consistency to be enforced by the storage system. (**Warning**: when objects are updated in the base tier, Ceph makes **no** attempt to sync these updates to the corresponding objects in the cache. Since this mode is considered experimental, a `--yes-i-really-mean-it` option must be passed in order to enable it.)
- **none** is used to completely disable caching.

## A word of caution[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#a-word-of-caution)

Cache tiering will *degrade* performance for most workloads.  Users should use extreme caution before using this feature.

- *Workload dependent*: Whether a cache will improve performance is highly dependent on the workload.  Because there is a cost associated with moving objects into or out of the cache, it can only be effective when there is a *large skew* in the access pattern in the data set, such that most of the requests touch a small number of objects.  The cache pool should be large enough to capture the working set for your workload to avoid thrashing.
- *Difficult to benchmark*: Most benchmarks that users run to measure performance will show terrible performance with cache tiering, in part because very few of them skew requests toward a small set of objects, it can take a long time for the cache to “warm up,” and because the warm-up cost can be high.
- *Usually slower*: For workloads that are not cache tiering-friendly, performance is often slower than a normal RADOS pool without cache tiering enabled.
- *librados object enumeration*: The librados-level object enumeration API is not meant to be coherent in the presence of the case.  If your application is using librados directly and relies on object enumeration, cache tiering will probably not work as expected. (This is not a problem for RGW, RBD, or CephFS.)
- *Complexity*: Enabling cache tiering means that a lot of additional machinery and complexity within the RADOS cluster is being used. This increases the probability that you will encounter a bug in the system that other users have not yet encountered and will put your deployment at a higher level of risk.

### Known Good Workloads[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#known-good-workloads)

- *RGW time-skewed*: If the RGW workload is such that almost all read operations are directed at recently written objects, a simple cache tiering configuration that destages recently written objects from the cache to the base tier after a configurable period can work well.

### Known Bad Workloads[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#known-bad-workloads)

The following configurations are *known to work poorly* with cache tiering.

- *RBD with replicated cache and erasure-coded base*: This is a common request, but usually does not perform well.  Even reasonably skewed workloads still send some small writes to cold objects, and because small writes are not yet supported by the erasure-coded pool, entire (usually 4 MB) objects must be migrated into the cache in order to satisfy a small (often 4 KB) write.  Only a handful of users have successfully deployed this configuration, and it only works for them because their data is extremely cold (backups) and they are not in any way sensitive to performance.
- *RBD with replicated cache and base*: RBD with a replicated base tier does better than when the base is erasure coded, but it is still highly dependent on the amount of skew in the workload, and very difficult to validate.  The user will need to have a good understanding of their workload and will need to tune the cache tiering parameters carefully.

## Setting Up Pools[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#setting-up-pools)

To set up cache tiering, you must have two pools. One will act as the backing storage and the other will act as the cache.

### Setting Up a Backing Storage Pool[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#setting-up-a-backing-storage-pool)

Setting up a backing storage pool typically involves one of two scenarios:

- **Standard Storage**: In this scenario, the pool stores multiple copies of an object in the Ceph Storage Cluster.
- **Erasure Coding:** In this scenario, the pool uses erasure coding to store data much more efficiently with a small performance tradeoff.

In the standard storage scenario, you can setup a CRUSH rule to establish the failure domain (e.g., osd, host, chassis, rack, row, etc.). Ceph OSD Daemons perform optimally when all storage drives in the rule are of the same size, speed (both RPMs and throughput) and type. See [CRUSH Maps](https://docs.ceph.com/en/latest/rados/operations/crush-map) for details on creating a rule. Once you have created a rule, create a backing storage pool.

In the erasure coding scenario, the pool creation arguments will generate the appropriate rule automatically. See [Create a Pool](https://docs.ceph.com/en/latest/rados/operations/pools#create-a-pool) for details.

In subsequent examples, we will refer to the backing storage pool as `cold-storage`.

### Setting Up a Cache Pool[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#setting-up-a-cache-pool)

Setting up a cache pool follows the same procedure as the standard storage scenario, but with this difference: the drives for the cache tier are typically high performance drives that reside in their own servers and have their own CRUSH rule.  When setting up such a rule, it should take account of the hosts that have the high performance drives while omitting the hosts that don’t. See [CRUSH Device Class](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crush-map-device-class) for details.

In subsequent examples, we will refer to the cache pool as `hot-storage` and the backing pool as `cold-storage`.

For cache tier configuration and default values, see [Pools - Set Pool Values](https://docs.ceph.com/en/latest/rados/operations/pools#set-pool-values).

## Creating a Cache Tier[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#creating-a-cache-tier)

Setting up a cache tier involves associating a backing storage pool with a cache pool

```
ceph osd tier add {storagepool} {cachepool}
```

For example

```
ceph osd tier add cold-storage hot-storage
```

To set the cache mode, execute the following:

```
ceph osd tier cache-mode {cachepool} {cache-mode}
```

For example:

```
ceph osd tier cache-mode hot-storage writeback
```

The cache tiers overlay the backing storage tier, so they require one additional step: you must direct all client traffic from the storage pool to the cache pool. To direct client traffic directly to the cache pool, execute the following:

```
ceph osd tier set-overlay {storagepool} {cachepool}
```

For example:

```
ceph osd tier set-overlay cold-storage hot-storage
```

## Configuring a Cache Tier[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#configuring-a-cache-tier)

Cache tiers have several configuration options. You may set cache tier configuration options with the following usage:

```
ceph osd pool set {cachepool} {key} {value}
```

See [Pools - Set Pool Values](https://docs.ceph.com/en/latest/rados/operations/pools#set-pool-values) for details.

### Target Size and Type[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#target-size-and-type)

Ceph’s production cache tiers use a [Bloom Filter](https://en.wikipedia.org/wiki/Bloom_filter) for the `hit_set_type`:

```
ceph osd pool set {cachepool} hit_set_type bloom
```

For example:

```
ceph osd pool set hot-storage hit_set_type bloom
```

The `hit_set_count` and `hit_set_period` define how many such HitSets to store, and how much time each HitSet should cover.

```
ceph osd pool set {cachepool} hit_set_count 12
ceph osd pool set {cachepool} hit_set_period 14400
ceph osd pool set {cachepool} target_max_bytes 1000000000000
```

Note

A larger `hit_set_count` results in more RAM consumed by the `ceph-osd` process.

Binning accesses over time allows Ceph to determine whether a Ceph client accessed an object at least once, or more than once over a time period (“age” vs “temperature”).

The `min_read_recency_for_promote` defines how many HitSets to check for the existence of an object when handling a read operation. The checking result is used to decide whether to promote the object asynchronously. Its value should be between 0 and `hit_set_count`. If it’s set to 0, the object is always promoted. If it’s set to 1, the current HitSet is checked. And if this object is in the current HitSet, it’s promoted. Otherwise not. For the other values, the exact number of archive HitSets are checked. The object is promoted if the object is found in any of the most recent `min_read_recency_for_promote` HitSets.

A similar parameter can be set for the write operation, which is `min_write_recency_for_promote`.

```
ceph osd pool set {cachepool} min_read_recency_for_promote 2
ceph osd pool set {cachepool} min_write_recency_for_promote 2
```

Note

The longer the period and the higher the `min_read_recency_for_promote` and `min_write_recency_for_promote``values, the more RAM the ``ceph-osd` daemon consumes. In particular, when the agent is active to flush or evict cache objects, all `hit_set_count` HitSets are loaded into RAM.

### Cache Sizing[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#cache-sizing)

The cache tiering agent performs two main functions:

- **Flushing:** The agent identifies modified (or dirty) objects and forwards them to the storage pool for long-term storage.
- **Evicting:** The agent identifies objects that haven’t been modified (or clean) and evicts the least recently used among them from the cache.

#### Absolute Sizing[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#absolute-sizing)

The cache tiering agent can flush or evict objects based upon the total number of bytes or the total number of objects. To specify a maximum number of bytes, execute the following:

```
ceph osd pool set {cachepool} target_max_bytes {#bytes}
```

For example, to flush or evict at 1 TB, execute the following:

```
ceph osd pool set hot-storage target_max_bytes 1099511627776
```

To specify the maximum number of objects, execute the following:

```
ceph osd pool set {cachepool} target_max_objects {#objects}
```

For example, to flush or evict at 1M objects, execute the following:

```
ceph osd pool set hot-storage target_max_objects 1000000
```

Note

Ceph is not able to determine the size of a cache pool automatically, so the configuration on the absolute size is required here, otherwise the flush/evict will not work. If you specify both limits, the cache tiering agent will begin flushing or evicting when either threshold is triggered.

Note

All client requests will be blocked only when  `target_max_bytes` or `target_max_objects` reached

#### Relative Sizing[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#relative-sizing)

The cache tiering agent can flush or evict objects relative to the size of the cache pool(specified by `target_max_bytes` / `target_max_objects` in [Absolute sizing](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#absolute-sizing)).  When the cache pool consists of a certain percentage of modified (or dirty) objects, the cache tiering agent will flush them to the storage pool. To set the `cache_target_dirty_ratio`, execute the following:

```
ceph osd pool set {cachepool} cache_target_dirty_ratio {0.0..1.0}
```

For example, setting the value to `0.4` will begin flushing modified (dirty) objects when they reach 40% of the cache pool’s capacity:

```
ceph osd pool set hot-storage cache_target_dirty_ratio 0.4
```

When the dirty objects reaches a certain percentage of its capacity, flush dirty objects with a higher speed. To set the `cache_target_dirty_high_ratio`:

```
ceph osd pool set {cachepool} cache_target_dirty_high_ratio {0.0..1.0}
```

For example, setting the value to `0.6` will begin aggressively flush dirty objects when they reach 60% of the cache pool’s capacity. obviously, we’d better set the value between dirty_ratio and full_ratio:

```
ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6
```

When the cache pool reaches a certain percentage of its capacity, the cache tiering agent will evict objects to maintain free capacity. To set the `cache_target_full_ratio`, execute the following:

```
ceph osd pool set {cachepool} cache_target_full_ratio {0.0..1.0}
```

For example, setting the value to `0.8` will begin flushing unmodified (clean) objects when they reach 80% of the cache pool’s capacity:

```
ceph osd pool set hot-storage cache_target_full_ratio 0.8
```

### Cache Age[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#cache-age)

You can specify the minimum age of an object before the cache tiering agent flushes a recently modified (or dirty) object to the backing storage pool:

```
ceph osd pool set {cachepool} cache_min_flush_age {#seconds}
```

For example, to flush modified (or dirty) objects after 10 minutes, execute the following:

```
ceph osd pool set hot-storage cache_min_flush_age 600
```

You can specify the minimum age of an object before it will be evicted from the cache tier:

```
ceph osd pool {cache-tier} cache_min_evict_age {#seconds}
```

For example, to evict objects after 30 minutes, execute the following:

```
ceph osd pool set hot-storage cache_min_evict_age 1800
```

## Removing a Cache Tier[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#removing-a-cache-tier)

Removing a cache tier differs depending on whether it is a writeback cache or a read-only cache.

### Removing a Read-Only Cache[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#removing-a-read-only-cache)

Since a read-only cache does not have modified data, you can disable and remove it without losing any recent changes to objects in the cache.

1. Change the cache-mode to `none` to disable it.

   ```
   ceph osd tier cache-mode {cachepool} none
   ```

   For example:

   ```
   ceph osd tier cache-mode hot-storage none
   ```

2. Remove the cache pool from the backing pool.

   ```
   ceph osd tier remove {storagepool} {cachepool}
   ```

   For example:

   ```
   ceph osd tier remove cold-storage hot-storage
   ```

### Removing a Writeback Cache[](https://docs.ceph.com/en/latest/rados/operations/cache-tiering/#removing-a-writeback-cache)

Since a writeback cache may have modified data, you must take steps to ensure that you do not lose any recent changes to objects in the cache before you disable and remove it.

1. Change the cache mode to `proxy` so that new and modified objects will flush to the backing storage pool.

   ```
   ceph osd tier cache-mode {cachepool} proxy
   ```

   For example:

   ```
   ceph osd tier cache-mode hot-storage proxy
   ```

2. Ensure that the cache pool has been flushed. This may take a few minutes:

   ```
   rados -p {cachepool} ls
   ```

   If the cache pool still has objects, you can flush them manually. For example:

   ```
   rados -p {cachepool} cache-flush-evict-all
   ```

3. Remove the overlay so that clients will not direct traffic to the cache.

   ```
   ceph osd tier remove-overlay {storagetier}
   ```

   For example:

   ```
   ceph osd tier remove-overlay cold-storage
   ```

4. Finally, remove the cache tier pool from the backing storage pool.

   ```
   ceph osd tier remove {storagepool} {cachepool}
   ```

   For example:

   ```
   ceph osd tier remove cold-storage hot-storage
   ```

​        

# Placement Groups[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#placement-groups)



## Autoscaling placement groups[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#autoscaling-placement-groups)

Placement groups (PGs) are an internal implementation detail of how Ceph distributes data.  You may enable *pg-autoscaling* to allow the cluster to make recommendations or automatically adjust the numbers of PGs (`pgp_num`) for each pool based on expected cluster and pool utilization.

Each pool has a `pg_autoscale_mode` property that can be set to `off`, `on`, or `warn`.

- `off`: Disable autoscaling for this pool.  It is up to the administrator to choose an appropriate `pgp_num` for each pool.  Please refer to [Choosing the number of Placement Groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-number-of-placement-groups) for more information.
- `on`: Enable automated adjustments of the PG count for the given pool.
- `warn`: Raise health alerts when the PG count should be adjusted

To set the autoscaling mode for an existing pool:

```
ceph osd pool set <pool-name> pg_autoscale_mode <mode>
```

For example to enable autoscaling on pool `foo`:

```
ceph osd pool set foo pg_autoscale_mode on
```

You can also configure the default `pg_autoscale_mode` that is set on any pools that are subsequently created:

```
ceph config set global osd_pool_default_pg_autoscale_mode <mode>
```

You can disable or enable the autoscaler for all pools with the `noautoscale` flag. By default this flag is set to  be `off`, but you can turn it `on` by using the command:

```
ceph osd pool set noautoscale
```

You can turn it `off` using the command:

```
ceph osd pool unset noautoscale
```

To `get` the value of the flag use the command:

```
ceph osd pool get noautoscale
```

### Viewing PG scaling recommendations[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#viewing-pg-scaling-recommendations)

You can view each pool, its relative utilization, and any suggested changes to the PG count with this command:

```
ceph osd pool autoscale-status
```

Output will be something like:

```
POOL    SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO BIAS PG_NUM  NEW PG_NUM  AUTOSCALE BULK
a     12900M                3.0        82431M  0.4695                                          8         128  warn      True
c         0                 3.0        82431M  0.0000        0.2000           0.9884  1.0      1          64  warn      True
b         0        953.6M   3.0        82431M  0.0347                                          8              warn      False
```

**SIZE** is the amount of data stored in the pool. **TARGET SIZE**, if present, is the amount of data the administrator has specified that they expect to eventually be stored in this pool.  The system uses the larger of the two values for its calculation.

**RATE** is the multiplier for the pool that determines how much raw storage capacity is consumed.  For example, a 3 replica pool will have a ratio of 3.0, while a k=4,m=2 erasure coded pool will have a ratio of 1.5.

**RAW CAPACITY** is the total amount of raw storage capacity on the OSDs that are responsible for storing this pool’s (and perhaps other pools’) data.  **RATIO** is the ratio of that total capacity that this pool is consuming (i.e., ratio = size * rate / raw capacity).

**TARGET RATIO**, if present, is the ratio of storage that the administrator has specified that they expect this pool to consume relative to other pools with target ratios set. If both target size bytes and ratio are specified, the ratio takes precedence.

**EFFECTIVE RATIO** is the target ratio after adjusting in two ways:

1. Subtracting any capacity expected to be used by pools with target size set
2. Normalizing the target ratios among pools with target ratio set so they collectively target the rest of the space. For example, 4 pools with target_ratio 1.0 would have an effective ratio of 0.25.

The system uses the larger of the actual ratio and the effective ratio for its calculation.

**BIAS** is used as a multiplier to manually adjust a pool’s PG based on prior information about how much PGs a specific pool is expected to have.

**PG_NUM** is the current number of PGs for the pool (or the current number of PGs that the pool is working towards, if a `pg_num` change is in progress).  **NEW PG_NUM**, if present, is what the system believes the pool’s `pg_num` should be changed to.  It is always a power of 2, and will only be present if the “ideal” value varies from the current value by more than a factor of 3 by default. This factor can be be adjusted with:

```
ceph osd pool set threshold 2.0
```

**AUTOSCALE**, is the pool `pg_autoscale_mode` and will be either `on`, `off`, or `warn`.

The final column, **BULK** determines if the pool is `bulk` and will be either `True` or `False`. A `bulk` pool means that the pool is expected to be large and should start out with large amount of PGs for performance purposes. On the other hand, pools without the `bulk` flag are expected to be smaller e.g., .mgr or meta pools.

### Automated scaling[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#automated-scaling)

Allowing the cluster to automatically scale `pgp_num` based on usage is the simplest approach.  Ceph will look at the total available storage and target number of PGs for the whole system, look at how much data is stored in each pool, and try to apportion PGs accordingly.  The system is relatively conservative with its approach, only making changes to a pool when the current number of PGs (`pg_num`) is more than a factor of 3 off from what it thinks it should be.

The target number of PGs per OSD is based on the `mon_target_pg_per_osd` configurable (default: 100), which can be adjusted with:

```
ceph config set global mon_target_pg_per_osd 100
```

The autoscaler analyzes pools and adjusts on a per-subtree basis. Because each pool may map to a different CRUSH rule, and each rule may distribute data across different devices, Ceph will consider utilization of each subtree of the hierarchy independently.  For example, a pool that maps to OSDs of class ssd and a pool that maps to OSDs of class hdd will each have optimal PG counts that depend on the number of those respective device types.

The autoscaler uses the bulk flag to determine which pool should start out with a full complement of PGs and only scales down when the usage ratio across the pool is not even. However, if the pool doesn’t have the bulk flag, the pool will start out with minimal PGs and only when there is more usage in the pool.

The autoscaler identifies any overlapping roots and prevents the pools with such roots from scaling because overlapping roots can cause problems with the scaling process.

To create pool with bulk flag:

```
ceph osd pool create <pool-name> --bulk
```

To set/unset bulk flag of existing pool:

```
ceph osd pool set <pool-name> bulk <true/false/1/0>
```

To get bulk flag of existing pool:

```
ceph osd pool get <pool-name> bulk
```



### Specifying expected pool size[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#specifying-expected-pool-size)

When a cluster or pool is first created, it will consume a small fraction of the total cluster capacity and will appear to the system as if it should only need a small number of placement groups. However, in most cases cluster administrators have a good idea which pools are expected to consume most of the system capacity over time. By providing this information to Ceph, a more appropriate number of PGs can be used from the beginning, preventing subsequent changes in `pg_num` and the overhead associated with moving data around when those adjustments are made.

The *target size* of a pool can be specified in two ways: either in terms of the absolute size of the pool (i.e., bytes), or as a weight relative to other pools with a `target_size_ratio` set.

For example:

```
ceph osd pool set mypool target_size_bytes 100T
```

will tell the system that mypool is expected to consume 100 TiB of space.  Alternatively:

```
ceph osd pool set mypool target_size_ratio 1.0
```

will tell the system that mypool is expected to consume 1.0 relative to the other pools with `target_size_ratio` set. If mypool is the only pool in the cluster, this means an expected use of 100% of the total capacity. If there is a second pool with `target_size_ratio` 1.0, both pools would expect to use 50% of the cluster capacity.

You can also set the target size of a pool at creation time with the optional `--target-size-bytes <bytes>` or `--target-size-ratio <ratio>` arguments to the `ceph osd pool create` command.

Note that if impossible target size values are specified (for example, a capacity larger than the total cluster) then a health warning (`POOL_TARGET_SIZE_BYTES_OVERCOMMITTED`) will be raised.

If both `target_size_ratio` and `target_size_bytes` are specified for a pool, only the ratio will be considered, and a health warning (`POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO`) will be issued.

### Specifying bounds on a pool’s PGs[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#specifying-bounds-on-a-pool-s-pgs)

It is also possible to specify a minimum number of PGs for a pool. This is useful for establishing a lower bound on the amount of parallelism client will see when doing IO, even when a pool is mostly empty.  Setting the lower bound prevents Ceph from reducing (or recommending you reduce) the PG number below the configured number.

You can set the minimum or maximum number of PGs for a pool with:

```
ceph osd pool set <pool-name> pg_num_min <num>
ceph osd pool set <pool-name> pg_num_max <num>
```

You can also specify the minimum or maximum PG count at pool creation time with the optional `--pg-num-min <num>` or `--pg-num-max <num>` arguments to the `ceph osd pool create` command.



## A preselection of pg_num[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#a-preselection-of-pg-num)

When creating a new pool with:

```
ceph osd pool create {pool-name} [pg_num]
```

it is optional to choose the value of `pg_num`.  If you do not specify `pg_num`, the cluster can (by default) automatically tune it for you based on how much data is stored in the pool (see above, [Autoscaling placement groups](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#pg-autoscaler)).

Alternatively, `pg_num` can be explicitly provided.  However, whether you specify a `pg_num` value or not does not affect whether the value is automatically tuned by the cluster after the fact.  To enable or disable auto-tuning:

```
ceph osd pool set {pool-name} pg_autoscale_mode (on|off|warn)
```

The “rule of thumb” for PGs per OSD has traditionally be 100.  With the additional of the balancer (which is also enabled by default), a value of more like 50 PGs per OSD is probably reasonable.  The challenge (which the autoscaler normally does for you), is to:

- have the PGs per pool proportional to the data in the pool, and
- end up with 50-100 PGs per OSDs, after the replication or erasuring-coding fan-out of each PG across OSDs is taken into consideration

## How are Placement Groups used ?[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#how-are-placement-groups-used)

A placement group (PG) aggregates objects within a pool because tracking object placement and object metadata on a per-object basis is computationally expensive–i.e., a system with millions of objects cannot realistically track placement on a per-object basis.

![img](https://docs.ceph.com/en/latest/_images/ditaa-a53f413321ac943aa924fb6b58c0b3716821f6a7.png)

The Ceph client will calculate which placement group an object should be in. It does this by hashing the object ID and applying an operation based on the number of PGs in the defined pool and the ID of the pool. See [Mapping PGs to OSDs](https://docs.ceph.com/en/latest/architecture#mapping-pgs-to-osds) for details.

The object’s contents within a placement group are stored in a set of OSDs. For instance, in a replicated pool of size two, each placement group will store objects on two OSDs, as shown below.

![img](https://docs.ceph.com/en/latest/_images/ditaa-9d7f8f18e202e8cc5e09f9691de9266064af43b6.png)

Should OSD #2 fail, another will be assigned to Placement Group #1 and will be filled with copies of all objects in OSD #1. If the pool size is changed from two to three, an additional OSD will be assigned to the placement group and will receive copies of all objects in the placement group.

Placement groups do not own the OSD; they share it with other placement groups from the same pool or even other pools. If OSD #2 fails, the Placement Group #2 will also have to restore copies of objects, using OSD #3.

When the number of placement groups increases, the new placement groups will be assigned OSDs. The result of the CRUSH function will also change and some objects from the former placement groups will be copied over to the new Placement Groups and removed from the old ones.

## Placement Groups Tradeoffs[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#placement-groups-tradeoffs)

Data durability and even distribution among all OSDs call for more placement groups but their number should be reduced to the minimum to save CPU and memory.



### Data durability[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#data-durability)

After an OSD fails, the risk of data loss increases until the data it contained is fully recovered. Let’s imagine a scenario that causes permanent data loss in a single placement group:

- The OSD fails and all copies of the object it contains are lost. For all objects within the placement group the number of replica suddenly drops from three to two.
- Ceph starts recovery for this placement group by choosing a new OSD to re-create the third copy of all objects.
- Another OSD, within the same placement group, fails before the new OSD is fully populated with the third copy. Some objects will then only have one surviving copies.
- Ceph picks yet another OSD and keeps copying objects to restore the desired number of copies.
- A third OSD, within the same placement group, fails before recovery is complete. If this OSD contained the only remaining copy of an object, it is permanently lost.

In a cluster containing 10 OSDs with 512 placement groups in a three replica pool, CRUSH will give each placement groups three OSDs. In the end, each OSDs will end up hosting (512 * 3) / 10 = ~150 Placement Groups. When the first OSD fails, the above scenario will therefore start recovery for all 150 placement groups at the same time.

The 150 placement groups being recovered are likely to be homogeneously spread over the 9 remaining OSDs. Each remaining OSD is therefore likely to send copies of objects to all others and also receive some new objects to be stored because they became part of a new placement group.

The amount of time it takes for this recovery to complete entirely depends on the architecture of the Ceph cluster. Let say each OSD is hosted by a 1TB SSD on a single machine and all of them are connected to a 10Gb/s switch and the recovery for a single OSD completes within M minutes. If there are two OSDs per machine using spinners with no SSD journal and a 1Gb/s switch, it will at least be an order of magnitude slower.

In a cluster of this size, the number of placement groups has almost no influence on data durability. It could be 128 or 8192 and the recovery would not be slower or faster.

However, growing the same Ceph cluster to 20 OSDs instead of 10 OSDs is likely to speed up recovery and therefore improve data durability significantly. Each OSD now participates in only ~75 placement groups instead of ~150 when there were only 10 OSDs and it will still require all 19 remaining OSDs to perform the same amount of object copies in order to recover. But where 10 OSDs had to copy approximately 100GB each, they now have to copy 50GB each instead. If the network was the bottleneck, recovery will happen twice as fast. In other words, recovery goes faster when the number of OSDs increases.

If this cluster grows to 40 OSDs, each of them will only host ~35 placement groups. If an OSD dies, recovery will keep going faster unless it is blocked by another bottleneck. However, if this cluster grows to 200 OSDs, each of them will only host ~7 placement groups. If an OSD dies, recovery will happen between at most of ~21 (7 * 3) OSDs in these placement groups: recovery will take longer than when there were 40 OSDs, meaning the number of placement groups should be increased.

No matter how short the recovery time is, there is a chance for a second OSD to fail while it is in progress. In the 10 OSDs cluster described above, if any of them fail, then ~17 placement groups (i.e. ~150 / 9 placement groups being recovered) will only have one surviving copy. And if any of the 8 remaining OSD fail, the last objects of two placement groups are likely to be lost (i.e. ~17 / 8 placement groups with only one remaining copy being recovered).

When the size of the cluster grows to 20 OSDs, the number of Placement Groups damaged by the loss of three OSDs drops. The second OSD lost will degrade ~4 (i.e. ~75 / 19 placement groups being recovered) instead of ~17 and the third OSD lost will only lose data if it is one of the four OSDs containing the surviving copy. In other words, if the probability of losing one OSD is 0.0001% during the recovery time frame, it goes from 17 * 10 * 0.0001% in the cluster with 10 OSDs to 4 * 20 * 0.0001% in the cluster with 20 OSDs.

In a nutshell, more OSDs mean faster recovery and a lower risk of cascading failures leading to the permanent loss of a Placement Group. Having 512 or 4096 Placement Groups is roughly equivalent in a cluster with less than 50 OSDs as far as data durability is concerned.

Note: It may take a long time for a new OSD added to the cluster to be populated with placement groups that were assigned to it. However there is no degradation of any object and it has no impact on the durability of the data contained in the Cluster.



### Object distribution within a pool[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#object-distribution-within-a-pool)

Ideally objects are evenly distributed in each placement group. Since CRUSH computes the placement group for each object, but does not actually know how much data is stored in each OSD within this placement group, the ratio between the number of placement groups and the number of OSDs may influence the distribution of the data significantly.

For instance, if there was a single placement group for ten OSDs in a three replica pool, only three OSD would be used because CRUSH would have no other choice. When more placement groups are available, objects are more likely to be evenly spread among them. CRUSH also makes every effort to evenly spread OSDs among all existing Placement Groups.

As long as there are one or two orders of magnitude more Placement Groups than OSDs, the distribution should be even. For instance, 256 placement groups for 3 OSDs, 512 or 1024 placement groups for 10 OSDs etc.

Uneven data distribution can be caused by factors other than the ratio between OSDs and placement groups. Since CRUSH does not take into account the size of the objects, a few very large objects may create an imbalance. Let say one million 4K objects totaling 4GB are evenly spread among 1024 placement groups on 10 OSDs. They will use 4GB / 10 = 400MB on each OSD. If one 400MB object is added to the pool, the three OSDs supporting the placement group in which the object has been placed will be filled with 400MB + 400MB = 800MB while the seven others will remain occupied with only 400MB.



### Memory, CPU and network usage[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#memory-cpu-and-network-usage)

For each placement group, OSDs and MONs need memory, network and CPU at all times and even more during recovery. Sharing this overhead by clustering objects within a placement group is one of the main reasons they exist.

Minimizing the number of placement groups saves significant amounts of resources.



## Choosing the number of Placement Groups[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#choosing-the-number-of-placement-groups)

If you have more than 50 OSDs, we recommend approximately 50-100 placement groups per OSD to balance out resource usage, data durability and distribution. If you have less than 50 OSDs, choosing among the [preselection](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#preselection) above is best. For a single pool of objects, you can use the following formula to get a baseline

> Total PGs = 

Where **pool size** is either the number of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by **ceph osd erasure-code-profile get**).

You should then check if the result makes sense with the way you designed your Ceph cluster to maximize [data durability](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#data-durability), [object distribution](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#object-distribution) and minimize [resource usage](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#resource-usage).

The result should always be **rounded up to the nearest power of two**.

Only a power of two will evenly balance the number of objects among placement groups. Other values will result in an uneven distribution of data across your OSDs. Their use should be limited to incrementally stepping from one power of two to another.

As an example, for a cluster with 200 OSDs and a pool size of 3 replicas, you would estimate your number of PGs as follows

> 

> . Nearest power of 2: 8192

When using multiple data pools for storing objects, you need to ensure that you balance the number of placement groups per pool with the number of placement groups per OSD so that you arrive at a reasonable total number of placement groups that provides reasonably low variance per OSD without taxing system resources or making the peering process too slow.

For instance a cluster of 10 pools each with 512 placement groups on ten OSDs is a total of 5,120 placement groups spread over ten OSDs, that is 512 placement groups per OSD. That does not use too many resources. However, if 1,000 pools were created with 512 placement groups each, the OSDs will handle ~50,000 placement groups each and it would require significantly more resources and time for peering.

You may find the [PGCalc](https://old.ceph.com/pgcalc/) tool helpful.



## Set the Number of Placement Groups[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#set-the-number-of-placement-groups)

To set the number of placement groups in a pool, you must specify the number of placement groups at the time you create the pool. See [Create a Pool](https://docs.ceph.com/en/latest/rados/operations/pools#createpool) for details.  Even after a pool is created you can also change the number of placement groups with:

```
ceph osd pool set {pool-name} pg_num {pg_num}
```

After you increase the number of placement groups, you must also increase the number of placement groups for placement (`pgp_num`) before your cluster will rebalance. The `pgp_num` will be the number of placement groups that will be considered for placement by the CRUSH algorithm. Increasing `pg_num` splits the placement groups but data will not be migrated to the newer placement groups until placement groups for placement, ie. `pgp_num` is increased. The `pgp_num` should be equal to the `pg_num`.  To increase the number of placement groups for placement, execute the following:

```
ceph osd pool set {pool-name} pgp_num {pgp_num}
```

When decreasing the number of PGs, `pgp_num` is adjusted automatically for you.

## Get the Number of Placement Groups[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#get-the-number-of-placement-groups)

To get the number of placement groups in a pool, execute the following:

```
ceph osd pool get {pool-name} pg_num
```

## Get a Cluster’s PG Statistics[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#get-a-cluster-s-pg-statistics)

To get the statistics for the placement groups in your cluster, execute the following:

```
ceph pg dump [--format {format}]
```

Valid formats are `plain` (default) and `json`.

## Get Statistics for Stuck PGs[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#get-statistics-for-stuck-pgs)

To get the statistics for all placement groups stuck in a specified state, execute the following:

```
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format <format>] [-t|--threshold <seconds>]
```

**Inactive** Placement groups cannot process reads or writes because they are waiting for an OSD with the most up-to-date data to come up and in.

**Unclean** Placement groups contain objects that are not replicated the desired number of times. They should be recovering.

**Stale** Placement groups are in an unknown state - the OSDs that host them have not reported to the monitor cluster in a while (configured by `mon_osd_report_timeout`).

Valid formats are `plain` (default) and `json`. The threshold defines the minimum number of seconds the placement group is stuck before including it in the returned statistics (default 300 seconds).

## Get a PG Map[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#get-a-pg-map)

To get the placement group map for a particular placement group, execute the following:

```
ceph pg map {pg-id}
```

For example:

```
ceph pg map 1.6c
```

Ceph will return the placement group map, the placement group, and the OSD status:

```
osdmap e13 pg 1.6c (1.6c) -> up [1,0] acting [1,0]
```

## Get a PGs Statistics[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#get-a-pgs-statistics)

To retrieve statistics for a particular placement group, execute the following:

```
ceph pg {pg-id} query
```

## Scrub a Placement Group[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#scrub-a-placement-group)

To scrub a placement group, execute the following:

```
ceph pg scrub {pg-id}
```

Ceph checks the primary and any replica nodes, generates a catalog of all objects in the placement group and compares them to ensure that no objects are missing or mismatched, and their contents are consistent.  Assuming the replicas all match, a final semantic sweep ensures that all of the snapshot-related object metadata is consistent. Errors are reported via logs.

To scrub all placement groups from a specific pool, execute the following:

```
ceph osd pool scrub {pool-name}
```

## Prioritize backfill/recovery of a Placement Group(s)[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#prioritize-backfill-recovery-of-a-placement-group-s)

You may run into a situation where a bunch of placement groups will require recovery and/or backfill, and some particular groups hold data more important than others (for example, those PGs may hold data for images used by running machines and other PGs may be used by inactive machines/less relevant data). In that case, you may want to prioritize recovery of those groups so performance and/or availability of data stored on those groups is restored earlier. To do this (mark particular placement group(s) as prioritized during backfill or recovery), execute the following:

```
ceph pg force-recovery {pg-id} [{pg-id #2}] [{pg-id #3} ...]
ceph pg force-backfill {pg-id} [{pg-id #2}] [{pg-id #3} ...]
```

This will cause Ceph to perform recovery or backfill on specified placement groups first, before other placement groups. This does not interrupt currently ongoing backfills or recovery, but causes specified PGs to be processed as soon as possible. If you change your mind or prioritize wrong groups, use:

```
ceph pg cancel-force-recovery {pg-id} [{pg-id #2}] [{pg-id #3} ...]
ceph pg cancel-force-backfill {pg-id} [{pg-id #2}] [{pg-id #3} ...]
```

This will remove “force” flag from those PGs and they will be processed in default order. Again, this doesn’t affect currently processed placement group, only those that are still queued.

The “force” flag is cleared automatically after recovery or backfill of group is done.

Similarly, you may use the following commands to force Ceph to perform recovery or backfill on all placement groups from a specified pool first:

```
ceph osd pool force-recovery {pool-name}
ceph osd pool force-backfill {pool-name}
```

or:

```
ceph osd pool cancel-force-recovery {pool-name}
ceph osd pool cancel-force-backfill {pool-name}
```

to restore to the default recovery or backfill priority if you change your mind.

Note that these commands could possibly break the ordering of Ceph’s internal priority computations, so use them with caution! Especially, if you have multiple pools that are currently sharing the same underlying OSDs, and some particular pools hold data more important than others, we recommend you use the following command to re-arrange all pools’s recovery/backfill priority in a better order:

```
ceph osd pool set {pool-name} recovery_priority {value}
```

For example, if you have 10 pools you could make the most important one priority 10, next 9, etc. Or you could leave most pools alone and have say 3 important pools all priority 1 or priorities 3, 2, 1 respectively.

## Revert Lost[](https://docs.ceph.com/en/latest/rados/operations/placement-groups/#revert-lost)

If the cluster has lost one or more objects, and you have decided to abandon the search for the lost data, you must mark the unfound objects as `lost`.

If all possible locations have been queried and objects are still lost, you may have to give up on the lost objects. This is possible given unusual combinations of failures that allow the cluster to learn about writes that were performed before the writes themselves are recovered.

Currently the only supported option is “revert”, which will either roll back to a previous version of the object or (if it was a new object) forget about it entirely. To mark the “unfound” objects as “lost”, execute the following:

```
ceph pg {pg-id} mark_unfound_lost revert|delete
```

Important

Use this feature with caution, because it may confuse applications that expect the object(s) to exist.

# Placement Group States[](https://docs.ceph.com/en/latest/rados/operations/pg-states/#placement-group-states)

When checking a cluster’s status (e.g., running `ceph -w` or `ceph -s`), Ceph will report on the status of the placement groups. A placement group has one or more states. The optimum state for placement groups in the placement group map is `active + clean`.

- *creating*

  Ceph is still creating the placement group.

- *activating*

  The placement group is peered but not yet active.

- *active*

  Ceph will process requests to the placement group.

- *clean*

  Ceph replicated all objects in the placement group the correct number of times.

- *down*

  A replica with necessary data is down, so the placement group is offline.

- *laggy*

  A replica is not acknowledging new leases from the primary in a timely fashion; IO is temporarily paused.

- *wait*

  The set of OSDs for this PG has just changed and IO is temporarily paused until the previous interval’s leases expire.

- *scrubbing*

  Ceph is checking the placement group metadata for inconsistencies.

- *deep*

  Ceph is checking the placement group data against stored checksums.

- *degraded*

  Ceph has not replicated some objects in the placement group the correct number of times yet.

- *inconsistent*

  Ceph detects inconsistencies in the one or more replicas of an object in the placement group (e.g. objects are the wrong size, objects are missing from one replica *after* recovery finished, etc.).

- *peering*

  The placement group is undergoing the peering process

- *repair*

  Ceph is checking the placement group and repairing any inconsistencies it finds (if possible).

- *recovering*

  Ceph is migrating/synchronizing objects and their replicas.

- *forced_recovery*

  High recovery priority of that PG is enforced by user.

- *recovery_wait*

  The placement group is waiting in line to start recover.

- *recovery_toofull*

  A recovery operation is waiting because the destination OSD is over its full ratio.

- *recovery_unfound*

  Recovery stopped due to unfound objects.

- *backfilling*

  Ceph is scanning and synchronizing the entire contents of a placement group instead of inferring what contents need to be synchronized from the logs of recent operations. Backfill is a special case of recovery.

- *forced_backfill*

  High backfill priority of that PG is enforced by user.

- *backfill_wait*

  The placement group is waiting in line to start backfill.

- *backfill_toofull*

  A backfill operation is waiting because the destination OSD is over the backfillfull ratio.

- *backfill_unfound*

  Backfill stopped due to unfound objects.

- *incomplete*

  Ceph detects that a placement group is missing information about writes that may have occurred, or does not have any healthy copies. If you see this state, try to start any failed OSDs that may contain the needed information. In the case of an erasure coded pool temporarily reducing min_size may allow recovery.

- *stale*

  The placement group is in an unknown state - the monitors have not received an update for it since the placement group mapping changed.

- *remapped*

  The placement group is temporarily mapped to a different set of OSDs from what CRUSH specified.

- *undersized*

  The placement group has fewer copies than the configured pool replication level.

- *peered*

  The placement group has peered, but cannot serve client IO due to not having enough copies to reach the pool’s configured min_size parameter.  Recovery may occur in this state, so the pg may heal up to min_size eventually.

- *snaptrim*

  Trimming snaps.

- *snaptrim_wait*

  Queued to trim snaps.

- *snaptrim_error*

  Error stopped trimming snaps.

- *unknown*

  The ceph-mgr hasn’t yet received any information about the PG’s state from an OSD since mgr started up.

# Placement Group Concepts[](https://docs.ceph.com/en/latest/rados/operations/pg-concepts/#placement-group-concepts)

When you execute commands like `ceph -w`, `ceph osd dump`, and other commands related to placement groups, Ceph may return values using some of the following terms:

- *Peering*

  The process of bringing all of the OSDs that store a Placement Group (PG) into agreement about the state of all of the objects (and their metadata) in that PG. Note that agreeing on the state does not mean that they all have the latest contents.

- *Acting Set*

  The ordered list of OSDs who are (or were as of some epoch) responsible for a particular placement group.

- *Up Set*

  The ordered list of OSDs responsible for a particular placement group for a particular epoch according to CRUSH. Normally this is the same as the *Acting Set*, except when the *Acting Set* has been explicitly overridden via `pg_temp` in the OSD Map.

- *Current Interval* or *Past Interval*

  A sequence of OSD map epochs during which the *Acting Set* and *Up Set* for particular placement group do not change.

- *Primary*

  The member (and by convention first) of the *Acting Set*, that is responsible for coordination peering, and is the only OSD that will accept client-initiated writes to objects in a placement group.

- *Replica*

  A non-primary OSD in the *Acting Set* for a placement group (and who has been recognized as such and *activated* by the primary).

- *Stray*

  An OSD that is not a member of the current *Acting Set*, but has not yet been told that it can delete its copies of a particular placement group.

- *Recovery*

  Ensuring that copies of all of the objects in a placement group are on all of the OSDs in the *Acting Set*.  Once *Peering* has been performed, the *Primary* can start accepting write operations, and *Recovery* can proceed in the background.

- *PG Info*

  Basic metadata about the placement group’s creation epoch, the version for the most recent write to the placement group, *last epoch started*, *last epoch clean*, and the beginning of the *current interval*.  Any inter-OSD communication about placement groups includes the *PG Info*, such that any OSD that knows a placement group exists (or once existed) also has a lower bound on *last epoch clean* or *last epoch started*.

- *PG Log*

  A list of recent updates made to objects in a placement group. Note that these logs can be truncated after all OSDs in the *Acting Set* have acknowledged up to a certain point.

- *Missing Set*

  Each OSD notes update log entries and if they imply updates to the contents of an object, adds that object to a list of needed updates.  This list is called the *Missing Set* for that `<OSD,PG>`.

- *Authoritative History*

  A complete, and fully ordered set of operations that, if performed, would bring an OSD’s copy of a placement group up to date.

- *Epoch*

  A (monotonically increasing) OSD map version number

- *Last Epoch Start*

  The last epoch at which all nodes in the *Acting Set* for a particular placement group agreed on an *Authoritative History*.  At this point, *Peering* is deemed to have been successful.

- *up_thru*

  Before a *Primary* can successfully complete the *Peering* process, it must inform a monitor that is alive through the current OSD map *Epoch* by having the monitor set its *up_thru* in the osd map.  This helps *Peering* ignore previous *Acting Sets* for which *Peering* never completed after certain sequences of failures, such as the second interval below: *acting set* = [A,B] *acting set* = [A] *acting set* = [] very shortly after (e.g., simultaneous failure, but staggered detection) *acting set* = [B] (B restarts, A does not)

- *Last Epoch Clean*

  The last *Epoch* at which all nodes in the *Acting set* for a particular placement group were completely up to date (both placement group logs and object contents). At this point, *recovery* is deemed to have been completed.

# Balancer[](https://docs.ceph.com/en/latest/rados/operations/balancer/#balancer)

The *balancer* can optimize the placement of PGs across OSDs in order to achieve a balanced distribution, either automatically or in a supervised fashion.

## Status[](https://docs.ceph.com/en/latest/rados/operations/balancer/#status)

The current status of the balancer can be checked at any time with:

```
ceph balancer status
```

## Automatic balancing[](https://docs.ceph.com/en/latest/rados/operations/balancer/#automatic-balancing)

The automatic balancing feature is enabled by default in `upmap` mode. Please refer to [Using the pg-upmap](https://docs.ceph.com/en/latest/rados/operations/upmap/#upmap) for more details. The balancer can be turned off with:

```
ceph balancer off
```

The balancer mode can be changed to `crush-compat` mode, which is backward compatible with older clients, and will make small changes to the data distribution over time to ensure that OSDs are equally utilized.

## Throttling[](https://docs.ceph.com/en/latest/rados/operations/balancer/#throttling)

No adjustments will be made to the PG distribution if the cluster is degraded (e.g., because an OSD has failed and the system has not yet healed itself).

When the cluster is healthy, the balancer will throttle its changes such that the percentage of PGs that are misplaced (i.e., that need to be moved) is below a threshold of (by default) 5%.  The `target_max_misplaced_ratio` threshold can be adjusted with:

```
ceph config set mgr target_max_misplaced_ratio .07   # 7%
```

Set the number of seconds to sleep in between runs of the automatic balancer:

```
ceph config set mgr mgr/balancer/sleep_interval 60
```

Set the time of day to begin automatic balancing in HHMM format:

```
ceph config set mgr mgr/balancer/begin_time 0000
```

Set the time of day to finish automatic balancing in HHMM format:

```
ceph config set mgr mgr/balancer/end_time 2400
```

Restrict automatic balancing to this day of the week or later. Uses the same conventions as crontab, 0 or 7 is Sunday, 1 is Monday, and so on:

```
ceph config set mgr mgr/balancer/begin_weekday 0
```

Restrict automatic balancing to this day of the week or earlier. Uses the same conventions as crontab, 0 or 7 is Sunday, 1 is Monday, and so on:

```
ceph config set mgr mgr/balancer/end_weekday 7
```

Pool IDs to which the automatic balancing will be limited. The default for this is an empty string, meaning all pools will be balanced. The numeric pool IDs can be gotten with the **ceph osd pool ls detail** command:

```
ceph config set mgr mgr/balancer/pool_ids 1,2,3
```

## Modes[](https://docs.ceph.com/en/latest/rados/operations/balancer/#modes)

There are currently two supported balancer modes:

1. **crush-compat**.  The CRUSH compat mode uses the compat weight-set feature (introduced in Luminous) to manage an alternative set of weights for devices in the CRUSH hierarchy.  The normal weights should remain set to the size of the device to reflect the target amount of data that we want to store on the device.  The balancer then optimizes the weight-set values, adjusting them up or down in small increments, in order to achieve a distribution that matches the target distribution as closely as possible.  (Because PG placement is a pseudorandom process, there is a natural amount of variation in the placement; by optimizing the weights we counter-act that natural variation.)

   Notably, this mode is *fully backwards compatible* with older clients: when an OSDMap and CRUSH map is shared with older clients, we present the optimized weights as the “real” weights.

   The primary restriction of this mode is that the balancer cannot handle multiple CRUSH hierarchies with different placement rules if the subtrees of the hierarchy share any OSDs.  (This is normally not the case, and is generally not a recommended configuration because it is hard to manage the space utilization on the shared OSDs.)

2. **upmap**.  Starting with Luminous, the OSDMap can store explicit mappings for individual OSDs as exceptions to the normal CRUSH placement calculation.  These upmap entries provide fine-grained control over the PG mapping.  This CRUSH mode will optimize the placement of individual PGs in order to achieve a balanced distribution.  In most cases, this distribution is “perfect,” which an equal number of PGs on each OSD (+/-1 PG, since they might not divide evenly).

   Note that using upmap requires that all clients be Luminous or newer.

The default mode is `upmap`.  The mode can be adjusted with:

```
ceph balancer mode crush-compat
```

## Supervised optimization[](https://docs.ceph.com/en/latest/rados/operations/balancer/#supervised-optimization)

The balancer operation is broken into a few distinct phases:

1. building a *plan*
2. evaluating the quality of the data distribution, either for the  current PG distribution, or the PG distribution that would result after  executing a *plan*
3. executing the *plan*

To evaluate and score the current distribution:

```
ceph balancer eval
```

You can also evaluate the distribution for a single pool with:

```
ceph balancer eval <pool-name>
```

Greater detail for the evaluation can be seen with:

```
ceph balancer eval-verbose ...
```

The balancer can generate a plan, using the currently configured mode, with:

```
ceph balancer optimize <plan-name>
```

The name is provided by the user and can be any useful identifying string.  The contents of a plan can be seen with:

```
ceph balancer show <plan-name>
```

All plans can be shown with:

```
ceph balancer ls
```

Old plans can be discarded with:

```
ceph balancer rm <plan-name>
```

Currently recorded plans are shown as part of the status command:

```
ceph balancer status
```

The quality of the distribution that would result after executing a plan can be calculated with:

```
ceph balancer eval <plan-name>
```

Assuming the plan is expected to improve the distribution (i.e., it  has a lower score than the current cluster state), the user can execute  that plan with:

```
ceph balancer execute <plan-name>
```

​        

# Using the pg-upmap[](https://docs.ceph.com/en/latest/rados/operations/upmap/#using-the-pg-upmap)

Starting in Luminous v12.2.z there is a new *pg-upmap* exception table in the OSDMap that allows the cluster to explicitly map specific PGs to specific OSDs.  This allows the cluster to fine-tune the data distribution to, in most cases, perfectly distributed PGs across OSDs.

The key caveat to this new mechanism is that it requires that all clients understand the new *pg-upmap* structure in the OSDMap.

## Enabling[](https://docs.ceph.com/en/latest/rados/operations/upmap/#enabling)

New clusters will have this module on by default. The cluster must only have luminous (and newer) clients. You can the turn the balancer off with:

```
ceph balancer off
```

To allow use of the feature on existing clusters, you must tell the cluster that it only needs to support luminous (and newer) clients with:

```
ceph osd set-require-min-compat-client luminous
```

This command will fail if any pre-luminous clients or daemons are connected to the monitors.  You can see what client versions are in use with:

```
ceph features
```

## Balancer module[](https://docs.ceph.com/en/latest/rados/operations/upmap/#balancer-module)

The balancer module for ceph-mgr will automatically balance the number of PGs per OSD.  See [Balancer](https://docs.ceph.com/en/latest/rados/operations/balancer/#balancer)

## Offline optimization[](https://docs.ceph.com/en/latest/rados/operations/upmap/#offline-optimization)

Upmap entries are updated with an offline optimizer built into `osdmaptool`.

1. Grab the latest copy of your osdmap:

   ```
   ceph osd getmap -o om
   ```

2. Run the optimizer:

   ```
   osdmaptool om --upmap out.txt [--upmap-pool <pool>]
            [--upmap-max <max-optimizations>] [--upmap-deviation <max-deviation>]
            [--upmap-active]
   ```

   It is highly recommended that optimization be done for each pool individually, or for sets of similarly-utilized pools.  You can specify the `--upmap-pool` option multiple times.  “Similar pools” means pools that are mapped to the same devices and store the same kind of data (e.g., RBD image pools, yes; RGW index pool and RGW data pool, no).

   The `max-optimizations` value is the maximum number of upmap entries to identify in the run.  The default is 10 like the ceph-mgr balancer module, but you should use a larger number if you are doing offline optimization. If it cannot find any additional changes to make it will stop early (i.e., when the pool distribution is perfect).

   The `max-deviation` value defaults to 5.  If an OSD PG count varies from the computed target number by less than or equal to this amount it will be considered perfect.

   The `--upmap-active` option simulates the behavior of the active balancer in upmap mode.  It keeps cycling until the OSDs are balanced and reports how many rounds and how long each round is taking.  The elapsed time for rounds indicates the CPU load ceph-mgr will be consuming when it tries to compute the next optimization plan.

3. Apply the changes:

   ```
   source out.txt
   ```

   The proposed changes are written to the output file `out.txt` in the example above.  These are normal ceph CLI commands that can be run to apply the changes to the cluster.

The above steps can be repeated as many times as necessary to achieve a perfect distribution of PGs for each set of pools.

You can see some (gory) details about what the tool is doing by passing `--debug-osd 10` and even more with `--debug-crush 10` to `osdmaptool`.

​        

# CRUSH Maps[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#crush-maps)

The CRUSH algorithm determines how to store and retrieve data by computing storage locations. CRUSH empowers Ceph clients to communicate with OSDs directly rather than through a centralized server or broker. With an algorithmically determined method of storing and retrieving data, Ceph avoids a single point of failure, a performance bottleneck, and a physical limit to its scalability.

CRUSH uses a map of your cluster (the CRUSH map) to pseudo-randomly map data to OSDs, distributing it across the cluster according to configured replication policy and failure domain.  For a detailed discussion of CRUSH, see [CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data](https://ceph.com/assets/pdfs/weil-crush-sc06.pdf)

CRUSH maps contain a list of OSDs, a hierarchy of ‘buckets’ for aggregating devices and buckets, and rules that govern how CRUSH replicates data within the cluster’s pools. By reflecting the underlying physical organization of the installation, CRUSH can model (and thereby address) the potential for correlated device failures. Typical factors include chassis, racks, physical proximity, a shared power source, and shared networking. By encoding this information into the cluster map, CRUSH placement policies distribute object replicas across failure domains while maintaining the desired distribution. For example, to address the possibility of concurrent failures, it may be desirable to ensure that data replicas are on devices using different shelves, racks, power supplies, controllers, and/or physical locations.

When you deploy OSDs they are automatically added to the CRUSH map under a `host` bucket named for the node on which they run.  This, combined with the configured CRUSH failure domain, ensures that replicas or erasure code shards are distributed across hosts and that a single host or other failure will not affect availability.  For larger clusters, administrators must carefully consider their choice of failure domain.  Separating replicas across racks, for example, is typical for mid- to large-sized clusters.

## CRUSH Location[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#crush-location)

The location of an OSD within the CRUSH map’s hierarchy is referred to as a `CRUSH location`.  This location specifier takes the form of a list of key and value pairs.  For example, if an OSD is in a particular row, rack, chassis and host, and is part of the ‘default’ CRUSH root (which is the case for most clusters), its CRUSH location could be described as:

```
root=default row=a rack=a2 chassis=a2a host=a2a1
```

Note:

1. Note that the order of the keys does not matter.
2. The key name (left of `=`) must be a valid CRUSH `type`.  By default these include `root`, `datacenter`, `room`, `row`, `pod`, `pdu`, `rack`, `chassis` and `host`. These defined types suffice for almost all clusters, but can be customized by modifying the CRUSH map.
3. Not all keys need to be specified.  For example, by default, Ceph automatically sets an `OSD`’s location to be `root=default host=HOSTNAME` (based on the output from `hostname -s`).

The CRUSH location for an OSD can be defined by adding the `crush location` option in `ceph.conf`.  Each time the OSD starts, it verifies it is in the correct location in the CRUSH map and, if it is not, it moves itself.  To disable this automatic CRUSH map management, add the following to your configuration file in the `[osd]` section:

```
osd crush update on start = false
```

Note that in most cases you will not need to manually configure this.

### Custom location hooks[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#custom-location-hooks)

A customized location hook can be used to generate a more complete CRUSH location on startup.  The CRUSH location is based on, in order of preference:

1. A `crush location` option in `ceph.conf`
2. A default of `root=default host=HOSTNAME` where the hostname is derived from the `hostname -s` command

A script can be written to provide additional location fields (for example, `rack` or `datacenter`) and the hook enabled via the config option:

```
crush location hook = /path/to/customized-ceph-crush-location
```

This hook is passed several arguments (below) and should output a single line to `stdout` with the CRUSH location description.:

```
--cluster CLUSTER --id ID --type TYPE
```

where the cluster name is typically `ceph`, the `id` is the daemon identifier (e.g., the OSD number or daemon identifier), and the daemon type is `osd`, `mds`, etc.

For example, a simple hook that additionally specifies a rack location based on a value in the file `/etc/rack` might be:

```
#!/bin/sh
echo "host=$(hostname -s) rack=$(cat /etc/rack) root=default"
```

## CRUSH structure[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#crush-structure)

The CRUSH map consists of a hierarchy that describes the physical topology of the cluster and a set of rules defining data placement policy.  The hierarchy has devices (OSDs) at the leaves, and internal nodes corresponding to other physical features or groupings: hosts, racks, rows, datacenters, and so on.  The rules describe how replicas are placed in terms of that hierarchy (e.g., ‘three replicas in different racks’).

### Devices[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#devices)

Devices are individual OSDs that store data, usually one for each storage drive. Devices are identified by an `id` (a non-negative integer) and a `name`, normally `osd.N` where `N` is the device id.

Since the Luminous release, devices may also have a *device class* assigned (e.g., `hdd` or `ssd` or `nvme`), allowing them to be conveniently targeted by CRUSH rules.  This is especially useful when mixing device types within hosts.



### Types and Buckets[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#types-and-buckets)

A bucket is the CRUSH term for internal nodes in the hierarchy: hosts, racks, rows, etc.  The CRUSH map defines a series of *types* that are used to describe these nodes.  Default types include:

- `osd` (or `device`)
- `host`
- `chassis`
- `rack`
- `row`
- `pdu`
- `pod`
- `room`
- `datacenter`
- `zone`
- `region`
- `root`

Most clusters use only a handful of these types, and others can be defined as needed.

The hierarchy is built with devices (normally type `osd`) at the leaves, interior nodes with non-device types, and a root node of type `root`.  For example,

![img](https://docs.ceph.com/en/latest/_images/ditaa-c6820cb15c82bde8e462dfb2ed29fa0ffaa542fe.png)

Each node (device or bucket) in the hierarchy has a *weight* that indicates the relative proportion of the total data that device or hierarchy subtree should store.  Weights are set at the leaves, indicating the size of the device, and automatically sum up the tree, such that the weight of the `root` node will be the total of all devices contained beneath it.  Normally weights are in units of terabytes (TB).

You can get a simple view the of CRUSH hierarchy for your cluster, including weights, with:

```
ceph osd tree
```

### Rules[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#rules)

CRUSH Rules define policy about how data is distributed across the devices in the hierarchy. They define placement and replication strategies or distribution policies that allow you to specify exactly how CRUSH places data replicas. For example, you might create a rule selecting a pair of targets for two-way mirroring, another rule for selecting three targets in two different data centers for three-way mirroring, and yet another rule for erasure coding (EC) across six storage devices. For a detailed discussion of CRUSH rules, refer to [CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data](https://ceph.com/assets/pdfs/weil-crush-sc06.pdf), and more specifically to **Section 3.2**.

CRUSH rules can be created via the CLI by specifying the *pool type* they will be used for (replicated or erasure coded), the *failure domain*, and optionally a *device class*. In rare cases rules must be written by hand by manually editing the CRUSH map.

You can see what rules are defined for your cluster with:

```
ceph osd crush rule ls
```

You can view the contents of the rules with:

```
ceph osd crush rule dump
```

### Device classes[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#device-classes)

Each device can optionally have a *class* assigned.  By default, OSDs automatically set their class at startup to hdd, ssd, or nvme based on the type of device they are backed by.

The device class for one or more OSDs can be explicitly set with:

```
ceph osd crush set-device-class <class> <osd-name> [...]
```

Once a device class is set, it cannot be changed to another class until the old class is unset with:

```
ceph osd crush rm-device-class <osd-name> [...]
```

This allows administrators to set device classes without the class being changed on OSD restart or by some other script.

A placement rule that targets a specific device class can be created with:

```
ceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <class>
```

A pool can then be changed to use the new rule with:

```
ceph osd pool set <pool-name> crush_rule <rule-name>
```

Device classes are implemented by creating a “shadow” CRUSH hierarchy for each device class in use that contains only devices of that class. CRUSH rules can then distribute data over the shadow hierarchy. This approach is fully backward compatible with old Ceph clients.  You can view the CRUSH hierarchy with shadow items with:

```
ceph osd crush tree --show-shadow
```

For older clusters created before Luminous that relied on manually crafted CRUSH maps to maintain per-device-type hierarchies, there is a *reclassify* tool available to help transition to device classes without triggering data movement (see [Migrating from a legacy SSD rule to device classes](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crush-reclassify)).

### Weights sets[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#weights-sets)

A *weight set* is an alternative set of weights to use when calculating data placement.  The normal weights associated with each device in the CRUSH map are set based on the device size and indicate how much data we *should* be storing where.  However, because CRUSH is a “probabilistic” pseudorandom placement process, there is always some variation from this ideal distribution, in the same way that rolling a die sixty times will not result in rolling exactly 10 ones and 10 sixes.  Weight sets allow the cluster to perform numerical optimization based on the specifics of your cluster (hierarchy, pools, etc.) to achieve a balanced distribution.

There are two types of weight sets supported:

> 1. A **compat** weight set is a single alternative set of weights for each device and node in the cluster.  This is not well-suited for correcting for all anomalies (for example, placement groups for different pools may be different sizes and have different load levels, but will be mostly treated the same by the balancer). However, compat weight sets have the huge advantage that they are *backward compatible* with previous versions of Ceph, which means that even though weight sets were first introduced in Luminous v12.2.z, older clients (e.g., firefly) can still connect to the cluster when a compat weight set is being used to balance data.
> 2. A **per-pool** weight set is more flexible in that it allows placement to be optimized for each data pool.  Additionally, weights can be adjusted for each position of placement, allowing the optimizer to correct for a subtle skew of data toward devices with small weights relative to their peers (and effect that is usually only apparently in very large clusters but which can cause balancing problems).

When weight sets are in use, the weights associated with each node in the hierarchy is visible as a separate column (labeled either `(compat)` or the pool name) from the command:

```
ceph osd tree
```

When both *compat* and *per-pool* weight sets are in use, data placement for a particular pool will use its own per-pool weight set if present.  If not, it will use the compat weight set if present.  If neither are present, it will use the normal CRUSH weights.

Although weight sets can be set up and manipulated by hand, it is recommended that the `ceph-mgr` *balancer* module be enabled to do so automatically when running Luminous or later releases.

## Modifying the CRUSH map[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#modifying-the-crush-map)



### Add/Move an OSD[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#add-move-an-osd)

To add or move an OSD in the CRUSH map of a running cluster:

```
ceph osd crush set {name} {weight} root={root} [{bucket-type}={bucket-name} ...]
```

Where:

```
name
```

- Description

  The full name of the OSD.

- Type

  String

- Required

  Yes

- Example

  `osd.0`

```
weight
```

- Description

  The CRUSH weight for the OSD, normally its size measure in terabytes (TB).

- Type

  Double

- Required

  Yes

- Example

  `2.0`

```
root
```

- Description

  The root node of the tree in which the OSD resides (normally `default`)

- Type

  Key/value pair.

- Required

  Yes

- Example

  `root=default`

```
bucket-type
```

- Description

  You may specify the OSD’s location in the CRUSH hierarchy.

- Type

  Key/value pairs.

- Required

  No

- Example

  `datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1`

The following example adds `osd.0` to the hierarchy, or moves the OSD from a previous location.

```
ceph osd crush set osd.0 1.0 root=default datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1
```

### Adjust OSD weight[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#adjust-osd-weight)

To adjust an OSD’s CRUSH weight in the CRUSH map of a running cluster, execute the following:

```
ceph osd crush reweight {name} {weight}
```

Where:

```
name
```

- Description

  The full name of the OSD.

- Type

  String

- Required

  Yes

- Example

  `osd.0`

```
weight
```

- Description

  The CRUSH weight for the OSD.

- Type

  Double

- Required

  Yes

- Example

  `2.0`



### Remove an OSD[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#remove-an-osd)

To remove an OSD from the CRUSH map of a running cluster, execute the following:

```
ceph osd crush remove {name}
```

Where:

```
name
```

- Description

  The full name of the OSD.

- Type

  String

- Required

  Yes

- Example

  `osd.0`

### Add a Bucket[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#add-a-bucket)

To add a bucket in the CRUSH map of a running cluster, execute the `ceph osd crush add-bucket` command:

```
ceph osd crush add-bucket {bucket-name} {bucket-type}
```

Where:

```
bucket-name
```

- Description

  The full name of the bucket.

- Type

  String

- Required

  Yes

- Example

  `rack12`

```
bucket-type
```

- Description

  The type of the bucket. The type must already exist in the hierarchy.

- Type

  String

- Required

  Yes

- Example

  `rack`

The following example adds the `rack12` bucket to the hierarchy:

```
ceph osd crush add-bucket rack12 rack
```

### Move a Bucket[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#move-a-bucket)

To move a bucket to a different location or position in the CRUSH map hierarchy, execute the following:

```
ceph osd crush move {bucket-name} {bucket-type}={bucket-name}, [...]
```

Where:

```
bucket-name
```

- Description

  The name of the bucket to move/reposition.

- Type

  String

- Required

  Yes

- Example

  `foo-bar-1`

```
bucket-type
```

- Description

  You may specify the bucket’s location in the CRUSH hierarchy.

- Type

  Key/value pairs.

- Required

  No

- Example

  `datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1`

### Remove a Bucket[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#remove-a-bucket)

To remove a bucket from the CRUSH hierarchy, execute the following:

```
ceph osd crush remove {bucket-name}
```

Note

A bucket must be empty before removing it from the CRUSH hierarchy.

Where:

```
bucket-name
```

- Description

  The name of the bucket that you’d like to remove.

- Type

  String

- Required

  Yes

- Example

  `rack12`

The following example removes the `rack12` bucket from the hierarchy:

```
ceph osd crush remove rack12
```

### Creating a compat weight set[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#creating-a-compat-weight-set)

To create a *compat* weight set:

```
ceph osd crush weight-set create-compat
```

Weights for the compat weight set can be adjusted with:

```
ceph osd crush weight-set reweight-compat {name} {weight}
```

The compat weight set can be destroyed with:

```
ceph osd crush weight-set rm-compat
```

### Creating per-pool weight sets[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#creating-per-pool-weight-sets)

To create a weight set for a specific pool,:

```
ceph osd crush weight-set create {pool-name} {mode}
```

Note

Per-pool weight sets require that all servers and daemons run Luminous v12.2.z or later.

Where:

```
pool-name
```

- Description

  The name of a RADOS pool

- Type

  String

- Required

  Yes

- Example

  `rbd`

```
mode
```

- Description

  Either `flat` or `positional`.  A *flat* weight set has a single weight for each device or bucket.  A *positional* weight set has a potentially different weight for each position in the resulting placement mapping.  For example, if a pool has a replica count of 3, then a positional weight set will have three weights for each device and bucket.

- Type

  String

- Required

  Yes

- Example

  `flat`

To adjust the weight of an item in a weight set:

```
ceph osd crush weight-set reweight {pool-name} {item-name} {weight [...]}
```

To list existing weight sets,:

```
ceph osd crush weight-set ls
```

To remove a weight set,:

```
ceph osd crush weight-set rm {pool-name}
```

### Creating a rule for a replicated pool[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#creating-a-rule-for-a-replicated-pool)

For a replicated pool, the primary decision when creating the CRUSH rule is what the failure domain is going to be.  For example, if a failure domain of `host` is selected, then CRUSH will ensure that each replica of the data is stored on a unique host.  If `rack` is selected, then each replica will be stored in a different rack. What failure domain you choose primarily depends on the size and topology of your cluster.

In most cases the entire cluster hierarchy is nested beneath a root node named `default`.  If you have customized your hierarchy, you may want to create a rule nested at some other node in the hierarchy.  It doesn’t matter what type is associated with that node (it doesn’t have to be a `root` node).

It is also possible to create a rule that restricts data placement to a specific *class* of device.  By default, Ceph OSDs automatically classify themselves as either `hdd` or `ssd`, depending on the underlying type of device being used.  These classes can also be customized.

To create a replicated rule,:

```
ceph osd crush rule create-replicated {name} {root} {failure-domain-type} [{class}]
```

Where:

```
name
```

- Description

  The name of the rule

- Type

  String

- Required

  Yes

- Example

  `rbd-rule`

```
root
```

- Description

  The name of the node under which data should be placed.

- Type

  String

- Required

  Yes

- Example

  `default`

```
failure-domain-type
```

- Description

  The type of CRUSH nodes across which we should separate replicas.

- Type

  String

- Required

  Yes

- Example

  `rack`

```
class
```

- Description

  The device class on which data should be placed.

- Type

  String

- Required

  No

- Example

  `ssd`

### Creating a rule for an erasure coded pool[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#creating-a-rule-for-an-erasure-coded-pool)

For an erasure-coded (EC) pool, the same basic decisions need to be made: what is the failure domain, which node in the hierarchy will data be placed under (usually `default`), and will placement be restricted to a specific device class.  Erasure code pools are created a bit differently, however, because they need to be constructed carefully based on the erasure code being used.  For this reason, you must include this information in the *erasure code profile*.  A CRUSH rule will then be created from that either explicitly or automatically when the profile is used to create a pool.

The erasure code profiles can be listed with:

```
ceph osd erasure-code-profile ls
```

An existing profile can be viewed with:

```
ceph osd erasure-code-profile get {profile-name}
```

Normally profiles should never be modified; instead, a new profile should be created and used when creating a new pool or creating a new rule for an existing pool.

An erasure code profile consists of a set of key=value pairs.  Most of these control the behavior of the erasure code that is encoding data in the pool.  Those that begin with `crush-`, however, affect the CRUSH rule that is created.

The erasure code profile properties of interest are:

> - **crush-root**: the name of the CRUSH node under which to place data [default: `default`].
> - **crush-failure-domain**: the CRUSH bucket type across which to distribute erasure-coded shards [default: `host`].
> - **crush-device-class**: the device class on which to place data [default: none, meaning all devices are used].
> - **k** and **m** (and, for the `lrc` plugin, **l**): these determine the number of erasure code shards, affecting the resulting CRUSH rule.

Once a profile is defined, you can create a CRUSH rule with:

```
ceph osd crush rule create-erasure {name} {profile-name}
```

### Deleting rules[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#deleting-rules)

Rules that are not in use by pools can be deleted with:

```
ceph osd crush rule rm {rule-name}
```



## Tunables[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#tunables)

Over time, we have made (and continue to make) improvements to the CRUSH algorithm used to calculate the placement of data.  In order to support the change in behavior, we have introduced a series of tunable options that control whether the legacy or improved variation of the algorithm is used.

In order to use newer tunables, both clients and servers must support the new version of CRUSH.  For this reason, we have created `profiles` that are named after the Ceph version in which they were introduced.  For example, the `firefly` tunables are first supported by the Firefly release, and will not work with older (e.g., Dumpling) clients.  Once a given set of tunables are changed from the legacy default behavior, the `ceph-mon` and `ceph-osd` will prevent older clients who do not support the new CRUSH features from connecting to the cluster.

### argonaut (legacy)[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#argonaut-legacy)

The legacy CRUSH behavior used by Argonaut and older releases works fine for most clusters, provided there are not many OSDs that have been marked out.

### bobtail (CRUSH_TUNABLES2)[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#bobtail-crush-tunables2)

The `bobtail` tunable profile fixes a few key misbehaviors:

> - For hierarchies with a small number of devices in the leaf buckets, some PGs map to fewer than the desired number of replicas.  This commonly happens for hierarchies with “host” nodes with a small number (1-3) of OSDs nested beneath each one.
> - For large clusters, some small percentages of PGs map to fewer than the desired number of OSDs.  This is more prevalent when there are multiple hierarchy layers in use (e.g., `row`, `rack`, `host`, `osd`).
> - When some OSDs are marked out, the data tends to get redistributed to nearby OSDs instead of across the entire hierarchy.

The new tunables are:

> - `choose_local_tries`: Number of local retries.  Legacy value is 2, optimal value is 0.
> - `choose_local_fallback_tries`: Legacy value is 5, optimal value is 0.
> - `choose_total_tries`: Total number of attempts to choose an item. Legacy value was 19, subsequent testing indicates that a value of 50 is more appropriate for typical clusters.  For extremely large clusters, a larger value might be necessary.
> - `chooseleaf_descend_once`: Whether a recursive chooseleaf attempt will retry, or only try once and allow the original placement to retry.  Legacy default is 0, optimal value is 1.

Migration impact:

> - Moving from `argonaut` to `bobtail` tunables triggers a moderate amount of data movement.  Use caution on a cluster that is already populated with data.

### firefly (CRUSH_TUNABLES3)[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#firefly-crush-tunables3)

The `firefly` tunable profile fixes a problem with `chooseleaf` CRUSH rule behavior that tends to result in PG mappings with too few results when too many OSDs have been marked out.

The new tunable is:

> - `chooseleaf_vary_r`: Whether a recursive chooseleaf attempt will start with a non-zero value of `r`, based on how many attempts the parent has already made.  Legacy default is `0`, but with this value CRUSH is sometimes unable to find a mapping.  The optimal value (in terms of computational cost and correctness) is `1`.

Migration impact:

> - For existing clusters that house lots of data, changing from `0` to `1` will cause a lot of data to move; a value of `4` or `5` will allow CRUSH to still find a valid mapping but will cause less data to move.

### straw_calc_version tunable (introduced with Firefly too)[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#straw-calc-version-tunable-introduced-with-firefly-too)

There were some problems with the internal weights calculated and stored in the CRUSH map for `straw` algorithm buckets.  Specifically, when there were items with a CRUSH weight of `0`, or both a mix of different and unique weights, CRUSH would distribute data incorrectly (i.e., not in proportion to the weights).

The new tunable is:

> - `straw_calc_version`: A value of `0` preserves the old, broken internal weight calculation; a value of `1` fixes the behavior.

Migration impact:

> - Moving to straw_calc_version `1` and then adjusting a straw bucket (by adding, removing, or reweighting an item, or by using the reweight-all command) can trigger a small to moderate amount of data movement *if* the cluster has hit one of the problematic conditions.

This tunable option is special because it has absolutely no impact concerning the required kernel version in the client side.

### hammer (CRUSH_V4)[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#hammer-crush-v4)

The `hammer` tunable profile does not affect the mapping of existing CRUSH maps simply by changing the profile.  However:

> - There is a new bucket algorithm (`straw2`) supported.  The new `straw2` bucket algorithm fixes several limitations in the original `straw`.  Specifically, the old `straw` buckets would change some mappings that should have changed when a weight was adjusted, while `straw2` achieves the original goal of only changing mappings to or from the bucket item whose weight has changed.
> - `straw2` is the default for any newly created buckets.

Migration impact:

> - Changing a bucket type from `straw` to `straw2` will result in a reasonably small amount of data movement, depending on how much the bucket item weights vary from each other.  When the weights are all the same no data will move, and when item weights vary significantly there will be more movement.

### jewel (CRUSH_TUNABLES5)[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#jewel-crush-tunables5)

The `jewel` tunable profile improves the overall behavior of CRUSH such that significantly fewer mappings change when an OSD is marked out of the cluster.  This results in significantly less data movement.

The new tunable is:

> - `chooseleaf_stable`: Whether a recursive chooseleaf attempt will use a better value for an inner loop that greatly reduces the number of mapping changes when an OSD is marked out.  The legacy value is `0`, while the new value of `1` uses the new approach.

Migration impact:

> - Changing this value on an existing cluster will result in a very large amount of data movement as almost every PG mapping is likely to change.

### Which client versions support CRUSH_TUNABLES[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#which-client-versions-support-crush-tunables)

> - argonaut series, v0.48.1 or later
> - v0.49 or later
> - Linux kernel version v3.6 or later (for the file system and RBD kernel clients)

### Which client versions support CRUSH_TUNABLES2[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#which-client-versions-support-crush-tunables2)

> - v0.55 or later, including bobtail series (v0.56.x)
> - Linux kernel version v3.9 or later (for the file system and RBD kernel clients)

### Which client versions support CRUSH_TUNABLES3[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#which-client-versions-support-crush-tunables3)

> - v0.78 (firefly) or later
> - Linux kernel version v3.15 or later (for the file system and RBD kernel clients)

### Which client versions support CRUSH_V4[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#which-client-versions-support-crush-v4)

> - v0.94 (hammer) or later
> - Linux kernel version v4.1 or later (for the file system and RBD kernel clients)

### Which client versions support CRUSH_TUNABLES5[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#which-client-versions-support-crush-tunables5)

> - v10.0.2 (jewel) or later
> - Linux kernel version v4.5 or later (for the file system and RBD kernel clients)

### Warning when tunables are non-optimal[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#warning-when-tunables-are-non-optimal)

Starting with version v0.74, Ceph will issue a health warning if the current CRUSH tunables don’t include all the optimal values from the `default` profile (see below for the meaning of the `default` profile). To make this warning go away, you have two options:

1. Adjust the tunables on the existing cluster.  Note that this will result in some data movement (possibly as much as 10%).  This is the preferred route, but should be taken with care on a production cluster where the data movement may affect performance.  You can enable optimal tunables with:

   ```
   ceph osd crush tunables optimal
   ```

   If things go poorly (e.g., too much load) and not very much progress has been made, or there is a client compatibility problem (old kernel CephFS or RBD clients, or pre-Bobtail `librados` clients), you can switch back with:

   ```
   ceph osd crush tunables legacy
   ```

2. You can make the warning go away without making any changes to CRUSH by adding the following option to your ceph.conf `[mon]` section:

   ```
   mon_warn_on_legacy_crush_tunables = false
   ```

   For the change to take effect, you will need to restart the monitors, or apply the option to running monitors with:

   ```
   ceph tell mon.\* config set mon_warn_on_legacy_crush_tunables false
   ```

### A few important points[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#a-few-important-points)

> - Adjusting these values will result in the shift of some PGs between storage nodes.  If the Ceph cluster is already storing a lot of data, be prepared for some fraction of the data to move.
> - The `ceph-osd` and `ceph-mon` daemons will start requiring the feature bits of new connections as soon as they get the updated map.  However, already-connected clients are effectively grandfathered in, and will misbehave if they do not support the new feature.
> - If the CRUSH tunables are set to non-legacy values and then later changed back to the default values, `ceph-osd` daemons will not be required to support the feature.  However, the OSD peering process requires examining and understanding old maps.  Therefore, you should not run old versions of the `ceph-osd` daemon if the cluster has previously used non-legacy CRUSH values, even if the latest version of the map has been switched back to using the legacy defaults.

### Tuning CRUSH[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#tuning-crush)

The simplest way to adjust CRUSH tunables is by applying them in matched sets known as *profiles*.  As of the Octopus release these are:

> - `legacy`: the legacy behavior from argonaut and earlier.
> - `argonaut`: the legacy values supported by the original argonaut release
> - `bobtail`: the values supported by the bobtail release
> - `firefly`: the values supported by the firefly release
> - `hammer`: the values supported by the hammer release
> - `jewel`: the values supported by the jewel release
> - `optimal`: the best (ie optimal) values of the current version of Ceph
> - `default`: the default values of a new cluster installed from scratch. These values, which depend on the current version of Ceph, are hardcoded and are generally a mix of optimal and legacy values. These values generally match the `optimal` profile of the previous LTS release, or the most recent release for which we generally expect most users to have up-to-date clients for.

You can apply a profile to a running cluster with the command:

```
ceph osd crush tunables {PROFILE}
```

Note that this may result in data movement, potentially quite a bit.  Study release notes and documentation carefully before changing the profile on a running cluster, and consider throttling recovery/backfill parameters to limit the impact of a bolus of backfill.

## Primary Affinity[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#primary-affinity)

When a Ceph Client reads or writes data, it first contacts the primary OSD in each affected PG’s acting set. By default, the first OSD in the acting set is the primary.  For example, in the acting set `[2, 3, 4]`, `osd.2` is listed first and thus is the primary (aka lead) OSD. Sometimes we know that an OSD is less well suited to act as the lead than are other OSDs (e.g., it has a slow drive or a slow controller). To prevent performance bottlenecks (especially on read operations) while maximizing utilization of your hardware, you can influence the selection of primary OSDs by adjusting primary affinity values, or by crafting a CRUSH rule that selects preferred OSDs first.

Tuning primary OSD selection is mainly useful for replicated pools, because by default read operations are served from the primary OSD for each PG. For erasure coded (EC) pools, a way to speed up read operations is to enable **fast read** as described in [Pool settings](https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref/#pool-settings).

A common scenario for primary affinity is when a cluster contains a mix of drive sizes, for example older racks with 1.9 TB SATA SSDS and newer racks with 3.84TB SATA SSDs.  On average the latter will be assigned double the number of PGs and thus will serve double the number of write and read operations, thus they’ll be busier than the former.  A rough assignment of primary affinity inversely proportional to OSD size won’t be 100% optimal, but it can readily achieve a 15% improvement in overall read throughput by utilizing SATA interface bandwidth and CPU cycles more evenly.

By default, all ceph OSDs have primary affinity of `1`, which indicates that any OSD may act as a primary with equal probability.

You can reduce a Ceph OSD’s primary affinity so that CRUSH is less likely to choose the OSD as primary in a PG’s acting set.:

```
ceph osd primary-affinity <osd-id> <weight>
```

You may set an OSD’s primary affinity to a real number in the range `[0-1]`, where `0` indicates that the OSD may **NOT** be used as a primary and `1` indicates that an OSD may be used as a primary.  When the weight is between these extremes, it is less likely that CRUSH will select that OSD as a primary.  The process for selecting the lead OSD is more nuanced than a simple probability based on relative affinity values, but measurable results can be achieved even with first-order approximations of desirable values.

### Custom CRUSH Rules[](https://docs.ceph.com/en/latest/rados/operations/crush-map/#custom-crush-rules)

There are occasional clusters that balance cost and performance by mixing SSDs and HDDs in the same replicated pool. By setting the primary affinity of HDD OSDs to `0` one can direct operations to the SSD in each acting set. An alternative is to define a CRUSH rule that always selects an SSD OSD as the first OSD, then selects HDDs for the remaining OSDs. Thus, each PG’s acting set will contain exactly one SSD OSD as the primary with the balance on HDDs.

For example, the CRUSH rule below:

```
rule mixed_replicated_rule {
        id 11
        type replicated
        step take default class ssd
        step chooseleaf firstn 1 type host
        step emit
        step take default class hdd
        step chooseleaf firstn 0 type host
        step emit
}
```

chooses an SSD as the first OSD.  Note that for an `N`-times replicated pool this rule selects `N+1` OSDs to guarantee that `N` copies are on different hosts, because the first SSD OSD might be co-located with any of the `N` HDD OSDs.

This extra storage requirement can be avoided by placing SSDs and HDDs in different hosts with the tradeoff that hosts with SSDs will receive all client requests.  You may thus consider faster CPU(s) for SSD hosts and more modest ones for HDD nodes, since the latter will normally only service recovery operations.  Here the CRUSH roots `ssd_hosts` and `hdd_hosts` strictly must not contain the same servers:

```
rule mixed_replicated_rule_two {
       id 1
       type replicated
       step take ssd_hosts class ssd
       step chooseleaf firstn 1 type host
       step emit
       step take hdd_hosts class hdd
       step chooseleaf firstn -1 type host
       step emit
}
```

Note also that on failure of an SSD, requests to a PG will be served temporarily from a (slower) HDD OSD until the PG’s data has been replicated onto the replacement primary SSD OSD.

# Manually editing a CRUSH Map[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#manually-editing-a-crush-map)

Note

Manually editing the CRUSH map is an advanced administrator operation.  All CRUSH changes that are necessary for the overwhelming majority of installations are possible via the standard ceph CLI and do not require manual CRUSH map edits.  If you have identified a use case where manual edits *are* necessary with recent Ceph releases, consider contacting the Ceph developers so that future versions of Ceph can obviate your corner case.

To edit an existing CRUSH map:

1. [Get the CRUSH map](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#getcrushmap).
2. [Decompile](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#decompilecrushmap) the CRUSH map.
3. Edit at least one of [Devices](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crushmapdevices), [Buckets](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crushmapbuckets) and [Rules](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crushmaprules).
4. [Recompile](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#compilecrushmap) the CRUSH map.
5. [Set the CRUSH map](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#setcrushmap).

For details on setting the CRUSH map rule for a specific pool, see [Set Pool Values](https://docs.ceph.com/en/latest/rados/operations/pools#setpoolvalues).



## Get a CRUSH Map[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#get-a-crush-map)

To get the CRUSH map for your cluster, execute the following:

```
ceph osd getcrushmap -o {compiled-crushmap-filename}
```

Ceph will output (-o) a compiled CRUSH map to the filename you specified. Since the CRUSH map is in a compiled form, you must decompile it first before you can edit it.



## Decompile a CRUSH Map[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#decompile-a-crush-map)

To decompile a CRUSH map, execute the following:

```
crushtool -d {compiled-crushmap-filename} -o {decompiled-crushmap-filename}
```



## Recompile a CRUSH Map[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#recompile-a-crush-map)

To compile a CRUSH map, execute the following:

```
crushtool -c {decompiled-crushmap-filename} -o {compiled-crushmap-filename}
```



## Set the CRUSH Map[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#setcrushmap)

To set the CRUSH map for your cluster, execute the following:

```
ceph osd setcrushmap -i {compiled-crushmap-filename}
```

Ceph will load (-i) a compiled CRUSH map from the filename you specified.

## Sections[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#sections)

There are six main sections to a CRUSH Map.

1. **tunables:** The preamble at the top of the map describes any *tunables* that differ from the historical / legacy CRUSH behavior. These correct for old bugs, optimizations, or other changes that have been made over the years to improve CRUSH’s behavior.
2. **devices:** Devices are individual OSDs that store data.
3. **types**: Bucket `types` define the types of buckets used in your CRUSH hierarchy. Buckets consist of a hierarchical aggregation of storage locations (e.g., rows, racks, chassis, hosts, etc.) and their assigned weights.
4. **buckets:** Once you define bucket types, you must define each node in the hierarchy, its type, and which devices or other nodes it contains.
5. **rules:** Rules define policy about how data is distributed across devices in the hierarchy.
6. **choose_args:** Choose_args are alternative weights associated with the hierarchy that have been adjusted to optimize data placement.  A single choose_args map can be used for the entire cluster, or one can be created for each individual pool.



## CRUSH Map Devices[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crush-map-devices)

Devices are individual OSDs that store data.  Usually one is defined here for each OSD daemon in your cluster.  Devices are identified by an `id` (a non-negative integer) and a `name`, normally `osd.N` where `N` is the device id.

Devices may also have a *device class* associated with them (e.g., `hdd` or `ssd`), allowing them to be conveniently targeted by a crush rule.

```
# devices
device {num} {osd.name} [class {class}]
```

For example:

```
# devices
device 0 osd.0 class ssd
device 1 osd.1 class hdd
device 2 osd.2
device 3 osd.3
```

In most cases, each device maps to a single `ceph-osd` daemon.  This is normally a single storage device, a pair of devices (for example, one for data and one for a journal or metadata), or in some cases a small RAID device.

## CRUSH Map Bucket Types[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crush-map-bucket-types)

The second list in the CRUSH map defines ‘bucket’ types. Buckets facilitate a hierarchy of nodes and leaves. Node (or non-leaf) buckets typically represent physical locations in a hierarchy. Nodes aggregate other nodes or leaves. Leaf buckets represent `ceph-osd` daemons and their corresponding storage media.

Tip

The term “bucket” used in the context of CRUSH means a node in the hierarchy, i.e. a location or a piece of physical hardware. It is a different concept from the term “bucket” when used in the context of RADOS Gateway APIs.

To add a bucket type to the CRUSH map, create a new line under your list of bucket types. Enter `type` followed by a unique numeric ID and a bucket name. By convention, there is one leaf bucket and it is `type 0`;  however, you may give it any name you like (e.g., osd, disk, drive, storage, etc.):

```
#types
type {num} {bucket-name}
```

For example:

```
# types
type 0 osd
type 1 host
type 2 chassis
type 3 rack
type 4 row
type 5 pdu
type 6 pod
type 7 room
type 8 datacenter
type 9 zone
type 10 region
type 11 root
```



## CRUSH Map Bucket Hierarchy[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crush-map-bucket-hierarchy)

The CRUSH algorithm distributes data objects among storage devices according to a per-device weight value, approximating a uniform probability distribution. CRUSH distributes objects and their replicas according to the hierarchical cluster map you define. Your CRUSH map represents the available storage devices and the logical elements that contain them.

To map placement groups to OSDs across failure domains, a CRUSH map defines a hierarchical list of bucket types (i.e., under `#types` in the generated CRUSH map). The purpose of creating a bucket hierarchy is to segregate the leaf nodes by their failure domains, such as hosts, chassis, racks, power distribution units, pods, rows, rooms, and data centers. With the exception of the leaf nodes representing OSDs, the rest of the hierarchy is arbitrary, and you may define it according to your own needs.

We recommend adapting your CRUSH map to your firms’s hardware naming conventions and using instances names that reflect the physical hardware. Your naming practice can make it easier to administer the cluster and troubleshoot problems when an OSD and/or other hardware malfunctions and the administrator need access to physical hardware.

In the following example, the bucket hierarchy has a leaf bucket named `osd`, and two node buckets named `host` and `rack` respectively.

![img](https://docs.ceph.com/en/latest/_images/ditaa-c26439fc5272af69daf61b47959ad8b96c7792a1.png)

Note

The higher numbered `rack` bucket type aggregates the lower numbered `host` bucket type.

Since leaf nodes reflect storage devices declared under the `#devices` list at the beginning of the CRUSH map, you do not need to declare them as bucket instances. The second lowest bucket type in your hierarchy usually aggregates the devices (i.e., it’s usually the computer containing the storage media, and uses whatever term you prefer to describe it, such as  “node”, “computer”, “server,” “host”, “machine”, etc.). In high density environments, it is increasingly common to see multiple hosts/nodes per chassis. You should account for chassis failure too–e.g., the need to pull a chassis if a node fails may result in bringing down numerous hosts/nodes and their OSDs.

When declaring a bucket instance, you must specify its type, give it a unique name (string), assign it a unique ID expressed as a negative integer (optional), specify a weight relative to the total capacity/capability of its item(s), specify the bucket algorithm (usually `straw2`), and the hash (usually `0`, reflecting hash algorithm `rjenkins1`). A bucket may have one or more items. The items may consist of node buckets or leaves. Items may have a weight that reflects the relative weight of the item.

You may declare a node bucket with the following syntax:

```
[bucket-type] [bucket-name] {
        id [a unique negative numeric ID]
        weight [the relative capacity/capability of the item(s)]
        alg [the bucket type: uniform | list | tree | straw | straw2 ]
        hash [the hash type: 0 by default]
        item [item-name] weight [weight]
}
```

For example, using the diagram above, we would define two host buckets and one rack bucket. The OSDs are declared as items within the host buckets:

```
host node1 {
        id -1
        alg straw2
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

host node2 {
        id -2
        alg straw2
        hash 0
        item osd.2 weight 1.00
        item osd.3 weight 1.00
}

rack rack1 {
        id -3
        alg straw2
        hash 0
        item node1 weight 2.00
        item node2 weight 2.00
}
```

Note

In the foregoing example, note that the rack bucket does not contain any OSDs. Rather it contains lower level host buckets, and includes the sum total of their weight in the item entry.

Bucket Types

Ceph supports five bucket types, each representing a tradeoff between performance and reorganization efficiency. If you are unsure of which bucket type to use, we recommend using a `straw2` bucket.  For a detailed discussion of bucket types, refer to [CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data](https://ceph.com/assets/pdfs/weil-crush-sc06.pdf), and more specifically to **Section 3.4**. The bucket types are:

> 1. **uniform**: Uniform buckets aggregate devices with **exactly** the same weight. For example, when firms commission or decommission hardware, they typically do so with many machines that have exactly the same physical configuration (e.g., bulk purchases). When storage devices have exactly the same weight, you may use the `uniform` bucket type, which allows CRUSH to map replicas into uniform buckets in constant time. With non-uniform weights, you should use another bucket algorithm.
>
> 2. **list**: List buckets aggregate their content as linked lists. Based on the RUSH P algorithm, a list is a natural and intuitive choice for an **expanding cluster**: either an object is relocated to the newest device with some appropriate probability, or it remains on the older devices as before. The result is optimal data migration when items are added to the bucket. Items removed from the middle or tail of the list, however, can result in a signiﬁcant amount of unnecessary movement, making list buckets most suitable for circumstances in which they **never (or very rarely) shrink**.
>
> 3. **tree**: Tree buckets use a binary search tree. They are more efficient than list buckets when a bucket contains a larger set of items. Based on the RUSH R algorithm, tree buckets reduce the placement time to O(log n), making them suitable for managing much larger sets of devices or nested buckets.
>
> 4. **straw**: List and Tree buckets use a divide and conquer strategy in a way that either gives certain items precedence (e.g., those at the beginning of a list) or obviates the need to consider entire subtrees of items at all. That improves the performance of the replica placement process, but can also introduce suboptimal reorganization behavior when the contents of a bucket change due an addition, removal, or re-weighting of an item. The straw bucket type allows all items to fairly “compete” against each other for replica placement through a process analogous to a draw of straws.
>
> 5. **straw2**: Straw2 buckets improve Straw to correctly avoid any data movement between items when neighbor weights change.
>
>    For example the weight of item A including adding it anew or removing it completely, there will be data movement only to or from item A.

Hash

Each bucket uses a hash algorithm. Currently, Ceph supports `rjenkins1`. Enter `0` as your hash setting to select `rjenkins1`.

Weighting Bucket Items

Ceph expresses bucket weights as doubles, which allows for fine weighting. A weight is the relative difference between device capacities. We recommend using `1.00` as the relative weight for a 1TB storage device. In such a scenario, a weight of `0.5` would represent approximately 500GB, and a weight of `3.00` would represent approximately 3TB. Higher level buckets have a weight that is the sum total of the leaf items aggregated by the bucket.

A bucket item weight is one dimensional, but you may also calculate your item weights to reflect the performance of the storage drive. For example, if you have many 1TB drives where some have relatively low data transfer rate and the others have a relatively high data transfer rate, you may weight them differently, even though they have the same capacity (e.g., a weight of 0.80 for the first set of drives with lower total throughput, and 1.20 for the second set of drives with higher total throughput).



## CRUSH Map Rules[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#crush-map-rules)

CRUSH maps support the notion of ‘CRUSH rules’, which are the rules that determine data placement for a pool. The default CRUSH map has a rule for each pool. For large clusters, you will likely create many pools where each pool may have its own non-default CRUSH rule.

Note

In most cases, you will not need to modify the default rule. When you create a new pool, by default the rule will be set to `0`.

CRUSH rules define placement and replication strategies or distribution policies that allow you to specify exactly how CRUSH places object replicas. For example, you might create a rule selecting a pair of targets for 2-way mirroring, another rule for selecting three targets in two different data centers for 3-way mirroring, and yet another rule for erasure coding over six storage devices. For a detailed discussion of CRUSH rules, refer to [CRUSH - Controlled, Scalable, Decentralized Placement of Replicated Data](https://ceph.com/assets/pdfs/weil-crush-sc06.pdf), and more specifically to **Section 3.2**.

A rule takes the following form:

```
rule <rulename> {

        id [a unique whole numeric ID]
        type [ replicated | erasure ]
        step take <bucket-name> [class <device-class>]
        step [choose|chooseleaf] [firstn|indep] <N> type <bucket-type>
        step emit
}
id
```

- Description

  A unique whole number for identifying the rule.

- Purpose

  A component of the rule mask.

- Type

  Integer

- Required

  Yes

- Default

  0

```
type
```

- Description

  Describes a rule for either a storage drive (replicated) or a RAID.

- Purpose

  A component of the rule mask.

- Type

  String

- Required

  Yes

- Default

  `replicated`

- Valid Values

  Currently only `replicated` and `erasure`

```
step take <bucket-name> [class <device-class>]
```

- Description

  Takes a bucket name, and begins iterating down the tree. If the `device-class` is specified, it must match a class previously used when defining a device. All devices that do not belong to the class are excluded.

- Purpose

  A component of the rule.

- Required

  Yes

- Example

  `step take data`

```
step choose firstn {num} type {bucket-type}
```

- Description

  Selects the number of buckets of the given type from within the current bucket. The number is usually the number of replicas in the pool (i.e., pool size). If `{num} == 0`, choose `pool-num-replicas` buckets (all available). If `{num} > 0 && < pool-num-replicas`, choose that many buckets. If `{num} < 0`, it means `pool-num-replicas - {num}`.

- Purpose

  A component of the rule.

- Prerequisite

  Follows `step take` or `step choose`.

- Example

  `step choose firstn 1 type row`

```
step chooseleaf firstn {num} type {bucket-type}
```

- Description

  Selects a set of buckets of `{bucket-type}` and chooses a leaf node (that is, an OSD) from the subtree of each bucket in the set of buckets. The number of buckets in the set is usually the number of replicas in the pool (i.e., pool size). If `{num} == 0`, choose `pool-num-replicas` buckets (all available). If `{num} > 0 && < pool-num-replicas`, choose that many buckets. If `{num} < 0`, it means `pool-num-replicas - {num}`.

- Purpose

  A component of the rule. Usage removes the need to select a device using two steps.

- Prerequisite

  Follows `step take` or `step choose`.

- Example

  `step chooseleaf firstn 0 type row`

```
step emit
```

- Description

  Outputs the current value and empties the stack. Typically used at the end of a rule, but may also be used to pick from different trees in the same rule.

- Purpose

  A component of the rule.

- Prerequisite

  Follows `step choose`.

- Example

  `step emit`

Important

A given CRUSH rule may be assigned to multiple pools, but it is not possible for a single pool to have multiple CRUSH rules.

```
firstn` versus `indep
```

- Description

  Controls the replacement strategy CRUSH uses when items (OSDs) are marked down in the CRUSH map. If this rule is to be used with replicated pools it should be `firstn` and if it’s for erasure-coded pools it should be `indep`. The reason has to do with how they behave when a previously-selected device fails. Let’s say you have a PG stored on OSDs 1, 2, 3, 4, 5. Then 3 goes down. With the “firstn” mode, CRUSH simply adjusts its calculation to select 1 and 2, then selects 3 but discovers it’s down, so it retries and selects 4 and 5, and then goes on to select a new OSD 6. So the final CRUSH mapping change is 1, 2, 3, 4, 5 -> 1, 2, 4, 5, 6. But if you’re storing an EC pool, that means you just changed the data mapped to OSDs 4, 5, and 6! So the “indep” mode attempts to not do that. You can instead expect it, when it selects the failed OSD 3, to try again and pick out 6, for a final transformation of: 1, 2, 3, 4, 5 -> 1, 2, 6, 4, 5



## Migrating from a legacy SSD rule to device classes[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#migrating-from-a-legacy-ssd-rule-to-device-classes)

It used to be necessary to manually edit your CRUSH map and maintain a parallel hierarchy for each specialized device type (e.g., SSD) in order to write rules that apply to those devices.  Since the Luminous release, the *device class* feature has enabled this transparently.

However, migrating from an existing, manually customized per-device map to the new device class rules in the trivial way will cause all data in the system to be reshuffled.

The `crushtool` has a few commands that can transform a legacy rule and hierarchy so that you can start using the new class-based rules. There are three types of transformations possible:

1. `--reclassify-root <root-name> <device-class>`

   This will take everything in the hierarchy beneath root-name and adjust any rules that reference that root via a `take <root-name>` to instead `take <root-name> class <device-class>`. It renumbers the buckets in such a way that the old IDs are instead used for the specified class’s “shadow tree” so that no data movement takes place.

   For example, imagine you have an existing rule like:

   ```
   rule replicated_rule {
      id 0
      type replicated
      step take default
      step chooseleaf firstn 0 type rack
      step emit
   }
   ```

   If you reclassify the root default as class hdd, the rule will become:

   ```
   rule replicated_rule {
      id 0
      type replicated
      step take default class hdd
      step chooseleaf firstn 0 type rack
      step emit
   }
   ```

2. `--set-subtree-class <bucket-name> <device-class>`

   This will mark every device in the subtree rooted at *bucket-name* with the specified device class.

   This is normally used in conjunction with the `--reclassify-root` option to ensure that all devices in that root are labeled with the correct class.  In some situations, however, some of those devices (correctly) have a different class and we do not want to relabel them.  In such cases, one can exclude the `--set-subtree-class` option.  This means that the remapping process will not be perfect, since the previous rule distributed across devices of multiple classes but the adjusted rules will only map to devices of the specified *device-class*, but that often is an accepted level of data movement when the number of outlier devices is small.

3. `--reclassify-bucket <match-pattern> <device-class> <default-parent>`

   This will allow you to merge a parallel type-specific hierarchy with  the normal hierarchy.  For example, many users have maps like:

   ```
   host node1 {
      id -2           # do not change unnecessarily
      # weight 109.152
      alg straw2
      hash 0  # rjenkins1
      item osd.0 weight 9.096
      item osd.1 weight 9.096
      item osd.2 weight 9.096
      item osd.3 weight 9.096
      item osd.4 weight 9.096
      item osd.5 weight 9.096
      ...
   }
   
   host node1-ssd {
      id -10          # do not change unnecessarily
      # weight 2.000
      alg straw2
      hash 0  # rjenkins1
      item osd.80 weight 2.000
      ...
   }
   
   root default {
      id -1           # do not change unnecessarily
      alg straw2
      hash 0  # rjenkins1
      item node1 weight 110.967
      ...
   }
   
   root ssd {
      id -18          # do not change unnecessarily
      # weight 16.000
      alg straw2
      hash 0  # rjenkins1
      item node1-ssd weight 2.000
      ...
   }
   ```

   This function will reclassify each bucket that matches a pattern.  The pattern can look like `%suffix` or `prefix%`. For example, in the above example, we would use the pattern `%-ssd`.  For each matched bucket, the remaining portion of the name (that matches the `%` wildcard) specifies the *base bucket*. All devices in the matched bucket are labeled with the specified device class and then moved to the base bucket.  If the base bucket does not exist (e.g., `node12-ssd` exists but `node12` does not), then it is created and linked underneath the specified *default parent* bucket.  In each case, we are careful to preserve the old bucket IDs for the new shadow buckets to prevent data movement.  Any rules with `take` steps referencing the old buckets are adjusted.

4. `--reclassify-bucket <bucket-name> <device-class> <base-bucket>`

   The same command can also be used without a wildcard to map a single bucket.  For example, in the previous example, we want the `ssd` bucket to be mapped to the `default` bucket.

The final command to convert the map comprised of the above fragments would be something like:

```
$ ceph osd getcrushmap -o original
$ crushtool -i original --reclassify \
    --set-subtree-class default hdd \
    --reclassify-root default hdd \
    --reclassify-bucket %-ssd ssd default \
    --reclassify-bucket ssd ssd default \
    -o adjusted
```

In order to ensure that the conversion is correct, there is a `--compare` command that will test a large sample of inputs to the CRUSH map and  ensure that the same result comes back out.  These inputs are controlled by the same options that apply to the `--test` command.  For the above example,:

```
$ crushtool -i original --compare adjusted
rule 0 had 0/10240 mismatched mappings (0)
rule 1 had 0/10240 mismatched mappings (0)
maps appear equivalent
```

If there were difference, you’d see what ratio of inputs are remapped in the parentheses.

If you are satisfied with the adjusted map, you can apply it to the cluster with something like:

```
ceph osd setcrushmap -i adjusted
```

## Tuning CRUSH, the hard way[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#tuning-crush-the-hard-way)

If you can ensure that all clients are running recent code, you can adjust the tunables by extracting the CRUSH map, modifying the values, and reinjecting it into the cluster.

- Extract the latest CRUSH map:

  ```
  ceph osd getcrushmap -o /tmp/crush
  ```

- Adjust tunables.  These values appear to offer the best behavior for both large and small clusters we tested with.  You will need to additionally specify the `--enable-unsafe-tunables` argument to `crushtool` for this to work.  Please use this option with extreme care.:

  ```
  crushtool -i /tmp/crush --set-choose-local-tries 0 --set-choose-local-fallback-tries 0 --set-choose-total-tries 50 -o /tmp/crush.new
  ```

- Reinject modified map:

  ```
  ceph osd setcrushmap -i /tmp/crush.new
  ```

## Legacy values[](https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/#legacy-values)

For reference, the legacy values for the CRUSH tunables can be set with:

```
crushtool -i /tmp/crush --set-choose-local-tries 2 --set-choose-local-fallback-tries 5 --set-choose-total-tries 19 --set-chooseleaf-descend-once 0 --set-chooseleaf-vary-r 0 -o /tmp/crush.legacy
```

Again, the special `--enable-unsafe-tunables` option is required. Further, as noted above, be careful running old versions of the `ceph-osd` daemon after reverting to legacy values as the feature bit is not perfectly enforced.

# Stretch Clusters[](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/#stretch-clusters)

## Stretch Clusters[](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/#id1)

Ceph generally expects all parts of its network and overall cluster to be equally reliable, with failures randomly distributed across the CRUSH map. So you may lose a switch that knocks out a number of OSDs, but we expect the remaining OSDs and monitors to route around that.

This is usually a good choice, but may not work well in some stretched cluster configurations where a significant part of your cluster is stuck behind a single network component. For instance, a single cluster which is located in multiple data centers, and you want to sustain the loss of a full DC.

There are two standard configurations we’ve seen deployed, with either two or three data centers (or, in clouds, availability zones). With two zones, we expect each site to hold a copy of the data, and for a third site to have a tiebreaker monitor (this can be a VM or high-latency compared to the main sites) to pick a winner if the network connection fails and both DCs remain alive. For three sites, we expect a copy of the data and an equal number of monitors in each site.

Note that the standard Ceph configuration will survive MANY failures of the network or data centers and it will never compromise data consistency.  If you bring back enough Ceph servers following a failure, it will recover. If you lose a data center, but can still form a quorum of monitors and have all the data available (with enough copies to satisfy pools’ `min_size`, or CRUSH rules that will re-replicate to meet it), Ceph will maintain availability.

What can’t it handle?

## Stretch Cluster Issues[](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/#stretch-cluster-issues)

No matter what happens, Ceph will not compromise on data integrity and consistency. If there’s a failure in your network or a loss of nodes and you can restore service, Ceph will return to normal functionality on its own.

But there are scenarios where you lose data availability despite having enough servers available to satisfy Ceph’s consistency and sizing constraints, or where you may be surprised to not satisfy Ceph’s constraints. The first important category of these failures resolve around inconsistent networks – if there’s a netsplit, Ceph may be unable to mark OSDs down and kick them out of the acting PG sets despite the primary being unable to replicate data. If this happens, IO will not be permitted, because Ceph can’t satisfy its durability guarantees.

The second important category of failures is when you think you have data replicated across data centers, but the constraints aren’t sufficient to guarantee this. For instance, you might have data centers A and B, and your CRUSH rule targets 3 copies and places a copy in each data center with a `min_size` of 2. The PG may go active with 2 copies in site A and no copies in site B, which means that if you then lose site A you have lost data and Ceph can’t operate on it. This situation is surprisingly difficult to avoid with standard CRUSH rules.

## Stretch Mode[](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/#id2)

The new stretch mode is designed to handle the 2-site case. Three sites are just as susceptible to netsplit issues, but are much more tolerant of component availability outages than 2-site clusters are.

To enter stretch mode, you must set the location of each monitor, matching your CRUSH map. For instance, to place `mon.a` in your first data center

```
$ ceph mon set_location a datacenter=site1
```

Next, generate a CRUSH rule which will place 2 copies in each data center. This will require editing the CRUSH map directly:

```
$ ceph osd getcrushmap > crush.map.bin
$ crushtool -d crush.map.bin -o crush.map.txt
```

Now edit the `crush.map.txt` file to add a new rule. Here there is only one other rule, so this is ID 1, but you may need to use a different rule ID. We also have two datacenter buckets named `site1` and `site2`:

```
rule stretch_rule {
        id 1
        min_size 1
        max_size 10
        type replicated
        step take site1
        step chooseleaf firstn 2 type host
        step emit
        step take site2
        step chooseleaf firstn 2 type host
        step emit
}
```

Finally, inject the CRUSH map to make the rule available to the cluster:

```
$ crushtool -c crush.map.txt -o crush2.map.bin
$ ceph osd setcrushmap -i crush2.map.bin
```

If you aren’t already running your monitors in connectivity mode, do so with the instructions in [Changing Monitor Elections](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections).

And lastly, tell the cluster to enter stretch mode. Here, `mon.e` is the tiebreaker and we are splitting across data centers. `mon.e` should be also set a datacenter, that will differ from `site1` and `site2`. For this purpose you can create another datacenter bucket named ``site3` in your CRUSH and place `mon.e` there

```
$ ceph mon set_location e datacenter=site3
$ ceph mon enable_stretch_mode e stretch_rule datacenter
```

When stretch mode is enabled, the OSDs will only take PGs active when they peer across data centers (or whatever other CRUSH bucket type you specified), assuming both are alive. Pools will increase in size from the default 3 to 4, expecting 2 copies in each site. OSDs will only be allowed to connect to monitors in the same data center. New monitors will not be allowed to join the cluster if they do not specify a location.

If all the OSDs and monitors from a data center become inaccessible at once, the surviving data center will enter a degraded stretch mode. This will issue a warning, reduce the min_size to 1, and allow the cluster to go active with data in the single remaining site. Note that we do not change the pool size, so you will also get warnings that the pools are too small – but a special stretch mode flag will prevent the OSDs from creating extra copies in the remaining data center (so it will only keep 2 copies, as before).

When the missing data center comes back, the cluster will enter recovery stretch mode. This changes the warning and allows peering, but still only requires OSDs from the data center which was up the whole time. When all PGs are in a known state, and are neither degraded nor incomplete, the cluster transitions back to regular stretch mode, ends the warning, restores min_size to its starting value (2) and requires both sites to peer, and stops requiring the always-alive site when peering (so that you can fail over to the other site, if necessary).

## Stretch Mode Limitations[](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/#stretch-mode-limitations)

As implied by the setup, stretch mode only handles 2 sites with OSDs.

While it is not enforced, you should run 2 monitors in each site plus a tiebreaker, for a total of 5. This is because OSDs can only connect to monitors in their own site when in stretch mode.

You cannot use erasure coded pools with stretch mode. If you try, it will refuse, and it will not allow you to create EC pools once in stretch mode.

You must create your own CRUSH rule which provides 2 copies in each site, and you must use 4 total copies with 2 in each site. If you have existing pools with non-default size/min_size, Ceph will object when you attempt to enable stretch mode.

Because it runs with `min_size 1` when degraded, you should only use stretch mode with all-flash OSDs.  This minimizes the time needed to recover once connectivity is restored, and thus minimizes the potential for data loss.

Hopefully, future development will extend this feature to support EC pools and running with more than 2 full sites.

## Other commands[](https://docs.ceph.com/en/latest/rados/operations/stretch-mode/#other-commands)

If your tiebreaker monitor fails for some reason, you can replace it. Turn on a new monitor and run

```
$ ceph mon set_new_tiebreaker mon.<new_mon_name>
```

This command will protest if the new monitor is in the same location as existing non-tiebreaker monitors. This command WILL NOT remove the previous tiebreaker monitor; you should do so yourself.

If you are writing your own tooling for deploying Ceph, you can use a new `--set-crush-location` option when booting monitors, instead of running `ceph mon set_location`. This option accepts only a single “bucket=loc” pair, eg `ceph-mon --set-crush-location 'datacenter=a'`, which must match the bucket type you specified when running `enable_stretch_mode`.

When in stretch degraded mode, the cluster will go into “recovery” mode automatically when the disconnected data center comes back. If that doesn’t work, or you want to enable recovery mode early, you can invoke

```
$ ceph osd force_recovery_stretch_mode --yes-i-really-mean-it
```

But this command should not be necessary; it is included to deal with unanticipated situations.

When in recovery mode, the cluster should go back into normal stretch mode when the PGs are healthy. If this doesn’t happen, or you want to force the cross-data-center peering early and are willing to risk data downtime (or have verified separately that all the PGs can peer, even if they aren’t fully recovered), you can invoke

```
$ ceph osd force_healthy_stretch_mode --yes-i-really-mean-it
```

This command should not be necessary; it is included to deal with unanticipated situations. But you might wish to invoke it to remove the `HEALTH_WARN` state which recovery mode generates.

# Configure Monitor Election Strategies[](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections/#configure-monitor-election-strategies)

By default, the monitors will use the `classic` mode. We recommend that you stay in this mode unless you have a very specific reason.

If you want to switch modes BEFORE constructing the cluster, change the `mon election default strategy` option. This option is an integer value:

- 1 for “classic”
- 2 for “disallow”
- 3 for “connectivity”

Once your cluster is running, you can change strategies by running

```
$ ceph mon set election_strategy {classic|disallow|connectivity}
```

## Choosing a mode[](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections/#choosing-a-mode)

The modes other than classic provide different features. We recommend you stay in classic mode if you don’t need the extra features as it is the simplest mode.

## The disallow Mode[](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections/#the-disallow-mode)

This mode lets you mark monitors as disallowd, in which case they will participate in the quorum and serve clients, but cannot be elected leader. You may wish to use this if you have some monitors which are known to be far away from clients. You can disallow a leader by running

```
$ ceph mon add disallowed_leader {name}
```

You can remove a monitor from the disallowed list, and allow it to become a leader again, by running

```
$ ceph mon rm disallowed_leader {name}
```

The list of disallowed_leaders is included when you run

```
$ ceph mon dump
```

## The connectivity Mode[](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections/#the-connectivity-mode)

This mode evaluates connection scores provided by each monitor for its peers and elects the monitor with the highest score. This mode is designed to handle network partitioning or *net-splits*, which may happen if your cluster is stretched across multiple data centers or otherwise has a non-uniform or unbalanced network topology.

This mode also supports disallowing monitors from being the leader using the same commands as above in disallow.

## Examining connectivity scores[](https://docs.ceph.com/en/latest/rados/operations/change-mon-elections/#examining-connectivity-scores)

The monitors maintain connection scores even if they aren’t in the connectivity election mode. You can examine the scores a monitor has by running

```
ceph daemon mon.{name} connection scores dump
```

Scores for individual connections range from 0-1 inclusive, and also include whether the connection is considered alive or dead (determined by whether it returned its latest ping within the timeout).

While this would be an unexpected occurrence, if for some reason you experience problems and troubleshooting makes you think your scores have become invalid, you can forget history and reset them by running

```
ceph daemon mon.{name} connection scores reset
```

While resetting scores has low risk (monitors will still quickly determine if a connection is alive or dead, and trend back to the previous scores if they were accurate!), it should also not be needed and is not recommended unless requested by your support team or a developer.

# Adding/Removing OSDs[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#adding-removing-osds)

When you have a cluster up and running, you may add OSDs or remove OSDs from the cluster at runtime.

## Adding OSDs[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#adding-osds)

When you want to expand a cluster, you may add an OSD at runtime. With Ceph, an OSD is generally one Ceph `ceph-osd` daemon for one storage drive within a host machine. If your host has multiple storage drives, you may map one `ceph-osd` daemon for each drive.

Generally, it’s a good idea to check the capacity of your cluster to see if you are reaching the upper end of its capacity. As your cluster reaches its `near full` ratio, you should add one or more OSDs to expand your cluster’s capacity.

Warning

Do not let your cluster reach its `full ratio` before adding an OSD. OSD failures that occur after the cluster reaches its `near full` ratio may cause the cluster to exceed its `full ratio`.

### Deploy your Hardware[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#deploy-your-hardware)

If you are adding a new host when adding a new OSD,  see [Hardware Recommendations](https://docs.ceph.com/en/latest/start/hardware-recommendations) for details on minimum recommendations for OSD hardware. To add an OSD host to your cluster, first make sure you have an up-to-date version of Linux installed, and you have made some initial preparations for your storage drives.  See [Filesystem Recommendations](https://docs.ceph.com/en/latest/rados/configuration/filesystem-recommendations) for details.

Add your OSD host to a rack in your cluster, connect it to the network and ensure that it has network connectivity. See the [Network Configuration Reference](https://docs.ceph.com/en/latest/rados/configuration/network-config-ref) for details.

### Install the Required Software[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#install-the-required-software)

For manually deployed clusters, you must install Ceph packages manually. See [Installing Ceph (Manual)](https://docs.ceph.com/en/latest/install) for details. You should configure SSH to a user with password-less authentication and root permissions.

### Adding an OSD (Manual)[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#adding-an-osd-manual)

This procedure sets up a `ceph-osd` daemon, configures it to use one drive, and configures the cluster to distribute data to the OSD. If your host has multiple drives, you may add an OSD for each drive by repeating this procedure.

To add an OSD, create a data directory for it, mount a drive to that directory, add the OSD to the cluster, and then add it to the CRUSH map.

When you add the OSD to the CRUSH map, consider the weight you give to the new OSD. Hard drive capacity grows 40% per year, so newer OSD hosts may have larger hard drives than older hosts in the cluster (i.e., they may have greater weight).

Tip

Ceph prefers uniform hardware across pools. If you are adding drives of dissimilar size, you can adjust their weights. However, for best performance, consider a CRUSH hierarchy with drives of the same type/size.

1. Create the OSD. If no UUID is given, it will be set automatically when the OSD starts up. The following command will output the OSD number, which you will need for subsequent steps.

   ```
   ceph osd create [{uuid} [{id}]]
   ```

   If the optional parameter {id} is given it will be used as the OSD id. Note, in this case the command may fail if the number is already in use.

   Warning

   In general, explicitly specifying {id} is not recommended. IDs are allocated as an array, and skipping entries consumes some extra memory. This can become significant if there are large gaps and/or clusters are large. If {id} is not specified, the smallest available is used.

2. Create the default directory on your new OSD.

   ```
   ssh {new-osd-host}
   sudo mkdir /var/lib/ceph/osd/ceph-{osd-number}
   ```

3. If the OSD is for a drive other than the OS drive, prepare it for use with Ceph, and mount it to the directory you just created:

   ```
   ssh {new-osd-host}
   sudo mkfs -t {fstype} /dev/{drive}
   sudo mount -o user_xattr /dev/{hdd} /var/lib/ceph/osd/ceph-{osd-number}
   ```

4. Initialize the OSD data directory.

   ```
   ssh {new-osd-host}
   ceph-osd -i {osd-num} --mkfs --mkkey
   ```

   The directory must be empty before you can run `ceph-osd`.

5. Register the OSD authentication key. The value of `ceph` for `ceph-{osd-num}` in the path is the `$cluster-$id`.  If your cluster name differs from `ceph`, use your cluster name instead.:

   ```
   ceph auth add osd.{osd-num} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-num}/keyring
   ```

6. Add the OSD to the CRUSH map so that the OSD can begin receiving data. The `ceph osd crush add` command allows you to add OSDs to the CRUSH hierarchy wherever you wish. If you specify at least one bucket, the command will place the OSD into the most specific bucket you specify, *and* it will move that bucket underneath any other buckets you specify. **Important:** If you specify only the root bucket, the command will attach the OSD directly to the root, but CRUSH rules expect OSDs to be inside of hosts.

   Execute the following:

   ```
   ceph osd crush add {id-or-name} {weight}  [{bucket-type}={bucket-name} ...]
   ```

   You may also decompile the CRUSH map, add the OSD to the device list, add the host as a bucket (if it’s not already in the CRUSH map), add the device as an item in the host, assign it a weight, recompile it and set it. See [Add/Move an OSD](https://docs.ceph.com/en/latest/rados/operations/crush-map#addosd) for details.



### Replacing an OSD[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#replacing-an-osd)

When disks fail, or if an administrator wants to reprovision OSDs with a new backend, for instance, for switching from FileStore to BlueStore, OSDs need to be replaced. Unlike [Removing the OSD](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#removing-the-osd), replaced OSD’s id and CRUSH map entry need to be keep intact after the OSD is destroyed for replacement.

1. Make sure it is safe to destroy the OSD:

   ```
   while ! ceph osd safe-to-destroy osd.{id} ; do sleep 10 ; done
   ```

2. Destroy the OSD first:

   ```
   ceph osd destroy {id} --yes-i-really-mean-it
   ```

3. Zap a disk for the new OSD, if the disk was used before for other purposes. It’s not necessary for a new disk:

   ```
   ceph-volume lvm zap /dev/sdX
   ```

4. Prepare the disk for replacement by using the previously destroyed OSD id:

   ```
   ceph-volume lvm prepare --osd-id {id} --data /dev/sdX
   ```

5. And activate the OSD:

   ```
   ceph-volume lvm activate {id} {fsid}
   ```

Alternatively, instead of preparing and activating, the device can be recreated in one call, like:

```
ceph-volume lvm create --osd-id {id} --data /dev/sdX
```

### Starting the OSD[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#starting-the-osd)

After you add an OSD to Ceph, the OSD is in your configuration. However, it is not yet running. The OSD is `down` and `in`. You must start your new OSD before it can begin receiving data. You may use `service ceph` from your admin host or start the OSD from its host machine:

```
sudo systemctl start ceph-osd@{osd-num}
```

Once you start your OSD, it is `up` and `in`.

### Observe the Data Migration[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#observe-the-data-migration)

Once you have added your new OSD to the CRUSH map, Ceph  will begin rebalancing the server by migrating placement groups to your new OSD. You can observe this process with  the [ceph](https://docs.ceph.com/en/latest/rados/operations/monitoring) tool.

```
ceph -w
```

You should see the placement group states change from `active+clean` to `active, some degraded objects`, and finally `active+clean` when migration completes. (Control-c to exit.)

## Removing OSDs (Manual)[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#removing-osds-manual)

When you want to reduce the size of a cluster or replace hardware, you may remove an OSD at runtime. With Ceph, an OSD is generally one Ceph `ceph-osd` daemon for one storage drive within a host machine. If your host has multiple storage drives, you may need to remove one `ceph-osd` daemon for each drive. Generally, it’s a good idea to check the capacity of your cluster to see if you are reaching the upper end of its capacity. Ensure that when you remove an OSD that your cluster is not at its `near full` ratio.

Warning

Do not let your cluster reach its `full ratio` when removing an OSD. Removing OSDs could cause the cluster to reach or exceed its `full ratio`.

### Take the OSD out of the Cluster[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#take-the-osd-out-of-the-cluster)

Before you remove an OSD, it is usually `up` and `in`.  You need to take it out of the cluster so that Ceph can begin rebalancing and copying its data to other OSDs.

```
ceph osd out {osd-num}
```

### Observe the Data Migration[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#id1)

Once you have taken your OSD `out` of the cluster, Ceph  will begin rebalancing the cluster by migrating placement groups out of the OSD you removed. You can observe  this process with  the [ceph](https://docs.ceph.com/en/latest/rados/operations/monitoring) tool.

```
ceph -w
```

You should see the placement group states change from `active+clean` to `active, some degraded objects`, and finally `active+clean` when migration completes. (Control-c to exit.)

Note

Sometimes, typically in a “small” cluster with few hosts (for instance with a small testing cluster), the fact to take `out` the OSD can spawn a CRUSH corner case where some PGs remain stuck in the `active+remapped` state. If you are in this case, you should mark the OSD `in` with:

> ```
> ceph osd in {osd-num}
> ```

to come back to the initial state and then, instead of marking `out` the OSD, set its weight to 0 with:

> ```
> ceph osd crush reweight osd.{osd-num} 0
> ```

After that, you can observe the data migration which should come to its end. The difference between marking `out` the OSD and reweighting it to 0 is that in the first case the weight of the bucket which contains the OSD is not changed whereas in the second case the weight of the bucket is updated (and decreased of the OSD weight). The reweight command could be sometimes favoured in the case of a “small” cluster.

### Stopping the OSD[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#stopping-the-osd)

After you take an OSD out of the cluster, it may still be running. That is, the OSD may be `up` and `out`. You must stop your OSD before you remove it from the configuration.

```
ssh {osd-host}
sudo systemctl stop ceph-osd@{osd-num}
```

Once you stop your OSD, it is `down`.

### Removing the OSD[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/#removing-the-osd)

This procedure removes an OSD from a cluster map, removes its authentication key, removes the OSD from the OSD map, and removes the OSD from the `ceph.conf` file. If your host has multiple drives, you may need to remove an OSD for each drive by repeating this procedure.

1. Let the cluster forget the OSD first. This step removes the OSD from the CRUSH map, removes its authentication key. And it is removed from the OSD map as well. Please note the [purge subcommand](https://docs.ceph.com/en/latest/man/8/ceph/#ceph-admin-osd) is introduced in Luminous, for older versions, please see below

   ```
   ceph osd purge {id} --yes-i-really-mean-it
   ```

2. Navigate to the host where you keep the master copy of the cluster’s `ceph.conf` file.

   ```
   ssh {admin-host}
   cd /etc/ceph
   vim ceph.conf
   ```

3. Remove the OSD entry from your `ceph.conf` file (if it exists).

   ```
   [osd.1]
           host = {hostname}
   ```

4. From the host where you keep the master copy of the cluster’s `ceph.conf` file, copy the updated `ceph.conf` file to the `/etc/ceph` directory of other hosts in your cluster.

If your Ceph cluster is older than Luminous, instead of using `ceph osd purge`, you need to perform this step manually:

1. Remove the OSD from the CRUSH map so that it no longer receives data. You may also decompile the CRUSH map, remove the OSD from the device list, remove the device as an item in the host bucket or remove the host  bucket (if it’s in the CRUSH map and you intend to remove the host), recompile the map and set it. See [Remove an OSD](https://docs.ceph.com/en/latest/rados/operations/crush-map#removeosd) for details.

   ```
   ceph osd crush remove {name}
   ```

2. Remove the OSD authentication key.

   ```
   ceph auth del osd.{osd-num}
   ```

   The value of `ceph` for `ceph-{osd-num}` in the path is the `$cluster-$id`. If your cluster name differs from `ceph`, use your cluster name instead.

3. Remove the OSD.

   ```
   ceph osd rm {osd-num}
   #for example
   ceph osd rm 1
   ```

​        

# Adding/Removing Monitors[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#adding-removing-monitors)

When you have a cluster up and running, you may add or remove monitors from the cluster at runtime. To bootstrap a monitor, see [Manual Deployment](https://docs.ceph.com/en/latest/install/manual-deployment) or [Monitor Bootstrap](https://docs.ceph.com/en/latest/dev/mon-bootstrap).



## Adding Monitors[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#adding-monitors)

Ceph monitors are lightweight processes that are the single source of truth for the cluster map. You can run a cluster with 1 monitor but we recommend at least 3 for a production cluster. Ceph monitors use a variation of the [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science)) algorithm to establish consensus about maps and other critical information across the cluster. Due to the nature of Paxos, Ceph requires a majority of monitors to be active to establish a quorum (thus establishing consensus).

It is advisable to run an odd number of monitors. An odd number of monitors is more resilient than an even number. For instance, with a two monitor deployment, no failures can be tolerated and still maintain a quorum; with three monitors, one failure can be tolerated; in a four monitor deployment, one failure can be tolerated; with five monitors, two failures can be tolerated.  This avoids the dreaded *split brain* phenomenon, and is why an odd number is best. In short, Ceph needs a majority of monitors to be active (and able to communicate with each other), but that majority can be achieved using a single monitor, or 2 out of 2 monitors, 2 out of 3, 3 out of 4, etc.

For small or non-critical deployments of multi-node Ceph clusters, it is advisable to deploy three monitors, and to increase the number of monitors to five for larger clusters or to survive a double failure.  There is rarely justification for seven or more.

Since monitors are lightweight, it is possible to run them on the same host as OSDs; however, we recommend running them on separate hosts, because fsync issues with the kernel may impair performance. Dedicated monitor nodes also minimize disruption since monitor and OSD daemons are not inactive at the same time when a node crashes or is taken down for maintenance.

Dedicated monitor nodes also make for cleaner maintenance by avoiding both OSDs and a mon going down if a node is rebooted, taken down, or crashes.

Note

A *majority* of monitors in your cluster must be able to reach each other in order to establish a quorum.

### Deploy your Hardware[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#deploy-your-hardware)

If you are adding a new host when adding a new monitor,  see [Hardware Recommendations](https://docs.ceph.com/en/latest/start/hardware-recommendations) for details on minimum recommendations for monitor hardware. To add a monitor host to your cluster, first make sure you have an up-to-date version of Linux installed (typically Ubuntu 16.04 or RHEL 7).

Add your monitor host to a rack in your cluster, connect it to the network and ensure that it has network connectivity.

### Install the Required Software[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#install-the-required-software)

For manually deployed clusters, you must install Ceph packages manually. See [Installing Packages](https://docs.ceph.com/en/latest/install/install-storage-cluster) for details. You should configure SSH to a user with password-less authentication and root permissions.



### Adding a Monitor (Manual)[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#adding-a-monitor-manual)

This procedure creates a `ceph-mon` data directory, retrieves the monitor map and monitor keyring, and adds a `ceph-mon` daemon to your cluster.  If this results in only two monitor daemons, you may add more monitors by repeating this procedure until you have a sufficient number of `ceph-mon` daemons to achieve a quorum.

At this point you should define your monitor’s id.  Traditionally, monitors have been named with single letters (`a`, `b`, `c`, …), but you are free to define the id as you see fit.  For the purpose of this document, please take into account that `{mon-id}` should be the id you chose, without the `mon.` prefix (i.e., `{mon-id}` should be the `a` on `mon.a`).

1. Create the default directory on the machine that will host your new monitor.

   ```
   ssh {new-mon-host}
   sudo mkdir /var/lib/ceph/mon/ceph-{mon-id}
   ```

2. Create a temporary directory `{tmp}` to keep the files needed during this process. This directory should be different from the monitor’s default directory created in the previous step, and can be removed after all the steps are executed.

   ```
   mkdir {tmp}
   ```

3. Retrieve the keyring for your monitors, where `{tmp}` is the path to the retrieved keyring, and `{key-filename}` is the name of the file containing the retrieved monitor key.

   ```
   ceph auth get mon. -o {tmp}/{key-filename}
   ```

4. Retrieve the monitor map, where `{tmp}` is the path to the retrieved monitor map, and `{map-filename}` is the name of the file containing the retrieved monitor map.

   ```
   ceph mon getmap -o {tmp}/{map-filename}
   ```

5. Prepare the monitor’s data directory created in the first step. You must specify the path to the monitor map so that you can retrieve the information about a quorum of monitors and their `fsid`. You must also specify a path to the monitor keyring:

   ```
   sudo ceph-mon -i {mon-id} --mkfs --monmap {tmp}/{map-filename} --keyring {tmp}/{key-filename}
   ```

6. Start the new monitor and it will automatically join the cluster. The daemon needs to know which address to bind to, via either the `--public-addr {ip}` or `--public-network {network}` argument. For example:

   ```
   ceph-mon -i {mon-id} --public-addr {ip:port}
   ```



## Removing Monitors[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#removing-monitors)

When you remove monitors from a cluster, consider that Ceph monitors use Paxos to establish consensus about the master cluster map. You must have a sufficient number of monitors to establish a quorum for consensus about the cluster map.



### Removing a Monitor (Manual)[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#removing-a-monitor-manual)

This procedure removes a `ceph-mon` daemon from your cluster.   If this procedure results in only two monitor daemons, you may add or remove another monitor until you have a number of `ceph-mon` daemons that can achieve a quorum.

1. Stop the monitor.

   ```
   service ceph -a stop mon.{mon-id}
   ```

2. Remove the monitor from the cluster.

   ```
   ceph mon remove {mon-id}
   ```

3. Remove the monitor entry from `ceph.conf`.



### Removing Monitors from an Unhealthy Cluster[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#removing-monitors-from-an-unhealthy-cluster)

This procedure removes a `ceph-mon` daemon from an unhealthy cluster, for example a cluster where the monitors cannot form a quorum.

1. Stop all `ceph-mon` daemons on all monitor hosts.

   ```
   ssh {mon-host}
   systemctl stop ceph-mon.target
   # and repeat for all mons
   ```

2. Identify a surviving monitor and log in to that host.

   ```
   ssh {mon-host}
   ```

3. Extract a copy of the monmap file.

   ```
   ceph-mon -i {mon-id} --extract-monmap {map-path}
   # in most cases, that's
   ceph-mon -i `hostname` --extract-monmap /tmp/monmap
   ```

4. Remove the non-surviving or problematic monitors.  For example, if you have three monitors, `mon.a`, `mon.b`, and `mon.c`, where only `mon.a` will survive, follow the example below:

   ```
   monmaptool {map-path} --rm {mon-id}
   # for example,
   monmaptool /tmp/monmap --rm b
   monmaptool /tmp/monmap --rm c
   ```

5. Inject the surviving map with the removed monitors into the surviving monitor(s).  For example, to inject a map into monitor `mon.a`, follow the example below:

   ```
   ceph-mon -i {mon-id} --inject-monmap {map-path}
   # for example,
   ceph-mon -i a --inject-monmap /tmp/monmap
   ```

6. Start only the surviving monitors.

7. Verify the monitors form a quorum (`ceph -s`).

8. You may wish to archive the removed monitors’ data directory in `/var/lib/ceph/mon` in a safe location, or delete it if you are confident the remaining monitors are healthy and are sufficiently redundant.



## Changing a Monitor’s IP Address[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address)

Important

Existing monitors are not supposed to change their IP addresses.

Monitors are critical components of a Ceph cluster, and they need to maintain a quorum for the whole system to work properly. To establish a quorum, the monitors need to discover each other. Ceph has strict requirements for discovering monitors.

Ceph clients and other Ceph daemons use `ceph.conf` to discover monitors. However, monitors discover each other using the monitor map, not `ceph.conf`. For example,  if you refer to [Adding a Monitor (Manual)](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#adding-a-monitor-manual) you will see that you need to obtain the current monmap for the cluster when creating a new monitor, as it is one of the required arguments of `ceph-mon -i {mon-id} --mkfs`. The following sections explain the consistency requirements for Ceph monitors, and a few safe ways to change a monitor’s IP address.

### Consistency Requirements[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#consistency-requirements)

A monitor always refers to the local copy of the monmap  when discovering other monitors in the cluster.  Using the monmap instead of `ceph.conf` avoids errors that could  break the cluster (e.g., typos in `ceph.conf` when specifying a monitor address or port). Since monitors use monmaps for discovery and they share monmaps with clients and other Ceph daemons, the monmap provides monitors with a strict guarantee that their consensus is valid.

Strict consistency also applies to updates to the monmap. As with any other updates on the monitor, changes to the monmap always run through a distributed consensus algorithm called [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science)). The monitors must agree on each update to the monmap, such as adding or removing a monitor, to ensure that each monitor in the quorum has the same version of the monmap. Updates to the monmap are incremental so that monitors have the latest agreed upon version, and a set of previous versions, allowing a monitor that has an older version of the monmap to catch up with the current state of the cluster.

If monitors discovered each other through the Ceph configuration file instead of through the monmap, it would introduce additional risks because the Ceph configuration files are not updated and distributed automatically. Monitors might inadvertently use an older `ceph.conf` file, fail to recognize a monitor, fall out of a quorum, or develop a situation where [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science)) is not able to determine the current state of the system accurately. Consequently,  making changes to an existing monitor’s IP address must be done with  great care.

### Changing a Monitor’s IP address (The Right Way)[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address-the-right-way)

Changing a monitor’s IP address in `ceph.conf` only is not sufficient to ensure that other monitors in the cluster will receive the update.  To change a monitor’s IP address, you must add a new monitor with the IP  address you want to use (as described in [Adding a Monitor (Manual)](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#adding-a-monitor-manual)),  ensure that the new monitor successfully joins the  quorum; then, remove the monitor that uses the old IP address. Then, update the `ceph.conf` file to ensure that clients and other daemons know the IP address of the new monitor.

For example, lets assume there are three monitors in place, such as

```
[mon.a]
        host = host01
        addr = 10.0.0.1:6789
[mon.b]
        host = host02
        addr = 10.0.0.2:6789
[mon.c]
        host = host03
        addr = 10.0.0.3:6789
```

To change `mon.c` to `host04` with the IP address  `10.0.0.4`, follow the steps in [Adding a Monitor (Manual)](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#adding-a-monitor-manual) by adding a  new monitor `mon.d`. Ensure that `mon.d` is  running before removing `mon.c`, or it will break the quorum. Remove `mon.c` as described on  [Removing a Monitor (Manual)](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#removing-a-monitor-manual). Moving all three  monitors would thus require repeating this process as many times as needed.

### Changing a Monitor’s IP address (The Messy Way)[](https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address-the-messy-way)

There may come a time when the monitors must be moved to a different network,  a different part of the datacenter or a different datacenter altogether. While  it is possible to do it, the process becomes a bit more hazardous.

In such a case, the solution is to generate a new monmap with updated IP addresses for all the monitors in the cluster, and inject the new map on each individual monitor.  This is not the most user-friendly approach, but we do not expect this to be something that needs to be done every other week.  As it is clearly stated on the top of this section, monitors are not supposed to change IP addresses.

Using the previous monitor configuration as an example, assume you want to move all the  monitors from the `10.0.0.x` range to `10.1.0.x`, and these networks  are unable to communicate.  Use the following procedure:

1. Retrieve the monitor map, where `{tmp}` is the path to the retrieved monitor map, and `{filename}` is the name of the file containing the retrieved monitor map.

   ```
   ceph mon getmap -o {tmp}/{filename}
   ```

2. The following example demonstrates the contents of the monmap.

   ```
   $ monmaptool --print {tmp}/{filename}
   
   monmaptool: monmap file {tmp}/{filename}
   epoch 1
   fsid 224e376d-c5fe-4504-96bb-ea6332a19e61
   last_changed 2012-12-17 02:46:41.591248
   created 2012-12-17 02:46:41.591248
   0: 10.0.0.1:6789/0 mon.a
   1: 10.0.0.2:6789/0 mon.b
   2: 10.0.0.3:6789/0 mon.c
   ```

3. Remove the existing monitors.

   ```
   $ monmaptool --rm a --rm b --rm c {tmp}/{filename}
   
   monmaptool: monmap file {tmp}/{filename}
   monmaptool: removing a
   monmaptool: removing b
   monmaptool: removing c
   monmaptool: writing epoch 1 to {tmp}/{filename} (0 monitors)
   ```

4. Add the new monitor locations.

   ```
   $ monmaptool --add a 10.1.0.1:6789 --add b 10.1.0.2:6789 --add c 10.1.0.3:6789 {tmp}/{filename}
   
   monmaptool: monmap file {tmp}/{filename}
   monmaptool: writing epoch 1 to {tmp}/{filename} (3 monitors)
   ```

5. Check new contents.

   ```
   $ monmaptool --print {tmp}/{filename}
   
   monmaptool: monmap file {tmp}/{filename}
   epoch 1
   fsid 224e376d-c5fe-4504-96bb-ea6332a19e61
   last_changed 2012-12-17 02:46:41.591248
   created 2012-12-17 02:46:41.591248
   0: 10.1.0.1:6789/0 mon.a
   1: 10.1.0.2:6789/0 mon.b
   2: 10.1.0.3:6789/0 mon.c
   ```

At this point, we assume the monitors (and stores) are installed at the new location. The next step is to propagate the modified monmap to the new monitors, and inject the modified monmap into each new monitor.

1. First, make sure to stop all your monitors.  Injection must be done while the daemon is not running.

2. Inject the monmap.

   ```
   ceph-mon -i {mon-id} --inject-monmap {tmp}/{filename}
   ```

3. Restart the monitors.

After this step, migration to the new location is complete and the monitors should operate successfully.

# Device Management[](https://docs.ceph.com/en/latest/rados/operations/devices/#device-management)

Ceph tracks which hardware storage devices (e.g., HDDs, SSDs) are consumed by which daemons, and collects health metrics about those devices in order to provide tools to predict and/or automatically respond to hardware failure.

## Device tracking[](https://docs.ceph.com/en/latest/rados/operations/devices/#device-tracking)

You can query which storage devices are in use with:

```
ceph device ls
```

You can also list devices by daemon or by host:

```
ceph device ls-by-daemon <daemon>
ceph device ls-by-host <host>
```

For any individual device, you can query information about its location and how it is being consumed with:

```
ceph device info <devid>
```

## Identifying physical devices[](https://docs.ceph.com/en/latest/rados/operations/devices/#identifying-physical-devices)

You can blink the drive LEDs on hardware enclosures to make the replacement of failed disks easy and less error-prone.  Use the following command:

```
device light on|off <devid> [ident|fault] [--force]
```

The `<devid>` parameter is the device identification. You can obtain this information using the following command:

```
ceph device ls
```

The `[ident|fault]` parameter is used to set the kind of light to blink. By default, the identification light is used.

Note

This command needs the Cephadm or the Rook [orchestrator](https://docs.ceph.com/docs/master/mgr/orchestrator/#orchestrator-cli-module) module enabled. The orchestrator module enabled is shown by executing the following command:

```
ceph orch status
```

The command behind the scene to blink the drive LEDs is lsmcli. If you need to customize this command you can configure this via a Jinja2 template:

```
ceph config-key set mgr/cephadm/blink_device_light_cmd "<template>"
ceph config-key set mgr/cephadm/<host>/blink_device_light_cmd "lsmcli local-disk-{{ ident_fault }}-led-{{'on' if on else 'off'}} --path '{{ path or dev }}'"
```

The Jinja2 template is rendered using the following arguments:

- - `on`

    A boolean value.

- - `ident_fault`

    A string containing ident or fault.

- - `dev`

    A string containing the device ID, e.g. SanDisk_X400_M.2_2280_512GB_162924424784.

- - `path`

    A string containing the device path, e.g. /dev/sda.



## Enabling monitoring[](https://docs.ceph.com/en/latest/rados/operations/devices/#enabling-monitoring)

Ceph can also monitor health metrics associated with your device.  For example, SATA hard disks implement a standard called SMART that provides a wide range of internal metrics about the device’s usage and health, like the number of hours powered on, number of power cycles, or unrecoverable read errors.  Other device types like SAS and NVMe implement a similar set of metrics (via slightly different standards). All of these can be collected by Ceph via the `smartctl` tool.

You can enable or disable health monitoring with:

```
ceph device monitoring on
```

or:

```
ceph device monitoring off
```

## Scraping[](https://docs.ceph.com/en/latest/rados/operations/devices/#scraping)

If monitoring is enabled, metrics will automatically be scraped at regular intervals.  That interval can be configured with:

```
ceph config set mgr mgr/devicehealth/scrape_frequency <seconds>
```

The default is to scrape once every 24 hours.

You can manually trigger a scrape of all devices with:

```
ceph device scrape-health-metrics
```

A single device can be scraped with:

```
ceph device scrape-health-metrics <device-id>
```

Or a single daemon’s devices can be scraped with:

```
ceph device scrape-daemon-health-metrics <who>
```

The stored health metrics for a device can be retrieved (optionally for a specific timestamp) with:

```
ceph device get-health-metrics <devid> [sample-timestamp]
```

## Failure prediction[](https://docs.ceph.com/en/latest/rados/operations/devices/#failure-prediction)

Ceph can predict life expectancy and device failures based on the health metrics it collects.  There are three modes:

- *none*: disable device failure prediction.
- *local*: use a pre-trained prediction model from the ceph-mgr daemon

The prediction mode can be configured with:

```
ceph config set global device_failure_prediction_mode <mode>
```

Prediction normally runs in the background on a periodic basis, so it may take some time before life expectancy values are populated.  You can see the life expectancy of all devices in output from:

```
ceph device ls
```

You can also query the metadata for a specific device with:

```
ceph device info <devid>
```

You can explicitly force prediction of a device’s life expectancy with:

```
ceph device predict-life-expectancy <devid>
```

If you are not using Ceph’s internal device failure prediction but have some external source of information about device failures, you can inform Ceph of a device’s life expectancy with:

```
ceph device set-life-expectancy <devid> <from> [<to>]
```

Life expectancies are expressed as a time interval so that uncertainty can be expressed in the form of a wide interval. The interval end can also be left unspecified.

## Health alerts[](https://docs.ceph.com/en/latest/rados/operations/devices/#health-alerts)

The `mgr/devicehealth/warn_threshold` controls how soon an expected device failure must be before we generate a health warning.

The stored life expectancy of all devices can be checked, and any appropriate health alerts generated, with:

```
ceph device check-health
```

## Automatic Mitigation[](https://docs.ceph.com/en/latest/rados/operations/devices/#automatic-mitigation)

If the `mgr/devicehealth/self_heal` option is enabled (it is by default), then for devices that are expected to fail soon the module will automatically migrate data away from them by marking the devices “out”.

The `mgr/devicehealth/mark_out_threshold` controls how soon an expected device failure must be before we automatically mark an osd “out”.

# BlueStore Migration[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#bluestore-migration)

Each OSD can run either BlueStore or FileStore, and a single Ceph cluster can contain a mix of both.  Users who have previously deployed FileStore are likely to want to transition to BlueStore in order to take advantage of the improved performance and robustness.  There are several strategies for making such a transition.

An individual OSD cannot be converted in place in isolation, however: BlueStore and FileStore are simply too different for that to be practical.  “Conversion” will rely either on the cluster’s normal replication and healing support or tools and strategies that copy OSD content from an old (FileStore) device to a new (BlueStore) one.

## Deploy new OSDs with BlueStore[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#deploy-new-osds-with-bluestore)

Any new OSDs (e.g., when the cluster is expanded) can be deployed using BlueStore.  This is the default behavior so no specific change is needed.

Similarly, any OSDs that are reprovisioned after replacing a failed drive can use BlueStore.

## Convert existing OSDs[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#convert-existing-osds)

### Mark out and replace[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#mark-out-and-replace)

The simplest approach is to mark out each device in turn, wait for the data to replicate across the cluster, reprovision the OSD, and mark it back in again.  It is simple and easy to automate.  However, it requires more data migration than should be necessary, so it is not optimal.

1. Identify a FileStore OSD to replace:

   ```
   ID=<osd-id-number>
   DEVICE=<disk-device>
   ```

   You can tell whether a given OSD is FileStore or BlueStore with:

   ```
   ceph osd metadata $ID | grep osd_objectstore
   ```

   You can get a current count of filestore vs bluestore with:

   ```
   ceph osd count-metadata osd_objectstore
   ```

2. Mark the filestore OSD out:

   ```
   ceph osd out $ID
   ```

3. Wait for the data to migrate off the OSD in question:

   ```
   while ! ceph osd safe-to-destroy $ID ; do sleep 60 ; done
   ```

4. Stop the OSD:

   ```
   systemctl kill ceph-osd@$ID
   ```

5. Make note of which device this OSD is using:

   ```
   mount | grep /var/lib/ceph/osd/ceph-$ID
   ```

6. Unmount the OSD:

   ```
   umount /var/lib/ceph/osd/ceph-$ID
   ```

7. Destroy the OSD data. Be *EXTREMELY CAREFUL* as this will destroy the contents of the device; be certain the data on the device is not needed (i.e., that the cluster is healthy) before proceeding.

   ```
   ceph-volume lvm zap $DEVICE
   ```

8. Tell the cluster the OSD has been destroyed (and a new OSD can be reprovisioned with the same ID):

   ```
   ceph osd destroy $ID --yes-i-really-mean-it
   ```

9. Reprovision a BlueStore OSD in its place with the same OSD ID. This requires you do identify which device to wipe based on what you saw mounted above. BE CAREFUL!

   ```
   ceph-volume lvm create --bluestore --data $DEVICE --osd-id $ID
   ```

10. Repeat.

You can allow the refilling of the replacement OSD to happen concurrently with the draining of the next OSD, or follow the same procedure for multiple OSDs in parallel, as long as you ensure the cluster is fully clean (all data has all replicas) before destroying any OSDs.  Failure to do so will reduce the redundancy of your data and increase the risk of (or potentially even cause) data loss.

Advantages:

- Simple.
- Can be done on a device-by-device basis.
- No spare devices or hosts are required.

Disadvantages:

- Data is copied over the network twice: once to some other OSD in the cluster (to maintain the desired number of replicas), and then again back to the reprovisioned BlueStore OSD.

### Whole host replacement[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#whole-host-replacement)

If you have a spare host in the cluster, or have sufficient free space to evacuate an entire host in order to use it as a spare, then the conversion can be done on a host-by-host basis with each stored copy of the data migrating only once.

First, you need have empty host that has no data.  There are two ways to do this: either by starting with a new, empty host that isn’t yet  part of the cluster, or by offloading data from an existing host that in the cluster.

#### Use a new, empty host[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#use-a-new-empty-host)

Ideally the host should have roughly the same capacity as other hosts you will be converting (although it doesn’t strictly matter).

```
NEWHOST=<empty-host-name>
```

Add the host to the CRUSH hierarchy, but do not attach it to the root:

```
ceph osd crush add-bucket $NEWHOST host
```

Make sure the ceph packages are installed.

#### Use an existing host[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#use-an-existing-host)

If you would like to use an existing host that is already part of the cluster, and there is sufficient free space on that host so that all of its data can be migrated off, then you can instead do:

```
OLDHOST=<existing-cluster-host-to-offload>
ceph osd crush unlink $OLDHOST default
```

where “default” is the immediate ancestor in the CRUSH map. (For smaller clusters with unmodified configurations this will normally be “default”, but it might also be a rack name.)  You should now see the host at the top of the OSD tree output with no parent:

```
$ bin/ceph osd tree
ID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF
-5             0 host oldhost
10   ssd 1.00000     osd.10        up  1.00000 1.00000
11   ssd 1.00000     osd.11        up  1.00000 1.00000
12   ssd 1.00000     osd.12        up  1.00000 1.00000
-1       3.00000 root default
-2       3.00000     host foo
 0   ssd 1.00000         osd.0     up  1.00000 1.00000
 1   ssd 1.00000         osd.1     up  1.00000 1.00000
 2   ssd 1.00000         osd.2     up  1.00000 1.00000
...
```

If everything looks good, jump directly to the “Wait for data migration to complete” step below and proceed from there to clean up the old OSDs.

#### Migration process[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#migration-process)

If you’re using a new host, start at step #1.  For an existing host, jump to step #5 below.

1. Provision new BlueStore OSDs for all devices:

   ```
   ceph-volume lvm create --bluestore --data /dev/$DEVICE
   ```

2. Verify OSDs join the cluster with:

   ```
   ceph osd tree
   ```

   You should see the new host `$NEWHOST` with all of the OSDs beneath it, but the host should *not* be nested beneath any other node in hierarchy (like `root default`).  For example, if `newhost` is the empty host, you might see something like:

   ```
   $ bin/ceph osd tree
   ID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF
   -5             0 host newhost
   10   ssd 1.00000     osd.10        up  1.00000 1.00000
   11   ssd 1.00000     osd.11        up  1.00000 1.00000
   12   ssd 1.00000     osd.12        up  1.00000 1.00000
   -1       3.00000 root default
   -2       3.00000     host oldhost1
    0   ssd 1.00000         osd.0     up  1.00000 1.00000
    1   ssd 1.00000         osd.1     up  1.00000 1.00000
    2   ssd 1.00000         osd.2     up  1.00000 1.00000
   ...
   ```

3. Identify the first target host to convert

   ```
   OLDHOST=<existing-cluster-host-to-convert>
   ```

4. Swap the new host into the old host’s position in the cluster:

   ```
   ceph osd crush swap-bucket $NEWHOST $OLDHOST
   ```

   At this point all data on `$OLDHOST` will start migrating to OSDs on `$NEWHOST`.  If there is a difference in the total capacity of the old and new hosts you may also see some data migrate to or from other nodes in the cluster, but as long as the hosts are similarly sized this will be a relatively small amount of data.

5. Wait for data migration to complete:

   ```
   while ! ceph osd safe-to-destroy $(ceph osd ls-tree $OLDHOST); do sleep 60 ; done
   ```

6. Stop all old OSDs on the now-empty `$OLDHOST`:

   ```
   ssh $OLDHOST
   systemctl kill ceph-osd.target
   umount /var/lib/ceph/osd/ceph-*
   ```

7. Destroy and purge the old OSDs:

   ```
   for osd in `ceph osd ls-tree $OLDHOST`; do
       ceph osd purge $osd --yes-i-really-mean-it
   done
   ```

8. Wipe the old OSD devices. This requires you do identify which devices are to be wiped manually (BE CAREFUL!). For each device,:

   ```
   ceph-volume lvm zap $DEVICE
   ```

9. Use the now-empty host as the new host, and repeat:

   ```
   NEWHOST=$OLDHOST
   ```

Advantages:

- Data is copied over the network only once.
- Converts an entire host’s OSDs at once.
- Can parallelize to converting multiple hosts at a time.
- No spare devices are required on each host.

Disadvantages:

- A spare host is required.
- An entire host’s worth of OSDs will be migrating data at a time.  This is like likely to impact overall cluster performance.
- All migrated data still makes one full hop over the network.

### Per-OSD device copy[](https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/#per-osd-device-copy)

A single logical OSD can be converted by using the `copy` function of `ceph-objectstore-tool`.  This requires that the host have a free device (or devices) to provision a new, empty BlueStore OSD.  For example, if each host in your cluster has 12 OSDs, then you’d need a 13th available device so that each OSD can be converted in turn before the old device is reclaimed to convert the next OSD.

Caveats:

- This strategy requires that a blank BlueStore OSD be prepared without allocating a new OSD ID, something that the `ceph-volume` tool doesn’t support.  More importantly, the setup of *dmcrypt* is closely tied to the OSD identity, which means that this approach does not work with encrypted OSDs.
- The device must be manually partitioned.
- An unsupported user-contributed script that shows this process may be found at https://github.com/ceph/ceph/blob/master/src/script/contrib/ceph-migrate-bluestore.bash

Advantages:

- Little or no data migrates over the network during the conversion, so long as the noout or norecover/norebalance flags are set on the OSD or the cluster while the process proceeds.

Disadvantages:

- Tooling is not fully implemented, supported, or documented.
- Each host must have an appropriate spare or empty device for staging.
- The OSD is offline during the conversion, which means new writes to PGs with the OSD in their acting set may not be ideally redundant until the subject OSD comes up and recovers. This increases the risk of data loss due to an overlapping failure.  However, if another OSD fails before conversion and start-up are complete, the original Filestore OSD can be started to provide access to its original data.