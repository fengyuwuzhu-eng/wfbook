# 升级指南

Red Hat Ceph Storage 7

## 升级 Red Hat Ceph Storage 集群

 Red Hat Ceph Storage Documentation Team  

[法律通告](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/index#idm139899019991232)

**摘要**

​				本文档提供了有关升级在 AMD64 和 Intel 64 构架中运行 Red Hat Enterprise Linux 的 Red Hat Ceph Storage 集群的信息。 		

​				红帽承诺替换我们的代码、文档和网页属性中存在问题的语言。我们从这四个术语开始：master、slave、黑名单和白名单。由于此项工作十分艰巨，这些更改将在即将推出的几个发行版本中逐步实施。详情请查看 [CTO Chris Wright 信息](https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language)。 		

------

# 第 1 章 使用 cephadm 升级 Red Hat Ceph Storage 集群

​			作为存储管理员，您可以使用 `cephadm` Orchestrator 使用 `ceph orch upgrade` 命令将 Red Hat Ceph Storage 5.3 或 6.1 集群升级到 Red Hat Ceph Storage 7。 	

注意

​				支持直接从 Red Hat Ceph Storage 5 升级到 Red Hat Ceph Storage 7。 		

​			自动化升级流程遵循 Ceph 最佳实践。例如： 	

- ​					升级顺序从 Ceph 管理器、Ceph 监控器开始，然后是其他守护进程。 			
- ​					只有在 Ceph 指示集群可用后，每一守护进程才会重新启动。 			

​			存储集群健康状态可能会在升级过程中切换到 `HEALTH_WARNING`。升级完成后，健康状态应该切回到 HEALTH_OK。 	

注意

​				升级成功后，您不会收到消息。运行 `ceph versions` 和 `ceph orch ps` 命令，以验证新的镜像 ID 和存储集群的版本。 		

## 1.1. RHCS 和 podman 版本间的兼容性注意事项

​				`Podman` 和 Red Hat Ceph Storage 具有不同的生命周期结束策略，这可能会导致查找兼容版本变得困难。 		

​				红帽建议为 Red Hat Ceph Storage 使用相应 Red Hat Enterprise Linux 版本附带的 `podman` 版本。如需了解更多详细信息，请参阅 [*Red Hat Ceph Storage: 支持的配置*](https://access.redhat.com/articles/1548993)知识库文章。如需了解更多帮助，请参阅 *Red Hat Ceph Storage 故障排除指南中的* [*联系红帽支持*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/troubleshooting_guide/#contacting-red-hat-support-for-service) 部分。 		

​				下表显示了 Red Hat Ceph Storage 7 和 `podman` 版本之间的版本兼容性。 		

| Ceph                   | Podman |      |      |       |      |      |
| ---------------------- | ------ | ---- | ---- | ----- | ---- | ---- |
|                        | 1.9    | 2.0  | 2.1  | 2.2   | 3.0  | >3.0 |
| Red Hat Ceph Storage 7 | false  | true | true | false | true | true |

警告

​					您必须使用版本 2.0.0 或更高版本的 Podman。 			

## 1.2. 升级 Red Hat Ceph Storage 集群

​				您可以使用 `ceph orch upgrade` 命令升级 Red Hat Ceph Storage 6.1 集群。 		

**先决条件**

- ​						正在运行的 Red Hat Ceph Storage 集群 5.3 或 6.1。 				
- ​						所有节点的根级别访问权限。 				
- ​						具有 sudo 的 Ansible 用户，对存储集群中所有节点的 `ssh` 访问和免密码访问。 				
- ​						存储集群中至少有两个 Ceph 管理器节点：一个活跃节点和一个备用节点。 				

注意

​					Red Hat Ceph Storage 5 还包括一个健康检查功能，如果它检测到存储集群中的任何守护进程正在运行多个版本的 Red  Hat Ceph Storage，它会返回 DAEMON_OLD_VERSION 警告。当守护进程继续运行多个版本的 Red Hat Ceph  Storage 时，会触发警告，超过 `mon_warn_older_version_delay` 选项中设置的时间值。默认情况下，`mon_warn_older_version_delay` 选项设置为 1 周。此设置允许大多数升级进行，而不会看到警告。如果升级过程暂停了较长的时间，您可以屏蔽健康警告： 			



```none
ceph health mute DAEMON_OLD_VERSION --sticky
```

​					升级完成后，取消健康警告： 			



```none
ceph health unmute DAEMON_OLD_VERSION
```

**流程**

1. ​						在 Ansible 管理节点上启用 Ceph Ansible 存储库： 				

   Red Hat Enterprise Linux 9

   ​							

   

   ```none
   subscription-manager repos --enable=rhceph-7-tools-for-rhel-9-x86_64-rpms
   ```

2. ​						更新 `cephadm` 和 `cephadm-ansible` 软件包： 				

   **示例**

   ​							

   

   ```none
   [root@admin ~]# dnf update cephadm
   [root@admin ~]# dnf update cephadm-ansible
   ```

3. ​						进入 `/usr/share/cephadm-ansible/` 目录： 				

   **示例**

   ​							

   

   ```none
   [root@admin ~]# cd /usr/share/cephadm-ansible
   ```

4. ​						在存储集群的 bootstrap 主机上，运行 preflight playbook，并将 `upgrade_ceph_packages` 参数设置为 `true` ： 				

   **语法**

   ​							

   

   ```none
   ansible-playbook -i INVENTORY_FILE cephadm-preflight.yml --extra-vars "ceph_origin=rhcs upgrade_ceph_packages=true"
   ```

   **示例**

   ​							

   

   ```none
   [ceph-admin@admin cephadm-ansible]$ ansible-playbook -i /etc/ansible/hosts cephadm-preflight.yml --extra-vars "ceph_origin=rhcs upgrade_ceph_packages=true"
   ```

   ​						此软件包升级所有节点上的 `cephadm`。 				

5. ​						登录 `cephadm` shell： 				

   **示例**

   ​							

   

   ```none
   [root@host01 ~]# cephadm shell
   ```

6. ​						确定所有主机都在线，并且存储集群处于健康状态： 				

   **示例**

   ​							

   

   ```none
   [ceph: root@host01 /]# ceph -s
   ```

7. ​						设置 OSD `noout`、`noscrub`、和 `nodeep-scrub` 标记，以防止 OSD 在升级过程中被标记为 out，并避免对集群造成不必要的负载： 				

   **示例**

   ​							

   

   ```none
   [ceph: root@host01 /]# ceph osd set noout
   [ceph: root@host01 /]# ceph osd set noscrub
   [ceph: root@host01 /]# ceph osd set nodeep-scrub
   ```

8. ​						检查服务版本和可用目标容器： 				

   **语法**

   ​							

   

   ```none
   ceph orch upgrade check IMAGE_NAME
   ```

   **示例**

   ​							

   

   ```none
   [ceph: root@host01 /]# ceph orch upgrade check registry.redhat.io/rhceph/rhceph-7-rhel9:latest
   ```

   注意

   ​							镜像名称适用于 Red Hat Enterprise Linux 8 和 Red Hat Enterprise Linux 9。 					

9. ​						升级存储集群： 				

   **语法**

   ​							

   

   ```none
   ceph orch upgrade start IMAGE_NAME
   ```

   **示例**

   ​							

   

   ```none
   [ceph: root@host01 /]# ceph orch upgrade start registry.redhat.io/rhceph/rhceph-7-rhel9:latest
   ```

   注意

   ​							要执行交错升级，请参阅[*执行交错升级*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/index#performing-a-staggered-upgrade_upgrade)。 					

   ​						在升级过程中，`ceph status` 输出中会出现一个进度条。 				

   **示例**

   ​							

   

   ```none
   [ceph: root@host01 /]# ceph status
   [...]
   progress:
       Upgrade to 18.2.0-128.el9cp (1s)
         [............................]
   ```

10. ​						验证 Ceph 集群的新 *IMAGE_ID* 和 *VERSION* ： 				

    **示例**

    ​							

    

    ```none
    [ceph: root@host01 /]# ceph versions
    [ceph: root@host01 /]# ceph orch ps
    ```

    注意

    ​							在升级 Ceph 集群后如果不使用 `cephadm-ansible` playbook，则必须升级客户端节点上的 `ceph-common` 软件包和客户端库。 					

    **示例**

    ​								

    

    ```none
    [root@client01 ~] dnf update ceph-common
    ```

    ​							验证您是否具有最新版本： 					

    **示例**

    ​								

    

    ```none
    [root@client01 ~] ceph --version
    ```

11. ​						升级完成后，取消设置 `noout`、`noscrub` 和 `nodeep-scrub` 标记： 				

    **示例**

    ​							

    

    ```none
    [ceph: root@host01 /]# ceph osd unset noout
    [ceph: root@host01 /]# ceph osd unset noscrub
    [ceph: root@host01 /]# ceph osd unset nodeep-scrub
    ```

## 1.3. 在断开连接的环境中升级 Red Hat Ceph Storage 集群

​				您可以使用 `--image` 标签在断开连接的环境中升级存储集群。 		

​				您可以使用 `ceph orch upgrade` 命令升级 Red Hat Ceph Storage 集群。 		

**先决条件**

- ​						正在运行的 Red Hat Ceph Storage 集群 5.3 或 6.1。 				
- ​						所有节点的根级别访问权限。 				
- ​						具有 sudo 的 Ansible 用户，对存储集群中所有节点的 `ssh` 访问和免密码访问。 				
- ​						存储集群中至少有两个 Ceph 管理器节点：一个活跃节点和一个备用节点。 				
- ​						将节点注册到 CDN 并附加订阅。 				
- ​						在断开连接的环境中检查客户容器镜像，并根据需要更改配置。如需了解更多详细信息，请参阅 *Red Hat Ceph Storage 安装指南中的* [*为断开连接的安装更改自定义容器镜像的配置*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/installation_guide#changing-configurations-of-custom-container-images-for-disconnected-installations_install)。 				

​				默认情况下，监控堆栈组件根据主 Ceph 镜像进行部署。对于存储集群的断开连接环境，您必须使用最新的监控堆栈组件镜像。 		

表 1.1. 监控堆栈的自定义镜像详情

| 监控堆栈组件  | 镜像详情                                                     |
| ------------- | ------------------------------------------------------------ |
| Prometheus    | registry.redhat.io/openshift4/ose-prometheus:v4.12           |
| Grafana       | registry.redhat.io/rhceph/grafana-rhel9:latest               |
| Node-exporter | registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.12 |
| AlertManager  | registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.12 |
| HAProxy       | registry.redhat.io/rhceph/rhceph-haproxy-rhel9:latest        |
| Keepalived    | registry.redhat.io/rhceph/keepalived-rhel9:latest            |
| SNMP Gateway  | registry.redhat.io/rhceph/snmp-notifier-rhel9:latest         |

**流程**

1. ​						在 Ansible 管理节点上启用 Ceph Ansible 存储库： 				

   Red Hat Enterprise Linux 9

   ​							

   

   ```none
   subscription-manager repos --enable=rhceph-7-tools-for-rhel-9-x86_64-rpms
   ```

2. ​						更新 `cephadm` 和 `cephadm-ansible` 软件包。 				

   **示例**

   ​							

   

   ```none
   [root@admin ~]# dnf update cephadm
   [root@admin ~]# dnf update cephadm-ansible
   ```

3. ​						进入 /usr/share/cephadm-ansible/ 目录： 				

   **示例**

   ​							

   

   ```none
   [root@admin ~]# cd /usr/share/cephadm-ansible
   ```

4. ​						运行 preflight playbook，将 `upgrade_ceph_packages` 参数设置为 `true`，在存储集群中 bootstrapped 主机上将 `ceph_origin` 参数设置为 `custom` ： 				

   **语法**

   ​							

   

   ```none
   ansible-playbook -i INVENTORY_FILE cephadm-preflight.yml --extra-vars "ceph_origin=custom upgrade_ceph_packages=true"
   ```

   **示例**

   ​							

   

   ```none
   [ceph-admin@admin ~]$ ansible-playbook -i /etc/ansible/hosts cephadm-preflight.yml --extra-vars "ceph_origin=custom upgrade_ceph_packages=true"
   ```

   ​						此软件包升级所有节点上的 `cephadm`。 				

5. ​						登录 `cephadm` shell： 				

   **示例**

   ​							

   

   ```none
   [root@node0 ~]# cephadm shell
   ```

6. ​						确定所有主机都在线，并且存储集群处于健康状态： 				

   **示例**

   ​							

   

   ```none
   [ceph: root@node0 /]# ceph -s
   ```

7. ​						设置 OSD `noout`、`noscrub`、和 `nodeep-scrub` 标记，以防止 OSD 在升级过程中被标记为 out，并避免对集群造成不必要的负载： 				

   **示例**

   ​							

   

   ```none
   [ceph: root@host01 /]# ceph osd set noout
   [ceph: root@host01 /]# ceph osd set noscrub
   [ceph: root@host01 /]# ceph osd set nodeep-scrub
   ```

8. ​						检查服务版本和可用目标容器： 				

   **语法**

   ​							

   

   ```none
   ceph orch upgrade check IMAGE_NAME
   ```

   **示例**

   ​							

   

   ```none
   [ceph: root@node0 /]# ceph orch upgrade check LOCAL_NODE_FQDN:5000/rhceph/rhceph-7-rhel9
   ```

9. ​						升级存储集群： 				

   **语法**

   ​							

   

   ```none
   ceph orch upgrade start IMAGE_NAME
   ```

   **示例**

   ​							

   

   ```none
   [ceph: root@node0 /]# ceph orch upgrade start LOCAL_NODE_FQDN:5000/rhceph/rhceph-7-rhel9
   ```

   ​						在升级过程中，`ceph status` 输出中会出现一个进度条。 				

   **示例**

   ​							

   

   ```none
   [ceph: root@node0 /]# ceph status
   [...]
   progress:
       Upgrade to 18.2.0-128.el9cp (1s)
         [............................]
   ```

10. ​						验证 Ceph 集群的新 *IMAGE_ID* 和 *VERSION* ： 				

    **示例**

    ​							

    

    ```none
    [ceph: root@node0 /]# ceph version
    [ceph: root@node0 /]# ceph versions
    [ceph: root@node0 /]# ceph orch ps
    ```

11. ​						升级完成后，取消设置 `noout`、`noscrub` 和 `nodeep-scrub` 标记： 				

    **示例**

    ​							

    

    ```none
    [ceph: root@host01 /]# ceph osd unset noout
    [ceph: root@host01 /]# ceph osd unset noscrub
    [ceph: root@host01 /]# ceph osd unset nodeep-scrub
    ```

**其它资源**

- ​						请参阅 *Red Hat Ceph Storage 安装指南*中的[*将 Red Hat Ceph Storage 节点注册到 CDN 并附加订阅*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/installation_guide#registering-the-red-hat-ceph-storage-nodes-to-the-cdn-and-attaching-subscriptions_install)部分。 				
- ​						请参阅 *Red Hat Ceph Storage 安装指南*中的[*为断开连接的安装配置私有 registry*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/installation_guide#configuring-a-private-registry-for-a-disconnected-installation_install) 一节。 				

# 第 2 章 将主机操作系统从 RHEL 8 升级到 RHEL 9

​			您可以使用 Leapp 程序执行 Red Hat Ceph Storage 主机操作系统从 Red Hat Enterprise Linux 8 升级到 Red Hat Enterprise Linux 9。 	

**先决条件**

- ​					正在运行的 Red Hat Ceph Storage 5.3 集群。 			

​			以下是容器化 Ceph 守护进程支持的组合。有关更多信息，请参阅 *Red Hat Ceph Storage 安装指南中的* [*colocation 的工作原理及其优点*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/installation_guide#how-colocation-works-and-its-advantages) 部分。 	

- ​					Ceph 元数据服务器(`ceph-mds`)、Ceph OSD (`ceph-osd`)和 Ceph 对象网关(`radosgw`) 			
- ​					Ceph Monitor (`ceph-mon`) *或* Ceph 管理器(`ceph-mgr`)、Ceph OSD (`ceph-osd`)和 Ceph 对象网关(`radosgw`) 			
- ​					Ceph Monitor (`ceph-mon`)、Ceph Manager (`ceph-mgr`)、Ceph OSD (`ceph-osd`)和 Ceph 对象网关(`radosgw`) 			

**流程**

1. ​					在带有服务的 Red Hat Enterprise Linux 8.8 上部署 Red Hat Ceph Storage 5.3。 			

注意

​				验证集群包含两个 admin 节点，以便在一个 *admin* 节点（使用 `_admin` 标签）执行主机升级时，可以使用第二个 admin 来管理集群。 		

​			具体说明，请参阅 *Red Hat Ceph Storage 安装指南*中的 [*Red Hat Ceph Storage 安装*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/installation_guide#red-hat-ceph-storage-installation)，以及*使用操作指南*中的[*服务规格部署 Ceph 守护进程*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/operations_guide/#deploying-the-ceph-daemons-using-the-service-specification)。 	

1. ​					在 Ceph OSD 上设置 `noout` 标志。 			

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph osd set noout
   ```

2. ​					使用 Leapp 程序一次执行主机升级一个节点。 			

   1. ​							在使用 Leapp 执行主机升级前，放置相应的节点维护模式。 					

      **语法**

      ​								

      

      ```none
      ceph orch host maintenance enter HOST
      ```

      **示例**

      ​								

      

      ```none
      ceph orch host maintenance enter host01
      ```

   2. ​							请参阅红帽客户门户网站中的在 Red Hat Enterprise Linux 产品文档中将 [*RHEL 8 升级到 RHEL 9*](https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/9/html/upgrading_from_rhel_8_to_rhel_9/)。 					

3. ​					验证 Ceph 集群的新 *IMAGE_ID* 和 *VERSION* ： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@node0 /]# ceph version
   [ceph: root@node0 /]# ceph orch ps
   ```

4. ​					按照 Red Hat Ceph Storage *升级指南中的升级 Red Hat Ceph Storage [\*集群步骤，继续 Red Hat Ceph Storage\*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/#upgrade-a-red-hat-ceph-storage-cluster-using-cephadm) 5.3 升级到 Red Hat Ceph Storage* 7。 			

# 第 3 章 在启用了扩展模式的将 RHCS 5 升级到 RHCS 7 到 RHEL 9 升级

​			您可以执行从 Red Hat Ceph Storage 5 升级到 Red Hat Ceph Storage 7，涉及 Red Hat Enterprise Linux 8 到启用了扩展模式的 Red Hat Enterprise Linux 9。 	

重要

​				在升级到最新版本的 Red Hat Ceph Storage 7 之前，升级到 Red Hat Ceph Storage 5 的最新版本。 		

**先决条件**

- ​					Red Hat Ceph Storage 5 on Red Hat Enterprise Linux 8，在启用了扩展模式的情况下运行了必要的主机和守护进程。 			
- ​					Ceph 二进制文件备份(`/usr/sbin/cephadm`)、ceph.pub (`/etc/ceph`)和来自管理节点的 Ceph 集群的公共 SSH 密钥。 			

**流程**

1. ​					登录到 Cephadm shell： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# cephadm shell
   ```

2. ​					在重新置备 admin 节点时，将第二个节点标记为 admin 以管理集群。 			

   **语法**

   ​						

   

   ```none
   ceph orch host label add HOSTNAME _admin
   ```

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph orch host label add host02_admin
   ```

3. ​					设置 `noout` 标志。 			

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph osd set noout
   ```

4. ​					排空主机中的所有守护进程： 			

   **语法**

   ​						

   

   ```none
   ceph orch host drain HOSTNAME --force
   ```

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph orch host drain host02 --force
   ```

   ​					`_no_schedule` 标签自动应用到阻止部署的主机。 			

5. ​					检查所有守护进程是否已从存储集群中移除： 			

   **语法**

   ​						

   

   ```none
   ceph orch ps HOSTNAME
   ```

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph orch ps host02
   ```

6. ​					zap 设备，以便在主机排空时有 OSD，则它们可用于在主机重新添加时重新部署 OSD。 			

   **语法**

   ​						

   

   ```none
   ceph orch device zap HOSTNAME DISK --force
   ```

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph orch device zap ceph-host02 /dev/vdb --force
   
   zap successful for /dev/vdb on ceph-host02
   ```

7. ​					检查移除 OSD 的状态： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph orch osd rm status
   ```

   ​					当 OSD 上没有剩余的放置组(PG)时，该 OSD 会停用并从存储集群中移除。 			

8. ​					从集群中删除主机： 			

   **语法**

   ​						

   

   ```none
   ceph orch host rm HOSTNAME --force
   ```

   **示例**

   ​						

   

   ```none
   [ceph: root@host01 /]# ceph orch host rm host02 --force
   ```

9. ​					将相应的主机从 RHEL 8 重新置备到 RHEL 9，如 [*从 RHEL 8 升级到 RHEL 9*](https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/9/html/upgrading_from_rhel_8_to_rhel_9/index) 所述。 			

10. ​					使用 `--limit` 选项运行 preflight playbook： 			

    **语法**

    ​						

    

    ```none
    ansible-playbook -i INVENTORY_FILE cephadm-preflight.yml --limit NEWHOST_NAME
    ```

    **示例**

    ​						

    

    ```none
    [ceph: root@host01 /]# ansible-playbook -i hosts cephadm-preflight.yml --extra-vars "ceph_origin={storage-product}" --limit host02
    ```

    ​					preflight playbook 在新主机上安装 `podman`、`lvm2`、`chronyd` 和 `cephadm`。安装完成后，`cephadm` 驻留在 `/usr/sbin/` 目录中。 			

11. ​					将集群的公共 SSH 密钥提取到文件夹： 			

    **语法**

    ​						

    

    ```none
    ceph cephadm get-pub-key ~/PATH
    ```

    **示例**

    ​						

    

    ```none
    [ceph: root@host01 /]# ceph cephadm get-pub-key ~/ceph.pub
    ```

12. ​					将 Ceph 集群的公共 SSH 密钥复制到重新置备的节点： 			

    **语法**

    ​						

    

    ```none
    ssh-copy-id -f -i ~/PATH root@HOST_NAME_2
    ```

    **示例**

    ​						

    

    ```none
    [ceph: root@host01 /]# ssh-copy-id -f -i ~/ceph.pub root@host02
    ```

    1. ​							可选：如果移除的主机有一个 monitor 守护进程，则在将主机添加到集群前，请添加 `--unmanaged` 标志来监控部署。 					

       **语法**

       ​								

       

       ```none
       ceph orch apply mon PLACEMENT --unmanaged
       ```

13. ​					再次将主机添加到集群中，并添加之前存在的标签： 			

    **语法**

    ​						

    

    ```none
    ceph orch host add HOSTNAME IP_ADDRESS --labels=LABELS
    ```

    1. ​							可选：如果移除的主机最初部署了一个监控器守护进程，则需要使用 location 属性手动添加 monitor 守护进程，如将 [*tiebreaker 替换为新的监控器*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/{troubleshooting_guide}#troubleshooting-clusters-in-stretch-mode#replacing-the-tiebreaker-with-a-new-monitor_diag) 中所述。 					

       **语法**

       ​								

       

       ```none
       ceph mon add HOSTNAME IP LOCATION
       ```

       **示例**

       ​								

       

       ```none
       [ceph: root@host01 /]# ceph mon add ceph-host02 10.0.211.62 datacenter=DC2
       ```

       **语法**

       ​								

       

       ```none
       ceph orch daemon add mon HOSTNAME
       ```

       **示例**

       ​								

       

       ```none
       [ceph: root@host01 /]# ceph orch daemon add mon ceph-host02
       ```

14. ​					验证 re-provisioned 主机上的守护进程使用相同的 ceph 版本成功运行： 			

    **语法**

    ​						

    

    ```none
    ceph orch ps
    ```

15. ​					将 monitor 守护进程放置重新设置为 `managed`。 			

    **语法**

    ​						

    

    ```none
    ceph orch apply mon PLACEMENT
    ```

16. ​					对所有主机重复上述步骤。 			

    1. ​							.arbiter monitor 无法排空或从主机中删除。因此，需要将仲裁 mon 重新置备到另一个 tie-breaker 节点，然后从主机排空或删除，[*如将 tiebreaker 替换为新的监控器*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/{troubleshooting_guide}#troubleshooting-clusters-in-stretch-mode#replacing-the-tiebreaker-with-a-new-monitor_diag) 中所述。 					

17. ​					按照相同的方法重新置备 admin 节点，并使用第二个管理节点管理集群。 			

18. ​					再次将备份文件添加到节点。 			

19. ​					.再次使用第二个管理节点将 admin 节点添加到集群中。将 `mon` 部署设置为 `unmanaged`。 			

20. ​					[*遵循将 tiebreaker 替换为新 monitor*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/6/html/troubleshooting_guide/troubleshooting-clusters-in-stretch-mode#replacing-the-tiebreaker-with-a-new-monitor_diag)，以重新添加旧的仲裁 mon 并删除之前创建的临时 mon。 			

21. ​					取消设置 `noout` 标志。 			

    **语法**

    ​						

    

    ```none
    ceph osd unset noout
    ```

22. ​					验证 Ceph 版本和集群状态，以确保所有 demons 在 Red Hat Enterprise Linux 升级后按预期工作。 			

23. ​					按照 [*使用 cephadm 升级 Red Hat Ceph Storage 集群*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/6/html/upgrade_guide/upgrade-a-red-hat-ceph-storage-cluster-using-cephadm#doc-wrapper) 来执行 Red Hat Ceph Storage 5 到 Red Hat Ceph Storage 7 升级。 			

# 第 4 章 交错升级

​			作为存储管理员，您可以选择以阶段性的形式升级 Red Hat Ceph Storage 组件，而不是一次升级所有组件。`ceph orch upgrade` 命令允许您指定选项来限制单个升级命令升级哪些守护进程。 	

注意

​				如果要从不支持交错升级的版本进行升级，您需要首先手动升级 Ceph Manager (`ceph-mgr`) 守护进程。有关从之前版本执行交错升级的更多信息，请参阅[*从之前版本进行升级*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/index#performing-a-staggered-upgrade-from-previous-releases_upgrade)。 		

## 4.1. 交错升级选项

​				`ceph orch upgrade` 命令支持几个选项来以阶段的形式升级集群组件。交错升级选项包括： 		

- ​						**--daemon_types**: `--daemon_types` 选项使用以逗号分隔的守护进程类型列表，只升级这些类型的守护进程。此选项的有效守护进程类型包括 `mgr`,`mon`,`crash`,`osd`,`mds`,`rgw`,`rbd-mirror`,`cephfs-mirror`, 和 `nfs`。 				
- ​						**--services**：`--services` 选项与 `--daemon-types` 相互排斥，每次只能使用其中一个选项，仅升级属于这些服务的守护进程。例如，您无法同时提供 OSD 和 RGW 服务。 				
- ​						**--hosts** ：您可以将 `--hosts` 选项与 `--daemon_types`、`--services` 结合使用，或者单独使用这个选项。`--hosts` 选项参数的格式与编配器 CLI 放置规格的命令行选项相同。 				
- ​						**--limit**：`--limit` 选项使用一个大于零的整数，它提供了 `cephadm` 要升级的守护进程的数量限制。您可以将 `--limit` 选项与 `--daemon_types`、`--services` 或 `--hosts` 组合使用。例如，如果要升级 `host01` 上的类型为 `osd` 的守护进程，并将限制设置为 `3`，则 `cephadm` 将最多升级 host01 上的三个 OSD 守护进程。 				

### 4.1.1. 执行交错升级

​					作为存储管理员，您可以使用 `ceph orch upgrade` 选项来限制单个升级命令升级哪些守护进程。 			

​					Cephadm 严格强制实施在升级场景中仍然存在的守护进程升级的顺序。当前升级顺序是： 			

- ​							Ceph Manager 节点 					
- ​							Ceph 监控节点 					
- ​							Ceph-crash 守护进程 					
- ​							Ceph OSD 节点 					
- ​							Ceph 元数据服务器 (MDS) 节点 					
- ​							Ceph 对象网关(RGW)节点 					
- ​							Ceph RBD-mirror 节点 					
- ​							CephFS-mirror 节点 					
- ​							Ceph NFS 节点 					

注意

​						如果您的参数所指定的守护进程升级没有遵循正常的顺序，则升级命令会阻止并记录在继续执行前需要先升级哪些守护进程。 				

**示例**

​							



```none
[ceph: root@host01 /]# ceph orch upgrade start --image  registry.redhat.io/rhceph/rhceph-7-rhel9:latest --hosts host02

Error EINVAL: Cannot start upgrade. Daemons with types earlier in upgrade order than daemons on given host need upgrading.
Please first upgrade mon.ceph-host01
```

**先决条件**

- ​							运行 Red Hat Ceph Storage 5.3 或 6.1 的集群。 					
- ​							所有节点的根级别访问权限。 					
- ​							存储集群中至少有两个 Ceph 管理器节点：一个活跃节点和一个备用节点。 					

**流程**

1. ​							登录 `cephadm` shell： 					

   **示例**

   ​								

   

   ```none
   [root@host01 ~]# cephadm shell
   ```

2. ​							确定所有主机都在线，并且存储集群处于健康状态： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph -s
   ```

3. ​							设置 OSD `noout`、`noscrub`、和 `nodeep-scrub` 标记，以防止 OSD 在升级过程中被标记为 out，并避免对集群造成不必要的负载： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph osd set noout
   [ceph: root@host01 /]# ceph osd set noscrub
   [ceph: root@host01 /]# ceph osd set nodeep-scrub
   ```

4. ​							检查服务版本和可用目标容器： 					

   **语法**

   ​								

   

   ```none
   ceph orch upgrade check IMAGE_NAME
   ```

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph orch upgrade check registry.redhat.io/rhceph/rhceph-7-rhel9:latest
   ```

5. ​							升级存储集群： 					

   1. ​									在特定主机上升级特定的守护进程类型： 							

      **语法**

      ​										

      

      ```none
      ceph orch upgrade start --image IMAGE_NAME --daemon-types DAEMON_TYPE1,DAEMON_TYPE2 --hosts HOST1,HOST2
      ```

      **示例**

      ​										

      

      ```none
      [ceph: root@host01 /]# ceph orch upgrade start --image registry.redhat.io/rhceph/rhceph-7-rhel9:latest --daemon-types mgr,mon --hosts host02,host03
      ```

   2. ​									指定特定的服务，并限制要升级的守护进程数量： 							

      **语法**

      ​										

      

      ```none
      ceph orch upgrade start --image IMAGE_NAME --services SERVICE1,SERVICE2 --limit LIMIT_NUMBER
      ```

      **示例**

      ​										

      

      ```none
      [ceph: root@host01 /]# ceph orch upgrade start --image registry.redhat.io/rhceph/rhceph-7-rhel9:latest --services rgw.example1,rgw1.example2 --limit 2
      ```

      注意

      ​										在交错升级时，如果使用限制参数，则在升级 Ceph Manager 守护进程后会刷新监控堆栈守护进程（包括 Prometheus 和 `node-exporter`）。由于使用了限制参数，Ceph Manager 升级需要更长时间才能完成。监控堆栈守护进程的版本可能不会在 Ceph 发行版本之间有所变化，在这种情况下，它们只会重新部署。 								

      注意

      ​										对参数进行限制的升级命令会验证开始升级前的选项，这样可能需要拉取新的容器镜像。因此，在您提供限制参数时，`upgrade start` 命令可能需要一段时间才能返回。 								

6. ​							要查看您仍然需要升级的守护进程，请运行 `ceph orch upgrade check` 或 `ceph versions` 命令： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph orch upgrade check --image registry.redhat.io/rhceph/rhceph-7-rhel9:latest
   ```

7. ​							要完成交错升级，请验证所有剩余的服务的升级： 					

   **语法**

   ​								

   

   ```none
   ceph orch upgrade start --image IMAGE_NAME
   ```

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph orch upgrade start --image registry.redhat.io/rhceph/rhceph-7-rhel9:latest
   ```

**验证**

- ​							验证 Ceph 集群的新 *IMAGE_ID* 和 *VERSION* ： 					

  **示例**

  ​								

  

  ```none
  [ceph: root@host01 /]# ceph versions
  [ceph: root@host01 /]# ceph orch ps
  ```

  1. ​									升级完成后，取消设置 `noout`、`noscrub` 和 `nodeep-scrub` 标记： 							

     **示例**

     ​										

     

     ```none
     [ceph: root@host01 /]# ceph osd unset noout
     [ceph: root@host01 /]# ceph osd unset noscrub
     [ceph: root@host01 /]# ceph osd unset nodeep-scrub
     ```

### 4.1.2. 从之前的版本执行标签升级

​					从 Red Hat Ceph Storage 5.2 开始，您可以通过提供必要的参数来在存储集群上执行标签升级 			

​					您可以通过提供必要的参数，在存储集群上执行标签升级。如果要从不支持交错升级的版本进行升级，您需要首先手动升级 Ceph Manager (`ceph-mgr`) 守护进程。升级 Ceph Manager 守护进程后，您可以传递限制参数来完成提取升级。 			

重要

​						在尝试此步骤前，验证您至少有两个正在运行的 Ceph Manager 守护进程。 				

**先决条件**

- ​							运行 Red Hat Ceph Storage 5.2 或更小的集群。 					
- ​							存储集群中至少有两个 Ceph 管理器节点：一个活跃节点和一个备用节点。 					

**流程**

1. ​							登录到 Cephadm shell： 					

   **示例**

   ​								

   

   ```none
   [root@host01 ~]# cephadm shell
   ```

2. ​							确定哪个 Ceph Manager 处于活跃状态且处于待机状态： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph -s
     cluster:
       id:     266ee7a8-2a05-11eb-b846-5254002d4916
       health: HEALTH_OK
   
   
     services:
       mon: 2 daemons, quorum host01,host02 (age 92s)
       mgr: host01.ndtpjh(active, since 16h), standbys: host02.pzgrhz
   ```

3. ​							手动升级每个备用 Ceph Manager 守护进程： 					

   **语法**

   ​								

   

   ```none
   ceph orch daemon redeploy mgr.ceph-HOST.MANAGER_ID --image IMAGE_ID
   ```

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph orch daemon redeploy mgr.ceph-host02.pzgrhz --image registry.redhat.io/rhceph/rhceph-7-rhel9:latest
   ```

4. ​							故障切换到升级的备用 Ceph Manager： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph mgr fail
   ```

5. ​							检查备用 Ceph Manager 现在是否活跃： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph -s
     cluster:
       id:     266ee7a8-2a05-11eb-b846-5254002d4916
       health: HEALTH_OK
   
   
     services:
       mon: 2 daemons, quorum host01,host02 (age 1h)
       mgr: host02.pzgrhz(active, since 25s), standbys: host01.ndtpjh
   ```

6. ​							验证活跃的 Ceph Manager 是否已升级到新版本： 					

   **语法**

   ​								

   

   ```none
   ceph tell mgr.ceph-HOST.MANAGER_ID version
   ```

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph tell mgr.host02.pzgrhz version
   {
       "version": "18.2.0-128.el8cp",
       "release": "reef",
       "release_type": "stable"
   }
   ```

7. ​							重复步骤 2 - 6，将剩余的 Ceph Manager 升级到新版本。 					

8. ​							检查所有 Ceph Manager 是否已升级到新版本： 					

   **示例**

   ​								

   

   ```none
   [ceph: root@host01 /]# ceph mgr versions
   {
       "ceph version 18.2.0-128.el8cp (600e227816517e2da53d85f2fab3cd40a7483372) pacific (stable)": 2
   }
   ```

9. ​							升级所有 Ceph Manager 后，您可以指定限制参数并完成交错升级的其余部分。 					

**其它资源**

- ​							有关执行升级和标签升级选项的更多信息，请参阅[*执行标签升级*](https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/7/html-single/upgrade_guide/index#performing-a-staggered-upgrade_upgrade)。 					

# 第 5 章 监控和管理存储集群的升级

​			运行 `ceph orch upgrade start` 命令以升级 Red Hat Ceph Storage 集群后，您可以检查升级过程的状态、暂停、恢复或停止。集群的健康状态在升级过程中更改为 `HEALTH_WARNING`。如果集群的主机离线，升级将暂停。 	

注意

​				您需要在另一个守护进程后升级一个守护进程类型。如果无法升级守护进程，则会暂停升级。 		

**先决条件**

- ​					正在运行的 Red Hat Ceph Storage 集群 6。 			
- ​					所有节点的根级别访问权限。 			
- ​					存储集群中至少有两个 Ceph 管理器节点：一个活跃节点和一个备用节点。 			
- ​					启动了存储集群的升级。 			

**流程**

1. ​					确定升级是否正在进行以及集群要升级到的版本： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@node0 /]# ceph orch upgrade status
   ```

   注意

   ​						升级成功后，您不会收到消息。运行 `ceph versions` 和 `ceph orch ps` 命令，以验证新的镜像 ID 和存储集群的版本。 				

2. ​					可选：暂停升级过程： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@node0 /]# ceph orch upgrade pause
   ```

3. ​					可选：恢复暂停的升级过程： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@node0 /]# ceph orch upgrade resume
   ```

4. ​					可选：停止升级过程： 			

   **示例**

   ​						

   

   ```none
   [ceph: root@node0 /]# ceph orch upgrade stop
   ```

# 第 6 章 升级错误消息故障排除

​			下表显示了一些 `cephadm` 升级错误消息：如果 `cephadm` 升级因任何原因失败，存储集群健康状态中会出现错误消息。 	

| 错误消息               | 描述                                                         |
| ---------------------- | ------------------------------------------------------------ |
| UPGRADE_NO_STANDBY_MGR | Ceph 需要活跃和备用管理器守护进程才能继续，但目前没有待机。  |
| UPGRADE_FAILED_PULL    | Ceph 无法拉取目标版本的容器镜像。如果您指定不存在的版本或容器镜像（如 1.2.3），或者无法从集群中的一个或多个主机访问容器 registry，则会出现此情况。 |

# 法律通告