# 监控

[TOC]

## 概述

Ceph Dashboard 使用 Prometheus、Grafana 和相关工具来存储和可视化集群利用率和性能的详细指标。Ceph 用户有三种选择：

1. 让 cephadm 部署和配置这些服务。这是引导新集群时的默认设置，除非使用 `--skip-monitoring-stack` 选项。
2. 手动部署和配置这些服务。这是推荐给在其环境中具有现有prometheus 服务的用户（以及 Ceph is running in Kubernetes with Rook 的情况）。
3. 完全跳过监视堆栈。一些 Ceph dashboard 的图形将不可用。

The monitoring stack consists of [Prometheus](https://prometheus.io/), Prometheus exporters ([Prometheus Module](https://docs.ceph.com/en/latest/mgr/prometheus/#mgr-prometheus), [Node exporter](https://prometheus.io/docs/guides/node-exporter/)), [Prometheus Alert Manager](https://prometheus.io/docs/alerting/alertmanager/) 和 [Grafana](https://grafana.com/).

> **Note:**
>
> Prometheus’ security model presumes that untrusted users have access to the Prometheus HTTP endpoint and logs. Untrusted users have access to all the (meta)data Prometheus collects that is contained in the database, plus a variety of operational and debugging information.
>
> However, Prometheus’ HTTP API is limited to read-only operations. Configurations can *not* be changed using the API and secrets are not exposed. Moreover, Prometheus has some built-in measures to mitigate the impact of denial of service attacks.
>
> Please see Prometheus’ Security model <https://prometheus.io/docs/operating/security/> for more detailed information.
>
> 普罗米修斯的安全模型假定不受信任的用户可以访问普罗米修斯HTTP端点和日志。不受信任的用户可以访问Prometheus收集的数据库中的所有（元）数据，以及各种操作和调试信息。
>
> 然而，Prometheus的HTTP API仅限于只读操作。不能使用API更改配置，并且不会泄露秘密。此外，普罗米修斯有一些内置的措施来减轻拒绝服务攻击的影响。
>
> 请参阅普罗米修斯的安全模型了解更多详细信息。

## 使用 cephadm 部署监控

The default behavior of `cephadm` is to deploy a basic monitoring stack.  It is however possible that you have a Ceph cluster without a monitoring stack, and you would like to add a monitoring stack to it. (Here are some ways that you might have come to have a Ceph cluster without a monitoring stack: You might have passed the `--skip-monitoring stack` option to `cephadm` during the installation of the cluster, or you might have converted an existing cluster (which had no monitoring stack) to cephadm management.)

cephadm 的默认行为是部署基本的监视堆栈。然而，您可能有一个没有监视堆栈的Ceph集群，并且您希望向其中添加监视堆栈。（这里有一些方法可以使Ceph集群没有监视堆栈：您可能在集群安装期间将--skip监视堆栈选项传递给cephadm，或者您可能将现有集群（没有监视堆栈）转换为cephadm管理。）

To set up monitoring on a Ceph cluster that has no monitoring, follow the steps below:要在没有监控的Ceph集群上设置监控，请执行以下步骤：

1. Deploy a node-exporter service on every node of the cluster.  The node-exporter provides host-level metrics like CPU and memory  utilization:在集群的每个节点上部署节点导出器服务。节点导出器提供主机级指标，如CPU和内存利用率：

   ```bash
   ceph orch apply node-exporter
   ```

2. 部署 alertmanager：

   ```bash
   ceph orch apply alertmanager
   ```

3. 部署 Prometheus 。一个 Prometheus 实例就足够了，但对于高可用性（HA），您可能需要部署两个：

   ```bash
   ceph orch apply prometheus
   ```

   或者:

   ```bash
   ceph orch apply prometheus --placement 'count:2'
   ```

4. 部署 grafana：

   ```bash
   ceph orch apply grafana
   ```

### Centralized Logging in Ceph

Ceph now provides centralized logging with Loki & Promtail.  Centralized Log Management (CLM) consolidates all log data and pushes it to a central repository, with an accessible and easy-to-use interface. Centralized logging is  designed to make your life easier. Some of the advantages are:

1. **Linear event timeline**: it is easier to troubleshoot issues analyzing a single chain of events than thousands of different logs from a hundred nodes.
2. **Real-time live log monitoring**: it is impractical to follow logs from thousands of different sources.
3. **Flexible retention policies**: with per-daemon logs, log rotation is usually set to a short interval (1-2 weeks) to save disk usage.
4. **Increased security & backup**: logs can contain sensitive information and expose usage patterns. Additionally, centralized logging allows for HA, etc.

Centralized Logging in Ceph is implemented using two new services - `loki` & `promtail`.

Loki: It is basically a log aggregation system and is used to query logs. It can be configured as a datasource in Grafana.

Promtail: It acts as an agent that gathers logs from the system and makes them available to Loki.

These two services are not deployed by default in a Ceph cluster. To  enable the centralized logging you can follow the steps mentioned here [Enable Centralized Logging in Dashboard](https://docs.ceph.com/en/latest/mgr/dashboard/#centralized-logging).

Ceph中的集中测井

Ceph现在使用Loki&Promtail提供集中日志记录。集中式日志管理（CLM）整合所有日志数据并将其推送到中央存储库，具有可访问且易于使用的界面。集中日志记录旨在让您的生活更轻松。其优点包括：

线性事件时间线：分析单个事件链比分析来自100个节点的数千个不同日志更容易解决问题。

实时实时日志监控：跟踪来自数千个不同来源的日志是不切实际的。

灵活的保留策略：对于每个守护程序日志，日志循环通常设置为短间隔（1-2周）以节省磁盘使用。

提高安全性和备份：日志可能包含敏感信息并暴露使用模式。此外，集中式日志记录允许HA等。

Ceph中的集中式日志记录是使用两个新服务实现的：loki和promtail。

Loki：它基本上是一个日志聚合系统，用于查询日志。它可以在Grafana中配置为数据源。

Promtail：它充当一个代理，从系统中收集日志并将其提供给Loki。

默认情况下，这两个服务不会部署在Ceph集群中。要启用集中式日志记录，您可以按照此处提到的步骤在仪表板中启用集中式日志。

### 网络和端口

所有监控服务都可以使用 yaml 服务规范配置它们绑定到的网络和端口。

```yaml
service_type: grafana
service_name: grafana
placement:
  count: 1
networks:
- 192.169.142.0/24
spec:
  port: 4200
```

### 使用自定义镜像

It is possible to install or upgrade monitoring components based on other images.  To do so, the name of the image to be used needs to be stored in the configuration first.  The following configuration options are available.

可以基于其他映像安装或升级监控组件。为此，需要首先将要使用的图像的名称存储在配置中。以下配置选项可用。

- `container_image_prometheus`
- `container_image_grafana`
- `container_image_alertmanager`
- `container_image_node_exporter`

可以使用 `ceph config` 命令设置自定义镜像

```bash
ceph config set mgr mgr/cephadm/<option_name> <value>
```

示例：

```bash
ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1
```

If there were already running monitoring stack daemon(s) of the type whose image you’ve changed, you must redeploy the daemon(s) in order to have them actually use the new image.

如果已经运行了您更改了其映像类型的监视堆栈守护程序，则必须重新部署守护程序，以便它们实际使用新映像。

例如，如果改变了 prometheus 的镜像

```bash
ceph orch redeploy prometheus
```

> **Note:**
>
> By setting a custom image, the default value will be overridden (but not overwritten).  The default value changes when updates become available. By setting a custom image, you will not be able to update the component you have set the custom image for automatically.  You will need to manually update the configuration (image name and tag) to be able to install updates.
>
> If you choose to go with the recommendations instead, you can reset the custom image you have set before.  After that, the default value will be used again.  Use `ceph config rm` to reset the configuration option
>
> 通过设置自定义图像，将覆盖（但不覆盖）默认值。当更新可用时，默认值会更改。通过设置自定义图像，您将无法自动更新为其设置自定义图像的组件。您需要手动更新配置（图像名称和标记），以便能够安装更新。
>
> 如果您选择使用建议，则可以重置之前设置的自定义图像。之后，将再次使用默认值。使用ceph-config rm重置配置选项
>
> ```bash
> ceph config rm mgr mgr/cephadm/<option_name>
> ```
>
> 例如：
>
> ```bash
> ceph config rm mgr mgr/cephadm/container_image_prometheus
> ```

### 使用自定义配置文件

By overriding cephadm templates, it is possible to completely customize the configuration files for monitoring services.

通过覆盖cephadm模板，可以完全定制监控服务的配置文件。

Internally, cephadm already uses [Jinja2](https://jinja.palletsprojects.com/en/2.11.x/) templates to generate the configuration files for all monitoring components. To be able to customize the configuration of Prometheus, Grafana or the Alertmanager it is possible to store a Jinja2 template for each service that will be used for configuration generation instead. This template will be evaluated every time a service of that kind is deployed or reconfigured. That way, the custom configuration is preserved and automatically applied on future deployments of these services.

在内部，cephadm已经使用Jinja2模板为所有监控组件生成配置文件。从17.2.3版开始，cephadm使用Prometheus http服务发现支持http sd config，以便从Ceph获取当前配置的目标。在内部，ceph-mgr在：8765/sd/（端口可通过可变服务发现端口配置）提供了一个服务发现端点，Prometheus使用该端点来获取所需的目标。

Customers with external monitoring stack can use ceph-mgr service discovery endpoint to get scraping configuration. Root certificate of the server can be obtained by the following command:

具有外部监控堆栈的客户可以使用ceph-mgr服务发现端点来获取抓取配置。可以通过以下命令获取服务器的根证书：

```bash
ceph orch sd dump cert
```

The configuration of Prometheus, Grafana, or Alertmanager may be customized by storing a Jinja2 template for each service. This template will be evaluated every time a service of that kind is deployed or reconfigured. That way, the custom configuration is preserved and automatically applied on future deployments of these services.

Prometheus、Grafana或Alertmanager的配置可以通过为每个服务存储Jinja2模板来定制。每次部署或重新配置此类服务时都会评估此模板。这样，将保留自定义配置并自动应用于这些服务的未来部署。

> **Note：**
>
> The configuration of the custom template is also preserved when the default configuration of cephadm changes. If the updated configuration is to be used, the custom template needs to be migrated *manually* after each upgrade of Ceph.
>
> 当cephadm的默认配置更改时，也会保留自定义模板的配置。如果要使用更新的配置，则需要在每次Ceph升级后手动迁移自定义模板。

#### Option names[](https://docs.ceph.com/en/latest/cephadm/services/monitoring/#option-names)

The following templates for files that will be generated by cephadm can be overridden. These are the names to be used when storing with `ceph config-key set`:

- `services/alertmanager/alertmanager.yml`
- `services/grafana/ceph-dashboard.yml`
- `services/grafana/grafana.ini`
- `services/prometheus/prometheus.yml`
- services/prometheus/alerting/custom_alerts.yml
- services/loki.yml
- services/promtail.yml

You can look up the file templates that are currently used by cephadm in `src/pybind/mgr/cephadm/templates`:

- `services/alertmanager/alertmanager.yml.j2`
- `services/grafana/ceph-dashboard.yml.j2`
- `services/grafana/grafana.ini.j2`
- `services/prometheus/prometheus.yml.j2`
- services/loki.yml.j2
- services/promtail.yml.j2

#### Usage

The following command applies a single line value:

```
ceph config-key set mgr/cephadm/<option_name> <value>
```

To set contents of files as template use the `-i` argument:

```
ceph config-key set mgr/cephadm/<option_name> -i $PWD/<filename>
```

> **Note：**
>
> When using files as input to `config-key` an absolute path to the file must be used.

Then the configuration file for the service needs to be recreated. This is done using reconfig. For more details see the following example.

#### 示例

```bash
# set the contents of ./prometheus.yml.j2 as template
ceph config-key set mgr/cephadm/services/prometheus/prometheus.yml \
  -i $PWD/prometheus.yml.j2

# reconfig the prometheus service
ceph orch reconfig prometheus
```

```bash
# set additional custom alerting rules for Prometheus
ceph config-key set mgr/cephadm/services/prometheus/alerting/custom_alerts.yml \
  -i $PWD/custom_alerts.yml

# Note that custom alerting rules are not parsed by Jinja and hence escaping
# will not be an issue.
```

## 不使用 cephadm 部署监控

If you have an existing prometheus monitoring infrastructure, or would like to manage it yourself, you need to configure it to integrate with your Ceph cluster.

- Enable the prometheus module in the ceph-mgr daemon

  ```
  ceph mgr module enable prometheus
  ```

  By default, ceph-mgr presents prometheus metrics on port 9283 on each host running a ceph-mgr daemon.  Configure prometheus to scrape these.

To make this integration easier, Ceph provides by means of ceph-mgr a service discovery endpoint at <https://<mgr-ip>:8765/sd/ which can be used by an external Prometheus to retrieve targets information. Information reported by this EP used the format specified by http_sd_config <https://prometheus.io/docs/prometheus/2.28/configuration/configuration/#http_sd_config>

* To enable the dashboard’s prometheus-based alerting, see [Enabling Prometheus Alerting](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-alerting).

- To enable dashboard integration with Grafana, see [Enabling the Embedding of Grafana Dashboards](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-grafana).

## 禁用监控

要禁用监控并删除支持监控的软件，请运行以下命令：

```bash
ceph orch rm grafana
ceph orch rm prometheus --force   # this will delete metrics data collected so far
ceph orch rm node-exporter
ceph orch rm alertmanager
ceph mgr module disable prometheus
```

## 设置 RBD-Image 监控

Due to performance reasons, monitoring of RBD images is disabled by default. For more information please see [Ceph Health Checks](https://docs.ceph.com/en/latest/mgr/prometheus/#prometheus-rbd-io-statistics). If disabled, the overview and details dashboards will stay empty in Grafana and the metrics will not be visible in Prometheus.

由于性能原因，默认情况下禁用RBD图像的监视。有关更多信息，请参阅Ceph Health Checks。如果禁用，Grafana中的概览和详细信息仪表板将保持空白，Prometheus中的指标将不可见。

## 设置 Prometheus

### 设置 Prometheus 保留大小和时间

Cephadm can configure Prometheus TSDB retention by specifying `retention_time` and `retention_size` values in the Prometheus service spec. The retention time value defaults to 15 days (15d). Users can set a different value/unit where supported units are: ‘y’, ‘w’, ‘d’, ‘h’, ‘m’ and ‘s’. The retention size value defaults to 0 (disabled). Supported units in this case are: ‘B’, ‘KB’, ‘MB’, ‘GB’, ‘TB’, ‘PB’ and ‘EB’.

Cephadm可以通过在Prometheus服务规范中指定保留时间和保留大小值来配置PrometheusTSDB保留。保留时间值默认为15天（15d）。用户可以设置不同的值/单位，其中支持的单位为：“y”、“w”、“d”、“h”、“m”和“s”。保留大小值默认为0（禁用）。本例中支持的单位为：“B”、“KB”、“MB”、“GB”、“TB”、“PB”和“EB”。

在下面的示例规范中，我们将保留时间设置为 1 年，大小设置为1GB 。

```yaml
service_type: prometheus
placement:
  count: 1
spec:
  retention_time: "1y"
  retention_size: "1GB"
```

> **Note：**
>
> If you already had Prometheus daemon(s) deployed before and are updating an existent spec as opposed to doing a fresh Prometheus deployment, you must also tell cephadm to redeploy the Prometheus daemon(s) to put this change into effect. This can be done with a `ceph orch redeploy prometheus` command.

## 设置 Grafana

### Manually setting the Grafana URL

Cephadm automatically configures Prometheus, Grafana, and Alertmanager in all cases except one.

In a some setups, the Dashboard user’s browser might not be able to access the Grafana URL that is configured in Ceph Dashboard. This can happen when the cluster and the accessing user are in different DNS zones.

If this is the case, you can use a configuration option for Ceph Dashboard to set the URL that the user’s browser will use to access Grafana. This value will never be altered by cephadm. To set this configuration option, issue the following command:

```bash
ceph dashboard set-grafana-frontend-api-url <grafana-server-api>
```

It might take a minute or two for services to be deployed. After the services have been deployed, you should see something like this when you issue the command `ceph orch ls`:

```bash
ceph orch ls
NAME           RUNNING  REFRESHED  IMAGE NAME                                      IMAGE ID        SPEC
alertmanager       1/1  6s ago     docker.io/prom/alertmanager:latest              0881eb8f169f  present
crash              2/2  6s ago     docker.io/ceph/daemon-base:latest-master-devel  mix           present
grafana            1/1  0s ago     docker.io/pcuzner/ceph-grafana-el8:latest       f77afcf0bcf6   absent
node-exporter      2/2  6s ago     docker.io/prom/node-exporter:latest             e5a616e4b9cf  present
prometheus         1/1  6s ago     docker.io/prom/prometheus:latest                e935122ab143  present
```

### Configuring SSL/TLS for Grafana

`cephadm` deploys Grafana using the certificate defined in the ceph key/value store. If no certificate is specified, `cephadm` generates a self-signed certificate during the deployment of the Grafana service.

A custom certificate can be configured using the following commands:

```bash
ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
```

Where hostname is the hostname for the host where grafana service is deployed.

If you have already deployed Grafana, run `reconfig` on the service to update its configuration:

```bash
ceph orch reconfig grafana
```

The `reconfig` command also sets the proper URL for Ceph Dashboard.

### Setting the initial admin password

By default, Grafana will not create an initial admin user. In order to create the admin user, please create a file `grafana.yaml` with this content:

```yaml
service_type: grafana
spec:
  initial_admin_password: mypassword
```

Then apply this specification:

```bash
ceph orch apply -i grafana.yaml
ceph orch redeploy grafana
```

Grafana will now create an admin user called `admin` with the given password.

## 设置 Alertmanager

### Adding Alertmanager webhooks

To add new webhooks to the Alertmanager configuration, add additional webhook urls like so:

```yaml
service_type: alertmanager
spec:
  user_data:
    default_webhook_urls:
    - "https://foo"
    - "https://bar"
```

Where `default_webhook_urls` is a list of additional URLs that are added to the default receivers’ `<webhook_configs>` configuration.

Run `reconfig` on the service to update its configuration:

```bash
ceph orch reconfig alertmanager
```

### Turn on Certificate Validation

If you are using certificates for alertmanager and want to make sure these certs are verified, you should set the “secure” option to true in your alertmanager spec (this defaults to false).

```yaml
service_type: alertmanager
spec:
  secure: true
```

If you already had alertmanager daemons running before applying the spec you must reconfigure them to update their configuration

```bash
ceph orch reconfig alertmanager
```

## 扩展阅读

- [Prometheus Module](https://docs.ceph.com/en/latest/mgr/prometheus/#mgr-prometheus)

作为存储管理员，您可以使用后端中带有 Cephadm 的 Ceph 编排器来部署监控和警报堆栈。监控堆栈由  Prometheus、Prometheus、Prometheus Alertmanager 和 Grafana 组成。用户需要在 YAML  配置文件中使用 Cephadm  定义这些服务，或者使用命令行界面来部署这些服务。部署多个相同类型的服务时，会部署一个高可用性的设置。节点导出器对此规则的一个例外。 	

注意

Red Hat Ceph Storage 5.0 不支持用于部署监控服务的自定义镜像，如 Prometheus、Grafana、Alertmanager 和 node-exporter。 		

以下监控服务可使用 Cephadm 部署： 	

- Prometheus 是监控和警报工具包。它收集 Prometheus 地区提供的数据，并在达到预定义阈值时触发预配置警报。Prometheus manager 模块提供了一个 Prometheus 导出器，用于从 `ceph-mgr` 中的收集点传递 Ceph 性能计数器。 			

  Prometheus 配置（包括提取目标）由 Cephadm 自动设置，如提供守护进程的指标。Cephadm 也会部署一组默认警报，如健康错误、10％ OSD 停机或 pgs inactive。 			

- Alertmanager 处理 Prometheus  服务器发送的警报。它重复数据删除、分组和将警报路由到正确的接收方。默认情况下，Ceph 控制面板自动配置为接收器。Alertmanager 处理 Prometheus 服务器发送的警报。可以使用 Alertmanager 来静默警报，但静默也可使用 Ceph 控制面板进行管理。 			

- Grafana 是视觉化和警报软件。此监控堆栈不使用 Grafana 的警报功能。用于警报，使用 Alertmanager。 			

  默认情况下，到 Grafana 的流量使用 TLS 加密。您可以提供自己的 TLS 证书，也可以使用自签名证书。如果在部署  Grafana 之前没有配置自定义证书，则会自动为 Grafana 创建和配置自签名证书。可以使用以下命令配置 Grafana 的自定义证书： 			

  ```none
   ceph config-key set mgr/cephadm/grafana_key -i PRESENT_WORKING_DIRECTORY/key.pem
   ceph config-key set mgr/cephadm/grafana_crt -i PRESENT_WORKING_DIRECTORY/certificate.pem
  ```

节点导出器是 Prometheus 的导出器，提供有关安装它的节点的数据。建议在所有节点上安装节点导出器。这可以通过将 'monitoring.yml 文件与 node-exporter 服务类型一起使用来实现。 	

## 部署监控堆栈

监控堆栈由 Prometheus、Prometheus、Prometheus Alertmanager 和 Grafana 组成。Ceph 控制面板利用这些组件存储和视觉化关于集群使用情况和性能的详细指标。 		

您可以使用 YAML 文件格式的服务规格部署监控堆栈。所有监控服务都可以在 `yml` 文件中具有它们绑定到的网络和端口。 				

**流程**

1. 在 Ceph 管理器守护进程中启用 prometheus 模块。这会公开内部 Ceph 指标，以便 Prometheus 可以读取它们：							

   ```none
   [ceph: root@host01 /]# ceph mgr module enable prometheus
   ```

   重要

   确保此命令在部署 Prometheus 之前运行。如果在部署前没有运行该命令，您必须重新部署 Prometheus 以更新配置： 					

   ```none
   ceph orch redeploy prometheus
   ```

2. 进入以下目录： 

   ```none
   cd /var/lib/ceph/DAEMON_PATH/		
   ```

   ```none
   [ceph: root@host01 mds/]# cd /var/lib/ceph/monitoring/
   ```

   注意如果目录 `监控` 不存在，请进行创建。 					

3. 创建 `monitoring.yml` 文件：

   ```none
   [ceph: root@host01 monitoring]# touch monitoring.yml
   ```

4. 使用类似以下示例的内容编辑规格文件： 

   ```yaml
   service_type: prometheus
   service_name: prometheus
   placement:
     hosts:
     - host01
   networks:
   - 192.169.142.0/24
   ---
   service_type: node-exporter
   ---
   service_type: alertmanager
   service_name: alertmanager
   placement:
     hosts:
     - host03
   networks:
   - 192.169.142.03/24
   ---
   service_type: grafana
   service_name: grafana
   placement:
     hosts:
     - host02
   networks:
   - 192.169.142.02/24
   ```

5. 应用监控服务：

   ```none
   ceph orch apply -i monitoring.yml
   ```

**验证**

- 列出服务：

  ```none
  [ceph: root@host01 /]# ceph orch ls
  ```

- 列出主机、守护进程和进程：		

  ```none
  ceph orch ps --service_name=SERVICE_NAME
  ```

  ```none
  [ceph: root@host01 /]# ceph orch ps --service_name=prometheus
  ```

重要

Prometheus、Grafana 和 Ceph 仪表板都会自动配置为相互通信，从而在 Ceph 控制面板中实现功能齐全的 Grafana 集成。 			

## 移除监控堆栈

您可以使用 `ceph orch rm` 命令删除监控堆栈。 				

**流程**

1. 使用 `ceph 或ch rm` 命令删除监控堆栈：

   ```none
   ceph orch rm SERVICE_NAME --force
   
   ceph orch rm grafana
   ceph orch rm prometheus
   ceph orch rm node-exporter
   ceph orch rm alertmanager
   ceph mgr module disable prometheus
   ```

2. 检查进程的状态：

   ```none
   ceph orch status
   ```

**验证**

- 列出服务： 

  ```none
  ceph orch ls
  ```

- 列出主机、守护进程和进程：

  ```none
  ceph orch ps
  ```





Ceph Dashboard uses [Prometheus](https://prometheus.io/), [Grafana](https://grafana.com/), and related tools to store and visualize detailed metrics on cluster utilization and performance.  Ceph users have three options:

1. Have cephadm deploy and configure these services.  This is the default when bootstrapping a new cluster unless the `--skip-monitoring-stack` option is used.
2. Deploy and configure these services manually.  This is recommended for users with existing prometheus services in their environment (and in cases where Ceph is running in Kubernetes with Rook).
3. Skip the monitoring stack completely.  Some Ceph dashboard graphs will not be available.

The monitoring stack consists of [Prometheus](https://prometheus.io/), Prometheus exporters ([Prometheus Module](https://docs.ceph.com/en/latest/mgr/prometheus/#mgr-prometheus), [Node exporter](https://prometheus.io/docs/guides/node-exporter/)), [Prometheus Alert Manager](https://prometheus.io/docs/alerting/alertmanager/) and [Grafana](https://grafana.com/).

Note

Prometheus’ security model presumes that untrusted users have access to the Prometheus HTTP endpoint and logs. Untrusted users have access to all the (meta)data Prometheus collects that is contained in the database, plus a variety of operational and debugging information.

However, Prometheus’ HTTP API is limited to read-only operations. Configurations can *not* be changed using the API and secrets are not exposed. Moreover, Prometheus has some built-in measures to mitigate the impact of denial of service attacks.

Please see Prometheus’ Security model <https://prometheus.io/docs/operating/security/> for more detailed information.

## Deploying monitoring with cephadm

By default, bootstrap will deploy a basic monitoring stack.  If you did not do this (by passing `--skip-monitoring-stack`, or if you converted an existing cluster to cephadm management, you can set up monitoring by following the steps below.

1. Enable the prometheus module in the ceph-mgr daemon.  This exposes the internal Ceph metrics so that prometheus can scrape them.

   ```
   ceph mgr module enable prometheus
   ```

2. Deploy a node-exporter service on every node of the cluster.  The node-exporter provides host-level metrics like CPU and memory  utilization.

   ```
   ceph orch apply node-exporter '*'
   ```

3. Deploy alertmanager

   ```
   ceph orch apply alertmanager 1
   ```

4. Deploy prometheus.  A single prometheus instance is sufficient, but for HA you may want to deploy two.

   ```
   ceph orch apply prometheus 1    # or 2
   ```

5. Deploy grafana

   ```
   ceph orch apply grafana 1
   ```

Cephadm takes care of the configuration of Prometheus, Grafana, and Alertmanager automatically.

However, there is one exception to this rule. In a some setups, the Dashboard user’s browser might not be able to access the Grafana URL configured in Ceph Dashboard. One such scenario is when the cluster and the accessing user are each in a different DNS zone.

For this case, there is an extra configuration option for Ceph Dashboard, which can be used to configure the URL for accessing Grafana by the user’s browser. This value will never be altered by cephadm. To set this configuration option, issue the following command:

```
$ ceph dashboard set-grafana-frontend-api-url <grafana-server-api>
```

It may take a minute or two for services to be deployed.  Once completed, you should see something like this from `ceph orch ls`

```
$ ceph orch ls
NAME           RUNNING  REFRESHED  IMAGE NAME                                      IMAGE ID        SPEC
alertmanager       1/1  6s ago     docker.io/prom/alertmanager:latest              0881eb8f169f  present
crash              2/2  6s ago     docker.io/ceph/daemon-base:latest-master-devel  mix           present
grafana            1/1  0s ago     docker.io/pcuzner/ceph-grafana-el8:latest       f77afcf0bcf6   absent
node-exporter      2/2  6s ago     docker.io/prom/node-exporter:latest             e5a616e4b9cf  present
prometheus         1/1  6s ago     docker.io/prom/prometheus:latest                e935122ab143  present
```

### Configuring SSL/TLS for Grafana

`cephadm` will deploy Grafana using the certificate defined in the ceph key/value store. If a certificate is not specified, `cephadm` will generate a self-signed certificate during deployment of the Grafana service.

A custom certificate can be configured using the following commands.

```
ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
```

If you already deployed Grafana, you need to `reconfig` the service for the configuration to be updated.

```
ceph orch reconfig grafana
```

The `reconfig` command also takes care of setting the right URL for Ceph Dashboard.

### Using custom images

It is possible to install or upgrade monitoring components based on other images.  To do so, the name of the image to be used needs to be stored in the configuration first.  The following configuration options are available.

- `container_image_prometheus`
- `container_image_grafana`
- `container_image_alertmanager`
- `container_image_node_exporter`

Custom images can be set with the `ceph config` command

```
ceph config set mgr mgr/cephadm/<option_name> <value>
```

For example

```
ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1
```

Note

By setting a custom image, the default value will be overridden (but not overwritten).  The default value changes when updates become available. By setting a custom image, you will not be able to update the component you have set the custom image for automatically.  You will need to manually update the configuration (image name and tag) to be able to install updates.

If you choose to go with the recommendations instead, you can reset the custom image you have set before.  After that, the default value will be used again.  Use `ceph config rm` to reset the configuration option

```
ceph config rm mgr mgr/cephadm/<option_name>
```

For example

```
ceph config rm mgr mgr/cephadm/container_image_prometheus
```

### Using custom configuration files

By overriding cephadm templates, it is possible to completely customize the configuration files for monitoring services.

Internally, cephadm already uses [Jinja2](https://jinja.palletsprojects.com/en/2.11.x/) templates to generate the configuration files for all monitoring components. To be able to customize the configuration of Prometheus, Grafana or the Alertmanager it is possible to store a Jinja2 template for each service that will be used for configuration generation instead. This template will be evaluated every time a service of that kind is deployed or reconfigured. That way, the custom configuration is preserved and automatically applied on future deployments of these services.

Note

The configuration of the custom template is also preserved when the default configuration of cephadm changes. If the updated configuration is to be used, the custom template needs to be migrated *manually*.

#### Option names

The following templates for files that will be generated by cephadm can be overridden. These are the names to be used when storing with `ceph config-key set`:

- `services/alertmanager/alertmanager.yml`
- `services/grafana/ceph-dashboard.yml`
- `services/grafana/grafana.ini`
- `services/prometheus/prometheus.yml`

You can look up the file templates that are currently used by cephadm in `src/pybind/mgr/cephadm/templates`:

- `services/alertmanager/alertmanager.yml.j2`
- `services/grafana/ceph-dashboard.yml.j2`
- `services/grafana/grafana.ini.j2`
- `services/prometheus/prometheus.yml.j2`

#### Usage

The following command applies a single line value:

```
ceph config-key set mgr/cephadm/<option_name> <value>
```

To set contents of files as template use the `-i` argument:

```
ceph config-key set mgr/cephadm/<option_name> -i $PWD/<filename>
```

Note

When using files as input to `config-key` an absolute path to the file must be used.

Then the configuration file for the service needs to be recreated. This is done using reconfig. For more details see the following example.

#### Example

```
# set the contents of ./prometheus.yml.j2 as template
ceph config-key set mgr/cephadm/services/prometheus/prometheus.yml \
  -i $PWD/prometheus.yml.j2

# reconfig the prometheus service
ceph orch reconfig prometheus
```

## Disabling monitoring

If you have deployed monitoring and would like to remove it, you can do so with

```
ceph orch rm grafana
ceph orch rm prometheus --force   # this will delete metrics data collected so far
ceph orch rm node-exporter
ceph orch rm alertmanager
ceph mgr module disable prometheus
```

## Deploying monitoring manually

If you have an existing prometheus monitoring infrastructure, or would like to manage it yourself, you need to configure it to integrate with your Ceph cluster.

- Enable the prometheus module in the ceph-mgr daemon

  ```
  ceph mgr module enable prometheus
  ```

  By default, ceph-mgr presents prometheus metrics on port 9283 on each host running a ceph-mgr daemon.  Configure prometheus to scrape these.

- To enable the dashboard’s prometheus-based alerting, see [Enabling Prometheus Alerting](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-alerting).

- To enable dashboard integration with Grafana, see [Enabling the Embedding of Grafana Dashboards](https://docs.ceph.com/en/latest/mgr/dashboard/#dashboard-grafana).

## Enabling RBD-Image monitoring

Due to performance reasons, monitoring of RBD images is disabled by default. For more information please see [RBD IO statistics](https://docs.ceph.com/en/latest/mgr/prometheus/#prometheus-rbd-io-statistics). If disabled, the overview and details dashboards will stay empty in Grafana and the metrics will not be visible in Prometheus.

## Other

开源的ceph-web项目，是非常简单的Web前端，通过ceph-rest-api获得数据并展示。

## Ceph-web

开源地址 https://github.com/tobegit3hub/ceph-web 。

目前ceph-web已经支持通过容器运行，执行下述命令即可一键启动Ceph监控工具。

```bash
docker run -d --net=host tobegit3hub/ceph-web
```

这样通过浏览器打开 http://127.0.0.1:8080 就可以看到以下管理界面。

![](../../Image/c/ceph_web.png)

![](../../Image/c/ceph_inkscope.jpg)