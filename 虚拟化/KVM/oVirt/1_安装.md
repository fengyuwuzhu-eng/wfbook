# 安装

[TOC]

## 自托管引擎

使用 Ansible 自动安装自托管引擎（self-hosted engine）。安装脚本（`hosted-engine --deploy`）在初始部署主机上运行，oVirt Engine（或 “engine” ）在该主机上创建的虚拟机中安装和配置。the oVirt Engine (or "engine")  is installed and configured on a virtual machine that is created on the  deployment host。Engine 和 Data Warehouse database 安装在 Engine 虚拟机上，但如果需要，可以在安装后迁移到单独的服务器。

可以运行 Engine 虚拟机的主机称为自托管引擎节点。至少需要两个自托管引擎节点来支持高可用性功能。

专用于 Engine 虚拟机的存储域称为自托管引擎存储域。此存储域由安装脚本创建，因此在开始安装之前必须准备好基础存储。

### 架构

oVirt Engine 作为虚拟机在其管理的同一环境中的自托管引擎节点（专用主机）上运行。自托管引擎环境需要更少的物理服务器，但需要更多的管理开销来部署和管理。引擎高度可用，无需外部 HA 管理。

自托管引擎环境的最低设置包括：

- 一个托管在自托管引擎节点上的虚拟机。Engine Appliance 用于自动安装 Enterprise Linux 8 虚拟机和该虚拟机上的Engine。
- 至少两个自托管引擎节点，用于虚拟机高可用性。可以使用Enterprise Linux 主机或 oVirt 节点。VDSM（host agent）在所有主机上运行，以便于与 oVirt Engine 进行通信。HA 服务在所有自托管引擎节点上运行，以管理引擎虚拟机的高可用性。
- 一个存储服务，可以在本地或远程服务器上托管，具体取决于所使用的存储类型。所有主机都必须可以访问存储服务。

![](../../../../Image/r/RHV_SHE_ARCHITECTURE1.png)

### 安装预览

自托管引擎安装使用 Ansible 和 Engine Appliance（预配置的引擎虚拟机映像）自动执行以下任务：

- 配置第一个自托管引擎节点。
- 在该节点上安装 Enterprise Linux 虚拟机。
- 在该虚拟机上安装和配置 oVirt 引擎。
- 配置自托管引擎存储域。

> **Note：**
>
> Engine Appliance 仅在安装期间使用。它不用于升级 Engine 。

安装自托管引擎环境涉及以下步骤：

1. 准备用于自托管引擎存储域和标准存储域的存储。可以使用以下存储类型之一：
   - NFS
   - iSCSI
   - Fibre Channel (FCP)
   - Gluster Storage
2. Install a deployment host to run the installation on.此主机将成为第一个自托管引擎节点。可以使用以下任一主机类型：
   - oVirt Node
   - Enterprise Linux
3. 安装并配置 oVirt 引擎：
   1. 启用和配置防火墙。
   2. 在部署主机上使用 `hosted-engine --deploy` 命令安装自托管引擎。
   3. 启用 oVirt Engine 存储库。
   4. 连接到管理门户以添加主机和存储域。
4. 将更多自托管引擎节点和标准主机添加到引擎。自托管引擎节点可以运行引擎虚拟机和其他虚拟机。标准主机可以运行所有其他虚拟机，但不能运行引擎虚拟机。
   1. 使用任一主机类型，或同时使用两者：
      - oVirt Node
      - Enterprise Linux
   2. 将主机作为自托管引擎节点添加到引擎。
   3. 将主机作为标准主机添加到引擎。
5. 向引擎添加更多存储域。除了引擎虚拟机之外，不建议其他任何人使用自托管引擎存储域。
6. 如果要在与引擎分离的服务器上托管任何数据库或服务，可以在安装完成后迁移它们。

> **Note：**
>
> 保持环境最新。由于经常发布已知问题的错误修复程序，请使用计划任务更新主机和引擎。

### 需求

#### oVirt Engine

##### 硬件

此处列出的最低和推荐硬件要求基于典型的中小型安装。具体需求因部署的大小和负载而异。

| 资源      | 最小值                                                       | 推荐值                                                       |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| CPU       | 双核 x86_64 CPU                                              | 一个四核 x86_64 CPU 或多个双核 x86_64 CPU                    |
| Memory    | 如果未安装数据仓库，并且现有进程未占用内存，4 GB 可用 RAM 。 | 16 GB 系统 RAM 。                                            |
| Hard Disk | 25 GB 的本地可访问、可写磁盘空间。                           | 50 GB 的本地可访问、可写磁盘空间。可以使用 [RHV Engine History Database Size Calculator](https://access.redhat.com/labs/rhevmhdsc/) 计算引擎历史数据库的适当磁盘空间。 |
| 网络接口  | 1个网络接口卡，带宽至少为 1 Gbps 。                          | 1个网络接口卡，带宽至少为 1 Gbps 。                          |

##### 浏览器

以下浏览器版本和操作系统可用于访问管理门户和 VM 门户。

浏览器测试分为以下几层：

- Tier 1: 经过充分测试的浏览器和操作系统组合。
- Tier 2: 浏览器和操作系统组合，经过部分测试，可能会工作。
- Tier 3: 浏览器和操作系统组合，未经测试，但可能有效。

| Support Tier | Operating System Family | Browser                                                      |
| ------------ | ----------------------- | ------------------------------------------------------------ |
| Tier 1       | Enterprise Linux        | Mozilla Firefox Extended Support Release (ESR) version       |
|              | Any                     | Most recent version of Google Chrome, Mozilla Firefox, or Microsoft Edge |
| Tier 2       |                         |                                                              |
| Tier 3       | Any                     | Earlier versions of Google Chrome or Mozilla Firefox         |
|              | Any                     | Other browsers                                               |

##### 客户端

在 Enterprise Linux 和 Windows 上，只能使用受支持的远程查看器（`virt-viewer`）客户端访问虚拟机控制台。要安装 `virt-viewer` ，请参阅《虚拟机管理指南》中的[在客户端计算机上安装支持组件](https://ovirt.org/documentation/virtual_machine_management_guide/index#sect-installing_supporting_components) 。安装 `virt-viewer` 需要管理员权限。

可以使用 SPICE、VNC 或 RDP（仅限Windows）协议访问虚拟机控制台。可以在 guest 操作系统中安装 QXLDOD 图形驱动程序，以改进 SPICE 的功能。SPICE 目前支持 2560 x 1600 像素的最大分辨率。

客户端操作系统 SPICE 支持

支持的 QXLDOD 驱动程序在 Enterprise Linux 7.2 及更高版本以及Windows 10 上可用。

> **Note：**
>
> SPICE 可以使用QXLDOD 驱动程序与 Windows 8 或 8.1 一起工作，但它既没有经过认证也没有经过测试。

##### 操作系统

oVirt Engine 必须安装在 Enterprise Linux 8.6 的 base 安装上。

请勿在 base 安装之后安装任何其他软件包，因为在尝试安装引擎所需的软件包时，这些软件包可能会导致依赖性问题。

除安装引擎所需的存储库外，不要启用其他存储库。

#### Host

要在虚拟化主机上使用 Enterprise Linux 9，必须禁用 UEFI 安全引导选项，原因是 [Bug 2081648 - dmidecode module fails to decode DMI data](https://bugzilla.redhat.com/show_bug.cgi?id=2081648).

##### CPU

所有 CPU 必须支持 Intel®64 或 AMD64 CPU 扩展以及 AMD-V™ 或启用 Intel VT® 硬件虚拟化扩展。还需要支持 No eXecute 标志（NX）。

支持以下 CPU 型号：

- AMD
  - Opteron G4
  - Opteron G5
  - EPYC
- Intel
  - Nehalem
  - Westmere
  - SandyBridge
  - IvyBridge
  - Haswell
  - Broadwell
  - Skylake Client
  - Skylake Server
  - Cascadelake Server

For each CPU model with security updates, the **CPU Type** lists a basic type and a secure type. For example:

Enterprise Linux 9 不支持 ppc64le 的虚拟化：CentOS Virtualization SIG 正在努力重新引入虚拟化支持，但尚未准备就绪。

oVirt 项目还提供了 64 位 ARM 架构（ARM64）的软件包，但仅作为技术预览。

对于每个具有安全更新的 CPU 型号，CPU 类型列出了基本类型和安全类型。例如：

- **Intel Cascadelake Server Family**
- **Secure Intel Cascadelake Server Family**

安全 CPU 类型包含最新更新。有关详细信息，请参阅 BZ#[1731395](https://bugzilla.redhat.com/1731395)

检查处理器是否支持所需标志。

您必须在 BIOS 中启用虚拟化。在此更改后关闭并重新启动主机，以确保应用更改。

程序：

1. 在 Enterprise Linux 或 oVirt Node 引导屏幕上，按任意键并从列表中选择 boot 或 boot with serial console 条目。

2. 按 Tab 键编辑所选选项的内核参数。

3. 确保在列出的最后一个内核参数后面有一个空格，然后追加参数 `rescue` 。

4. 按Enter键进入救援模式。

5. 在提示符下，确定处理器是否具有所需的扩展，并通过运行以下命令来启用这些扩展：

   ```bash
   grep -E 'svm|vmx' /proc/cpuinfo | grep nx
   ```

如果显示任何输出，则处理器具有硬件虚拟化功能。如果没有显示输出，则您的处理器可能仍然支持硬件虚拟化；在某些情况下，制造商禁用 BIOS 中的虚拟化扩展。如果您认为是这样，请查阅系统的BIOS 和制造商提供的主板手册。

##### Memory

所需的最小 RAM 为 2 GB 。对于集群级别 4.2 到 4.5，oVirt Node 中每个虚拟机支持的最大 RAM 为 6 TB 。对于集群级别 4.6 到 4.7 ，oVirt Node 中每个虚拟机支持的最大 RAM 为 16 TB 。

然而，所需的 RAM 数量取决于 guest 操作系统要求、guest 应用程序要求以及 guest 内存活动和使用情况。KVM can also overcommit physical RAM for virtualized guests, allowing  you to provision guests with RAM requirements greater than what is  physically present, on the assumption that the guests are not all  working concurrently at peak load. KVM does this by only allocating RAM  for guests as required and shifting underutilized guests into swap.

##### Storage

主机需要存储空间来存储配置、日志、内核转储，并用作交换空间。存储可以是本地的或基于网络的。oVirt  Node can boot with one, some, or all of its default  allocations  in network storage. 可以使用网络存储中的一个、一些或所有默认分配来启动。如果网络断开连接，从网络存储引导可能会导致冻结。Booting from network storage can result in a freeze. 添加 a drop-in 多路径配置文件可以帮助解决网络连接丢失的问题。如果 oVirt Node 从SAN存储启动并失去连接，则文件将变为只读，直到网络连接恢复。使用网络存储可能会导致性能降级。

下面列出了主机安装的最低存储要求。但是，使用默认分配，这会占用更多的存储空间。

- / (root)              - 6 GB

- /home              - 1 GB

- /tmp                 - 1 GB

- /boot                - 1 GB

- /var                   - 5 GB

- /var/crash        - 10 GB

- /var/log            - 8 GB

- /var/log/audit - 2 GB

- /var/tmp          - 10 GB

- swap                 - 1 GB

  详见 [What is the recommended swap size for Red Hat platforms?](https://access.redhat.com/solutions/15244) 。

- Anaconda 在卷组中保留 20% 的精简池大小，用于将来的元数据扩展。这是为了防止开箱即用 an out-of-the-box  configuration 配置在正常使用条件下耗尽空间。也不支持在安装期间过度提供精简池。

- **Minimum Total - 64 GiB**

如果还安装了用于自托管引擎安装的 Engine Appliance ， `/var/tmp` 必须至少为 10 GB 。

If you plan to use memory overcommitment, add enough swap space to provide virtual memory for all of virtual machines. See [Memory Optimization](https://ovirt.org/documentation/administration_guide/index#Memory_Optimization).如果计划使用内存过度使用，请添加足够的交换空间以为所有虚拟机提供虚拟内存。请参阅内存优化。

##### PCI Device

主机必须至少有一个最小带宽为 1 Gbps 的网络接口。每个主机应有两个网络接口，其中一个专用于支持网络密集型活动，如虚拟机迁移。此类操作的性能受到可用带宽的限制。

有关如何将PCI Express和传统PCI设备与基于Intel Q35的虚拟机一起使用的信息，For information about how to use PCI Express and conventional PCI devices with Intel Q35-based virtual machines, see [*Using PCI Express and Conventional PCI Devices with the Q35 Virtual Machine*](https://access.redhat.com/articles/3201152).

##### 设备分配

If you plan to implement device assignment and PCI passthrough so  that a virtual machine can use a specific PCIe device from a host,  ensure the following requirements are met:

- CPU must support IOMMU (for example, VT-d or AMD-Vi). IBM POWER8 supports IOMMU by default.
- 固件必须支持 IOMMU。
- CPU root ports used must support ACS or ACS-equivalent capability.
- PCIe devices must support ACS or ACS-equivalent capability.
- All PCIe switches and bridges between the PCIe device and the root  port should support ACS. For example, if a switch does not support ACS,  all devices behind that switch share the same IOMMU group, and can only  be assigned to the same virtual machine.
- For GPU support, Enterprise Linux 8 supports PCI device assignment of PCIe-based NVIDIA K-Series Quadro (model 2000 series or higher), GRID,  and Tesla as non-VGA graphics devices. Currently up to two GPUs may be  attached to a virtual machine in addition to one of the standard,  emulated VGA interfaces. The emulated VGA is used for pre-boot and  installation and the NVIDIA GPU takes over when the NVIDIA graphics  drivers are loaded. Note that the NVIDIA Quadro 2000 is not supported,  nor is the Quadro K420 card.

Check vendor specification and datasheets to confirm that your hardware meets these requirements. The `lspci -v` command can be used to print information for PCI devices already installed on a system.

如果您计划实施设备分配和PCI直通，以便虚拟机可以从主机使用特定的PCIe设备，请确保满足以下要求：

CPU必须支持IOMMU（例如VT-d或AMD Vi）。IBM POWER8 默认支持 IOMMU。

使用的CPU根端口必须支持ACS或ACS等效功能。

PCIe设备必须支持ACS或ACS同等功能。

PCIe设备和根端口之间的所有PCIe交换机和网桥都应支持ACS。例如，如果交换机不支持ACS，则该交换机后面的所有设备共享同一IOMMU组，并且只能分配给同一虚拟机。

对于GPU支持，Enterprise Linux 8支持基于PCIe的NVIDIA  K系列Quadro（2000系列或更高型号）、GRID和Tesla作为非VGA图形设备的PCI设备分配。目前，除了一个标准的模拟VGA接口之外，最多可以将两个GPU连接到虚拟机。模拟VGA用于预引导和安装，NVIDIA GPU在加载NVIDIA图形驱动程序时接管。请注意，NVIDIA Quadro 2000不受支持，Quadro K420卡也不受支持。

检查供应商规范和数据表，确认您的硬件满足这些要求。lspci-v命令可用于打印已安装在系统上的PCI设备的信息。

#####  vGPU

主机必须满足以下要求，该主机上的虚拟机才能使用 vGPU：

- vGPU 兼容的 GPU 。
- GPU-enabled host kernel支持GPU的主机内核
- 已安装具有正确驱动程序的GPU
- Select a vGPU type and the number of instances that you would like to use with this virtual machine using the **Manage vGPU** dialog in the **Administration Portal** **Host Devices** tab of the virtual machine.使用虚拟机的“管理门户主机设备”选项卡中的“管理v GPU”对话框，选择要用于此虚拟机的v GPU类型和实例数。
- vGPU-capable drivers installed on each host in the cluster
- vGPU-supported virtual machine operating system with vGPU drivers installed
- 群集中每个主机上安装的支持 vGPU 的驱动程序
- 安装了v GPU驱动程序的v GPU支持的虚拟机操作系统

#### 网络

##### 2.3.1.General requirements

oVirt requires IPv6 to remain enabled on the physical or virtual machine running the Engine. [Do not disable IPv6](https://access.redhat.com/solutions/8709) on the Engine machine, even if your systems do not use it.

o Virt要求在运行引擎的物理或虚拟机上保持启用IPv6。不要在引擎计算机上禁用IPv6，即使您的系统不使用它。

##### 2.3.2. Network range for self-hosted engine deployment

The self-hosted engine deployment process temporarily uses a `/24` network address under `192.168`. It defaults to `192.168.222.0/24`, and if this address is in use, it tries other `/24` addresses under `192.168` until it finds one that is not in use. If it does not find an unused network address in this range, deployment fails.

When installing the self-hosted engine using the command line, you can set the deployment script to use an alternate `/24` network range with the option `--ansible-extra-vars=he_ipv4_subnet_prefix=*PREFIX*`, where `*PREFIX*` is the prefix for the default range. For example:

```
# hosted-engine --deploy --ansible-extra-vars=he_ipv4_subnet_prefix=192.168.222
```

|      | You can only set another range by installing oVirt as a self-hosted engine using the command line. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

##### 2.3.3. Firewall Requirements for DNS, NTP, and IPMI Fencing

The firewall requirements for all of the following topics are special cases that require individual consideration.

DNS and NTP

oVirt does not create a DNS or NTP server, so the firewall does not need to have open ports for incoming traffic.

By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, define  exceptions for requests that are sent to DNS and NTP servers.

|      | The oVirt Engine and all hosts (oVirt Node and Enterprise Linux host) must have a fully qualified domain name and full, perfectly-aligned  forward and reverse name resolution.  Running a DNS service as a virtual machine in the oVirt environment  is not supported. All DNS services the oVirt environment uses must be  hosted outside of the environment.  Use DNS instead of the `/etc/hosts` file for name resolution. Using a hosts file typically requires more work and has a greater chance for errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

IPMI and Other Fencing Mechanisms (optional)

For IPMI (Intelligent Platform Management Interface) and other  fencing mechanisms, the firewall does not need to have open ports for  incoming traffic.

By default, Enterprise Linux allows outbound IPMI traffic to ports on any destination address. If you disable outgoing traffic, make  exceptions for requests being sent to your IPMI or fencing servers.

Each oVirt Node and Enterprise Linux host in the cluster must be able to connect to the fencing devices of all other hosts in the cluster. If the cluster hosts are experiencing an error (network error, storage  error…) and cannot function as hosts, they must be able to connect to  other hosts in the data center.

The specific port number depends on the type of the fence agent you are using and how it is configured.

The firewall requirement tables in the following sections do not represent this option.

##### 2.3.4. oVirt Engine Firewall Requirements

The oVirt Engine requires that a number of ports be opened to allow network traffic through the system’s firewall.

The `engine-setup` script can configure the firewall automatically.

The firewall configuration documented here assumes a default configuration.

| ID   | Port(s) | Protocol | Source                                                       | Destination                                    | Purpose                                                      | Encrypted by default                               |
| ---- | ------- | -------- | ------------------------------------------------------------ | ---------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------- |
| M1   | -       | ICMP     | oVirt Nodes Enterprise Linux hosts                           | oVirt Engine                                   | Optional. May help in diagnosis.                             | No                                                 |
| M2   | 22      | TCP      | System(s) used for maintenance of the Engine including backend configuration, and software upgrades. | oVirt Engine                                   | Secure Shell (SSH) access. Optional.                         | Yes                                                |
| M3   | 2222    | TCP      | Clients accessing virtual machine serial consoles.           | oVirt Engine                                   | Secure Shell (SSH) access to enable connection to virtual machine serial consoles. | Yes                                                |
| M4   | 80, 443 | TCP      | Administration Portal clients VM Portal clients oVirt Nodes Enterprise Linux hosts REST API clients | oVirt Engine                                   | Provides HTTP (port 80, not encrypted) and HTTPS (port 443, encrypted) access to the Engine. HTTP redirects connections to HTTPS. | Yes                                                |
| M5   | 6100    | TCP      | Administration Portal clients VM Portal clients              | oVirt Engine                                   | Provides websocket proxy access for a web-based console client, `noVNC`, when the websocket proxy is running on the Engine. | No                                                 |
| M6   | 7410    | UDP      | oVirt Nodes Enterprise Linux hosts                           | oVirt Engine                                   | If Kdump is enabled on the hosts, open this port for the fence_kdump listener on the Engine. See [fence_kdump Advanced Configuration](https://ovirt.org/documentation/administration_guide/index#sect-fence_kdump_Advanced_Configuration). `fence_kdump` doesn’t provide a way to encrypt the connection. However, you can  manually configure this port to block access from hosts that are not  eligible. | No                                                 |
| M7   | 54323   | TCP      | Administration Portal clients                                | oVirt Engine (`ovirt-imageio` service)         | Required for communication with the `ovirt-imageo` service.  | Yes                                                |
| M8   | 6642    | TCP      | oVirt Nodes Enterprise Linux hosts                           | Open Virtual Network (OVN) southbound database | Connect to Open Virtual Network (OVN) database               | Yes                                                |
| M9   | 9696    | TCP      | Clients of external network provider for OVN                 | External network provider for OVN              | OpenStack Networking API                                     | Yes, with configuration generated by engine-setup. |
| M10  | 35357   | TCP      | Clients of external network provider for OVN                 | External network provider for OVN              | OpenStack Identity API                                       | Yes, with configuration generated by engine-setup. |
| M11  | 53      | TCP, UDP | oVirt Engine                                                 | DNS Server                                     | DNS lookup requests from ports above 1023 to port 53, and responses. Open by default. | No                                                 |
| M12  | 123     | UDP      | oVirt Engine                                                 | NTP Server                                     | NTP requests from ports above 1023 to port 123, and responses.  Open by default. | No                                                 |

|      | A port for the OVN northbound database (6641) is not listed because,  in the default configuration, the only client for the OVN northbound  database (6641) is `ovirt-provider-ovn`. Because they both run on the same host, their communication is not visible to the network.  By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, make  exceptions for the Engine to send requests to DNS and NTP servers. Other nodes may also require DNS and NTP. In that case, consult the  requirements for those nodes and configure the firewall accordingly. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

##### 2.3.5. Host Firewall Requirements

Enterprise Linux hosts and oVirt Nodes (oVirt Node) require a number  of ports to be opened to allow network traffic through the system’s  firewall. The firewall rules are automatically configured by default  when adding a new host to the Engine, overwriting any pre-existing  firewall configuration.

To disable automatic firewall configuration when adding a new host, clear the **Automatically configure host firewall** check box under **Advanced Parameters**.

| ID   | Port(s)       | Protocol | Source                                          | Destination                        | Purpose                                                      | Encrypted by default                                         |
| ---- | ------------- | -------- | ----------------------------------------------- | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| H1   | 22            | TCP      | oVirt Engine                                    | oVirt Nodes Enterprise Linux hosts | Secure Shell (SSH) access. Optional.                         | Yes                                                          |
| H2   | 2223          | TCP      | oVirt Engine                                    | oVirt Nodes Enterprise Linux hosts | Secure Shell (SSH) access to enable connection to virtual machine serial consoles. | Yes                                                          |
| H3   | 161           | UDP      | oVirt Nodes Enterprise Linux hosts              | oVirt Engine                       | Simple network management protocol (SNMP). Only required if you want Simple  Network Management Protocol traps sent from the host to one or more  external SNMP managers. Optional. | No                                                           |
| H4   | 111           | TCP      | NFS storage server                              | oVirt Nodes Enterprise Linux hosts | NFS connections. Optional.                                   | No                                                           |
| H5   | 5900 - 6923   | TCP      | Administration Portal clients VM Portal clients | oVirt Nodes Enterprise Linux hosts | Remote guest console access via VNC and SPICE. These ports must be open to facilitate client access to virtual machines. | Yes (optional)                                               |
| H6   | 5989          | TCP, UDP | Common Information Model Object Manager (CIMOM) | oVirt Nodes Enterprise Linux hosts | Used by Common Information Model Object Managers (CIMOM) to monitor virtual  machines running on the host. Only required if you want to use a CIMOM  to monitor the virtual machines in your virtualization environment. Optional. | No                                                           |
| H7   | 9090          | TCP      | oVirt Engine Client machines                    | oVirt Nodes Enterprise Linux hosts | Required to access the Cockpit web interface, if installed.  | Yes                                                          |
| H8   | 16514         | TCP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Virtual machine migration using **libvirt**.                 | Yes                                                          |
| H9   | 49152 - 49215 | TCP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Virtual machine migration and fencing using VDSM. These ports must be open to  facilitate both automated and manual migration of virtual machines. | Yes. Depending on agent for fencing, migration is done through libvirt. |
| H10  | 54321         | TCP      | oVirt Engine oVirt Nodes Enterprise Linux hosts | oVirt Nodes Enterprise Linux hosts | VDSM communications with the Engine and other virtualization hosts. | Yes                                                          |
| H11  | 54322         | TCP      | oVirt Engine `ovirt-imageio` service            | oVirt Nodes Enterprise Linux hosts | Required for communication with the `ovirt-imageo` service.  | Yes                                                          |
| H12  | 6081          | UDP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Required, when Open Virtual Network (OVN) is used as a network provider, to allow OVN to create tunnels between hosts. | No                                                           |
| H13  | 53            | TCP, UDP | oVirt Nodes Enterprise Linux hosts              | DNS Server                         | DNS lookup requests from ports above 1023 to port 53, and responses. This port is required and open by default. | No                                                           |
| H14  | 123           | UDP      | oVirt Nodes Enterprise Linux hosts              | NTP Server                         | NTP requests from ports above 1023 to port 123, and responses. This port is required and open by default. |                                                              |
| H15  | 4500          | TCP, UDP | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |
| H16  | 500           | UDP      | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |
| H17  | -             | AH, ESP  | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |

|      | By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, make  exceptions for the oVirt Nodes  Enterprise Linux hosts to send requests to DNS and NTP servers. Other nodes may also require DNS and NTP. In that case, consult the  requirements for those nodes and configure the firewall accordingly. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

##### 2.3.6. Database Server Firewall Requirements

oVirt supports the use of a remote database server for the Engine database (`engine`) and the Data Warehouse database (`ovirt-engine-history`). If you plan to use a remote database server, it must allow connections  from the Engine and the Data Warehouse service (which can be separate  from the Engine).

Similarly, if you plan to access a local or remote Data Warehouse  database from an external system, the database must allow connections  from that system.

|      | Accessing the Engine database from external systems is not supported. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| ID   | Port(s) | Protocol | Source                              | Destination                                                  | Purpose                                           | Encrypted by default                                         |
| ---- | ------- | -------- | ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ |
| D1   | 5432    | TCP, UDP | oVirt Engine Data Warehouse service | Engine (`engine`) database server Data Warehouse (`ovirt-engine-history`) database server | Default port for PostgreSQL database connections. | [No, but can be enabled](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH). |
| D2   | 5432    | TCP, UDP | External systems                    | Data Warehouse (`ovirt-engine-history`) database server      | Default port for PostgreSQL database connections. | Disabled by default. [No, but can be enabled](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH). |

##### 2.3.7. Maximum Transmission Unit Requirements

The recommended Maximum Transmission Units (MTU) setting for Hosts  during deployment is 1500. It is possible to update this setting after  the environment is set up to a different MTU. For more information on  changing the MTU setting, see [How to change the Hosted Engine VM network MTU](https://access.redhat.com/solutions/4129641).

### 3. Preparing Storage for oVirt

You need to prepare storage to be used for storage domains in the new environment. A oVirt environment must have at least one data storage  domain, but adding more is recommended.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

A data domain holds the virtual hard disks and OVF files of all the  virtual machines and templates in a data center, and cannot be shared  across data centers while active (but can be migrated between data  centers). Data domains of multiple storage types can be added to the  same data center, provided they are all shared, rather than local,  domains.

You can use one of the following storage types:

- [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Preparing_NFS_Storage_SHE_cli_deploy)
- [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Preparing_iSCSI_Storage_SHE_cli_deploy)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Preparing_FCP_Storage_SHE_cli_deploy)
- [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Preparing_Red_Hat_Gluster_Storage_SHE_cli_deploy)

Prerequisites

- Self-hosted engines must have an additional data domain with at least 74 GiB dedicated to the Engine virtual machine. The self-hosted engine  installer creates this domain. Prepare the storage for this domain  before installation.

  |      | Extending or otherwise changing the self-hosted engine storage domain after deployment of the self-hosted engine is not supported. Any such  change might prevent the self-hosted engine from booting. |
  | ---- | ------------------------------------------------------------ |
  |      |                                                              |

- When using a block storage domain, either FCP or iSCSI, a single  target LUN is the only supported setup for a self-hosted engine.

- If you use iSCSI storage, the self-hosted engine storage domain must  use a dedicated iSCSI target. Any additional storage domains must use a  different iSCSI target.

- It is strongly recommended to create additional data storage domains  in the same data center as the self-hosted engine storage domain. If you deploy the self-hosted engine in a data center with only one active  data storage domain, and that storage domain is corrupted, you cannot  add new storage domains or remove the corrupted storage domain. You must redeploy the self-hosted engine.

#### 3.1. Preparing NFS Storage

Set up NFS shares on your file storage or remote server to serve as  storage domains on Red Hat Enterprise Virtualization Host systems. After exporting the shares on the remote storage and configuring them in the  Red Hat Virtualization Manager, the shares will be automatically  imported on the Red Hat Virtualization hosts.

For information on setting up, configuring, mounting and exporting NFS, see [*Managing file systems*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_file_systems/index) for Red Hat Enterprise Linux 8.

Specific system user accounts and system user groups are required by  oVirt so the Engine can store data in the storage domains represented by the exported directories. The following procedure sets the permissions  for one directory. You must repeat the `chown` and `chmod` steps for all of the directories you intend to use as storage domains in oVirt.

Prerequisites

1. Install the NFS `utils` package.

   ```
   # dnf install nfs-utils -y
   ```

2. To check the enabled versions:

   ```
   # cat /proc/fs/nfsd/versions
   ```

3. Enable the following services:

   ```
   # systemctl enable nfs-server
   # systemctl enable rpcbind
   ```

Procedure

1. Create the group `kvm`:

   ```
   # groupadd kvm -g 36
   ```

2. Create the user `vdsm` in the group `kvm`:

   ```
   # useradd vdsm -u 36 -g kvm
   ```

3. Create the `storage` directory and modify the access rights.

   ```
   # mkdir /storage
   # chmod 0755 /storage
   # chown 36:36 /storage/
   ```

4. Add the `storage` directory to `/etc/exports` with the relevant permissions.

   ```
   # vi /etc/exports
   # cat /etc/exports
    /storage *(rw)
   ```

5. Restart the following services:

   ```
   # systemctl restart rpcbind
   # systemctl restart nfs-server
   ```

6. To see which export are available for a specific IP address:

   ```
   # exportfs
    /nfs_server/srv
                  10.46.11.3/24
    /nfs_server       <world>
   ```

|      | If changes in `/etc/exports` have been made after starting the services, the `exportfs -ra` command can be used to reload the changes. After performing all the above stages, the exports directory should be  ready and can be tested on a different host to check that it is usable. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 3.2. Preparing iSCSI Storage

oVirt supports iSCSI storage, which is a storage domain created from a volume group made up of LUNs. Volume groups and LUNs cannot be attached to more than one storage domain at a time.

For information on setting up and configuring iSCSI storage, see [Getting started with iSCSI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_storage_devices/index#getting-started-with-iscsi_managing-storage-devices) in *Managing storage devices* for Red Hat Enterprise Linux 8.

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 3.3. Preparing FCP Storage

oVirt supports SAN storage by creating a storage domain from a volume group made of pre-existing LUNs. Neither volume groups nor LUNs can be  attached to more than one storage domain at a time.

oVirt system administrators need a working knowledge of Storage Area  Networks (SAN) concepts. SAN usually uses Fibre Channel Protocol (FCP)  for traffic between hosts and shared external storage. For this reason,  SAN may occasionally be referred to as FCP storage.

For information on setting up and configuring FCP or multipathing on Enterprise Linux, see the [*Storage Administration Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Storage_Administration_Guide/index.html) and [*DM Multipath Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/DM_Multipath/index.html).

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }  }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 3.4. Preparing Gluster Storage

For information on setting up and configuring Gluster Storage, see the [*Gluster Storage Installation Guide*](https://docs.gluster.org/en/latest/Install-Guide/Overview/).

#### 3.5. Customizing Multipath Configurations for SAN Vendors

If your RHV environment is configured to use multipath connections  with SANs, you can customize the multipath configuration settings to  meet requirements specified by your storage vendor. These customizations can override both the default settings and settings that are specified  in `/etc/multipath.conf`.

To override the multipath settings, do not customize `/etc/multipath.conf`. Because VDSM owns `/etc/multipath.conf`, installing or upgrading VDSM or oVirt can overwrite this file including any customizations it contains. This overwriting can cause severe  storage failures.

Instead, you create a file in the `/etc/multipath/conf.d` directory that contains the settings you want to customize or override.

VDSM executes the files in `/etc/multipath/conf.d` in  alphabetical order. So, to control the order of execution, you begin the filename with a number that makes it come last. For example, `/etc/multipath/conf.d/90-myfile.conf`.

To avoid causing severe storage failures, follow these guidelines:

- Do not modify `/etc/multipath.conf`. If the file contains user modifications, and the file is overwritten, it can cause unexpected storage problems.
- Do not override the `user_friendly_names` and `find_multipaths` settings. For details, see [Recommended Settings for Multipath.conf](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#ref-Recommended_Settings_for_Multipath_conf_SHE_cli_deploy).
- Avoid overriding the `no_path_retry` and `polling_interval` settings unless a storage vendor specifically requires you to do so. For details, see [Recommended Settings for Multipath.conf](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#ref-Recommended_Settings_for_Multipath_conf_SHE_cli_deploy).

|      | Not following these guidelines can cause catastrophic storage errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

- VDSM is configured to use the multipath module. To verify this, enter:

  ```
  # vdsm-tool is-configured --module multipath
  ```

Procedure

1. Create a new configuration file in the `/etc/multipath/conf.d` directory.

2. Copy the individual setting you want to override from `/etc/multipath.conf` to the new configuration file in `/etc/multipath/conf.d/<my_device>.conf`. Remove any comment marks, edit the setting values, and save your changes.

3. Apply the new configuration settings by entering:

   ```
   # systemctl reload multipathd
   ```

   |      | Do not restart the multipathd service. Doing so generates errors in the VDSM logs. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

Verification steps

1. Test that the new configuration performs as expected on a  non-production cluster in a variety of failure scenarios. For example,  disable all of the storage connections.
2. Enable one connection at a time and verify that doing so makes the storage domain reachable.

Additional resources

- [Recommended Settings for Multipath.conf](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#ref-Recommended_Settings_for_Multipath_conf_SHE_cli_deploy)
- [*Enterprise Linux DM Multipath*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/dm_multipath/)
- [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing)
- [How do I customize /etc/multipath.conf on my RHVH hypervisors? What values must not change and why?](https://access.redhat.com/solutions/3234761)

#### 3.6. Recommended Settings for Multipath.conf

Do not override the following settings:

- user_friendly_names  no

  Device names must be consistent across all hypervisors. For example, `/dev/mapper/{WWID}`. The default value of this setting, `no`, prevents the assignment of arbitrary and inconsistent device names such as `/dev/mapper/mpath{N}` on various hypervisors, which can lead to unpredictable system behavior.    Do not change this setting to `user_friendly_names  yes`. User-friendly names are likely to cause unpredictable system behavior or failures, and are not supported.

- `find_multipaths	no`

  This setting controls whether oVirt Node tries to access devices  through multipath only if more than one path is available. The current  value, `no`, allows oVirt to access devices through multipath even if only one path is available.    Do not override this setting.

Avoid overriding the following settings unless required by the storage system vendor:

- `no_path_retry	4`

  This setting controls the number of polling attempts to retry when no paths are available. Before oVirt version 4.2, the value of `no_path_retry` was `fail` because QEMU had trouble with the I/O queuing when no paths were available. The `fail` value made it fail quickly and paused the virtual machine. oVirt version 4.2 changed this value to `4` so when multipathd detects the last path has failed, it checks all of  the paths four more times. Assuming the default 5-second polling  interval, checking the paths takes 20 seconds. If no path is up,  multipathd tells the kernel to stop queuing and fails all outstanding  and future I/O until a path is restored. When a path is restored, the  20-second delay is reset for the next time all paths fail. For more  details, see [the commit that changed this setting](https://gerrit.ovirt.org/#/c/88082/).

- `polling_interval	5`

  This setting determines the number of seconds between polling  attempts to detect whether a path is open or has failed. Unless the  vendor provides a clear reason for increasing the value, keep the  VDSM-generated default so the system responds to path failures sooner.

### 4. Installing the Self-hosted Engine Deployment Host

A self-hosted engine can be deployed from a [oVirt Node](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Installing_Red_Hat_Virtualization_Hosts_SHE_deployment_host) or a [Enterprise Linux host](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Installing_Red_Hat_Enterprise_Linux_Hosts_SHE_deployment_host).

|      | If you plan to use bonded interfaces for high availability or VLANs  to separate different types of traffic (for example, for storage or  management connections), you should configure them on the host before  beginning the self-hosted engine deployment. See [Networking Recommendations](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#networking-recommendations) in the *Planning and Prerequisites Guide*. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 4.1. Installing oVirt Nodes

oVirt Node (oVirt Node) is a minimal operating system based on  Enterprise Linux that is designed to provide a simple method for setting up a physical machine to act as a hypervisor in a oVirt environment.  The minimal operating system contains only the packages required for the machine to act as a hypervisor, and features a Cockpit web interface  for monitoring the host and performing administrative tasks. See [Running Cockpit](http://cockpit-project.org/running.html) for the minimum browser requirements.

oVirt Node supports NIST 800-53 partitioning requirements to improve  security. oVirt Node uses a NIST 800-53 partition layout by default.

The host must meet the minimum  [host requirements](https://ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Write the oVirt Node Installation ISO disk image to a USB, CD, or DVD.

4. Start the machine on which you are installing oVirt Node, booting from the prepared installation media.

5. From the boot menu, select **Install oVirt Node 4.5** and press `Enter`.

   |      | You can also press the `Tab` key to edit the kernel  parameters. Kernel parameters must be separated by a space, and you can  boot the system using the specified kernel parameters by pressing the `Enter` key. Press the `Esc` key to clear any changes to the kernel parameters and return to the boot menu. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Select a language, and click **Continue**.

7. Select a keyboard layout from the **Keyboard Layout** screen and click **Done**.

8. Select the device on which to install oVirt Node from the **Installation Destination** screen. Optionally, enable encryption. Click **Done**.

   |      | Use the **Automatically configure partitioning** option. |
   | ---- | -------------------------------------------------------- |
   |      |                                                          |

9. Select a time zone from the **Time & Date** screen and click **Done**.

10. Select a network from the **Network & Host Name** screen and click **Configure…** to configure the connection details.

    |      | To use the connection every time the system boots, select the **Connect automatically with priority** check box. For more information, see [Configuring network and host name options](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation#network-hostname_configuring-system-settings) in the *Enterprise Linux 8 Installation Guide*. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    Enter a host name in the **Host Name** field, and click **Done**.

11. Optional: Configure **Security Policy** and **Kdump**. See [Customizing your RHEL installation using the GUI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation) in *Performing a standard RHEL installation* for Enterprise Linux 8 for more information on each of the sections in the **Installation Summary** screen.

12. Click **Begin Installation**.

13. Set a root password and, optionally, create an additional user while oVirt Node installs.

    |      | Do not create untrusted users on oVirt Node, as this can lead to exploitation of local security vulnerabilities. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

14. Click **Reboot** to complete the installation.

    |      | When oVirt Node restarts, `nodectl check` performs a health check on the host and displays the result when you log in on the command line. The message `node status: OK` or `node status: DEGRADED` indicates the health status. Run `nodectl check` to get more information. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    |      | If necessary, you can [ prevent kernel modules from loading automatically](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#proc-Preventing_Kernel_Modules_from_Loading_Automatically_Install_nodes_RHVH). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

#### 4.2. Installing Enterprise Linux hosts

A Enterprise Linux host is based on a standard basic installation of Enterprise Linux 8 on a physical server, with the `Enterprise Linux Server` and `oVirt` repositories enabled.

The oVirt project also provides packages for Enterprise Linux 9 but only as a Technology Preview.

For detailed installation instructions, see the [*Performing a standard EL installation*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index.html).

The host must meet the minimum [host requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Virtualization must be enabled in your host’s BIOS settings. For  information on changing your host’s BIOS settings, refer to your host’s  hardware documentation. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Do not install third-party watchdogs on Enterprise Linux hosts. They can interfere with the watchdog daemon provided by VDSM. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5. Installing the oVirt Engine

#### 5.1. Manually installing the Engine Appliance

When you deploy the self-hosted engine, the following sequence of events takes place:

1. The installer installs the Engine Appliance to the deployment host.
2. The appliance installs the Engine virtual machine.
3. The appliance installs the Engine on the Engine virtual machine.

However, you can install the appliance manually on the deployment  host beforehand if you need to. The appliance is large and network  connectivity issues might cause the appliance installation to take a  long time, or possibly fail.

Procedure

1. On Enterprise Linux hosts:

   1. Reset the `virt` module:

      ```
      # dnf module reset virt
      ```

      |      | If this module is already enabled in the Advanced Virtualization  stream, this step is not necessary, but it has no negative impact.  You can see the value of the stream by entering:  `# dnf module list virt` |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   2. Enable the `virt` module in the Advanced Virtualization stream with the following command:

- For oVirt 4.4.2:

  ```
  # dnf module enable virt:8.2
  ```

- For oVirt 4.4.3 to 4.4.5:

  ```
  # dnf module enable virt:8.3
  ```

- For oVirt 4.4.6 to 4.4.10:

  ```
  # dnf module enable virt:av
  ```

- For oVirt 4.5 and later:

  ```
  # dnf module enable virt:rhel
  ```

  |      | Starting with EL 8.6 the Advanced virtualization packages will use the standard `virt:rhel` module. For EL 8.4 and 8.5, only one Advanced Virtualization stream is used, `rhel:av`. |
  | ---- | ------------------------------------------------------------ |
  |      |                                                              |

  1. Synchronize installed packages to update them to the latest available versions:

     ```
     # dnf distro-sync --nobest
     ```

  2. Install the Engine Appliance to the host manually:

     ```
     # dnf install ovirt-engine-appliance
     ```

Now, when you deploy the self-hosted engine, the installer detects that the appliance is already installed.

#### 5.2. Enabling and configuring the firewall

`firewalld` must be installed and running before you run  the self-hosted deployment script. You must also have an active zone  with an interface configured.

Prerequisites

- `firewalld` is installed. `hosted-engine-setup` requires the `firewalld` package, so you do not need to do any additional steps.

Procedure

1. Start `firewalld`:

   ```
   # systemctl unmask firewalld
   # systemctl start firewalld
   ```

   To ensure firewalld starts automatically at system start, enter the following command as root:

   ```
   # systemctl enable firewalld
   ```

2. Ensure that firewalld is running:

   ```
   # systemctl status firewalld
   ```

3. Ensure that your management interface is in a firewall zone via

   ```
   # firewall-cmd --get-active-zones
   ```

Now you are ready to deploy the self-hosted engine.

#### 5.3. Deploying the self-hosted engine using the command line

You can deploy a self-hosted engine from the command line. After installing the setup package, you run the command `hosted-engine --deploy`, and a script collects the details of your environment and uses them to configure the host and the Engine.

You can customize the Engine virtual machine during deployment, either manually, by pausing the deployment, or using automation.

- Setting the variable `he_pause_host` to `true` pauses deployment after installing the Engine and adding the deployment host to the Engine.

- Setting the variable `he_pause_before_engine_setup` to `true` pauses the deployment before installing the Engine and before restoring the Engine when using `he_restore_from_file`.

  |      | When the `he_pause_host` or `he_pause_before_engine_setup` variables are set to true a lock file is created at `/tmp` with the suffix `_he_setup_lock` on the deployment host. You can then manually customize the virtual  machine as needed. The deployment continues after you delete the lock  file, or after 24 hours, whichever comes first. |
  | ---- | ------------------------------------------------------------ |
  |      |                                                              |

- Adding an Ansible playbook to any of the following directories on the deployment host automatically runs the playbook. Add the playbook under one of the following directories under `/usr/share/ansible/collections/ansible_collections/redhat/rhv/roles/hosted_engine_setup/hooks/`:

  - `enginevm_before_engine_setup`
  - `enginevm_after_engine_setup`
  - `after_add_host`
  - `after_setup`

Prerequisites

- FQDNs prepared for your Engine and the host. Forward and reverse lookup records must both be set in the DNS.

- When using a block storage domain, either FCP or iSCSI, a single  target LUN is the only supported setup for a self-hosted engine.

- Optional: If you want to customize the Engine virtual machine during  deployment using automation, an Ansible playbook must be added. See [Customizing the Engine virtual machine using automation during deployment](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#customizing_engine_vm_during_deployment_auto_SHE_cli_deploy).

- The self-hosted engine setup script requires ssh public key access  using 2048-bit RSA keys from the engine virtual machine to the root  account of its bare metal host. In `/etc/ssh/sshd_config`, these values must be set as follows:

  - `PubkeyAcceptedKeyTypes` must allow 2048-bit RSA keys or stronger.

    By default, this setting uses system-wide crypto policies. For more information, see the manual page `crypto-policies(7)`.

    |      | oVirt Node hosts that are registered with the Engine in versions  earlier than 4.4.5.5 require RSA 2048 for backward compatibility until  all the keys are migrated.  oVirt Node hosts registered for 4.4.5.5 and later use the strongest  algorithm that is supported by both the Engine and oVirt Node. The `PubkeyAcceptedKeyTypes` setting helps determine which algorithm is used. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

  - `PermitRoolLogin` is set to `without-password` or `yes`

  - `PubkeyAuthentication` is set to `yes`

Procedure

1. Install the deployment tool:

   ```
   # dnf install ovirt-hosted-engine-setup
   ```

2. Use the `tmux` window manager to run the script to avoid losing the session in case of network or terminal disruption.

   Install and run `tmux`:

   ```
   # dnf -y install tmux
   # tmux
   ```

3. Start the deployment script:

   ```
   # hosted-engine --deploy
   ```

   Alternatively, to pause the deployment after adding the deployment host to the Engine, use the command line option `--ansible-extra-vars=he_pause_host=true`:

   ```
   # hosted-engine --deploy --ansible-extra-vars=he_pause_host=true
   ```

   |      | To escape the script at any time, use the Ctrl+D keyboard combination to abort deployment. In the event of session timeout or connection disruption, run `tmux attach` to recover the deployment session. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

4. When prompted, enter **Yes** to begin the deployment:

   ```
   Continuing will configure this host for serving as hypervisor and will create a local VM with a running engine.
   The locally running engine will be used to configure a new storage domain and create a VM there.
   At the end the disk of the local VM will be moved to the shared storage.
   Are you sure you want to continue? (Yes, No)[Yes]:
   ```

5. Configure the network. Check that the gateway shown is correct and press Enter. Enter a pingable address on the same subnet so the script can check the host’s connectivity.

   ```
   Please indicate a pingable gateway IP address [X.X.X.X]:
   ```

6. The script detects possible NICs to use as a management bridge for the environment. Enter one of them or press Enter to accept the default.

   ```
   Please indicate a nic to set ovirtmgmt bridge on: (ens1, ens0) [ens1]:
   ```

7. Specify how to check network connectivity. The default is `dns`.

   ```
   Please specify which way the network connectivity should be checked (ping, dns, tcp, none) [dns]:
   ```

   - `**ping**`

     Attempts to ping the gateway.

   - `**dns**`

     Checks the connection to the DNS server.

   - `**tcp**`

     Creates a TCP connection to a host and port combination. You need to  specify a destination IP address and port. Once the connection is  successfully created, the network is considered to be alive. Ensure that the given host is able to accept incoming TCP connections on the given  port.

   - `**none**`

     The network is always considered connected.

8. Enter a name for the data center in which to deploy the host for the self-hosted engine. The default name is **Default**.

   ```
   Please enter the name of the data center where you want to deploy this hosted-engine host.
   Data center [Default]:
   ```

9. Enter a name for the cluster in which to deploy the host for the self-hosted engine. The default name is **Default**.

   ```
   Please enter the name of the cluster where you want to deploy this hosted-engine host.
   Cluster [Default]:
   ```

10. If you want to use a custom appliance for the virtual machine  installation, enter the path to the OVA archive. Otherwise, leave this  field empty to use the Engine Appliance.

11. To deploy with a custom Engine Appliance appliance image, specify the path to the OVA archive. Otherwise, leave this field empty to use the  Engine Appliance.

    ```
    If you want to deploy with a custom engine appliance image, please specify the path to the OVA archive you would like to use.
     Entering no value will use the image from the rhvm-appliance rpm, installing it if needed.
     Appliance image path []:
    ```

12. Enter the CPU and memory configuration for the Engine virtual machine:

    ```
    Please specify the number of virtual CPUs for the VM. The default is the appliance OVF value [4]:
    Please specify the memory size of the VM in MB. The default is the maximum available [6824]:
    ```

13. Specify the FQDN for the Engine virtual machine, such as `manager.example.com`:

    ```
    Please provide the FQDN you would like to use for the engine.
    Note: This will be the FQDN of the engine VM you are now going to launch,
    it should not point to the base host or to any other existing machine.
    Engine VM FQDN []:
    ```

14. Specify the domain of the Engine virtual machine. For example, if the FQDN is `manager.example.com`, then enter `example.com`.

    ```
    Please provide the domain name you would like to use for the engine appliance.
    Engine VM domain: [example.com]
    ```

15. Create the root password for the Engine, and reenter it to confirm:

    ```
    Enter root password that will be used for the engine appliance:
    Confirm appliance root password:
    ```

16. Optional: Enter an SSH public key to enable you to log in to the  Engine virtual machine as the root user without entering a password, and specify whether to enable SSH access for the root user:

    ```
    You may provide an SSH public key, that will be added by the deployment script to the authorized_keys file of the root user in the engine appliance.
    This should allow you passwordless login to the engine machine after deployment.
    If you provide no key, authorized_keys will not be touched.
    SSH public key []:
    
    Do you want to enable ssh access for the root user (yes, no, without-password) [yes]:
    ```

17. Optional: You can apply the DISA STIG security profile on the Engine  virtual machine. The DISA STIG profile is the default OpenSCAP profile.

    ```
    Do you want to apply a default OpenSCAP security profile? (Yes, No) [No]:
    ```

18. Enter a MAC address for the Engine virtual machine, or accept a  randomly generated one. If you want to provide the Engine virtual  machine with an IP address via DHCP, ensure that you have a valid DHCP  reservation for this MAC address. The deployment script will not  configure the DHCP server for you.

    ```
    You may specify a unicast MAC address for the VM or accept a randomly generated default [00:16:3e:3d:34:47]:
    ```

19. Enter the Engine virtual machine’s networking details:

    ```
    How should the engine VM network be configured (DHCP, Static)[DHCP]?
    ```

    If you specified **Static**, enter the IP address of the Engine virtual machine:

    |      | The static IP address must belong to the same subnet as the host. For example, if the host is in 10.1.1.0/24, the Engine virtual machine’s IP must be in the same subnet range (10.1.1.1-254/24).  For IPv6, oVirt supports only static addressing. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    ```
    Please enter the IP address to be used for the engine VM [x.x.x.x]:
    Please provide a comma-separated list (max 3) of IP addresses of domain name servers for the engine VM
    Engine VM DNS (leave it empty to skip):
    ```

20. Specify whether to add entries for the Engine virtual machine and the base host to the virtual machine’s `/etc/hosts` file. You must ensure that the host names are resolvable.

    ```
    Add lines for the appliance itself and for this host to /etc/hosts on the engine VM?
    Note: ensuring that this host could resolve the engine VM hostname is still up to you.
    Add lines to /etc/hosts? (Yes, No)[Yes]:
    ```

21. Provide the name and TCP port number of the SMTP server, the email  address used to send email notifications, and a comma-separated list of  email addresses to receive these notifications. Alternatively, press Enter to accept the defaults:

    ```
    Please provide the name of the SMTP server through which we will send notifications [localhost]:
    Please provide the TCP port number of the SMTP server [25]:
    Please provide the email address from which notifications will be sent [root@localhost]:
    Please provide a comma-separated list of email addresses which will get notifications [root@localhost]:
    ```

22. Create a password for the `admin@internal` user to access the Administration Portal and reenter it to confirm:

    ```
    Enter engine admin password:
    Confirm engine admin password:
    ```

23. Specify the hostname of the deployment host:

    ```
    Please provide the hostname of this host on the management network [hostname.example.com]:
    ```

    The script creates the virtual machine. By default, the script first  downloads and installs the Engine Appliance, which increases the  installation time.

24. Optional: If you set the variable `he_pause_host: true`,  the deployment pauses after adding the deployment host to the Engine.  You can now log in from the deployment host to the Engine virtual  machine to customize it. You can log in with either the FQDN or the IP  address of the Engine. For example, if the FQDN of the Engine is `manager.example.com`:

    ```
    $ ssh root@manager.example.com
    ```

    |      | In the installation log, the IP address is in `local_vm_ip`. The installation log is the most recent instance of `/var/log/ovirt-hosted-engine-setup/ovirt-hosted-engine-setup-ansible-bootstrap_local_vm*`. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    1. Customize the Engine virtual machine as needed.
    2. When you are done, log in to the Administration Portal using a  browser with the Engine FQDN and make sure that the host’s state is **Up**.
    3. Delete the lock file and the deployment script automatically continues, configuring the Engine virtual machine.

25. Select the type of storage to use:

    ```
    Please specify the storage you would like to use (glusterfs, iscsi, fc, nfs)[nfs]:
    ```

    - For NFS, enter the version, full address and path to the storage, and any mount options:

      ```
      Please specify the nfs version you would like to use (auto, v3, v4, v4_1)[auto]:
      Please specify the full shared storage connection path to use (example: host:/path): storage.example.com:/hosted_engine/nfs
      If needed, specify additional mount options for the connection to the hosted-engine storage domain []:
      ```

    - For iSCSI, enter the portal details and select a target and LUN from  the auto-detected lists. You can only select one iSCSI target during the deployment, but multipathing is supported to connect all portals of the same portal group.

      |      | To specify more than one iSCSI target, you must enable multipathing before deploying the self-hosted engine. See [*Enterprise Linux DM Multipath*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/dm_multipath/) for details. There is also a [Multipath Helper](https://access.redhat.com/labs/multipathhelper/#/) tool that generates a script to install and configure multipath with different options. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      ```
      Please specify the iSCSI portal IP address:
      Please specify the iSCSI portal port [3260]:
      Please specify the iSCSI discover user:
      Please specify the iSCSI discover password:
      Please specify the iSCSI portal login user:
      Please specify the iSCSI portal login password:
      
      The following targets have been found:
      	[1]	iqn.2017-10.com.redhat.example:he
      		TPGT: 1, portals:
      			192.168.1.xxx:3260
      			192.168.2.xxx:3260
      			192.168.3.xxx:3260
      
      Please select a target (1) [1]: 1
      
      The following luns have been found on the requested target:
        [1] 360003ff44dc75adcb5046390a16b4beb   199GiB  MSFT   Virtual HD
            status: free, paths: 1 active
      
      Please select the destination LUN (1) [1]:
      ```

    - For Gluster storage, enter the full address and path to the storage, and any mount options:

      |      | Only replica 1 and replica 3 Gluster storage are supported. Ensure you configure the volume as follows:  `gluster volume set *VOLUME_NAME* group virt gluster volume set *VOLUME_NAME* performance.strict-o-direct on gluster volume set *VOLUME_NAME* network.remote-dio off gluster volume set *VOLUME_NAME* storage.owner-uid 36 gluster volume set *VOLUME_NAME* storage.owner-gid 36 gluster volume set *VOLUME_NAME* network.ping-timeout 30` |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      ```
      Please specify the full shared storage connection path to use (example: host:/path): storage.example.com:/hosted_engine/gluster_volume
      If needed, specify additional mount options for the connection to the hosted-engine storage domain []:
      ```

    - For Fibre Channel, select a LUN from the auto-detected list. The host bus adapters must be configured and connected, and the LUN must not  contain any existing data. To reuse an existing LUN, see [Reusing LUNs](https://ovirt.org/documentation/administration_guide/index#Reusing_LUNs) in the *Administration Guide*.

      ```
      The following luns have been found on the requested target:
      [1] 3514f0c5447600351   30GiB   XtremIO XtremApp
      		status: used, paths: 2 active
      
      [2] 3514f0c5447600352   30GiB   XtremIO XtremApp
      		status: used, paths: 2 active
      
      Please select the destination LUN (1, 2) [1]:
      ```

26. Enter the disk size of the Engine virtual machine:

    ```
    Please specify the size of the VM disk in GB: [50]:
    ```

    When the deployment completes successfully, one data center, cluster, host, storage domain, and the Engine virtual machine are already  running. You can log in to the Administration Portal to add any other  resources.

27. Optional: Install and configure Red Hat Single Sign On so that you  can add additional users to the environment. For more information, see [Installing and Configuring Red Hat Single Sign-On](https://ovirt.org/documentation/administration_guide/index#Configuring_Red_Hat_SSO) in the *Administration Guide*.

28. Optional: Deploy Grafana so you can monitor and display reports from your oVirt environment. For more information, see [Configuring Grafana](https://ovirt.org/documentation/administration_guide/index#configuring_grafana) in the *Administration Guide*.

The Engine virtual machine, the host running it, and the self-hosted  engine storage domain are flagged with a gold crown in the  Administration Portal.

|      | Both the Engine’s I/O scheduler and the hypervisor that hosts the  Engine reorder I/O requests. This double reordering might delay I/O  requests to the storage layer, impacting performance.  Depending on your data center, you might improve performance by changing the I/O scheduler to `none`. For more information, see [Available disk schedulers](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/setting-the-disk-scheduler_monitoring-and-managing-system-status-and-performance) in *Monitoring and managing system status and performance* for RHEL. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

The next step is to enable the oVirt Engine repositories.

#### 5.4. Enabling the oVirt Engine Repositories

Ensure the correct repositories are enabled.

For oVirt 4.5: If you are going to install on RHEL or derivatives please follow [Installing on RHEL or derivatives](https://www.ovirt.org/download/install_on_rhel.html) first.

```
# dnf install -y centos-release-ovirt45
```

For oVirt 4.4:

```
# dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

Common procedure valid for both 4.4 and 4.5:

You can check which repositories are currently enabled by running `dnf repolist`.

1. Enable the `javapackages-tools` module.

   ```
   # dnf module -y enable javapackages-tools
   ```

2. Enable the `pki-deps` module.

   ```
   # dnf module -y enable pki-deps
   ```

3. Enable version 12 of the `postgresql` module.

   ```
   # dnf module -y enable postgresql:12
   ```

4. Enable version 2.3 of the `mod_auth_openidc` module.

   ```
   # dnf module -y enable mod_auth_openidc:2.3
   ```

5. Enable version 14 of the `nodejs` module:

   ```
   # dnf module -y enable nodejs:14
   ```

6. Synchronize installed packages to update them to the latest available versions.

   ```
   # dnf distro-sync --nobest
   ```

   Additional resources

   For information on modules and module streams, see the following sections in *Installing, managing, and removing user-space components*

   - [Module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#module-streams_introduction-to-modules)
   - [Selecting a stream before installation of packages](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#selecting-a-stream-before-installation-of-packages_installing-rhel-8-content)
   - [Resetting module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#resetting-module-streams_removing-rhel-8-content)
   - [Switching to a later stream](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#switching-to-a-later-stream_managing-versions-of-appstream-content)

Log in to the Administration Portal, where you can add hosts and storage to the environment:

#### 5.5. Connecting to the Administration Portal

Access the Administration Portal using a web browser.

1. In a web browser, navigate to `https://*manager-fqdn*/ovirt-engine`, replacing *manager-fqdn* with the FQDN that you provided during installation.

   |      | You can access the Administration Portal using alternate host names  or IP addresses. To do so, you need to add a configuration file under **/etc/ovirt-engine/engine.conf.d/**. For example:  `# vi /etc/ovirt-engine/engine.conf.d/99-custom-sso-setup.conf SSO_ALTERNATE_ENGINE_FQDNS="_alias1.example.com alias2.example.com_"`  The list of alternate host names needs to be separated by spaces. You can also add the IP address of the Engine to the list, but using IP  addresses instead of DNS-resolvable host names is not recommended. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

2. Click **Administration Portal**. An SSO login page displays. SSO login enables you to log in to the Administration and VM Portal at the same time.

3. Enter your **User Name** and **Password**. If you are logging in for the first time, use the user name **admin** along with the password that you specified during installation.

4. Select the **Domain** to authenticate against. If you are logging in using the internal **admin** user name, select the **internal** domain.

5. Click **Log In**.

6. You can view the Administration Portal in multiple languages. The  default selection is chosen based on the locale settings of your web  browser. If you want to view the Administration Portal in a language  other than the default, select your preferred language from the  drop-down list on the welcome page.

To log out of the oVirt Administration Portal, click your user name in the header bar and click **Sign Out**. You are logged out of all portals and the Engine welcome screen displays.

### 6. Installing Hosts for oVirt

oVirt supports two types of hosts: [oVirt Nodes (oVirt Node)](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Red_Hat_Virtualization_Hosts_SHE_cli_deploy) and [Enterprise Linux hosts](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Red_Hat_Enterprise_Linux_hosts_SHE_cli_deploy). Depending on your environment, you may want to use one type only, or  both. At least two hosts are required for features such as migration and high availability.

See [Recommended practices for configuring host networks](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Recommended_practices_for_configuring_host_networks_SHE_cli_deploy) for networking information.

|      | SELinux is in enforcing mode upon installation. To verify, run `getenforce`. SELinux must be in enforcing mode on all hosts and Managers for your oVirt environment to be supported. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| Host Type                 | Other Names                       | Description                                                  |
| ------------------------- | --------------------------------- | ------------------------------------------------------------ |
| **oVirt Node**            | oVirt Node, thin host             | This is a minimal operating system based on Enterprise Linux. It is  distributed as an ISO file from the Customer Portal and contains only  the packages required for the machine to act as a host. |
| **Enterprise Linux host** | Enterprise Linux host, thick host | Enterprise Linux systems with the appropriate repositories enabled can be used as hosts. |

Host Compatibility

When you create a new data center, you can set the compatibility  version. Select the compatibility version that suits all the hosts in  the data center. Once set, version regression is not allowed. For a  fresh oVirt installation, the latest compatibility version is set in the default data center and default cluster; to use an earlier  compatibility version, you must create additional data centers and  clusters.

#### 6.1. oVirt Nodes

##### 6.1.1. Installing oVirt Nodes

oVirt Node (oVirt Node) is a minimal operating system based on  Enterprise Linux that is designed to provide a simple method for setting up a physical machine to act as a hypervisor in a oVirt environment.  The minimal operating system contains only the packages required for the machine to act as a hypervisor, and features a Cockpit web interface  for monitoring the host and performing administrative tasks. See [Running Cockpit](http://cockpit-project.org/running.html) for the minimum browser requirements.

oVirt Node supports NIST 800-53 partitioning requirements to improve  security. oVirt Node uses a NIST 800-53 partition layout by default.

The host must meet the minimum  [host requirements](https://ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Write the oVirt Node Installation ISO disk image to a USB, CD, or DVD.

4. Start the machine on which you are installing oVirt Node, booting from the prepared installation media.

5. From the boot menu, select **Install oVirt Node 4.5** and press `Enter`.

   |      | You can also press the `Tab` key to edit the kernel  parameters. Kernel parameters must be separated by a space, and you can  boot the system using the specified kernel parameters by pressing the `Enter` key. Press the `Esc` key to clear any changes to the kernel parameters and return to the boot menu. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Select a language, and click **Continue**.

7. Select a keyboard layout from the **Keyboard Layout** screen and click **Done**.

8. Select the device on which to install oVirt Node from the **Installation Destination** screen. Optionally, enable encryption. Click **Done**.

   |      | Use the **Automatically configure partitioning** option. |
   | ---- | -------------------------------------------------------- |
   |      |                                                          |

9. Select a time zone from the **Time & Date** screen and click **Done**.

10. Select a network from the **Network & Host Name** screen and click **Configure…** to configure the connection details.

    |      | To use the connection every time the system boots, select the **Connect automatically with priority** check box. For more information, see [Configuring network and host name options](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation#network-hostname_configuring-system-settings) in the *Enterprise Linux 8 Installation Guide*. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    Enter a host name in the **Host Name** field, and click **Done**.

11. Optional: Configure **Security Policy** and **Kdump**. See [Customizing your RHEL installation using the GUI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation) in *Performing a standard RHEL installation* for Enterprise Linux 8 for more information on each of the sections in the **Installation Summary** screen.

12. Click **Begin Installation**.

13. Set a root password and, optionally, create an additional user while oVirt Node installs.

    |      | Do not create untrusted users on oVirt Node, as this can lead to exploitation of local security vulnerabilities. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

14. Click **Reboot** to complete the installation.

    |      | When oVirt Node restarts, `nodectl check` performs a health check on the host and displays the result when you log in on the command line. The message `node status: OK` or `node status: DEGRADED` indicates the health status. Run `nodectl check` to get more information. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    |      | If necessary, you can [ prevent kernel modules from loading automatically](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#proc-Preventing_Kernel_Modules_from_Loading_Automatically_Install_nodes_RHVH). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

##### 6.1.2. Installing a third-party package on oVirt-node

If you need a package that is not included in the oVirt provided  repository, you need to provide the repository before you can install  the package.

Prerequisites

- The path to the repository that includes the package you want to install.
- You are logged in to the host with root permissions.

Procedure

1. Open an existing `.repo` file or create a new one in `/etc/yum.repos.d/`.

2. Add an entry to the `.repo` file. For example, to install `sssd-ldap`, add the following entry to a new `.repo` file name `third-party.repo`:

   ```
   # imgbased: set-enabled
   [custom-sssd-ldap]
   name = Provides sssd-ldap
   mirrorlist=http://mirrorlist.centos.org/?release=$stream&arch=$basearch&repo=BaseOS&infra=$infra
   #baseurl=http://mirror.centos.org/$contentdir/$stream/BaseOS/$basearch/os/
   gpgcheck=1
   enabled=1
   gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial
   includepkgs = sssd-ldap
   ```

3. Install ` sssd-ldap`:

   ```
   # dnf install sssd-ldap
   ```

##### 6.1.3. Advanced Installation

###### Custom Partitioning

Custom partitioning on oVirt Node (oVirt Node) is not recommended. Use the **Automatically configure partitioning** option in the **Installation Destination** window.

If your installation requires custom partitioning, select the `I will configure partitioning` option during the installation, and note that the following restrictions apply:

- Ensure the default **LVM Thin Provisioning** option is selected in the **Manual Partitioning** window.

- The following directories are required and must be on thin provisioned logical volumes:

  - root (`/`)

  - `/home`

  - `/tmp`

  - `/var`

  - `/var/crash`

  - `/var/log`

  - `/var/log/audit`

    |      | Do not create a separate partition for `/usr`. Doing so will cause the installation to fail.  `/usr` must be on a logical volume that is able to change versions along with oVirt Node, and therefore should be left on root (`/`). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    For information about the required storage sizes for each partition, see [Storage Requirements](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Storage_Requirements_SHE_cli_deploy).

- The `/boot` directory should be defined as a standard partition.

- The `/var` directory must be on a separate volume or disk.

- Only XFS or Ext4 file systems are supported.

**Configuring Manual Partitioning in a Kickstart File**

The following example demonstrates how to configure manual partitioning in a Kickstart file.

```
clearpart --all
part /boot --fstype xfs --size=1000 --ondisk=sda
part pv.01 --size=42000 --grow
volgroup HostVG pv.01 --reserved-percent=20
logvol swap --vgname=HostVG --name=swap --fstype=swap --recommended
logvol none --vgname=HostVG --name=HostPool --thinpool --size=40000 --grow
logvol / --vgname=HostVG --name=root --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=6000 --grow
logvol /var --vgname=HostVG --name=var --thin --fstype=ext4 --poolname=HostPool
--fsoptions="defaults,discard" --size=15000
logvol /var/crash --vgname=HostVG --name=var_crash --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=10000
logvol /var/log --vgname=HostVG --name=var_log --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=8000
logvol /var/log/audit --vgname=HostVG --name=var_audit --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=2000
logvol /home --vgname=HostVG --name=home --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=1000
logvol /tmp --vgname=HostVG --name=tmp --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=1000
```

|      | If you use `logvol --thinpool --grow`, you must also include `volgroup --reserved-space` or `volgroup --reserved-percent` to reserve space in the volume group for the thin pool to grow. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

###### Installing a DUD driver on a host without installer support

There are times when installing oVirt Node (oVirt Node) requires a  Driver Update Disk (DUD), such as when using a hardware RAID device that is not supported by the default configuration of oVirt Node. In  contrast with Enterprise Linux hosts, oVirt Node does not fully support  using a DUD. Subsequently the host fails to boot normally after  installation because it does not see RAID. Instead it boots into  emergency mode.

Example output:

```
Warning: /dev/test/rhvh-4.4-20210202.0+1 does not exist
Warning: /dev/test/swap does not exist
Entering emergency mode. Exit the shell to continue.
```

In such a case you can manually add the drivers before finishing the installation.

Prerequisites

- A machine onto which you are installing oVirt Node.
- A DUD.
- If you are using a USB drive for the DUD and oVirt Node, you must have at least two available USB ports.

Procedure

1. Load the DUD on the host machine.

   You can search for DUDs or modules for CentOS Stream at the following locations:

   - [DUDs at the ELRepo Project](https://elrepo.org/linux/dud/el8/x86_64/)
   - [Kmods Special Interest Group](https://wiki.centos.org/SpecialInterestGroup/Kmods)

2. Install oVirt Node. See [Installing oVirt Nodes](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Installing_Red_Hat_Virtualization_Hosts_SHE_cli_deploy) in *Installing oVirt as a self-hosted engine using the command line*.

   |      | When installation completes, do not reboot the system. |
   | ---- | ------------------------------------------------------ |
   |      |                                                        |

   |      | If you want to access the DUD using SSH, do the following:   Add the string **` inst.sshd`** to the kernel command line:  `<*kernel_command_line*> inst.sshd`   Enable networking during the installation. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

3. Enter the console mode, by pressing Ctrl + Alt + F3. Alternatively you can connect to it using SSH.

4. Mount the DUD:

   ```
   # mkdir /mnt/dud
   # mount -r /dev/<dud_device> /mnt/dud
   ```

5. Copy the RPM file inside the DUD to the target machine’s disk:

   ```
   # cp /mnt/dud/rpms/<path>/<rpm_file>.rpm /mnt/sysroot/root/
   ```

   For example:

   ```
   # cp /mnt/dud/rpms/x86_64/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm /mnt/sysroot/root/
   ```

6. Change the root directory to `/mnt/sysroot`:

   ```
   # chroot /mnt/sysroot
   ```

7. Back up the current initrd images. For example:

   ```
   # cp -p /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
   # cp -p /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
   ```

8. Install the RPM file for the driver from the copy you made earlier.

   For example:

   ```
   # dnf install /root/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm
   ```

   |      | This package is not visible on the system after you reboot into the  installed environment, so if you need it, for example, to rebuild the `initramfs`, you need to install that package once again, after which the package remains.  If you update the host using `dnf`, the driver update persists, so you do not need to repeat this process. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   |      | If you do not have an internet connection, use the `rpm` command instead of `dnf`:  `# rpm -ivh /root/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm` |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

9. Create a new image, forcefully adding the driver:

   ```
   # dracut --force --add-drivers <module_name> --kver <kernel_version>
   ```

   For example:

   ```
   # dracut --force --add-drivers 3w-9xxx --kver 4.18.0-240.15.1.el8_3.x86_64
   ```

10. Check the results. The new image should be larger, and include the  driver. For example, compare the sizes of the original, backed-up image  file and the new image file.

    In this example, the new image file is 88739013 bytes, larger than the original 88717417 bytes:

    ```
    # ls -ltr /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img*
    -rw-------. 1 root root 88717417 Jun  2 14:29 /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
    -rw-------. 1 root root 88739013 Jun  2 17:47 /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img
    ```

    The new drivers should be part of the image file. For example, the 3w-9xxx module should be included:

    ```
    # lsinitrd /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img | grep 3w-9xxx
    drwxr-xr-x   2 root     root            0 Feb 22 15:57 usr/lib/modules/4.18.0-240.15.1.el8_3.x86_64/weak-updates/3w-9xxx
    lrwxrwxrwx   1 root     root           55 Feb 22 15:57 usr/lib/modules/4.18.0-240.15.1.el8_3.x86_64/weak-updates/3w-9xxx/3w-9xxx.ko-../../../4.18.0-240.el8.x86_64/extra/3w-9xxx/3w-9xxx.ko
    drwxr-xr-x   2 root     root            0 Feb 22 15:57 usr/lib/modules/4.18.0-240.el8.x86_64/extra/3w-9xxx
    -rw-r--r--   1 root     root        80121 Nov 10  2020 usr/lib/modules/4.18.0-240.el8.x86_64/extra/3w-9xxx/3w-9xxx.ko
    ```

11. Copy the image to the the directory under `/boot` that contains the kernel to be used in the layer being installed, for example:

    ```
    # cp -p /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img
    ```

12. Exit chroot.

13. Exit the shell.

14. If you used Ctrl + Alt + F3 to access a virtual terminal, then move back to the installer by pressing Ctrl + Alt + F_<n>_, usually F1 or F5

15. At the installer screen, reboot.

Verification

The machine should reboot successfully.

###### Automating oVirt Node deployment

You can install oVirt Node (oVirt Node) without a physical media  device by booting from a PXE server over the network with a Kickstart  file that contains the answers to the installation questions.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

General instructions for installing from a PXE server with a Kickstart file are available in the [*Enterprise Linux Installation Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/chap-kickstart-installations), as oVirt Node is installed in much the same way as Enterprise Linux.  oVirt Node-specific instructions, with examples for deploying oVirt Node with Red Hat Satellite, are described below.

The automated oVirt Node deployment has 3 stages:

- [Preparing the Installation Environment](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Preparing_the_Installation_Environment)
- [Configuring the PXE Server and the Boot Loader](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Configuring_the_PXE_Server_and_the_Boot_Loader)
- [Creating and Running a Kickstart File](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Creating_and_Running_a_Kickstart_File)

###### Preparing the installation environment

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Make the oVirt Node ISO image available over the network. See [Installation Source on a Network](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/sect-making-media-additional-sources.html#sect-making-media-sources-network) in the *Enterprise Linux Installation Guide*.

4. Extract the **squashfs.img** hypervisor image file from the oVirt Node ISO:

   ```
   # mount -o loop /path/to/oVirt Node-ISO /mnt/rhvh
   # cp /mnt/rhvh/Packages/redhat-virtualization-host-image-update* /tmp
   # cd /tmp
   # rpm2cpio redhat-virtualization-host-image-update* | cpio -idmv
   ```

   |      | This **squashfs.img** file, located in the `/tmp/usr/share/redhat-virtualization-host/image/` directory, is called **redhat-virtualization-host-\*version_number\*_version.squashfs.img**. It contains the hypervisor image for installation on the physical machine. It should not be confused with the **/LiveOS/squashfs.img** file, which is used by the Anaconda `inst.stage2` option. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

###### Configuring the PXE server and the boot loader

1. Configure the PXE server. See [Preparing for a Network Installation](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-installation-server-setup.html) in the *Enterprise Linux Installation Guide*.

2. Copy the oVirt Node boot images to the `/tftpboot` directory:

   ```
   # cp mnt/rhvh/images/pxeboot/{vmlinuz,initrd.img} /var/lib/tftpboot/pxelinux/
   ```

3. Create a `rhvh` label specifying the oVirt Node boot images in the boot loader configuration:

   ```
   LABEL rhvh
   MENU LABEL Install oVirt Node
   KERNEL /var/lib/tftpboot/pxelinux/vmlinuz
   APPEND initrd=/var/lib/tftpboot/pxelinux/initrd.img inst.stage2=URL/to/oVirt Node-ISO
   ```

   oVirt Node Boot loader configuration example for Red Hat Satellite

   If you are using information from Red Hat Satellite to provision the  host, you must create a global or host group level parameter called `rhvh_image` and populate it with the directory URL where the ISO is mounted or extracted:

   ```
   <%#
   kind: PXELinux
   name: oVirt Node PXELinux
   %>
   # Created for booting new hosts
   #
   
   DEFAULT rhvh
   
   LABEL rhvh
   KERNEL <%= @kernel %>
   APPEND initrd=<%= @initrd %> inst.ks=<%= foreman_url("provision") %> inst.stage2=<%= @host.params["rhvh_image"] %> intel_iommu=on console=tty0 console=ttyS1,115200n8 ssh_pwauth=1 local_boot_trigger=<%= foreman_url("built") %>
   IPAPPEND 2
   ```

4. Make the content of the oVirt Node ISO locally available and export it to the network, for example, using an HTTPD server:

   ```
   # cp -a /mnt/rhvh/ /var/www/html/rhvh-install
   # curl URL/to/oVirt Node-ISO/rhvh-install
   ```

###### Creating and running a Kickstart file

1. Create a Kickstart file and make it available over the network. See [Kickstart Installations](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-kickstart-installations.html) in the *Enterprise Linux Installation Guide*.

2. Ensure that the Kickstart file meets the following oVirt-specific requirements:

   - The `%packages` section is not required for oVirt Node. Instead, use the `liveimg` option and specify the **redhat-virtualization-host-\*version_number\*_version.squashfs.img** file from the oVirt Node ISO image:

     ```
     liveimg --url=example.com/tmp/usr/share/redhat-virtualization-host/image/redhat-virtualization-host-version_number_version.squashfs.img
     ```

   - Autopartitioning is highly recommended, but use caution: ensure that the local disk is detected first, include the `ignoredisk` command, and specify the local disk to ignore, such as `sda`.  To ensure that a particular drive is used, oVirt recommends using `ignoredisk --only-use=/dev/disk/<*path*>` or `ignoredisk --only-use=/dev/disk/<*ID*>`:

     ```
     autopart --type=thinp
     ignoredisk --only-use=sda
     ignoredisk --only-use=/dev/disk/<path>
     ignoredisk --only-use=/dev/disk/<ID>
     ```

     |      | Autopartitioning requires thin provisioning.  The `--no-home` option does not work in oVirt Node because `/home` is a required directory. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     If your installation requires manual partitioning, see [Custom Partitioning](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Custom_Partitioning_SHE_cli_deploy) for a list of limitations that apply to partitions and an example of manual partitioning in a Kickstart file.

   - A `%post` section that calls the `nodectl init` command is required:

     ```
     %post
     nodectl init
     %end
     ```

     |      | Ensure that the `nodectl init` command is at the very end of the `%post` section but before the reboot code, if any. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     Kickstart example for deploying oVirt Node on its own

     This Kickstart example shows you how to deploy oVirt Node. You can include additional commands and options as required.

     |      | This example assumes that all disks are empty and can be initialized. If you have attached disks with data, either remove them or add them to the `ignoredisks` property. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     ```
     liveimg --url=http://FQDN/tmp/usr/share/redhat-virtualization-host/image/redhat-virtualization-host-version_number_version.squashfs.img
     clearpart --all
     autopart --type=thinp
     rootpw --plaintext ovirt
     timezone --utc America/Phoenix
     zerombr
     text
     
     reboot
     
     %post --erroronfail
     nodectl init
     %end
     ```

3. Add the Kickstart file location to the boot loader configuration file on the PXE server:

   ```
   APPEND initrd=/var/tftpboot/pxelinux/initrd.img inst.stage2=URL/to/oVirt Node-ISO inst.ks=URL/to/oVirt Node-ks.cfg
   ```

4. Install oVirt Node following the instructions in [Booting from the Network Using PXE](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-booting-installer-x86.html#sect-booting-from-pxe-x86) in the *Enterprise Linux Installation Guide*.

### 6.2. Enterprise Linux hosts

#### 6.2.1. Installing Enterprise Linux hosts

A Enterprise Linux host is based on a standard basic installation of Enterprise Linux 8 on a physical server, with the `Enterprise Linux Server` and `oVirt` repositories enabled.

The oVirt project also provides packages for Enterprise Linux 9 but only as a Technology Preview.

For detailed installation instructions, see the [*Performing a standard EL installation*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index.html).

The host must meet the minimum [host requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Virtualization must be enabled in your host’s BIOS settings. For  information on changing your host’s BIOS settings, refer to your host’s  hardware documentation. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Do not install third-party watchdogs on Enterprise Linux hosts. They can interfere with the watchdog daemon provided by VDSM. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 6.2.2. Installing Cockpit on Enterprise Linux hosts

You can install Cockpit for monitoring the host’s resources and performing administrative tasks.

Procedure

1. Install the dashboard packages:

   ```
   # dnf install cockpit-ovirt-dashboard
   ```

2. Enable and start the `cockpit.socket` service:

   ```
   # systemctl enable cockpit.socket
   # systemctl start cockpit.socket
   ```

3. Check if Cockpit is an active service in the firewall:

   ```
   # firewall-cmd --list-services
   ```

   You should see `cockpit` listed. If it is not, enter the following with root permissions to add `cockpit` as a service to your firewall:

   ```
   # firewall-cmd --permanent --add-service=cockpit
   ```

   The `--permanent` option keeps the `cockpit` service active after rebooting.

You can log in to the Cockpit web interface at `https://*HostFQDNorIP*:9090`.

### 6.3. Recommended Practices for Configuring Host Networks

|      | Always use the oVirt Engine to modify the network configuration of hosts in your clusters. Otherwise, you might create an unsupported  configuration. For details, see [Network Manager Stateful Configuration (nmstate)](https://ovirt.org/documentation/administration_guide/index#con-Network-Manager-Stateful-Configuration-nmstate). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

If your network environment is complex, you may need to configure a  host network manually before adding the host to the oVirt Engine.

Consider the following practices for configuring a host network:

- Configure the network with Cockpit. Alternatively, you can use `nmtui` or `nmcli`.

- If a network is not required for a self-hosted engine deployment or  for adding a host to the Engine, configure the network in the  Administration Portal after adding the host to the Engine. See [Creating a New Logical Network in a Data Center or Cluster](https://ovirt.org/documentation/administration_guide/index#Creating_a_new_logical_network_in_a_data_center_or_cluster).

- Use the following naming conventions:

  - VLAN devices: `*VLAN_NAME_TYPE_RAW_PLUS_VID_NO_PAD*`
  - VLAN interfaces: `*physical_device*.*VLAN_ID*` (for example, `eth0.23`, `eth1.128`, `enp3s0.50`)
  - Bond interfaces: `bond*number*` (for example, `bond0`, `bond1`)
  - VLANs on bond interfaces: `bond*number*.*VLAN_ID*` (for example, `bond0.50`, `bond1.128`)

- Use [network bonding](https://ovirt.org/documentation/administration_guide/index#sect-Network_Bonding). Network teaming is not supported in oVirt and will cause errors if the  host is used to deploy a self-hosted engine or added to the Engine.

- Use recommended bonding modes:

  - If the `ovirtmgmt` network is not used by virtual machines, the network may use any supported bonding mode.
  - If the `ovirtmgmt` network is used by virtual machines, see [*Which bonding modes work when used with a bridge that virtual machine guests or containers connect to?*](https://access.redhat.com/solutions/67546).
  - oVirt’s default bonding mode is `(Mode 4) Dynamic Link Aggregation`. If your switch does not support Link Aggregation Control Protocol (LACP), use `(Mode 1) Active-Backup`. See [Bonding Modes](https://ovirt.org/documentation/administration_guide/index#Bonding_Modes) for details.

- Configure a VLAN on a physical NIC as in the following example (although `nmcli` is used, you can use any tool):

  ```
  # nmcli connection add type vlan con-name vlan50 ifname eth0.50 dev eth0 id 50
  # nmcli con mod vlan50 +ipv4.dns 8.8.8.8 +ipv4.addresses 123.123.0.1/24 +ipv4.gateway 123.123.0.254
  ```

- Configure a VLAN on a bond as in the following example (although `nmcli` is used, you can use any tool):

  ```
  # nmcli connection add type bond con-name bond0 ifname bond0 bond.options "mode=active-backup,miimon=100" ipv4.method disabled ipv6.method ignore
  # nmcli connection add type ethernet con-name eth0 ifname eth0 master bond0 slave-type bond
  # nmcli connection add type ethernet con-name eth1 ifname eth1 master bond0 slave-type bond
  # nmcli connection add type vlan con-name vlan50 ifname bond0.50 dev bond0 id 50
  # nmcli con mod vlan50 +ipv4.dns 8.8.8.8 +ipv4.addresses 123.123.0.1/24 +ipv4.gateway 123.123.0.254
  ```

- Do not disable `firewalld`.

- Customize the firewall rules in the Administration Portal after adding the host to the Engine. See [Configuring Host Firewall Rules](https://ovirt.org/documentation/administration_guide/index#Configuring_Host_Firewall_Rules).

### 6.4. Adding Self-Hosted Engine Nodes to the oVirt Engine

Add self-hosted engine nodes in the same way as a standard host, with an additional step to deploy the host as a self-hosted engine node. The shared storage domain is automatically detected and the node can be  used as a failover host to host the Engine virtual machine when  required. You can also attach standard hosts to a self-hosted engine  environment, but they cannot host the Engine virtual machine. Have at  least two self-hosted engine nodes to ensure the Engine virtual machine  is highly available. You can also add additional hosts using the REST  API. See [Hosts](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-hosts) in the *REST API Guide*.

Prerequisites

- All self-hosted engine nodes must be in the same cluster.
- If you are reusing a self-hosted engine node, remove its existing self-hosted engine configuration. See [Removing a Host from a Self-Hosted Engine Environment](https://ovirt.org/documentation/administration_guide/index#removing_a_host_from_a_self-hosted_engine_environment).

Procedure

1. In the Administration Portal, click **Compute** **Hosts**.

2. Click **New**.

   For information on additional host settings, see [Explanation of Settings and Controls in the New Host and Edit Host Windows](https://ovirt.org/documentation/administration_guide#sect-Explanation_of_Settings_and_Controls_in_the_New_Host_and_Edit_Host_Windows) in the *Administration Guide*.

3. Use the drop-down list to select the **Data Center** and **Host Cluster** for the new host.

4. Enter the **Name** and the **Address** of the new host. The standard SSH port, port 22, is auto-filled in the **SSH Port** field.

5. Select an authentication method to use for the Engine to access the host.

   - Enter the root user’s password to use password authentication.
   - Alternatively, copy the key displayed in the **SSH PublicKey** field to **/root/.ssh/authorized_keys** on the host to use public key authentication.

6. Optionally, configure power management, where the host has a  supported power management card. For information on power management  configuration, see [Host Power Management Settings Explained](https://ovirt.org/documentation/administration_guide#Host_Power_Management_settings_explained) in the *Administration Guide*.

7. Click the **Hosted Engine** tab.

8. Select **Deploy**.

9. Click **OK**.

### 6.5. Adding Standard Hosts to the oVirt Engine

|      | Always use the oVirt Engine to modify the network configuration of hosts in your clusters. Otherwise, you might create an unsupported  configuration. For details, see [Network Manager Stateful Configuration (nmstate)](https://ovirt.org/documentation/administration_guide/index#con-Network-Manager-Stateful-Configuration-nmstate). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Adding a host to your oVirt environment can take some time, as the  following steps are completed by the platform: virtualization checks,  installation of packages, and creation of a bridge.

Procedure

1. From the Administration Portal, click **Compute** **Hosts**.
2. Click **New**.
3. Use the drop-down list to select the **Data Center** and **Host Cluster** for the new host.
4. Enter the **Name** and the **Address** of the new host. The standard SSH port, port 22, is auto-filled in the **SSH Port** field.
5. Select an authentication method to use for the Engine to access the host.
   - Enter the root user’s password to use password authentication.
   - Alternatively, copy the key displayed in the **SSH PublicKey** field to **/root/.ssh/authorized_keys** on the host to use public key authentication.
6. Optionally, click the **Advanced Parameters** button to change the following advanced host settings:
   - Disable automatic firewall configuration.
   - Add a host SSH fingerprint to increase security. You can add it manually, or fetch it automatically.
7. Optionally configure power management, where the host has a supported power management card. For information on power management  configuration, see [Host Power Management Settings Explained](https://ovirt.org/documentation/administration_guide/index#Host_Power_Management_settings_explained) in the *Administration Guide*.
8. Click **OK**.

The new host displays in the list of hosts with a status of `Installing`, and you can view the progress of the installation in the **Events** section of the **Notification Drawer** (![EventsIcon](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/common/images/EventsIcon.png)). After a brief delay the host status changes to `Up`.

## 7. Adding Storage for oVirt

Add storage as data domains in the new environment. A oVirt  environment must have at least one data domain, but adding more is  recommended.

Add the storage you prepared earlier:

- [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Adding_NFS_Storage_SHE_cli_deploy)
- [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Adding_iSCSI_Storage_SHE_cli_deploy)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Adding_FCP_Storage_SHE_cli_deploy)
- [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Adding_Red_Hat_Gluster_Storage_SHE_cli_deploy)

|      | If you are using iSCSI storage, new data domains must not use the same iSCSI target as the self-hosted engine storage domain. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Creating additional data domains in the same data center as the  self-hosted engine storage domain is highly recommended. If you deploy  the self-hosted engine in a data center with only one active data  storage domain, and that storage domain is corrupted, you will not be  able to add new storage domains or remove the corrupted storage domain;  you will have to redeploy the self-hosted engine. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 7.1. Adding NFS Storage

This procedure shows you how to attach existing NFS storage to your oVirt environment as a data domain.

If you require an ISO or export domain, use this procedure, but select **ISO** or **Export** from the **Domain Function** list.

Procedure

1. In the Administration Portal, click **Storage** **Domains**.
2. Click **New Domain**.
3. Enter a **Name** for the storage domain.
4. Accept the default values for the **Data Center**, **Domain Function**, **Storage Type**, **Format**, and **Host** lists.
5. Enter the **Export Path** to be used for the storage domain. The export path should be in the format of *123.123.0.10:/data* (for IPv4), *[2001:0:0:0:0:0:0:5db1]:/data* (for IPv6), or *domain.example.com:/data*.
6. Optionally, you can configure the advanced parameters:
   1. Click **Advanced Parameters**.
   2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
   3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
   4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
7. Click **OK**.

The new NFS data domain has a status of `Locked` until the disk is prepared. The data domain is then automatically attached to the data center.

### 7.2. Adding iSCSI Storage

This procedure shows you how to attach existing iSCSI storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** of the new storage domain.

4. Select a **Data Center** from the drop-down list.

5. Select **Data** as the **Domain Function** and **iSCSI** as the **Storage Type**.

6. Select an active host as the **Host**.

   |      | Communication to the storage domain is from the selected host and not directly from the Engine. Therefore, all hosts must have access to the  storage device before the storage domain can be configured. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. The Engine can map iSCSI targets to LUNs or LUNs to iSCSI targets. The **New Domain** window automatically displays known targets with unused LUNs when the  iSCSI storage type is selected. If the target that you are using to add  storage does not appear, you can use target discovery to find it;  otherwise proceed to the next step.

   1. Click **Discover Targets** to enable target discovery options. When targets have been discovered and logged in to, the **New Domain** window automatically displays targets with LUNs unused by the environment.

      |      | LUNs used externally for the environment are also displayed. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      You can use the **Discover Targets** options to add LUNs on many targets or multiple paths to the same LUNs.

      |      | If you use the REST API method `discoveriscsi` to discover the iscsi targets, you can use an FQDN or an IP address, but you must  use the iscsi details from the discovered targets results to log in  using the REST API method `iscsilogin`. See [discoveriscsi](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-host-methods-discover_iscsi) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   2. Enter the FQDN or IP address of the iSCSI host in the **Address** field.

   3. Enter the port with which to connect to the host when browsing for targets in the **Port** field. The default is `3260`.

   4. If CHAP is used to secure the storage, select the **User Authentication** check box. Enter the **CHAP user name** and **CHAP password**.

      |      | You can define credentials for an iSCSI target for a specific host with the REST API. See [StorageServerConnectionExtensions: add](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-storage_server_connection_extensions-methods-add) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   5. Click **Discover**.

   6. Select one or more targets from the discovery results and click **Login** for one target or **Login All** for multiple targets.

      |      | If more than one path access is required, you must discover and log  in to the target through all the required paths. Modifying a storage  domain to add additional paths is currently not supported. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      |      | When using the REST API `iscsilogin` method to log in, you must use the iscsi details from the discovered targets results in the `discoveriscsi` method. See [iscsilogin](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-host-methods-iscsi_login) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

8. Click the **+** button next to the desired target. This expands the entry and displays all unused LUNs attached to the target.

9. Select the check box for each LUN that you are using to create the storage domain.

10. Optionally, you can configure the advanced parameters:

    1. Click **Advanced Parameters**.
    2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
    3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
    4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
    5. Select the **Discard After Delete** check box to enable  the discard after delete option. This option can be edited after the  domain is created. This option is only available to block storage  domains.

11. Click **OK**.

If you have configured multiple storage connection paths to the same target, follow the procedure in [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing) to complete iSCSI bonding.

If you want to migrate your current storage network to an iSCSI bond, see [Migrating a Logical Network to an iSCSI Bond](https://ovirt.org/documentation/administration_guide/index#Migrating_a_logical_network_to_an_iscsi_bond).

### 7.3. Adding FCP Storage

This procedure shows you how to attach existing FCP storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** of the storage domain.

4. Select an FCP **Data Center** from the drop-down list.

   If you do not yet have an appropriate FCP data center, select `(none)`.

5. Select the **Domain Function** and the **Storage Type** from the drop-down lists. The storage domain types that are not compatible with the chosen data center are not available.

6. Select an active host in the **Host** field. If this is not the first data domain in a data center, you must select the data center’s SPM host.

   |      | All communication to the storage domain is through the selected host  and not directly from the oVirt Engine. At least one active host must  exist in the system and be attached to the chosen data center. All hosts must have access to the storage device before the storage domain can be configured. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. The **New Domain** window automatically displays known targets with unused LUNs when **Fibre Channel** is selected as the storage type. Select the **LUN ID** check box to select all of the available LUNs.

8. Optionally, you can configure the advanced parameters.

   1. Click **Advanced Parameters**.
   2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
   3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
   4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
   5. Select the **Discard After Delete** check box to enable  the discard after delete option. This option can be edited after the  domain is created. This option is only available to block storage  domains.

9. Click **OK**.

The new FCP data domain remains in a `Locked` status while it is being prepared for use. When ready, it is automatically attached to the data center.

### 7.4. Adding Gluster Storage

To use Gluster Storage with oVirt, see [*Configuring oVirt with Gluster Storage*](https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.4/html/configuring_red_hat_virtualization_with_red_hat_gluster_storage/).

For the Gluster Storage versions that are supported with oVirt, see [Red Hat Gluster Storage Version Compatibility and Support](https://access.redhat.com/articles/2356261).

## Appendix A: Troubleshooting a Self-hosted Engine Deployment

To confirm whether the self-hosted engine has already been deployed, run `hosted-engine --check-deployed`. An error will only be displayed if the self-hosted engine has not been deployed.

### Troubleshooting the Engine Virtual Machine

Check the status of the Engine virtual machine by running `hosted-engine --vm-status`.

|      | Any changes made to the Engine virtual machine will take about 20  seconds before they are reflected in the status command output. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Depending on the `Engine status` in the output, see the following suggestions to find or fix the issue.

#### Engine status: "health": "good", "vm": "up"  "detail": "up"

1. If the Engine virtual machine is up and running as normal, you will see the following output:

   ```
   --== Host 1 status ==--
   
   Status up-to-date              : True
   Hostname                       : hypervisor.example.com
   Host ID                        : 1
   Engine status                  : {"health": "good", "vm": "up", "detail": "up"}
   Score                          : 3400
   stopped                        : False
   Local maintenance              : False
   crc32                          : 99e57eba
   Host timestamp                 : 248542
   ```

2. If the output is normal but you cannot connect to the Engine, check the network connection.

#### Engine status: "reason": "failed liveliness check", "health": "bad", "vm": "up", "detail": "up"

1. If the `health` is `bad` and the `vm` is `up`, the HA services will try to restart the Engine virtual machine to get  the Engine back. If it does not succeed within a few minutes, enable the global maintenance mode from the command line so that the hosts are no  longer managed by the HA services.

   ```
   # hosted-engine --set-maintenance --mode=global
   ```

2. Connect to the console. When prompted, enter the operating system’s root password. For more console options, see [How to access Hosted Engine VM console from RHEV-H host?](https://access.redhat.com/solutions/2221461).

   ```
   # hosted-engine --console
   ```

3. Ensure that the Engine virtual machine’s operating system is running by logging in.

4. Check the status of the `ovirt-engine` service:

   ```
   # systemctl status -l ovirt-engine
   # journalctl -u ovirt-engine
   ```

5. Check the following logs: **/var/log/messages**, **/var/log/ovirt-engine/engine.log,** and **/var/log/ovirt-engine/server.log**.

6. After fixing the issue, reboot the Engine virtual machine manually from one of the self-hosted engine nodes:

   ```
   # hosted-engine --vm-shutdown
   # hosted-engine --vm-start
   ```

   |      | When the self-hosted engine nodes are in global maintenance mode, the Engine virtual machine must be rebooted manually. If you try to reboot  the Engine virtual machine by sending a `reboot` command from the command line, the Engine virtual machine will remain powered off. This is by design. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. On the Engine virtual machine, verify that the `ovirt-engine` service is up and running:

   ```
    # systemctl status ovirt-engine.service
   ```

8. After ensuring the Engine virtual machine is up and running, close  the console session and disable the maintenance mode to enable the HA  services again:

   ```
   # hosted-engine --set-maintenance --mode=none
   ```

#### Engine status: "vm": "down", "health": "bad", "detail": "unknown", "reason": "vm not running on this host"

|      | This message is expected on a host that is not currently running the Engine virtual machine. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

1. If you have more than one host in your environment, ensure that  another host is not currently trying to restart the Engine virtual  machine.

2. Ensure that you are not in global maintenance mode.

3. Check the **ovirt-ha-agent** logs in **/var/log/ovirt-hosted-engine-ha/agent.log**.

4. Try to reboot the Engine virtual machine manually from one of the self-hosted engine nodes:

   ```
   # hosted-engine --vm-shutdown
   # hosted-engine --vm-start
   ```

#### Engine status: "vm": "unknown", "health": "unknown", "detail": "unknown", "reason": "failed to getVmStats"

This status means that `ovirt-ha-agent` failed to get the virtual machine’s details from VDSM.

1. Check the VDSM logs in **/var/log/vdsm/vdsm.log**.
2. Check the **ovirt-ha-agent** logs in **/var/log/ovirt-hosted-engine-ha/agent.log**.

#### Engine status: The self-hosted engine’s configuration has not been retrieved from shared storage

If you receive the status `The hosted engine configuration has  not been retrieved from shared storage. Please ensure that  ovirt-ha-agent is running and the storage server is reachable` there is an issue with the `ovirt-ha-agent` service, or with the storage, or both.

1. Check the status of `ovirt-ha-agent` on the host:

   ```
   # systemctl status -l ovirt-ha-agent
   # journalctl -u ovirt-ha-agent
   ```

2. If the `ovirt-ha-agent` is down, restart it:

   ```
   # systemctl start ovirt-ha-agent
   ```

3. Check the `ovirt-ha-agent` logs in **/var/log/ovirt-hosted-engine-ha/agent.log**.

4. Check that you can ping the shared storage.

5. Check whether the shared storage is mounted.

#### Additional Troubleshooting Commands

- `hosted-engine --reinitialize-lockspace`: This command is  used when the sanlock lockspace is broken. Ensure that the global  maintenance mode is enabled and that the Engine virtual machine is  stopped before reinitializing the sanlock lockspaces.
- `hosted-engine --clean-metadata`: Remove the metadata for a host’s agent from the global status database. This makes all other  hosts forget about this host. Ensure that the target host is down and  that the global maintenance mode is enabled.
- `hosted-engine --check-liveliness`: This command checks the liveliness page of the ovirt-engine service. You can also check by connecting to `https://*engine-fqdn*/ovirt-engine/services/health/` in a web browser.
- `hosted-engine --connect-storage`: This command instructs  VDSM to prepare all storage connections needed for the host and the  Engine virtual machine. This is normally run in the back-end during the  self-hosted engine deployment. Ensure that the global maintenance mode  is enabled if you need to run this command to troubleshoot storage  issues.

### Cleaning Up a Failed Self-hosted Engine Deployment

If a self-hosted engine deployment was interrupted, subsequent  deployments will fail with an error message. The error will differ  depending on the stage in which the deployment failed.

If you receive an error message, you can run the cleanup script on  the deployment host to clean up the failed deployment. However, it’s  best to reinstall your base operating system and start the deployment  from the beginning.

|      | The cleanup script has the following limitations:   A disruption in the network connection while the script is running  might cause the script to fail to remove the management bridge or to  recreate a working network configuration.  The script is not designed to clean up any shared storage device used during a failed deployment. You need to clean the shared storage device before you can reuse it in a subsequent deployment. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Run `/usr/sbin/ovirt-hosted-engine-cleanup` and select `y` to remove anything left over from the failed self-hosted engine deployment.

   ```
   # /usr/sbin/ovirt-hosted-engine-cleanup
   This will de-configure the host to run ovirt-hosted-engine-setup from scratch.
   Caution, this operation should be used with care.
   Are you sure you want to proceed? [y/n]
   ```

2. Define whether to reinstall on the same shared storage device or select a different shared storage device.

   - To deploy the installation on the same storage domain, clean up the  storage domain by running the following command in the appropriate  directory on the server for NFS, Gluster, PosixFS or local storage  domains:

     ```
     # rm -rf storage_location/*
     ```

   - For iSCSI or Fibre Channel Protocol (FCP) storage, see [How to Clean Up a Failed Self-hosted Engine Deployment?](https://access.redhat.com/solutions/2121581) for information on how to clean up the storage.

   - Reboot the self-hosted engine host or select a different shared storage device.

     |      | The reboot is needed to make sure all the connections to the storage are cleaned before the next attempt. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

3. Redeploy the self-hosted engine.

## Appendix B: Customizing the Engine virtual machine using automation during deployment

You can use automation to adjust or otherwise customize the Engine  virtual machine during deployment by using one or more Ansible  playbooks. You can run playbooks at the following points during  deployment:

- before the self-hosted engine setup
- after the self-hosted engine setup, but before storage is configured
- after adding the deployment host to the Engine
- after the deployment completes entirely

Procedure

1. Write one or more Ansible playbooks to run on the Engine virtual machine at specific points in the deployment process.

2. Add the playbooks to the appropriate directory under `/usr/share/ansible/collections/ansible_collections/redhat/rhv/roles/hosted_engine_setup/hooks/`:

   - `enginevm_before_engine_setup`

     Run the playbook before the self-hosted engine setup.

   - `enginevm_after_engine_setup`

     Run the playbook after the self-hosted engine setup, but before storage is configured.

   - `after_add_host`

     Run the playbook after adding the deployment host to the Engine.

   - `after_setup`

     Run the playbook after deployment is completed.

When you run the self-hosted-engine installer, the deployment script runs the `ovirt-engine-setup` role, which automatically runs any playbooks in either of these directories.

Additional resources

- [Deploying the self-hosted engine using the command line](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/installing_red_hat_virtualization_as_a_self-hosted_engine_using_the_command_line/index#Deploying_the_Self-Hosted_Engine_Using_the_CLI_install_RHVM)
- [Automating Configuration Tasks using Ansible](https://ovirt.org/documentation/administration_guide/index#chap-Automating_RHV_Configuration_using_Ansible)
- [Intro to playbooks](https://docs.ansible.com/ansible/latest/user_guide/playbooks_intro.html) in the Ansible documentation

## Appendix C: Migrating Databases and Services to a Remote Server

Although you cannot configure remote databases and services during  the automated installation, you can migrate them to a separate server  post-installation.

### Migrating Data Warehouse to a Separate Machine

This section describes how to migrate the Data Warehouse database and service from the oVirt Engine machine to a separate machine. Hosting  the Data Warehouse service on a separate machine reduces the load on  each individual machine, and avoids potential conflicts caused by  sharing CPU and memory resources with other processes.

|      | oVirt only supports installing the Data Warehouse database, the Data  Warehouse service and Grafana all on the same machine as each other,  even though you can install each of these components on separate  machines from each other. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

You have the following migration options:

- You can migrate the Data Warehouse service away from the Engine  machine and connect it with the existing Data Warehouse database (`ovirt_engine_history`).
- You can migrate the Data Warehouse database away from the Engine machine and then migrate the Data Warehouse service.

#### Migrating the Data Warehouse Database to a Separate Machine

Procedure

1. Create a backup of the Data Warehouse database and configuration files on the Engine:

   ```
   # engine-backup --mode=backup --scope=dwhdb --scope=files --file=file_name --log=log_file_name
   ```

2. Copy the backup file from the Engine to the new machine:

   ```
   # scp /tmp/file_name root@new.dwh.server.com:/tmp
   ```

3. Install `engine-backup` on the new machine:

   ```
   # dnf install ovirt-engine-tools-backup
   ```

4. Install the PostgreSQL server package:

   ```
   # dnf install postgresql-server postgresql-contrib
   ```

5. Initialize the PostgreSQL database, start the `postgresql` service, and ensure that this service starts on boot:

   ```
   # su - postgres -c 'initdb'
   # systemctl enable postgresql
   # systemctl start postgresql
   ```

6. Restore the Data Warehouse database on the new machine. *file_name* is the backup file copied from the Engine.

   ```
   # engine-backup --mode=restore --scope=files --scope=dwhdb --file=file_name --log=log_file_name --provision-dwh-db --restore-permissions
   ```

The Data Warehouse database is now hosted on a separate machine from  that on which the Engine is hosted. After successfully restoring the  Data Warehouse database, a prompt instructs you to run the `engine-setup` command. Before running this command, migrate the Data Warehouse service.

#### Migrating the Data Warehouse Service to a Separate Machine

You can migrate the Data Warehouse service installed and configured  on the oVirt Engine to a separate machine. Hosting the Data Warehouse  service on a separate machine helps to reduce the load on the Engine  machine.

Notice that this procedure migrates the Data Warehouse service only.

To migrate the Data Warehouse database (`ovirt_engine_history`) prior to migrating the Data Warehouse service, see [Migrating the Data Warehouse Database to a Separate Machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH).

|      | oVirt only supports installing the Data Warehouse database, the Data  Warehouse service and Grafana all on the same machine as each other,  even though you can install each of these components on separate  machines from each other. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

- You must have installed and configured the Engine and Data Warehouse on the same machine.
- To set up the new Data Warehouse machine, you must have the following:
  - The password from the Engine’s **/etc/ovirt-engine/engine.conf.d/10-setup-database.conf** file.
  - Allowed access from the Data Warehouse machine to the Engine database machine’s TCP port 5432.
  - The username and password for the Data Warehouse database from the Engine’s **/etc/ovirt-engine-dwh/ovirt-engine-dwhd.conf.d/10-setup-database.conf** file. If you migrated the `ovirt_engine_history` database using [Migrating the Data Warehouse Database to a Separate Machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH), the backup includes these credentials, which you defined during the database setup on that machine.

Installing this scenario requires four steps:

1. [Setting up the New Data Warehouse Machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#setting_up_machine)
2. [Stopping the Data Warehouse service on the Engine machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#stopping_dwh_service)
3. [Configuring the new Data Warehouse machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#configuring_new_machine)
4. [Disabling the Data Warehouse package on the Engine machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#disabling_dwh_package)

##### Setting up the New Data Warehouse Machine

Enable the oVirt repositories and install the Data Warehouse setup package on a Enterprise Linux 8 machine:

1. Enable the `pki-deps` module.

   ```
   # dnf module -y enable pki-deps
   ```

2. Ensure that all packages currently installed are up to date:

   ```
   # dnf upgrade --nobest
   ```

3. Install the `ovirt-engine-dwh-setup` package:

   ```
   # dnf install ovirt-engine-dwh-setup
   ```

##### Stopping the Data Warehouse Service on the Engine Machine

1. Stop the Data Warehouse service:

   ```
   # systemctl stop ovirt-engine-dwhd.service
   ```

2. If the database is hosted on a remote machine, you must manually grant access by editing the postgres.conf file. Edit the `/var/lib/pgsql/data/postgresql.conf` file and modify the listen_addresses line so that it matches the following:

   ```
   listen_addresses = '*'
   ```

   If the line does not exist or has been commented out, add it manually.

   If the database is hosted on the Engine machine and was configured  during a clean setup of the oVirt Engine, access is granted by default.

   See [Migrating the Data Warehouse Database to a Separate Machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH) for more information on how to configure and migrate the Data Warehouse database.

3. Restart the postgresql service:

   ```
   # systemctl restart postgresql
   ```

##### Configuring the New Data Warehouse Machine

The order of the options or settings shown in this section may differ depending on your environment.

1. If you are migrating both the `ovirt_engine_history` database and the Data Warehouse service to the **same** machine, run the following, otherwise proceed to the next step.

   ```
   # sed -i '/^ENGINE_DB_/d' \
           /etc/ovirt-engine-dwh/ovirt-engine-dwhd.conf.d/10-setup-database.conf
   
   # sed -i \
        -e 's;^\(OVESETUP_ENGINE_CORE/enable=bool\):True;\1:False;' \
        -e '/^OVESETUP_CONFIG\/fqdn/d' \
        /etc/ovirt-engine-setup.conf.d/20-setup-ovirt-post.conf
   ```

2. Run the `engine-setup` command to begin configuration of Data Warehouse on the machine:

   ```
   # engine-setup
   ```

3. Press **Enter** to accept the automatically detected host name, or enter an alternative host name and press **Enter**:

   ```
   Host fully qualified DNS name of this server [autodetected host name]:
   ```

4. Press `Enter` to automatically configure the firewall, or type `No` and press `Enter` to maintain existing settings:

   ```
   Setup can automatically configure the firewall on this system.
   Note: automatic configuration of the firewall may overwrite current settings.
   Do you want Setup to configure the firewall? (Yes, No) [Yes]:
   ```

   If you choose to automatically configure the firewall, and no  firewall managers are active, you are prompted to select your chosen  firewall manager from a list of supported options. Type the name of the  firewall manager and press `Enter`. This applies even in cases where only one option is listed.

5. Enter the fully qualified domain name and password for the Engine. Press **Enter** to accept the default values in each other field:

   ```
   Host fully qualified DNS name of the engine server []: engine-fqdn
   Setup needs to do some actions on the remote engine server. Either automatically, using ssh as root to access it, or you will be prompted to manually perform each such action.
   Please choose one of the following:
   1 - Access remote engine server using ssh as root
   2 - Perform each action manually, use files to copy content around
   (1, 2) [1]:
   ssh port on remote engine server [22]:
   root password on remote engine server engine-fqdn: password
   ```

6. Enter the FQDN and password for the Engine database machine. Press `Enter` to accept the default values in each other field:

   ```
   Engine database host []: manager-db-fqdn
   Engine database port [5432]:
   Engine database secured connection (Yes, No) [No]:
   Engine database name [engine]:
   Engine database user [engine]:
   Engine database password: password
   ```

7. Confirm your installation settings:

   ```
   Please confirm installation settings (OK, Cancel) [OK]:
   ```

The Data Warehouse service is now configured on the remote machine.  Proceed to disable the Data Warehouse service on the Engine machine.

##### Disabling the Data Warehouse Service on the Engine Machine

1. On the Engine machine, restart the Engine:

   ```
   # service ovirt-engine restart
   ```

2. Run the following command to modify the file **/etc/ovirt-engine-setup.conf.d/20-setup-ovirt-post.conf** and set the options to `False`:

   ```
   # sed -i \
        -e 's;^\(OVESETUP_DWH_CORE/enable=bool\):True;\1:False;' \
        -e 's;^\(OVESETUP_DWH_CONFIG/remoteEngineConfigured=bool\):True;\1:False;' \
        /etc/ovirt-engine-setup.conf.d/20-setup-ovirt-post.conf
   
   # sed -i \
        -e 's;^\(OVESETUP_GRAFANA_CORE/enable=bool\):True;\1:False;' \
        /etc/ovirt-engine-setup.conf.d/20-setup-ovirt-post.conf
   ```

3. Disable the Data Warehouse service:

   ```
   # systemctl disable ovirt-engine-dwhd.service
   ```

4. Remove the Data Warehouse files:

   ```
   # rm -f /etc/ovirt-engine-dwh/ovirt-engine-dwhd.conf.d/* .conf /var/lib/ovirt-engine-dwh/backups/*
   ```

The Data Warehouse service is now hosted on a separate machine from the Engine.

## Appendix D: Configuring a Host for PCI Passthrough

|      | This is one in a series of topics that show how to set up and configure SR-IOV on oVirt. For more information, see [Setting Up and Configuring SR-IOV](https://ovirt.org/documentation/administration_guide/index#setting-up-and-configuring-sr-iov) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Enabling PCI passthrough allows a virtual machine to use a host  device as if the device were directly attached to the virtual machine.  To enable the PCI passthrough function, you must enable virtualization  extensions and the IOMMU function. The following procedure requires you  to reboot the host. If the host is attached to the Engine already,  ensure you place the host into maintenance mode first.

Prerequisites

- Ensure that the host hardware meets the requirements for PCI device passthrough and assignment. See [PCI Device Requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#PCI_Device_Requirements_RHV_planning) for more information.

Configuring a Host for PCI Passthrough

1. Enable the virtualization extension and IOMMU extension in the BIOS. See [Enabling Intel VT-x and AMD-V virtualization hardware extensions in BIOS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Virtualization_Deployment_and_Administration_Guide/sect-Troubleshooting-Enabling_Intel_VT_x_and_AMD_V_virtualization_hardware_extensions_in_BIOS.html) in the *Enterprise Linux Virtualization Deployment and Administration Guide* for more information.
2. Enable the IOMMU flag in the kernel by selecting the **Hostdev Passthrough & SR-IOV** check box when adding the host to the Engine or by editing the **grub** configuration file manually.
   - To enable the IOMMU flag from the Administration Portal, see [Adding Standard Hosts to the oVirt Engine](https://ovirt.org/documentation/administration_guide#Adding_standard_hosts_to_the_Manager) and [Kernel Settings Explained](https://ovirt.org/documentation/administration_guide#Kernel_Settings_Explained).
   - To edit the **grub** configuration file manually, see  [Enabling IOMMU Manually](https://www.ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index.html#Enabling_IOMMU_Manually).
3. For GPU passthrough, you need to run additional configuration steps on both the host and the guest system. See [GPU device passthrough: Assigning a host GPU to a single virtual machine](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/setting_up_an_nvidia_gpu_for_a_virtual_machine_in_red_hat_virtualization/index#proc_nvidia_gpu_passthrough_nvidia_gpu_passthrough) in *Setting up an NVIDIA GPU for a virtual machine in Red Hat Virtualization* for more information.

Enabling IOMMU Manually

1. Enable IOMMU by editing the grub configuration file.

   |      | If you are using IBM POWER8 hardware, skip this step as IOMMU is enabled by default. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   - For Intel, boot the machine, and append `intel_iommu=on` to the end of the `GRUB_CMDLINE_LINUX` line in the **grub** configuration file.

     ```
     # vi /etc/default/grub
     ...
     GRUB_CMDLINE_LINUX="nofb splash=quiet console=tty0 ... intel_iommu=on
     ...
     ```

   - For AMD, boot the machine, and append `amd_iommu=on` to the end of the `GRUB_CMDLINE_LINUX` line in the **grub** configuration file.

     ```
     # vi /etc/default/grub
     …
     GRUB_CMDLINE_LINUX="nofb splash=quiet console=tty0 … amd_iommu=on
     …
     ```

     |      | If `intel_iommu=on` or an AMD IOMMU is detected, you can try adding `iommu=pt`. The `pt` option only enables IOMMU for devices used in passthrough and provides  better host performance. However, the option might not be supported on  all hardware. Revert to the previous option if the `pt` option doesn’t work for your host.  If the passthrough fails because the hardware does not support interrupt remapping, you can consider enabling the `allow_unsafe_interrupts` option if the virtual machines are trusted. The `allow_unsafe_interrupts` is not enabled by default because enabling it potentially exposes the  host to MSI attacks from virtual machines. To enable the option:  `# vi /etc/modprobe.d options vfio_iommu_type1 allow_unsafe_interrupts=1` |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

2. Refresh the **grub.cfg** file and reboot the host for these changes to take effect:

   ```
   # grub2-mkconfig -o /boot/grub2/grub.cfg
   ```

   ```
   # reboot
   ```

## Appendix E: Preventing kernel modules from loading automatically

You can prevent a kernel module from being loaded automatically,  whether the module is loaded directly, loaded as a dependency from  another module, or during the boot process.

Procedure

1. The module name must be added to a configuration file for the `modprobe` utility.  This file must reside in the configuration directory `/etc/modprobe.d`.

   For more information on this configuration directory, see the man page `modprobe.d`.

2. Ensure the module is not configured to get loaded in any of the following:

   - `/etc/modprobe.conf`
   - `/etc/modprobe.d/*`
   - `/etc/rc.modules`
   - `/etc/sysconfig/modules/*`

   ```
   # modprobe --showconfig <_configuration_file_name_>
   ```

3. If the module appears in the output, ensure it is ignored and not loaded:

   ```
   # modprobe --ignore-install <_module_name_>
   ```

4. Unload the module from the running system, if it is loaded:

   ```
   # modprobe -r <_module_name_>
   ```

5. Prevent the module from being loaded directly by adding the `blacklist` line to a configuration file specific to the system - for example `/etc/modprobe.d/local-dontload.conf`:

   ```
   # echo "blacklist <_module_name_> >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | This step does not prevent a module from loading if it is a required or an optional dependency of another module. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Prevent optional modules from being loading on demand:

   ```
   # echo "install <_module_name_>/bin/false" >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | If the excluded module is required for other hardware, excluding it might cause unexpected side effects. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. Make a backup copy of your `initramfs`:

   ```
   # cp /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.$(date +%m-%d-%H%M%S).bak
   ```

8. If the kernel module is part of the `initramfs`, rebuild your initial `ramdisk` image, omitting the module:

   ```
   # dracut --omit-drivers <_module_name_> -f
   ```

9. Get the current kernel command line parameters:

   ```
   # grub2-editenv - list | grep kernelopts
   ```

10. Append `<_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>` to the generated output:

    ```
    # grub2-editenv - set kernelopts="<> <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

    For example:

    ```
    # grub2-editenv - set kernelopts="root=/dev/mapper/rhel_example-root ro crashkernel=auto resume=/dev/mapper/rhel_example-swap rd.lvm.lv=rhel_example/root rd.lvm.lv=rhel_example/swap <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

11. Make a backup copy of the `kdump initramfs`:

    ```
    # cp /boot/initramfs-$(uname -r)kdump.img /boot/initramfs-$(uname -r)kdump.img.$(date +%m-%d-%H%M%S).bak
    ```

12. Append `rd.driver.blacklist=<_module_name_>` to the `KDUMP_COMMANDLINE_APPEND` setting in `/etc/sysconfig/kdump` to omit it from the `kdump initramfs`:

    ```
    # sed -i '/^KDUMP_COMMANDLINE_APPEND=/s/"$/ rd.driver.blacklist=module_name"/' /etc/sysconfig/kdump
    ```

13. Restart the `kdump` service to pick up the changes to the `kdump initrd`:

    ```
      # kdumpctl restart
    ```

14. Rebuild the `kdump` initial `ramdisk` image:

    ```
      # mkdumprd -f /boot/initramfs-$(uname -r)kdump.img
    ```

15. Reboot the system.

### Removing a module temporarily

You can remove a module temporarily.

Procedure

1. Run `modprobe` to remove any currently-loaded module:

   ```
   # modprobe -r <module name>
   ```

2. If the module cannot be unloaded, a process or another module might  still be using the module. If so, terminate the process and run the `modpole` command written above another time to unload the module.

## Appendix F: Legal notice

Certain portions of this text first appeared in [Red Hat Virtualization 4.4 Installing Red Hat Virtualization as a self-hosted engine using the command line](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/installing_red_hat_virtualization_as_a_self-hosted_engine_using_the_command_line/index). Copyright © 2022 Red Hat, Inc. Licensed under a [Creative Commons Attribution-ShareAlike 4.0 Unported License](https://creativecommons.org/licenses/by-sa/4.0/).

## a standalone Engine with local databases

独立引擎安装是手动和可定制的。必须安装 Enterprise Linux 机器，然后运行配置脚本（`engine-setup`）并提供有关如何配置 oVirt 引擎的信息。在引擎运行后添加主机和存储。虚拟机高可用性至少需要两台主机。

在本地数据库环境中，引擎配置脚本可以自动创建引擎数据库和数据仓库数据库。或者，您可以在运行 `engine-setup` 之前在 Engine计算机上手动创建这些数据库。

### 架构

oVirt 引擎在物理服务器上运行，或在单独的虚拟化环境中托管的虚拟机上运行。独立引擎更易于部署和管理，但需要额外的物理服务器。只有在使用 Red Hat 的高可用性附加组件等产品进行外部管理时，引擎才具有高可用性。

独立引擎环境的最低设置包括：

- 一台 oVirt Engine 机器。引擎通常部署在物理服务器上。但是，它也可以部署在虚拟机上，只要该虚拟机托管在单独的环境中。引擎必须在 Enterprise Linux 8 上运行。
- 虚拟机高可用性至少需要两台主机。可以使用 Enterprise Linux 主机或 oVirt 节点。VDSM（host agent）在所有主机上运行，以便于与 oVirt Engine 进行通信。
- 一个存储服务，可以在本地或远程服务器上托管，具体取决于所使用的存储类型。所有主机都必须可以访问存储服务。

![](../../../../Image/r/RHV_STANDARD_ARCHITECTURE1.png)

### 安装预览

安装带有本地数据库的独立引擎环境涉及以下步骤：

1. 安装并配置o Virt引擎：
   1. 为引擎安装 Enterprise Linux 机器。
   2. 启用 oVirt Engine 存储库。
   3. 使用 `engine-setup` 安装和配置 oVirt 引擎。
   4. 连接到管理门户以添加主机和存储域。
2. 安装要运行虚拟机的主机：
   1. 使用任一主机类型，或同时使用两者：
      - oVirt Node
      - Enterprise Linux
   2. 将主机添加到引擎。
3. 准备用于存储域的存储。您可以使用以下存储类型之一：
   - NFS
   - iSCSI
   - Fibre Channel (FCP）
   - POSIX-compliant file system
   - Local storage
   - Gluster Storage
4. 将存储域添加到引擎。

## 2. Requirements

### 2.1. oVirt Engine Requirements

#### 2.1.1. Hardware Requirements

The minimum and recommended hardware requirements outlined here are  based on a typical small to medium-sized installation. The exact  requirements vary between deployments based on sizing and load.

The oVirt Engine runs on Enterprise Linux operating systems like [CentOS Linux](https://www.centos.org/) or [Red Hat Enterprise Linux](https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux).

| Resource          | Minimum                                                      | Recommended                                                  |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| CPU               | A dual core x86_64 CPU.                                      | A quad core x86_64 CPU or multiple dual core x86_64 CPUs.    |
| Memory            | 4 GB of available system RAM if Data Warehouse is not installed and if memory is not being consumed by existing processes. | 16 GB of system RAM.                                         |
| Hard Disk         | 25 GB of locally accessible, writable disk space.            | 50 GB of locally accessible, writable disk space. You can use the [RHV Engine History Database Size Calculator](https://access.redhat.com/labs/rhevmhdsc/) to calculate the appropriate disk space for the Engine history database size. |
| Network Interface | 1 Network Interface Card (NIC) with bandwidth of at least 1 Gbps. | 1 Network Interface Card (NIC) with bandwidth of at least 1 Gbps. |

#### 2.1.2. Browser Requirements

The following browser versions and operating systems can be used to access the Administration Portal and the VM Portal.

Browser testing is divided into tiers:

- Tier 1: Browser and operating system combinations that are fully tested.
- Tier 2: Browser and operating system combinations that are partially tested, and are likely to work.
- Tier 3: Browser and operating system combinations that are not tested, but may work.

| Support Tier | Operating System Family | Browser                                                      |
| ------------ | ----------------------- | ------------------------------------------------------------ |
| Tier 1       | Enterprise Linux        | Mozilla Firefox Extended Support Release (ESR) version       |
|              | Any                     | Most recent version of Google Chrome, Mozilla Firefox, or Microsoft Edge |
| Tier 2       |                         |                                                              |
| Tier 3       | Any                     | Earlier versions of Google Chrome or Mozilla Firefox         |
|              | Any                     | Other browsers                                               |

#### 2.1.3. Client Requirements

Virtual machine consoles can only be accessed using supported Remote Viewer (`virt-viewer`) clients on Enterprise Linux and Windows. To install `virt-viewer`, see [Installing Supporting Components on Client Machines](https://ovirt.org/documentation/virtual_machine_management_guide/index#sect-installing_supporting_components) in the *Virtual Machine Management Guide*. Installing `virt-viewer` requires Administrator privileges.

You can access virtual machine consoles using the SPICE, VNC, or RDP  (Windows only) protocols. You can install the QXLDOD graphical driver in the guest operating system to improve the functionality of SPICE. SPICE currently supports a maximum resolution of 2560x1600 pixels.

Client Operating System SPICE Support

Supported QXLDOD drivers are available on Enterprise Linux 7.2 and later, and Windows 10.

|      | SPICE may work with Windows 8 or 8.1 using QXLDOD drivers, but it is neither certified nor tested. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 2.1.4. Operating System Requirements

The oVirt Engine must be installed on a base installation of Enterprise Linux 8.6.

Do not install any additional packages after the base installation,  as they may cause dependency issues when attempting to install the  packages required by the Engine.

Do not enable additional repositories other than those required for the Engine installation.

### 2.2. Host Requirements

To use Enterprise Linux 9 on virtualization hosts, the UEFI Secure Boot option must be disabled due to [Bug 2081648 - dmidecode module fails to decode DMI data](https://bugzilla.redhat.com/show_bug.cgi?id=2081648).

#### 2.2.1. CPU Requirements

All CPUs must have support for the Intel® 64 or AMD64 CPU extensions, and the AMD-V™ or Intel VT® hardware virtualization extensions enabled. Support for the No eXecute flag (NX) is also required.

The following CPU models are supported:

- AMD
  - Opteron G4
  - Opteron G5
  - EPYC
- Intel
  - Nehalem
  - Westmere
  - SandyBridge
  - IvyBridge
  - Haswell
  - Broadwell
  - Skylake Client
  - Skylake Server
  - Cascadelake Server
- IBM
  - POWER8
  - POWER9

Enterprise Linux 9 doesn’t support virtualization for ppc64le: [CentOS Virtualization SIG](https://wiki.centos.org/SpecialInterestGroup/Virtualization) is working on re-introducing virtualization support but it’s not ready yet.

The oVirt project also provides packages for the 64-bit ARM architecture (ARM 64) but only as a Technology Preview.

For each CPU model with security updates, the **CPU Type** lists a basic type and a secure type. For example:

- **Intel Cascadelake Server Family**
- **Secure Intel Cascadelake Server Family**

The Secure CPU type contains the latest updates. For details, see BZ#[1731395](https://bugzilla.redhat.com/1731395)

##### Checking if a Processor Supports the Required Flags

You must enable virtualization in the BIOS. Power off and reboot the  host after this change to ensure that the change is applied.

Procedure

1. At the Enterprise Linux or oVirt Node boot screen, press any key and select the **Boot** or **Boot with serial console** entry from the list.

2. Press `Tab` to edit the kernel parameters for the selected option.

3. Ensure there is a space after the last kernel parameter listed, and append the parameter `rescue`.

4. Press `Enter` to boot into rescue mode.

5. At the prompt, determine that your processor has the required extensions and that they are enabled by running this command:

   ```
   # grep -E 'svm|vmx' /proc/cpuinfo | grep nx
   ```

If any output is shown, the processor is hardware virtualization  capable. If no output is shown, your processor may still support  hardware virtualization; in some circumstances manufacturers disable the virtualization extensions in the BIOS. If you believe this to be the  case, consult the system’s BIOS and the motherboard manual provided by  the manufacturer.

#### 2.2.2. Memory Requirements

The minimum required RAM is 2 GB. For cluster levels 4.2 to 4.5, the  maximum supported RAM per VM in oVirt Node is 6 TB. For cluster levels  4.6 to 4.7, the maximum supported RAM per VM in oVirt Node is 16 TB.

However, the amount of RAM required varies depending on guest  operating system requirements, guest application requirements, and guest memory activity and usage. KVM can also overcommit physical RAM for  virtualized guests, allowing you to provision guests with RAM  requirements greater than what is physically present, on the assumption  that the guests are not all working concurrently at peak load. KVM does  this by only allocating RAM for guests as required and shifting  underutilized guests into swap.

#### 2.2.3. Storage Requirements

Hosts require storage to store configuration, logs, kernel dumps, and for use as swap space. Storage can be local or network-based. oVirt  Node (oVirt Node) can boot with one, some, or all of its default  allocations  in network storage. Booting from network storage can result in a freeze if there is a network disconnect. Adding a drop-in  multipath configuration file can help address losses in network  connectivity. If oVirt Node boots from SAN storage and loses  connectivity, the files become read-only until network connectivity  restores. Using network storage might result in a performance downgrade.

The minimum storage requirements of oVirt Node are documented in this section. The storage requirements for Enterprise Linux hosts vary based on the amount of disk space used by their existing configuration but  are expected to be greater than those of oVirt Node.

The minimum storage requirements for host installation are listed  below. However, use the default allocations, which use more storage  space.

- / (root) - 6 GB
- /home - 1 GB
- /tmp - 1 GB
- /boot - 1 GB
- /var - 5 GB
- /var/crash - 10 GB
- /var/log - 8 GB
- /var/log/audit - 2 GB
- /var/tmp - 10 GB
- swap - 1 GB. See [What is the recommended swap size for Red Hat platforms?](https://access.redhat.com/solutions/15244) for details.
- Anaconda reserves 20% of the thin pool size within the volume group  for future metadata expansion. This is to prevent an out-of-the-box  configuration from running out of space under normal usage conditions.  Overprovisioning of thin pools during installation is also not  supported.
- **Minimum Total - 64 GiB**

If you are also installing the Engine Appliance for self-hosted engine installation, `/var/tmp` must be at least 10 GB.

If you plan to use memory overcommitment, add enough swap space to provide virtual memory for all of virtual machines. See [Memory Optimization](https://ovirt.org/documentation/administration_guide/index#Memory_Optimization).

#### 2.2.4. PCI Device Requirements

Hosts must have at least one network interface with a minimum  bandwidth of 1 Gbps. Each host should have two network interfaces, with  one dedicated to supporting network-intensive activities, such as  virtual machine migration. The performance of such operations is limited by the bandwidth available.

For information about how to use PCI Express and conventional PCI devices with Intel Q35-based virtual machines, see [*Using PCI Express and Conventional PCI Devices with the Q35 Virtual Machine*](https://access.redhat.com/articles/3201152).

#### 2.2.5. Device Assignment Requirements

If you plan to implement device assignment and PCI passthrough so  that a virtual machine can use a specific PCIe device from a host,  ensure the following requirements are met:

- CPU must support IOMMU (for example, VT-d or AMD-Vi). IBM POWER8 supports IOMMU by default.
- Firmware must support IOMMU.
- CPU root ports used must support ACS or ACS-equivalent capability.
- PCIe devices must support ACS or ACS-equivalent capability.
- All PCIe switches and bridges between the PCIe device and the root  port should support ACS. For example, if a switch does not support ACS,  all devices behind that switch share the same IOMMU group, and can only  be assigned to the same virtual machine.
- For GPU support, Enterprise Linux 8 supports PCI device assignment of PCIe-based NVIDIA K-Series Quadro (model 2000 series or higher), GRID,  and Tesla as non-VGA graphics devices. Currently up to two GPUs may be  attached to a virtual machine in addition to one of the standard,  emulated VGA interfaces. The emulated VGA is used for pre-boot and  installation and the NVIDIA GPU takes over when the NVIDIA graphics  drivers are loaded. Note that the NVIDIA Quadro 2000 is not supported,  nor is the Quadro K420 card.

Check vendor specification and datasheets to confirm that your hardware meets these requirements. The `lspci -v` command can be used to print information for PCI devices already installed on a system.

#### 2.2.6. vGPU Requirements

A host must meet the following requirements in order for virtual machines on that host to use a vGPU:

- vGPU-compatible GPU
- GPU-enabled host kernel
- Installed GPU with correct drivers
- Select a vGPU type and the number of instances that you would like to use with this virtual machine using the **Manage vGPU** dialog in the **Administration Portal** **Host Devices** tab of the virtual machine.
- vGPU-capable drivers installed on each host in the cluster
- vGPU-supported virtual machine operating system with vGPU drivers installed

### 2.3. Networking requirements

#### 2.3.1. General requirements

oVirt requires IPv6 to remain enabled on the physical or virtual machine running the Engine. [Do not disable IPv6](https://access.redhat.com/solutions/8709) on the Engine machine, even if your systems do not use it.

#### 2.3.2. Firewall Requirements for DNS, NTP, and IPMI Fencing

The firewall requirements for all of the following topics are special cases that require individual consideration.

DNS and NTP

oVirt does not create a DNS or NTP server, so the firewall does not need to have open ports for incoming traffic.

By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, define  exceptions for requests that are sent to DNS and NTP servers.

|      | The oVirt Engine and all hosts (oVirt Node and Enterprise Linux host) must have a fully qualified domain name and full, perfectly-aligned  forward and reverse name resolution.  Running a DNS service as a virtual machine in the oVirt environment  is not supported. All DNS services the oVirt environment uses must be  hosted outside of the environment.  Use DNS instead of the `/etc/hosts` file for name resolution. Using a hosts file typically requires more work and has a greater chance for errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

IPMI and Other Fencing Mechanisms (optional)

For IPMI (Intelligent Platform Management Interface) and other  fencing mechanisms, the firewall does not need to have open ports for  incoming traffic.

By default, Enterprise Linux allows outbound IPMI traffic to ports on any destination address. If you disable outgoing traffic, make  exceptions for requests being sent to your IPMI or fencing servers.

Each oVirt Node and Enterprise Linux host in the cluster must be able to connect to the fencing devices of all other hosts in the cluster. If the cluster hosts are experiencing an error (network error, storage  error…) and cannot function as hosts, they must be able to connect to  other hosts in the data center.

The specific port number depends on the type of the fence agent you are using and how it is configured.

The firewall requirement tables in the following sections do not represent this option.

#### 2.3.3. oVirt Engine Firewall Requirements

The oVirt Engine requires that a number of ports be opened to allow network traffic through the system’s firewall.

The `engine-setup` script can configure the firewall automatically.

The firewall configuration documented here assumes a default configuration.

| ID   | Port(s) | Protocol | Source                                                       | Destination                                    | Purpose                                                      | Encrypted by default                               |
| ---- | ------- | -------- | ------------------------------------------------------------ | ---------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------- |
| M1   | -       | ICMP     | oVirt Nodes Enterprise Linux hosts                           | oVirt Engine                                   | Optional. May help in diagnosis.                             | No                                                 |
| M2   | 22      | TCP      | System(s) used for maintenance of the Engine including backend configuration, and software upgrades. | oVirt Engine                                   | Secure Shell (SSH) access. Optional.                         | Yes                                                |
| M3   | 2222    | TCP      | Clients accessing virtual machine serial consoles.           | oVirt Engine                                   | Secure Shell (SSH) access to enable connection to virtual machine serial consoles. | Yes                                                |
| M4   | 80, 443 | TCP      | Administration Portal clients VM Portal clients oVirt Nodes Enterprise Linux hosts REST API clients | oVirt Engine                                   | Provides HTTP (port 80, not encrypted) and HTTPS (port 443, encrypted) access to the Engine. HTTP redirects connections to HTTPS. | Yes                                                |
| M5   | 6100    | TCP      | Administration Portal clients VM Portal clients              | oVirt Engine                                   | Provides websocket proxy access for a web-based console client, `noVNC`, when the websocket proxy is running on the Engine. | No                                                 |
| M6   | 7410    | UDP      | oVirt Nodes Enterprise Linux hosts                           | oVirt Engine                                   | If Kdump is enabled on the hosts, open this port for the fence_kdump listener on the Engine. See [fence_kdump Advanced Configuration](https://ovirt.org/documentation/administration_guide/index#sect-fence_kdump_Advanced_Configuration). `fence_kdump` doesn’t provide a way to encrypt the connection. However, you can  manually configure this port to block access from hosts that are not  eligible. | No                                                 |
| M7   | 54323   | TCP      | Administration Portal clients                                | oVirt Engine (`ovirt-imageio` service)         | Required for communication with the `ovirt-imageo` service.  | Yes                                                |
| M8   | 6642    | TCP      | oVirt Nodes Enterprise Linux hosts                           | Open Virtual Network (OVN) southbound database | Connect to Open Virtual Network (OVN) database               | Yes                                                |
| M9   | 9696    | TCP      | Clients of external network provider for OVN                 | External network provider for OVN              | OpenStack Networking API                                     | Yes, with configuration generated by engine-setup. |
| M10  | 35357   | TCP      | Clients of external network provider for OVN                 | External network provider for OVN              | OpenStack Identity API                                       | Yes, with configuration generated by engine-setup. |
| M11  | 53      | TCP, UDP | oVirt Engine                                                 | DNS Server                                     | DNS lookup requests from ports above 1023 to port 53, and responses. Open by default. | No                                                 |
| M12  | 123     | UDP      | oVirt Engine                                                 | NTP Server                                     | NTP requests from ports above 1023 to port 123, and responses.  Open by default. | No                                                 |

|      | A port for the OVN northbound database (6641) is not listed because,  in the default configuration, the only client for the OVN northbound  database (6641) is `ovirt-provider-ovn`. Because they both run on the same host, their communication is not visible to the network.  By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, make  exceptions for the Engine to send requests to DNS and NTP servers. Other nodes may also require DNS and NTP. In that case, consult the  requirements for those nodes and configure the firewall accordingly. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 2.3.4. Host Firewall Requirements

Enterprise Linux hosts and oVirt Nodes (oVirt Node) require a number  of ports to be opened to allow network traffic through the system’s  firewall. The firewall rules are automatically configured by default  when adding a new host to the Engine, overwriting any pre-existing  firewall configuration.

To disable automatic firewall configuration when adding a new host, clear the **Automatically configure host firewall** check box under **Advanced Parameters**.

| ID   | Port(s)       | Protocol | Source                                          | Destination                        | Purpose                                                      | Encrypted by default                                         |
| ---- | ------------- | -------- | ----------------------------------------------- | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| H1   | 22            | TCP      | oVirt Engine                                    | oVirt Nodes Enterprise Linux hosts | Secure Shell (SSH) access. Optional.                         | Yes                                                          |
| H2   | 2223          | TCP      | oVirt Engine                                    | oVirt Nodes Enterprise Linux hosts | Secure Shell (SSH) access to enable connection to virtual machine serial consoles. | Yes                                                          |
| H3   | 161           | UDP      | oVirt Nodes Enterprise Linux hosts              | oVirt Engine                       | Simple network management protocol (SNMP). Only required if you want Simple  Network Management Protocol traps sent from the host to one or more  external SNMP managers. Optional. | No                                                           |
| H4   | 111           | TCP      | NFS storage server                              | oVirt Nodes Enterprise Linux hosts | NFS connections. Optional.                                   | No                                                           |
| H5   | 5900 - 6923   | TCP      | Administration Portal clients VM Portal clients | oVirt Nodes Enterprise Linux hosts | Remote guest console access via VNC and SPICE. These ports must be open to facilitate client access to virtual machines. | Yes (optional)                                               |
| H6   | 5989          | TCP, UDP | Common Information Model Object Manager (CIMOM) | oVirt Nodes Enterprise Linux hosts | Used by Common Information Model Object Managers (CIMOM) to monitor virtual  machines running on the host. Only required if you want to use a CIMOM  to monitor the virtual machines in your virtualization environment. Optional. | No                                                           |
| H7   | 9090          | TCP      | oVirt Engine Client machines                    | oVirt Nodes Enterprise Linux hosts | Required to access the Cockpit web interface, if installed.  | Yes                                                          |
| H8   | 16514         | TCP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Virtual machine migration using **libvirt**.                 | Yes                                                          |
| H9   | 49152 - 49215 | TCP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Virtual machine migration and fencing using VDSM. These ports must be open to  facilitate both automated and manual migration of virtual machines. | Yes. Depending on agent for fencing, migration is done through libvirt. |
| H10  | 54321         | TCP      | oVirt Engine oVirt Nodes Enterprise Linux hosts | oVirt Nodes Enterprise Linux hosts | VDSM communications with the Engine and other virtualization hosts. | Yes                                                          |
| H11  | 54322         | TCP      | oVirt Engine `ovirt-imageio` service            | oVirt Nodes Enterprise Linux hosts | Required for communication with the `ovirt-imageo` service.  | Yes                                                          |
| H12  | 6081          | UDP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Required, when Open Virtual Network (OVN) is used as a network provider, to allow OVN to create tunnels between hosts. | No                                                           |
| H13  | 53            | TCP, UDP | oVirt Nodes Enterprise Linux hosts              | DNS Server                         | DNS lookup requests from ports above 1023 to port 53, and responses. This port is required and open by default. | No                                                           |
| H14  | 123           | UDP      | oVirt Nodes Enterprise Linux hosts              | NTP Server                         | NTP requests from ports above 1023 to port 123, and responses. This port is required and open by default. |                                                              |
| H15  | 4500          | TCP, UDP | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |
| H16  | 500           | UDP      | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |
| H17  | -             | AH, ESP  | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |

|      | By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, make  exceptions for the oVirt Nodes  Enterprise Linux hosts to send requests to DNS and NTP servers. Other nodes may also require DNS and NTP. In that case, consult the  requirements for those nodes and configure the firewall accordingly. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 2.3.5. Database Server Firewall Requirements

oVirt supports the use of a remote database server for the Engine database (`engine`) and the Data Warehouse database (`ovirt-engine-history`). If you plan to use a remote database server, it must allow connections  from the Engine and the Data Warehouse service (which can be separate  from the Engine).

Similarly, if you plan to access a local or remote Data Warehouse  database from an external system, the database must allow connections  from that system.

|      | Accessing the Engine database from external systems is not supported. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| ID   | Port(s) | Protocol | Source                              | Destination                                                  | Purpose                                           | Encrypted by default                                         |
| ---- | ------- | -------- | ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ |
| D1   | 5432    | TCP, UDP | oVirt Engine Data Warehouse service | Engine (`engine`) database server Data Warehouse (`ovirt-engine-history`) database server | Default port for PostgreSQL database connections. | [No, but can be enabled](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH). |
| D2   | 5432    | TCP, UDP | External systems                    | Data Warehouse (`ovirt-engine-history`) database server      | Default port for PostgreSQL database connections. | Disabled by default. [No, but can be enabled](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH). |

#### 2.3.6. Maximum Transmission Unit Requirements

The recommended Maximum Transmission Units (MTU) setting for Hosts  during deployment is 1500. It is possible to update this setting after  the environment is set up to a different MTU. For more information on  changing the MTU setting, see [How to change the Hosted Engine VM network MTU](https://access.redhat.com/solutions/4129641).

### 安装 oVirt Engine

#### 准备 oVirt Engine Machine

oVirt Engine 必须在 Enterprise Linux 8 上运行。

默认情况下，oVirt Engine 的配置脚本、 `engine-setup` 会在 Engine 机器上自动创建和配置 Engine 数据库和数据仓库数据库。要手动设置数据库或两者，请参阅 [Preparing a Local Manually-Configured PostgreSQL Database](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_a_Local_Manually-Configured_PostgreSQL_Database_SM_localDB_deploy) 。

#### 启用 oVirt Engine Repositories

```bash
# oVirt 4.5
dnf install -y centos-release-ovirt45
# oVirt 4.4
dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

可以通过运行 `dnf repolist` 检查当前启用的存储库。

1. 启用 `javapackages-tools` 模块。

   ```bash
   dnf module -y enable javapackages-tools
   ```

2. 启用 `pki-deps` 模块。

   ```bash
   dnf module -y enable pki-deps
   ```

3. 启用 version 12 of the `postgresql` 模块。

   ```bash
   dnf module -y enable postgresql:12
   ```

4. 启用 version 2.3 of the `mod_auth_openidc` 模块。

   ```basic
   dnf module -y enable mod_auth_openidc:2.3
   ```

5. 启用 version 14 of the `nodejs` 模块。

   ```bash
   dnf module -y enable nodejs:14
   ```

6. 同步已安装的软件包以将其更新到最新的可用版本。

   ```bash
   dnf distro-sync --nobest
   ```


#### 安装和配置 oVirt Engine

安装 oVirtEngine 的软件包和依赖项，并使用 `engine-setup` 命令对其进行配置。脚本会问您一系列问题，在您为所有问题提供所需的值后，应用该配置并启动 `ovirt-engine` 服务。

>  **Note:**
>
> `engine-setup` 命令引导您完成几个不同的配置阶段，每个阶段都包含需要用户输入的几个步骤。方括号中提供了建议的配置默认值；如果建议的值对于给定步骤是可接受的，请按 Enter 键接受该值。
>
> 您可以运行 `engine-setup --accept-defaults` 以自动接受所有具有默认答案的问题。仅当您熟悉 `engine-setup` 时，才应谨慎使用此选项。

1. 确保所有程序包都是最新的：

   ```bash
   dnf upgrade --nobest
   ```

   如果更新了任何与内核相关的软件包，请重新启动计算机。
   
2. 安装 `ovirt-engine` 及其依赖的软件包：

   ```bash
   dnf install ovirt-engine
   ```

3. 执行 `engine-setup` 命令开始配置 oVirt Engine ：

   ```bash
   engine-setup
   ```

4. 可选：键入 Yes 并按 Enter 键在此计算机上设置 Cinderlib 集成：

   ```bash
   Set up Cinderlib integration
   (Currently in tech preview)
   (Yes, No) [No]:
   ```

   > **Note：**
   >
   > Cinderlib 仅是技术预览功能。Red Hat 生产服务级别协议（SLA）不支持技术预览功能，功能可能不完整，Red  Hat 不建议将其用于生产。这些功能提供了对即将推出的产品功能的早期访问，使客户能够在开发过程中测试功能并提供反馈。有关 Red Hat  Technology Preview 功能支持范围的更多信息，请参阅 [Red Hat Technology Preview Features Support Scope](https://access.redhat.com/support/offerings/techpreview/) 。

5. 按 `Enter` 键在此计算机上配置引擎：

   ```bash
   Configure Engine on this host (Yes, No) [Yes]:
   ```

6. Optional: Install Open Virtual Network (OVN). Selecting `Yes` installs an OVN server on the Engine machine and adds it to oVirt as an external network provider. This action also configures the Default  cluster to use OVN as its default network provider.

   可选：安装开放虚拟网络（OVN）。选择是将OVN服务器安装在Engine机器上，并将其作为外部网络提供商添加到o Virt。此操作还将默认集群配置为使用OVN作为其默认网络提供商。

   ```bash
   Configuring ovirt-provider-ovn also sets the Default cluster’s default network provider to ovirt-provider-ovn.
   Non-Default clusters may be configured with an OVN after installation.
   Configure ovirt-provider-ovn (Yes, No) [Yes]:
   ```

7. Optional: Allow `engine-setup` to configure a WebSocket Proxy server for allowing users to connect to virtual machines through the `noVNC` console:

   可选：允许引擎设置配置Web套接字代理服务器，以允许用户通过无VNC控制台连接到虚拟机：

   ```bash
   Configure WebSocket Proxy on this machine? (Yes, No) [Yes]:
   ```

8. 选择是否在此计算机上配置数据仓库。

   ```bash
   Please note: Data Warehouse is required for the engine. If you choose to not configure it on this host, you have to configure it on a remote host, and then configure the engine on this host so that it can access the database of the remote Data Warehouse host.
   Configure Data Warehouse on this host (Yes, No) [Yes]:
   ```

   > Note:
   >
   > oVirt only supports installing the Data Warehouse database, the Data  Warehouse service, and Grafana all on the same machine as each other.
   >
   > oVirt 只支持在同一台机器上安装数据仓库数据库、数据仓库服务和Grafana。

9. 按下 `Enter` 配置 Grafana :

   ```bash
   Configure Grafana on this host (Yes, No) [Yes]:
   ```

10. Optional: Allow access to a virtual machine’s serial console from the command line.

    可选：允许从命令行访问虚拟机的串行控制台。

    ```bash
    Configure VM Console Proxy on this host (Yes, No) [Yes]:
    ```

    Additional configuration is required on the client machine to use this feature. See [Opening a Serial Console to a Virtual Machine](https://ovirt.org/documentation/virtual_machine_management_guide/index#Opening_a_Serial_Console_to_a_Virtual_Machine) in the *Virtual Machine Management Guide*.要使用此功能，需要在客户端计算机上进行其他配置。请参阅《虚拟机管理指南》中的“向虚拟机打开串行控制台”。

11. Press `Enter` to accept the automatically detected host name, or enter an alternative host name and press `Enter`. Note that the automatically detected host name may be incorrect if you are using virtual hosts.按Enter键接受自动检测到的主机名，或输入其他主机名并按Enter键。请注意，如果您使用的是虚拟主机，则自动检测到的主机名可能不正确。

    ```
    Host fully qualified DNS name of this server [autodetected host name]:
    ```

12. The `engine-setup` command checks your firewall  configuration and offers to open the ports used by the Engine for  external communication, such as ports 80 and 443. If you do not allow `engine-setup` to modify your firewall configuration, you must manually open the ports used by the Engine. `firewalld` is configured as the firewall manager.引擎设置命令检查防火墙配置，并提供打开引擎用于外部通信的端口，例如端口80和443。如果不允许引擎设置修改防火墙配置，则必须手动打开引擎使用的端口。firewalld被配置为防火墙管理器。

    ```
    Setup can automatically configure the firewall on this system.
    Note: automatic configuration of the firewall may overwrite current settings.
    Do you want Setup to configure the firewall? (Yes, No) [Yes]:
    ```

    If you choose to automatically configure the firewall, and no  firewall managers are active, you are prompted to select your chosen  firewall manager from a list of supported options. Type the name of the  firewall manager and press `Enter`. This applies even in cases where only one option is listed.

    如果您选择自动配置防火墙，并且没有防火墙管理器处于活动状态，系统会提示您从支持的选项列表中选择所选的防火墙管理器。键入防火墙管理器的名称，然后按Enter键。即使在仅列出一个选项的情况下，这也适用。

13. Specify whether to configure the Data Warehouse database on this machine, or on another machine:指定是在此计算机上还是在另一台计算机上配置数据仓库数据库：

    ```
    Where is the DWH database located? (Local, Remote) [Local]:
    ```

    - If you select `Local`, the `engine-setup`  script can configure your database automatically (including adding a  user and a database), or it can connect to a preconfigured local  database:如果选择“本地”，引擎设置脚本可以自动配置数据库（包括添加用户和数据库），也可以连接到预配置的本地数据库：

      ```
      Setup can configure the local postgresql server automatically for the DWH to run. This may conflict with existing applications.
      Would you like Setup to automatically configure postgresql and create DWH database, or prefer to perform that manually? (Automatic, Manual) [Automatic]:
      ```

      - If you select `Automatic` by pressing `Enter`, no further action is required here.

      - If you select `Manual`, input the following values for the manually configured local database:

      - 如果通过按Enter键选择“自动”，则此处不需要进一步操作。

        如果选择手动，请为手动配置的本地数据库输入以下值：
        
      - 

        ```
        DWH database secured connection (Yes, No) [No]:
        DWH database name [ovirt_engine_history]:
        DWH database user [ovirt_engine_history]:
        DWH database password:
        ```

        |      | `engine-setup` requests these values after the Engine database is configured in the next step. |
        | ---- | ------------------------------------------------------------ |
        |      |                                                              |

    - If you select `Remote` (for example, if you are installing the Data Warehouse service on the Engine machine, but have configured a remote Data Warehouse database), input the following values for the  remote database server:

      ```
      DWH database host [localhost]:
      DWH database port [5432]:
      DWH database secured connection (Yes, No) [No]:
      DWH database name [ovirt_engine_history]:
      DWH database user [ovirt_engine_history]:
      DWH database password:
      ```

      |      | `engine-setup` requests these values after the Engine database is configured in the next step. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

    - If you select `Remote`, you are prompted to enter the username and password for the Grafana database user:

      ```
      Grafana database user [ovirt_engine_history_grafana]:
      Grafana database password:
      ```

14. Specify whether to configure the Engine database on this machine, or on another machine:

    ```
    Where is the Engine database located? (Local, Remote) [Local]:
    ```

    - If you select `Local`, the `engine-setup`  command can configure your database automatically (including adding a  user and a database), or it can connect to a preconfigured local  database:

      ```
      Setup can configure the local postgresql server automatically for the engine to run. This may conflict with existing applications.
      Would you like Setup to automatically configure postgresql and create Engine database, or prefer to perform that manually? (Automatic, Manual) [Automatic]:
      ```

      - If you select `Automatic` by pressing `Enter`, no further action is required here.

      - If you select `Manual`, input the following values for the manually configured local database:

        ```
        Engine database secured connection (Yes, No) [No]:
        Engine database name [engine]:
        Engine database user [engine]:
        Engine database password:
        ```

15. Set a password for the automatically created administrative user of the oVirt Engine:

    ```
    Engine admin password:
    Confirm engine admin password:
    ```

16. Select **Gluster**, **Virt**, or **Both**:

    ```
    Application mode (Both, Virt, Gluster) [Both]:
    ```

    - **Both** - offers the greatest flexibility. In most cases, select **Both**.
    - **Virt** - allows you to run virtual machines in the environment.
    - **Gluster** - only allows you to manage GlusterFS from the Administration Portal.

17. If you installed the OVN provider, you can choose to use the default credentials, or specify an alternative.

    ```
    Use default credentials (admin@internal) for ovirt-provider-ovn (Yes, No) [Yes]:
    oVirt OVN provider user[admin@internal]:
    oVirt OVN provider password:
    ```

18. Set the default value for the `wipe_after_delete` flag, which wipes the blocks of a virtual disk when the disk is deleted.

    ```
    Default SAN wipe after delete (Yes, No) [No]:
    ```

19. The Engine uses certificates to communicate securely with its hosts.  This certificate can also optionally be used to secure HTTPS  communications with the Engine. Provide the organization name for the  certificate:

    ```
    Organization name for certificate [autodetected domain-based name]:
    ```

20. Optionally allow `engine-setup` to make the landing page of the Engine the default page presented by the Apache web server:

    ```
    Setup can configure the default page of the web server to present the application home page. This may conflict with existing applications.
    Do you wish to set the application as the default web page of the server? (Yes, No) [Yes]:
    ```

21. By default, external SSL (HTTPS) communication with the Engine is  secured with the self-signed certificate created earlier in the  configuration to securely communicate with hosts. Alternatively, choose  another certificate for external HTTPS connections; this does not affect how the Engine communicates with hosts:

    ```
    Setup can configure apache to use SSL using a certificate issued from the internal CA.
    Do you wish Setup to configure that, or prefer to perform that manually? (Automatic, Manual) [Automatic]:
    ```

22. You can specify a unique password for the Grafana admin user, or use same one as the Engine admin password:

    ```
    Use Engine admin password as initial Grafana admin password (Yes, No) [Yes]:
    ```

23. Choose how long Data Warehouse will retain collected data:

    ```
    Please choose Data Warehouse sampling scale:
    (1) Basic
    (2) Full
    (1, 2)[1]:
    ```

    `Full` uses the default values for the data storage settings listed in the [*Data Warehouse Guide*](https://ovirt.org/documentation/data_warehouse_guide/index#Application_Settings_for_the_Data_Warehouse_service_in_ovirt-engine-dwhd_file) (recommended when Data Warehouse is installed on a remote server).

    `Basic` reduces the values of `DWH_TABLES_KEEP_HOURLY` to `720` and `DWH_TABLES_KEEP_DAILY` to `0`, easing the load on the Engine machine. Use `Basic` when the Engine and Data Warehouse are installed on the same machine.

24. Review the installation settings, and press `Enter` to accept the values and proceed with the installation:

    ```
    Please confirm installation settings (OK, Cancel) [OK]:
    ```

When your environment has been configured, `engine-setup` displays details about how to access your environment.

Next steps

If you chose to manually configure the firewall, `engine-setup` provides a custom list of ports that need to be opened, based on the options selected during setup. `engine-setup` also saves your answers to a file that can be used to reconfigure the  Engine using the same values, and outputs the location of the log file  for the oVirt Engine configuration process.

- If you intend to link your oVirt environment with a directory server, configure the date and time to synchronize with the system clock used  by the directory server to avoid unexpected account expiry issues. See [Synchronizing the System Clock with a Remote Server](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-Configuring_the_Date_and_Time.html#sect-Configuring_the_Date_and_Time-timedatectl-NTP) in the *Enterprise Linux System Administrator’s Guide* for more information.
- Install the certificate authority according to the instructions  provided by your browser. You can get the certificate authority’s  certificate by navigating to `http://<manager-fqdn>/ovirt-engine/services/pki-resource?resource=ca-certificate&format=X509-PEM-CA`, replacing <manager-fqdn> with the FQDN that you provided during the installation.

Log in to the Administration Portal, where you can add hosts and storage to the environment:

#### 3.4. Connecting to the Administration Portal

Access the Administration Portal using a web browser.

1. In a web browser, navigate to `https://*manager-fqdn*/ovirt-engine`, replacing *manager-fqdn* with the FQDN that you provided during installation.

   |      | You can access the Administration Portal using alternate host names  or IP addresses. To do so, you need to add a configuration file under **/etc/ovirt-engine/engine.conf.d/**. For example:  `# vi /etc/ovirt-engine/engine.conf.d/99-custom-sso-setup.conf SSO_ALTERNATE_ENGINE_FQDNS="_alias1.example.com alias2.example.com_"`  The list of alternate host names needs to be separated by spaces. You can also add the IP address of the Engine to the list, but using IP  addresses instead of DNS-resolvable host names is not recommended. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

2. Click **Administration Portal**. An SSO login page displays. SSO login enables you to log in to the Administration and VM Portal at the same time.

3. Enter your **User Name** and **Password**. If you are logging in for the first time, use the user name **admin** along with the password that you specified during installation.

4. Select the **Domain** to authenticate against. If you are logging in using the internal **admin** user name, select the **internal** domain.

5. Click **Log In**.

6. You can view the Administration Portal in multiple languages. The  default selection is chosen based on the locale settings of your web  browser. If you want to view the Administration Portal in a language  other than the default, select your preferred language from the  drop-down list on the welcome page.

To log out of the oVirt Administration Portal, click your user name in the header bar and click **Sign Out**. You are logged out of all portals and the Engine welcome screen displays.

## 4. Installing Hosts for oVirt

oVirt supports two types of hosts: [oVirt Nodes (oVirt Node)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Red_Hat_Virtualization_Hosts_SM_localDB_deploy) and [Enterprise Linux hosts](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Red_Hat_Enterprise_Linux_hosts_SM_localDB_deploy). Depending on your environment, you may want to use one type only, or  both. At least two hosts are required for features such as migration and high availability.

See [Recommended practices for configuring host networks](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Recommended_practices_for_configuring_host_networks_SM_localDB_deploy) for networking information.

|      | SELinux is in enforcing mode upon installation. To verify, run `getenforce`. SELinux must be in enforcing mode on all hosts and Managers for your oVirt environment to be supported. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| Host Type                 | Other Names                       | Description                                                  |
| ------------------------- | --------------------------------- | ------------------------------------------------------------ |
| **oVirt Node**            | oVirt Node, thin host             | This is a minimal operating system based on Enterprise Linux. It is  distributed as an ISO file from the Customer Portal and contains only  the packages required for the machine to act as a host. |
| **Enterprise Linux host** | Enterprise Linux host, thick host | Enterprise Linux systems with the appropriate repositories enabled can be used as hosts. |

Host Compatibility

When you create a new data center, you can set the compatibility  version. Select the compatibility version that suits all the hosts in  the data center. Once set, version regression is not allowed. For a  fresh oVirt installation, the latest compatibility version is set in the default data center and default cluster; to use an earlier  compatibility version, you must create additional data centers and  clusters.

#### 4.1. oVirt Nodes

#### 4.1.1. Installing oVirt Nodes

oVirt Node (oVirt Node) is a minimal operating system based on  Enterprise Linux that is designed to provide a simple method for setting up a physical machine to act as a hypervisor in a oVirt environment.  The minimal operating system contains only the packages required for the machine to act as a hypervisor, and features a Cockpit web interface  for monitoring the host and performing administrative tasks. See [Running Cockpit](http://cockpit-project.org/running.html) for the minimum browser requirements.

oVirt Node supports NIST 800-53 partitioning requirements to improve  security. oVirt Node uses a NIST 800-53 partition layout by default.

The host must meet the minimum  [host requirements](https://ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Write the oVirt Node Installation ISO disk image to a USB, CD, or DVD.

4. Start the machine on which you are installing oVirt Node, booting from the prepared installation media.

5. From the boot menu, select **Install oVirt Node 4.5** and press `Enter`.

   |      | You can also press the `Tab` key to edit the kernel  parameters. Kernel parameters must be separated by a space, and you can  boot the system using the specified kernel parameters by pressing the `Enter` key. Press the `Esc` key to clear any changes to the kernel parameters and return to the boot menu. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Select a language, and click **Continue**.

7. Select a keyboard layout from the **Keyboard Layout** screen and click **Done**.

8. Select the device on which to install oVirt Node from the **Installation Destination** screen. Optionally, enable encryption. Click **Done**.

   |      | Use the **Automatically configure partitioning** option. |
   | ---- | -------------------------------------------------------- |
   |      |                                                          |

9. Select a time zone from the **Time & Date** screen and click **Done**.

10. Select a network from the **Network & Host Name** screen and click **Configure…** to configure the connection details.

    |      | To use the connection every time the system boots, select the **Connect automatically with priority** check box. For more information, see [Configuring network and host name options](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation#network-hostname_configuring-system-settings) in the *Enterprise Linux 8 Installation Guide*. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    Enter a host name in the **Host Name** field, and click **Done**.

11. Optional: Configure **Security Policy** and **Kdump**. See [Customizing your RHEL installation using the GUI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation) in *Performing a standard RHEL installation* for Enterprise Linux 8 for more information on each of the sections in the **Installation Summary** screen.

12. Click **Begin Installation**.

13. Set a root password and, optionally, create an additional user while oVirt Node installs.

    |      | Do not create untrusted users on oVirt Node, as this can lead to exploitation of local security vulnerabilities. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

14. Click **Reboot** to complete the installation.

    |      | When oVirt Node restarts, `nodectl check` performs a health check on the host and displays the result when you log in on the command line. The message `node status: OK` or `node status: DEGRADED` indicates the health status. Run `nodectl check` to get more information. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    |      | If necessary, you can [ prevent kernel modules from loading automatically](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#proc-Preventing_Kernel_Modules_from_Loading_Automatically_Install_nodes_RHVH). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

#### 4.1.2. Installing a third-party package on oVirt-node

If you need a package that is not included in the oVirt provided  repository, you need to provide the repository before you can install  the package.

Prerequisites

- The path to the repository that includes the package you want to install.
- You are logged in to the host with root permissions.

Procedure

1. Open an existing `.repo` file or create a new one in `/etc/yum.repos.d/`.

2. Add an entry to the `.repo` file. For example, to install `sssd-ldap`, add the following entry to a new `.repo` file name `third-party.repo`:

   ```
   # imgbased: set-enabled
   [custom-sssd-ldap]
   name = Provides sssd-ldap
   mirrorlist=http://mirrorlist.centos.org/?release=$stream&arch=$basearch&repo=BaseOS&infra=$infra
   #baseurl=http://mirror.centos.org/$contentdir/$stream/BaseOS/$basearch/os/
   gpgcheck=1
   enabled=1
   gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial
   includepkgs = sssd-ldap
   ```

3. Install ` sssd-ldap`:

   ```
   # dnf install sssd-ldap
   ```

#### 4.1.3. Advanced Installation

##### Custom Partitioning

Custom partitioning on oVirt Node (oVirt Node) is not recommended. Use the **Automatically configure partitioning** option in the **Installation Destination** window.

If your installation requires custom partitioning, select the `I will configure partitioning` option during the installation, and note that the following restrictions apply:

- Ensure the default **LVM Thin Provisioning** option is selected in the **Manual Partitioning** window.

- The following directories are required and must be on thin provisioned logical volumes:

  - root (`/`)

  - `/home`

  - `/tmp`

  - `/var`

  - `/var/crash`

  - `/var/log`

  - `/var/log/audit`

    |      | Do not create a separate partition for `/usr`. Doing so will cause the installation to fail.  `/usr` must be on a logical volume that is able to change versions along with oVirt Node, and therefore should be left on root (`/`). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    For information about the required storage sizes for each partition, see [Storage Requirements](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Storage_Requirements_SM_localDB_deploy).

- The `/boot` directory should be defined as a standard partition.

- The `/var` directory must be on a separate volume or disk.

- Only XFS or Ext4 file systems are supported.

**Configuring Manual Partitioning in a Kickstart File**

The following example demonstrates how to configure manual partitioning in a Kickstart file.

```
clearpart --all
part /boot --fstype xfs --size=1000 --ondisk=sda
part pv.01 --size=42000 --grow
volgroup HostVG pv.01 --reserved-percent=20
logvol swap --vgname=HostVG --name=swap --fstype=swap --recommended
logvol none --vgname=HostVG --name=HostPool --thinpool --size=40000 --grow
logvol / --vgname=HostVG --name=root --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=6000 --grow
logvol /var --vgname=HostVG --name=var --thin --fstype=ext4 --poolname=HostPool
--fsoptions="defaults,discard" --size=15000
logvol /var/crash --vgname=HostVG --name=var_crash --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=10000
logvol /var/log --vgname=HostVG --name=var_log --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=8000
logvol /var/log/audit --vgname=HostVG --name=var_audit --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=2000
logvol /home --vgname=HostVG --name=home --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=1000
logvol /tmp --vgname=HostVG --name=tmp --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=1000
```

|      | If you use `logvol --thinpool --grow`, you must also include `volgroup --reserved-space` or `volgroup --reserved-percent` to reserve space in the volume group for the thin pool to grow. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

##### Installing a DUD driver on a host without installer support

There are times when installing oVirt Node (oVirt Node) requires a  Driver Update Disk (DUD), such as when using a hardware RAID device that is not supported by the default configuration of oVirt Node. In  contrast with Enterprise Linux hosts, oVirt Node does not fully support  using a DUD. Subsequently the host fails to boot normally after  installation because it does not see RAID. Instead it boots into  emergency mode.

Example output:

```
Warning: /dev/test/rhvh-4.4-20210202.0+1 does not exist
Warning: /dev/test/swap does not exist
Entering emergency mode. Exit the shell to continue.
```

In such a case you can manually add the drivers before finishing the installation.

Prerequisites

- A machine onto which you are installing oVirt Node.
- A DUD.
- If you are using a USB drive for the DUD and oVirt Node, you must have at least two available USB ports.

Procedure

1. Load the DUD on the host machine.

   You can search for DUDs or modules for CentOS Stream at the following locations:

   - [DUDs at the ELRepo Project](https://elrepo.org/linux/dud/el8/x86_64/)
   - [Kmods Special Interest Group](https://wiki.centos.org/SpecialInterestGroup/Kmods)

2. Install oVirt Node. See [Installing oVirt Nodes](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Installing_Red_Hat_Virtualization_Hosts_SHE_cli_deploy) in *Installing oVirt as a self-hosted engine using the command line*.

   |      | When installation completes, do not reboot the system. |
   | ---- | ------------------------------------------------------ |
   |      |                                                        |

   |      | If you want to access the DUD using SSH, do the following:   Add the string **` inst.sshd`** to the kernel command line:  `<*kernel_command_line*> inst.sshd`   Enable networking during the installation. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

3. Enter the console mode, by pressing Ctrl + Alt + F3. Alternatively you can connect to it using SSH.

4. Mount the DUD:

   ```
   # mkdir /mnt/dud
   # mount -r /dev/<dud_device> /mnt/dud
   ```

5. Copy the RPM file inside the DUD to the target machine’s disk:

   ```
   # cp /mnt/dud/rpms/<path>/<rpm_file>.rpm /mnt/sysroot/root/
   ```

   For example:

   ```
   # cp /mnt/dud/rpms/x86_64/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm /mnt/sysroot/root/
   ```

6. Change the root directory to `/mnt/sysroot`:

   ```
   # chroot /mnt/sysroot
   ```

7. Back up the current initrd images. For example:

   ```
   # cp -p /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
   # cp -p /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
   ```

8. Install the RPM file for the driver from the copy you made earlier.

   For example:

   ```
   # dnf install /root/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm
   ```

   |      | This package is not visible on the system after you reboot into the  installed environment, so if you need it, for example, to rebuild the `initramfs`, you need to install that package once again, after which the package remains.  If you update the host using `dnf`, the driver update persists, so you do not need to repeat this process. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   |      | If you do not have an internet connection, use the `rpm` command instead of `dnf`:  `# rpm -ivh /root/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm` |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

9. Create a new image, forcefully adding the driver:

   ```
   # dracut --force --add-drivers <module_name> --kver <kernel_version>
   ```

   For example:

   ```
   # dracut --force --add-drivers 3w-9xxx --kver 4.18.0-240.15.1.el8_3.x86_64
   ```

10. Check the results. The new image should be larger, and include the  driver. For example, compare the sizes of the original, backed-up image  file and the new image file.

    In this example, the new image file is 88739013 bytes, larger than the original 88717417 bytes:

    ```
    # ls -ltr /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img*
    -rw-------. 1 root root 88717417 Jun  2 14:29 /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
    -rw-------. 1 root root 88739013 Jun  2 17:47 /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img
    ```

    The new drivers should be part of the image file. For example, the 3w-9xxx module should be included:

    ```
    # lsinitrd /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img | grep 3w-9xxx
    drwxr-xr-x   2 root     root            0 Feb 22 15:57 usr/lib/modules/4.18.0-240.15.1.el8_3.x86_64/weak-updates/3w-9xxx
    lrwxrwxrwx   1 root     root           55 Feb 22 15:57 usr/lib/modules/4.18.0-240.15.1.el8_3.x86_64/weak-updates/3w-9xxx/3w-9xxx.ko-../../../4.18.0-240.el8.x86_64/extra/3w-9xxx/3w-9xxx.ko
    drwxr-xr-x   2 root     root            0 Feb 22 15:57 usr/lib/modules/4.18.0-240.el8.x86_64/extra/3w-9xxx
    -rw-r--r--   1 root     root        80121 Nov 10  2020 usr/lib/modules/4.18.0-240.el8.x86_64/extra/3w-9xxx/3w-9xxx.ko
    ```

11. Copy the image to the the directory under `/boot` that contains the kernel to be used in the layer being installed, for example:

    ```
    # cp -p /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img
    ```

12. Exit chroot.

13. Exit the shell.

14. If you used Ctrl + Alt + F3 to access a virtual terminal, then move back to the installer by pressing Ctrl + Alt + F_<n>_, usually F1 or F5

15. At the installer screen, reboot.

Verification

The machine should reboot successfully.

##### Automating oVirt Node deployment

You can install oVirt Node (oVirt Node) without a physical media  device by booting from a PXE server over the network with a Kickstart  file that contains the answers to the installation questions.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

General instructions for installing from a PXE server with a Kickstart file are available in the [*Enterprise Linux Installation Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/chap-kickstart-installations), as oVirt Node is installed in much the same way as Enterprise Linux.  oVirt Node-specific instructions, with examples for deploying oVirt Node with Red Hat Satellite, are described below.

The automated oVirt Node deployment has 3 stages:

- [Preparing the Installation Environment](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_the_Installation_Environment)
- [Configuring the PXE Server and the Boot Loader](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Configuring_the_PXE_Server_and_the_Boot_Loader)
- [Creating and Running a Kickstart File](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Creating_and_Running_a_Kickstart_File)

###### Preparing the installation environment

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Make the oVirt Node ISO image available over the network. See [Installation Source on a Network](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/sect-making-media-additional-sources.html#sect-making-media-sources-network) in the *Enterprise Linux Installation Guide*.

4. Extract the **squashfs.img** hypervisor image file from the oVirt Node ISO:

   ```
   # mount -o loop /path/to/oVirt Node-ISO /mnt/rhvh
   # cp /mnt/rhvh/Packages/redhat-virtualization-host-image-update* /tmp
   # cd /tmp
   # rpm2cpio redhat-virtualization-host-image-update* | cpio -idmv
   ```

   |      | This **squashfs.img** file, located in the `/tmp/usr/share/redhat-virtualization-host/image/` directory, is called **redhat-virtualization-host-\*version_number\*_version.squashfs.img**. It contains the hypervisor image for installation on the physical machine. It should not be confused with the **/LiveOS/squashfs.img** file, which is used by the Anaconda `inst.stage2` option. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

###### Configuring the PXE server and the boot loader

1. Configure the PXE server. See [Preparing for a Network Installation](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-installation-server-setup.html) in the *Enterprise Linux Installation Guide*.

2. Copy the oVirt Node boot images to the `/tftpboot` directory:

   ```
   # cp mnt/rhvh/images/pxeboot/{vmlinuz,initrd.img} /var/lib/tftpboot/pxelinux/
   ```

3. Create a `rhvh` label specifying the oVirt Node boot images in the boot loader configuration:

   ```
   LABEL rhvh
   MENU LABEL Install oVirt Node
   KERNEL /var/lib/tftpboot/pxelinux/vmlinuz
   APPEND initrd=/var/lib/tftpboot/pxelinux/initrd.img inst.stage2=URL/to/oVirt Node-ISO
   ```

   oVirt Node Boot loader configuration example for Red Hat Satellite

   If you are using information from Red Hat Satellite to provision the  host, you must create a global or host group level parameter called `rhvh_image` and populate it with the directory URL where the ISO is mounted or extracted:

   ```
   <%#
   kind: PXELinux
   name: oVirt Node PXELinux
   %>
   # Created for booting new hosts
   #
   
   DEFAULT rhvh
   
   LABEL rhvh
   KERNEL <%= @kernel %>
   APPEND initrd=<%= @initrd %> inst.ks=<%= foreman_url("provision") %> inst.stage2=<%= @host.params["rhvh_image"] %> intel_iommu=on console=tty0 console=ttyS1,115200n8 ssh_pwauth=1 local_boot_trigger=<%= foreman_url("built") %>
   IPAPPEND 2
   ```

4. Make the content of the oVirt Node ISO locally available and export it to the network, for example, using an HTTPD server:

   ```
   # cp -a /mnt/rhvh/ /var/www/html/rhvh-install
   # curl URL/to/oVirt Node-ISO/rhvh-install
   ```

###### Creating and running a Kickstart file

1. Create a Kickstart file and make it available over the network. See [Kickstart Installations](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-kickstart-installations.html) in the *Enterprise Linux Installation Guide*.

2. Ensure that the Kickstart file meets the following oVirt-specific requirements:

   - The `%packages` section is not required for oVirt Node. Instead, use the `liveimg` option and specify the **redhat-virtualization-host-\*version_number\*_version.squashfs.img** file from the oVirt Node ISO image:

     ```
     liveimg --url=example.com/tmp/usr/share/redhat-virtualization-host/image/redhat-virtualization-host-version_number_version.squashfs.img
     ```

   - Autopartitioning is highly recommended, but use caution: ensure that the local disk is detected first, include the `ignoredisk` command, and specify the local disk to ignore, such as `sda`.  To ensure that a particular drive is used, oVirt recommends using `ignoredisk --only-use=/dev/disk/<*path*>` or `ignoredisk --only-use=/dev/disk/<*ID*>`:

     ```
     autopart --type=thinp
     ignoredisk --only-use=sda
     ignoredisk --only-use=/dev/disk/<path>
     ignoredisk --only-use=/dev/disk/<ID>
     ```

     |      | Autopartitioning requires thin provisioning.  The `--no-home` option does not work in oVirt Node because `/home` is a required directory. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     If your installation requires manual partitioning, see [Custom Partitioning](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Custom_Partitioning_SM_localDB_deploy) for a list of limitations that apply to partitions and an example of manual partitioning in a Kickstart file.

   - A `%post` section that calls the `nodectl init` command is required:

     ```
     %post
     nodectl init
     %end
     ```

     |      | Ensure that the `nodectl init` command is at the very end of the `%post` section but before the reboot code, if any. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     Kickstart example for deploying oVirt Node on its own

     This Kickstart example shows you how to deploy oVirt Node. You can include additional commands and options as required.

     |      | This example assumes that all disks are empty and can be initialized. If you have attached disks with data, either remove them or add them to the `ignoredisks` property. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     ```
     liveimg --url=http://FQDN/tmp/usr/share/redhat-virtualization-host/image/redhat-virtualization-host-version_number_version.squashfs.img
     clearpart --all
     autopart --type=thinp
     rootpw --plaintext ovirt
     timezone --utc America/Phoenix
     zerombr
     text
     
     reboot
     
     %post --erroronfail
     nodectl init
     %end
     ```

3. Add the Kickstart file location to the boot loader configuration file on the PXE server:

   ```
   APPEND initrd=/var/tftpboot/pxelinux/initrd.img inst.stage2=URL/to/oVirt Node-ISO inst.ks=URL/to/oVirt Node-ks.cfg
   ```

4. Install oVirt Node following the instructions in [Booting from the Network Using PXE](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-booting-installer-x86.html#sect-booting-from-pxe-x86) in the *Enterprise Linux Installation Guide*.

### 4.2. Enterprise Linux hosts

#### 4.2.1. Installing Enterprise Linux hosts

A Enterprise Linux host is based on a standard basic installation of Enterprise Linux 8 on a physical server, with the `Enterprise Linux Server` and `oVirt` repositories enabled.

The oVirt project also provides packages for Enterprise Linux 9 but only as a Technology Preview.

For detailed installation instructions, see the [*Performing a standard EL installation*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index.html).

The host must meet the minimum [host requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Virtualization must be enabled in your host’s BIOS settings. For  information on changing your host’s BIOS settings, refer to your host’s  hardware documentation. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Do not install third-party watchdogs on Enterprise Linux hosts. They can interfere with the watchdog daemon provided by VDSM. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 4.2.2. Installing Cockpit on Enterprise Linux hosts

You can install Cockpit for monitoring the host’s resources and performing administrative tasks.

Procedure

1. Install the dashboard packages:

   ```
   # dnf install cockpit-ovirt-dashboard
   ```

2. Enable and start the `cockpit.socket` service:

   ```
   # systemctl enable cockpit.socket
   # systemctl start cockpit.socket
   ```

3. Check if Cockpit is an active service in the firewall:

   ```
   # firewall-cmd --list-services
   ```

   You should see `cockpit` listed. If it is not, enter the following with root permissions to add `cockpit` as a service to your firewall:

   ```
   # firewall-cmd --permanent --add-service=cockpit
   ```

   The `--permanent` option keeps the `cockpit` service active after rebooting.

You can log in to the Cockpit web interface at `https://*HostFQDNorIP*:9090`.

### 4.3. Recommended Practices for Configuring Host Networks

|      | Always use the oVirt Engine to modify the network configuration of hosts in your clusters. Otherwise, you might create an unsupported  configuration. For details, see [Network Manager Stateful Configuration (nmstate)](https://ovirt.org/documentation/administration_guide/index#con-Network-Manager-Stateful-Configuration-nmstate). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

If your network environment is complex, you may need to configure a  host network manually before adding the host to the oVirt Engine.

Consider the following practices for configuring a host network:

- Configure the network with Cockpit. Alternatively, you can use `nmtui` or `nmcli`.

- If a network is not required for a self-hosted engine deployment or  for adding a host to the Engine, configure the network in the  Administration Portal after adding the host to the Engine. See [Creating a New Logical Network in a Data Center or Cluster](https://ovirt.org/documentation/administration_guide/index#Creating_a_new_logical_network_in_a_data_center_or_cluster).

- Use the following naming conventions:

  - VLAN devices: `*VLAN_NAME_TYPE_RAW_PLUS_VID_NO_PAD*`
  - VLAN interfaces: `*physical_device*.*VLAN_ID*` (for example, `eth0.23`, `eth1.128`, `enp3s0.50`)
  - Bond interfaces: `bond*number*` (for example, `bond0`, `bond1`)
  - VLANs on bond interfaces: `bond*number*.*VLAN_ID*` (for example, `bond0.50`, `bond1.128`)

- Use [network bonding](https://ovirt.org/documentation/administration_guide/index#sect-Network_Bonding). Network teaming is not supported in oVirt and will cause errors if the  host is used to deploy a self-hosted engine or added to the Engine.

- Use recommended bonding modes:

  - If the `ovirtmgmt` network is not used by virtual machines, the network may use any supported bonding mode.
  - If the `ovirtmgmt` network is used by virtual machines, see [*Which bonding modes work when used with a bridge that virtual machine guests or containers connect to?*](https://access.redhat.com/solutions/67546).
  - oVirt’s default bonding mode is `(Mode 4) Dynamic Link Aggregation`. If your switch does not support Link Aggregation Control Protocol (LACP), use `(Mode 1) Active-Backup`. See [Bonding Modes](https://ovirt.org/documentation/administration_guide/index#Bonding_Modes) for details.

- Configure a VLAN on a physical NIC as in the following example (although `nmcli` is used, you can use any tool):

  ```
  # nmcli connection add type vlan con-name vlan50 ifname eth0.50 dev eth0 id 50
  # nmcli con mod vlan50 +ipv4.dns 8.8.8.8 +ipv4.addresses 123.123.0.1/24 +ipv4.gateway 123.123.0.254
  ```

- Configure a VLAN on a bond as in the following example (although `nmcli` is used, you can use any tool):

  ```
  # nmcli connection add type bond con-name bond0 ifname bond0 bond.options "mode=active-backup,miimon=100" ipv4.method disabled ipv6.method ignore
  # nmcli connection add type ethernet con-name eth0 ifname eth0 master bond0 slave-type bond
  # nmcli connection add type ethernet con-name eth1 ifname eth1 master bond0 slave-type bond
  # nmcli connection add type vlan con-name vlan50 ifname bond0.50 dev bond0 id 50
  # nmcli con mod vlan50 +ipv4.dns 8.8.8.8 +ipv4.addresses 123.123.0.1/24 +ipv4.gateway 123.123.0.254
  ```

- Do not disable `firewalld`.

- Customize the firewall rules in the Administration Portal after adding the host to the Engine. See [Configuring Host Firewall Rules](https://ovirt.org/documentation/administration_guide/index#Configuring_Host_Firewall_Rules).

### 4.4. Adding Standard Hosts to the oVirt Engine

|      | Always use the oVirt Engine to modify the network configuration of hosts in your clusters. Otherwise, you might create an unsupported  configuration. For details, see [Network Manager Stateful Configuration (nmstate)](https://ovirt.org/documentation/administration_guide/index#con-Network-Manager-Stateful-Configuration-nmstate). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Adding a host to your oVirt environment can take some time, as the  following steps are completed by the platform: virtualization checks,  installation of packages, and creation of a bridge.

Procedure

1. From the Administration Portal, click **Compute** **Hosts**.
2. Click **New**.
3. Use the drop-down list to select the **Data Center** and **Host Cluster** for the new host.
4. Enter the **Name** and the **Address** of the new host. The standard SSH port, port 22, is auto-filled in the **SSH Port** field.
5. Select an authentication method to use for the Engine to access the host.
   - Enter the root user’s password to use password authentication.
   - Alternatively, copy the key displayed in the **SSH PublicKey** field to **/root/.ssh/authorized_keys** on the host to use public key authentication.
6. Optionally, click the **Advanced Parameters** button to change the following advanced host settings:
   - Disable automatic firewall configuration.
   - Add a host SSH fingerprint to increase security. You can add it manually, or fetch it automatically.
7. Optionally configure power management, where the host has a supported power management card. For information on power management  configuration, see [Host Power Management Settings Explained](https://ovirt.org/documentation/administration_guide/index#Host_Power_Management_settings_explained) in the *Administration Guide*.
8. Click **OK**.

The new host displays in the list of hosts with a status of `Installing`, and you can view the progress of the installation in the **Events** section of the **Notification Drawer** (![EventsIcon](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/common/images/EventsIcon.png)). After a brief delay the host status changes to `Up`.

## 5. Preparing Storage for oVirt

You need to prepare storage to be used for storage domains in the new environment. A oVirt environment must have at least one data storage  domain, but adding more is recommended.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

A data domain holds the virtual hard disks and OVF files of all the  virtual machines and templates in a data center, and cannot be shared  across data centers while active (but can be migrated between data  centers). Data domains of multiple storage types can be added to the  same data center, provided they are all shared, rather than local,  domains.

You can use one of the following storage types:

- [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_NFS_Storage_SM_localDB_deploy)
- [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_iSCSI_Storage_SM_localDB_deploy)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_FCP_Storage_SM_localDB_deploy)

- [POSIX-compliant file system](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_POSIX_Storage_SM_localDB_deploy)
- [Local storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_Local_Storage_SM_localDB_deploy)
- [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Preparing_Red_Hat_Gluster_Storage_SM_localDB_deploy)

### 5.1. Preparing NFS Storage

Set up NFS shares on your file storage or remote server to serve as  storage domains on Red Hat Enterprise Virtualization Host systems. After exporting the shares on the remote storage and configuring them in the  Red Hat Virtualization Manager, the shares will be automatically  imported on the Red Hat Virtualization hosts.

For information on setting up, configuring, mounting and exporting NFS, see [*Managing file systems*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_file_systems/index) for Red Hat Enterprise Linux 8.

Specific system user accounts and system user groups are required by  oVirt so the Engine can store data in the storage domains represented by the exported directories. The following procedure sets the permissions  for one directory. You must repeat the `chown` and `chmod` steps for all of the directories you intend to use as storage domains in oVirt.

Prerequisites

1. Install the NFS `utils` package.

   ```
   # dnf install nfs-utils -y
   ```

2. To check the enabled versions:

   ```
   # cat /proc/fs/nfsd/versions
   ```

3. Enable the following services:

   ```
   # systemctl enable nfs-server
   # systemctl enable rpcbind
   ```

Procedure

1. Create the group `kvm`:

   ```
   # groupadd kvm -g 36
   ```

2. Create the user `vdsm` in the group `kvm`:

   ```
   # useradd vdsm -u 36 -g kvm
   ```

3. Create the `storage` directory and modify the access rights.

   ```
   # mkdir /storage
   # chmod 0755 /storage
   # chown 36:36 /storage/
   ```

4. Add the `storage` directory to `/etc/exports` with the relevant permissions.

   ```
   # vi /etc/exports
   # cat /etc/exports
    /storage *(rw)
   ```

5. Restart the following services:

   ```
   # systemctl restart rpcbind
   # systemctl restart nfs-server
   ```

6. To see which export are available for a specific IP address:

   ```
   # exportfs
    /nfs_server/srv
                  10.46.11.3/24
    /nfs_server       <world>
   ```

|      | If changes in `/etc/exports` have been made after starting the services, the `exportfs -ra` command can be used to reload the changes. After performing all the above stages, the exports directory should be  ready and can be tested on a different host to check that it is usable. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.2. Preparing iSCSI Storage

oVirt supports iSCSI storage, which is a storage domain created from a volume group made up of LUNs. Volume groups and LUNs cannot be attached to more than one storage domain at a time.

For information on setting up and configuring iSCSI storage, see [Getting started with iSCSI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_storage_devices/index#getting-started-with-iscsi_managing-storage-devices) in *Managing storage devices* for Red Hat Enterprise Linux 8.

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.3. Preparing FCP Storage

oVirt supports SAN storage by creating a storage domain from a volume group made of pre-existing LUNs. Neither volume groups nor LUNs can be  attached to more than one storage domain at a time.

oVirt system administrators need a working knowledge of Storage Area  Networks (SAN) concepts. SAN usually uses Fibre Channel Protocol (FCP)  for traffic between hosts and shared external storage. For this reason,  SAN may occasionally be referred to as FCP storage.

For information on setting up and configuring FCP or multipathing on Enterprise Linux, see the [*Storage Administration Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Storage_Administration_Guide/index.html) and [*DM Multipath Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/DM_Multipath/index.html).

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }  }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.4. Preparing POSIX-compliant File System Storage

POSIX file system support allows you to mount file systems using the  same mount options that you would normally use when mounting them  manually from the command line. This functionality is intended to allow  access to storage not exposed using NFS, iSCSI, or FCP.

Any POSIX-compliant file system used as a storage domain in oVirt  must be a clustered file system, such as Global File System 2 (GFS2),  and must support sparse files and direct I/O. The Common Internet File  System (CIFS), for example, does not support direct I/O, making it  incompatible with oVirt.

For information on setting up and configuring POSIX-compliant file system storage, see [*Enterprise Linux Global File System 2*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Global_File_System_2/index.html).

|      | Do **not** mount NFS storage by creating a POSIX-compliant file system storage domain. Always create an NFS storage domain instead. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.5. Preparing local storage

On oVirt Node (oVirt Node), local storage should always be defined on a file system that is separate from `/` (root). Use a separate logical volume or disk, to prevent possible loss of data during upgrades.

Procedure for Enterprise Linux hosts

1. On the host, create the directory to be used for the local storage:

   ```
   # mkdir -p /data/images
   ```

2. Ensure that the directory has permissions allowing read/write access to the **vdsm** user (UID 36) and **kvm** group (GID 36):

   ```
   # chown 36:36 /data /data/images
   # chmod 0755 /data /data/images
   ```

Procedure for oVirt Nodes

Create the local storage on a logical volume:

1. Create a local storage directory:

   ```
   # mkdir /data
   # lvcreate -L $SIZE rhvh -n data
   # mkfs.ext4 /dev/mapper/rhvh-data
   # echo "/dev/mapper/rhvh-data /data ext4 defaults,discard 1 2" >> /etc/fstab
   # mount /data
   ```

2. Mount the new local storage:

   ```
   # mount -a
   ```

3. Ensure that the directory has permissions allowing read/write access to the **vdsm** user (UID 36) and **kvm** group (GID 36):

   ```
   # chown 36:36 /data /rhvh-data
   # chmod 0755 /data /rhvh-data
   ```

### 5.6. Preparing Gluster Storage

For information on setting up and configuring Gluster Storage, see the [*Gluster Storage Installation Guide*](https://docs.gluster.org/en/latest/Install-Guide/Overview/).

### 5.7. Customizing Multipath Configurations for SAN Vendors

If your RHV environment is configured to use multipath connections  with SANs, you can customize the multipath configuration settings to  meet requirements specified by your storage vendor. These customizations can override both the default settings and settings that are specified  in `/etc/multipath.conf`.

To override the multipath settings, do not customize `/etc/multipath.conf`. Because VDSM owns `/etc/multipath.conf`, installing or upgrading VDSM or oVirt can overwrite this file including any customizations it contains. This overwriting can cause severe  storage failures.

Instead, you create a file in the `/etc/multipath/conf.d` directory that contains the settings you want to customize or override.

VDSM executes the files in `/etc/multipath/conf.d` in  alphabetical order. So, to control the order of execution, you begin the filename with a number that makes it come last. For example, `/etc/multipath/conf.d/90-myfile.conf`.

To avoid causing severe storage failures, follow these guidelines:

- Do not modify `/etc/multipath.conf`. If the file contains user modifications, and the file is overwritten, it can cause unexpected storage problems.
- Do not override the `user_friendly_names` and `find_multipaths` settings. For details, see [Recommended Settings for Multipath.conf](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#ref-Recommended_Settings_for_Multipath_conf_SM_localDB_deploy).
- Avoid overriding the `no_path_retry` and `polling_interval` settings unless a storage vendor specifically requires you to do so. For details, see [Recommended Settings for Multipath.conf](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#ref-Recommended_Settings_for_Multipath_conf_SM_localDB_deploy).

|      | Not following these guidelines can cause catastrophic storage errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

- VDSM is configured to use the multipath module. To verify this, enter:

  ```
  # vdsm-tool is-configured --module multipath
  ```

Procedure

1. Create a new configuration file in the `/etc/multipath/conf.d` directory.

2. Copy the individual setting you want to override from `/etc/multipath.conf` to the new configuration file in `/etc/multipath/conf.d/<my_device>.conf`. Remove any comment marks, edit the setting values, and save your changes.

3. Apply the new configuration settings by entering:

   ```
   # systemctl reload multipathd
   ```

   |      | Do not restart the multipathd service. Doing so generates errors in the VDSM logs. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

Verification steps

1. Test that the new configuration performs as expected on a  non-production cluster in a variety of failure scenarios. For example,  disable all of the storage connections.
2. Enable one connection at a time and verify that doing so makes the storage domain reachable.

Additional resources

- [Recommended Settings for Multipath.conf](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#ref-Recommended_Settings_for_Multipath_conf_SHE_cli_deploy)
- [*Enterprise Linux DM Multipath*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/dm_multipath/)
- [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing)
- [How do I customize /etc/multipath.conf on my RHVH hypervisors? What values must not change and why?](https://access.redhat.com/solutions/3234761)

### 5.8. Recommended Settings for Multipath.conf

Do not override the following settings:

- user_friendly_names  no

  Device names must be consistent across all hypervisors. For example, `/dev/mapper/{WWID}`. The default value of this setting, `no`, prevents the assignment of arbitrary and inconsistent device names such as `/dev/mapper/mpath{N}` on various hypervisors, which can lead to unpredictable system behavior.    Do not change this setting to `user_friendly_names  yes`. User-friendly names are likely to cause unpredictable system behavior or failures, and are not supported.

- `find_multipaths	no`

  This setting controls whether oVirt Node tries to access devices  through multipath only if more than one path is available. The current  value, `no`, allows oVirt to access devices through multipath even if only one path is available.    Do not override this setting.

Avoid overriding the following settings unless required by the storage system vendor:

- `no_path_retry	4`

  This setting controls the number of polling attempts to retry when no paths are available. Before oVirt version 4.2, the value of `no_path_retry` was `fail` because QEMU had trouble with the I/O queuing when no paths were available. The `fail` value made it fail quickly and paused the virtual machine. oVirt version 4.2 changed this value to `4` so when multipathd detects the last path has failed, it checks all of  the paths four more times. Assuming the default 5-second polling  interval, checking the paths takes 20 seconds. If no path is up,  multipathd tells the kernel to stop queuing and fails all outstanding  and future I/O until a path is restored. When a path is restored, the  20-second delay is reset for the next time all paths fail. For more  details, see [the commit that changed this setting](https://gerrit.ovirt.org/#/c/88082/).

- `polling_interval	5`

  This setting determines the number of seconds between polling  attempts to detect whether a path is open or has failed. Unless the  vendor provides a clear reason for increasing the value, keep the  VDSM-generated default so the system responds to path failures sooner.

## 6. Adding Storage for oVirt

Add storage as data domains in the new environment. A oVirt  environment must have at least one data domain, but adding more is  recommended.

Add the storage you prepared earlier:

- [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Adding_NFS_Storage_SM_localDB_deploy)
- [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Adding_iSCSI_Storage_SM_localDB_deploy)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Adding_FCP_Storage_SM_localDB_deploy)

- [POSIX-compliant file system](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Adding_POSIX_Storage_SM_localDB_deploy)
- [Local storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Adding_Local_Storage_SM_localDB_deploy)
- [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Adding_Red_Hat_Gluster_Storage_SM_localDB_deploy)

### 6.1. Adding NFS Storage

This procedure shows you how to attach existing NFS storage to your oVirt environment as a data domain.

If you require an ISO or export domain, use this procedure, but select **ISO** or **Export** from the **Domain Function** list.

Procedure

1. In the Administration Portal, click **Storage** **Domains**.
2. Click **New Domain**.
3. Enter a **Name** for the storage domain.
4. Accept the default values for the **Data Center**, **Domain Function**, **Storage Type**, **Format**, and **Host** lists.
5. Enter the **Export Path** to be used for the storage domain. The export path should be in the format of *123.123.0.10:/data* (for IPv4), *[2001:0:0:0:0:0:0:5db1]:/data* (for IPv6), or *domain.example.com:/data*.
6. Optionally, you can configure the advanced parameters:
   1. Click **Advanced Parameters**.
   2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
   3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
   4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
7. Click **OK**.

The new NFS data domain has a status of `Locked` until the disk is prepared. The data domain is then automatically attached to the data center.

### 6.2. Adding iSCSI Storage

This procedure shows you how to attach existing iSCSI storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** of the new storage domain.

4. Select a **Data Center** from the drop-down list.

5. Select **Data** as the **Domain Function** and **iSCSI** as the **Storage Type**.

6. Select an active host as the **Host**.

   |      | Communication to the storage domain is from the selected host and not directly from the Engine. Therefore, all hosts must have access to the  storage device before the storage domain can be configured. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. The Engine can map iSCSI targets to LUNs or LUNs to iSCSI targets. The **New Domain** window automatically displays known targets with unused LUNs when the  iSCSI storage type is selected. If the target that you are using to add  storage does not appear, you can use target discovery to find it;  otherwise proceed to the next step.

   1. Click **Discover Targets** to enable target discovery options. When targets have been discovered and logged in to, the **New Domain** window automatically displays targets with LUNs unused by the environment.

      |      | LUNs used externally for the environment are also displayed. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      You can use the **Discover Targets** options to add LUNs on many targets or multiple paths to the same LUNs.

      |      | If you use the REST API method `discoveriscsi` to discover the iscsi targets, you can use an FQDN or an IP address, but you must  use the iscsi details from the discovered targets results to log in  using the REST API method `iscsilogin`. See [discoveriscsi](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-host-methods-discover_iscsi) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   2. Enter the FQDN or IP address of the iSCSI host in the **Address** field.

   3. Enter the port with which to connect to the host when browsing for targets in the **Port** field. The default is `3260`.

   4. If CHAP is used to secure the storage, select the **User Authentication** check box. Enter the **CHAP user name** and **CHAP password**.

      |      | You can define credentials for an iSCSI target for a specific host with the REST API. See [StorageServerConnectionExtensions: add](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-storage_server_connection_extensions-methods-add) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   5. Click **Discover**.

   6. Select one or more targets from the discovery results and click **Login** for one target or **Login All** for multiple targets.

      |      | If more than one path access is required, you must discover and log  in to the target through all the required paths. Modifying a storage  domain to add additional paths is currently not supported. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      |      | When using the REST API `iscsilogin` method to log in, you must use the iscsi details from the discovered targets results in the `discoveriscsi` method. See [iscsilogin](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-host-methods-iscsi_login) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

8. Click the **+** button next to the desired target. This expands the entry and displays all unused LUNs attached to the target.

9. Select the check box for each LUN that you are using to create the storage domain.

10. Optionally, you can configure the advanced parameters:

    1. Click **Advanced Parameters**.
    2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
    3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
    4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
    5. Select the **Discard After Delete** check box to enable  the discard after delete option. This option can be edited after the  domain is created. This option is only available to block storage  domains.

11. Click **OK**.

If you have configured multiple storage connection paths to the same target, follow the procedure in [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing) to complete iSCSI bonding.

If you want to migrate your current storage network to an iSCSI bond, see [Migrating a Logical Network to an iSCSI Bond](https://ovirt.org/documentation/administration_guide/index#Migrating_a_logical_network_to_an_iscsi_bond).

### 6.3. Adding FCP Storage

This procedure shows you how to attach existing FCP storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** of the storage domain.

4. Select an FCP **Data Center** from the drop-down list.

   If you do not yet have an appropriate FCP data center, select `(none)`.

5. Select the **Domain Function** and the **Storage Type** from the drop-down lists. The storage domain types that are not compatible with the chosen data center are not available.

6. Select an active host in the **Host** field. If this is not the first data domain in a data center, you must select the data center’s SPM host.

   |      | All communication to the storage domain is through the selected host  and not directly from the oVirt Engine. At least one active host must  exist in the system and be attached to the chosen data center. All hosts must have access to the storage device before the storage domain can be configured. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. The **New Domain** window automatically displays known targets with unused LUNs when **Fibre Channel** is selected as the storage type. Select the **LUN ID** check box to select all of the available LUNs.

8. Optionally, you can configure the advanced parameters.

   1. Click **Advanced Parameters**.
   2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
   3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
   4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
   5. Select the **Discard After Delete** check box to enable  the discard after delete option. This option can be edited after the  domain is created. This option is only available to block storage  domains.

9. Click **OK**.

The new FCP data domain remains in a `Locked` status while it is being prepared for use. When ready, it is automatically attached to the data center.

### 6.4. Adding POSIX-compliant File System Storage

This procedure shows you how to attach existing POSIX-compliant file system storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** for the storage domain.

4. Select the **Data Center** to be associated with the storage domain. The data center selected must be of type **POSIX (POSIX compliant FS)**. Alternatively, select `(none)`.

5. Select `Data` from the **Domain Function** drop-down list, and `POSIX compliant FS` from the **Storage Type** drop-down list.

   If applicable, select the **Format** from the drop-down menu.

6. Select a host from the **Host** drop-down list.

7. Enter the **Path** to the POSIX file system, as you would normally provide it to the `mount` command.

8. Enter the **VFS Type**, as you would normally provide it to the `mount` command using the `-t` argument. See `man mount` for a list of valid VFS types.

9. Enter additional **Mount Options**, as you would normally provide them to the `mount` command using the `-o` argument. The mount options should be provided in a comma-separated list. See `man mount` for a list of valid mount options.

10. Optionally, you can configure the advanced parameters.

    1. Click **Advanced Parameters**.
    2. Enter a percentage value in the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
    3. Enter a GB value in the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
    4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.

11. Click **OK**.

### 6.5. Adding a local storage domain

When adding a local storage domain to a host, setting the path to the local storage directory automatically creates and places the host in a  local data center, local cluster, and local storage domain.

Procedure

1. Click **Compute** **Hosts** and select the host.
2. Click **Management** **Maintenance** and **OK**. The host’s status changes to **Maintenance**.
3. Click **Management** **Configure Local Storage**.
4. Click the **Edit** buttons next to the **Data Center**, **Cluster**, and **Storage** fields to configure and name the local storage domain.
5. Set the path to your local storage in the text entry field.
6. If applicable, click the **Optimization** tab to configure the memory optimization policy for the new local storage cluster.
7. Click **OK**.

The Engine sets up the local data center with a local cluster, local storage domain. It also changes the host’s status to **Up**.

Verification

1. Click **Storage** **Domains**.
2. Locate the local storage domain you just added.

The domain’s status should be **Active** (![status active icon](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/common/images/status_active_icon.png)), and the value in the **Storage Type** column should be **Local on Host**.

You can now upload a disk image in the new local storage domain.

### 6.6. Adding Gluster Storage

To use Gluster Storage with oVirt, see [*Configuring oVirt with Gluster Storage*](https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.4/html/configuring_red_hat_virtualization_with_red_hat_gluster_storage/).

For the Gluster Storage versions that are supported with oVirt, see [Red Hat Gluster Storage Version Compatibility and Support](https://access.redhat.com/articles/2356261).

## Appendix A: Preparing a Local Manually Configured PostgreSQL Database

Use this procedure to set up the Engine database. Set up this  database before you configure the Engine; you must supply the database  credentials during `engine-setup`.

|      | The `engine-setup` and `engine-backup --mode=restore` commands only support system error messages in the `en_US.UTF8` locale, even if the system locale is different.  The locale settings in the `postgresql.conf` file must be set to `en_US.UTF8`. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | The database name must contain only numbers, underscores, and lowercase letters. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### Enabling the oVirt Engine Repositories

Ensure the correct repositories are enabled.

For oVirt 4.5: If you are going to install on RHEL or derivatives please follow [Installing on RHEL or derivatives](https://www.ovirt.org/download/install_on_rhel.html) first.

```
# dnf install -y centos-release-ovirt45
```

For oVirt 4.4:

```
# dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

Common procedure valid for both 4.4 and 4.5:

You can check which repositories are currently enabled by running `dnf repolist`.

1. Enable the `javapackages-tools` module.

   ```
   # dnf module -y enable javapackages-tools
   ```

2. Enable version 12 of the `postgresql` module.

   ```
   # dnf module -y enable postgresql:12
   ```

3. Enable version 2.3 of the `mod_auth_openidc` module.

   ```
   # dnf module -y enable mod_auth_openidc:2.3
   ```

4. Enable version 14 of the `nodejs` module:

   ```
   # dnf module -y enable nodejs:14
   ```

5. Synchronize installed packages to update them to the latest available versions.

   ```
   # dnf distro-sync --nobest
   ```

   Additional resources

   For information on modules and module streams, see the following sections in *Installing, managing, and removing user-space components*

   - [Module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#module-streams_introduction-to-modules)
   - [Selecting a stream before installation of packages](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#selecting-a-stream-before-installation-of-packages_installing-rhel-8-content)
   - [Resetting module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#resetting-module-streams_removing-rhel-8-content)
   - [Switching to a later stream](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#switching-to-a-later-stream_managing-versions-of-appstream-content)

### Initializing the PostgreSQL Database

1. Install the PostgreSQL server package:

   ```
   # dnf install postgresql-server postgresql-contrib
   ```

2. Initialize the PostgreSQL database instance:

   ```
   # postgresql-setup --initdb
   ```

3. Start the `postgresql` service, and ensure that this service starts on boot:

   ```
   # systemctl enable postgresql
   # systemctl start postgresql
   ```

4. Connect to the `psql` command line interface as the `postgres` user:

   ```
   # su - postgres -c psql
   ```

5. Create a default user. The Engine’s default user is `engine` and Data Warehouse’s default user is `ovirt_engine_history`:

   ```
   postgres=# create role user_name with login encrypted password 'password';
   ```

6. Create a database. The Engine’s default database name is `engine` and Data Warehouse’s default database name is `ovirt_engine_history`:

   ```
   postgres=# create database database_name owner user_name template template0 encoding 'UTF8' lc_collate 'en_US.UTF-8' lc_ctype 'en_US.UTF-8';
   ```

7. Connect to the new database:

   ```
   postgres=# \c database_name
   ```

8. Add the `uuid-ossp` extension:

   ```
   database_name=# CREATE EXTENSION "uuid-ossp";
   ```

9. Add the `plpgsql` language if it does not exist:

   ```
   database_name=# CREATE LANGUAGE plpgsql;
   ```

10. Quit the `psql` interface:

    ```
    database_name=# \q
    ```

11. Edit the `/var/lib/pgsql/data/pg_hba.conf` file to enable  md5 client authentication, so the engine can access the database  locally. Add the following line immediately below the line that starts  with `local` at the bottom of the file:

    ```
    host    database_name    user_name    0.0.0.0/0    md5
    host    database_name    user_name    ::0/0   md5
    ```

12. Update the PostgreSQL server’s configuration. Edit the `/var/lib/pgsql/data/postgresql.conf` file and add the following lines to the bottom of the file:

    ```
    autovacuum_vacuum_scale_factor=0.01
    autovacuum_analyze_scale_factor=0.075
    autovacuum_max_workers=6
    maintenance_work_mem=65536
    max_connections=150
    work_mem=8192
    ```

13. Restart the `postgresql` service:

    ```
    # systemctl restart postgresql
    ```

14. Optionally, set up [SSL](http://www.postgresql.org/docs/12/static/ssl-tcp.html#SSL-FILE-USAGE) to secure database connections.

Return to [Configuring the Engine](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Configuring_the_Red_Hat_Virtualization_Manager_install_RHVM), and answer `Local` and `Manual` when asked about the database.

## Appendix B: Configuring a Host for PCI Passthrough

|      | This is one in a series of topics that show how to set up and configure SR-IOV on oVirt. For more information, see [Setting Up and Configuring SR-IOV](https://ovirt.org/documentation/administration_guide/index#setting-up-and-configuring-sr-iov) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Enabling PCI passthrough allows a virtual machine to use a host  device as if the device were directly attached to the virtual machine.  To enable the PCI passthrough function, you must enable virtualization  extensions and the IOMMU function. The following procedure requires you  to reboot the host. If the host is attached to the Engine already,  ensure you place the host into maintenance mode first.

Prerequisites

- Ensure that the host hardware meets the requirements for PCI device passthrough and assignment. See [PCI Device Requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#PCI_Device_Requirements_RHV_planning) for more information.

Configuring a Host for PCI Passthrough

1. Enable the virtualization extension and IOMMU extension in the BIOS. See [Enabling Intel VT-x and AMD-V virtualization hardware extensions in BIOS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Virtualization_Deployment_and_Administration_Guide/sect-Troubleshooting-Enabling_Intel_VT_x_and_AMD_V_virtualization_hardware_extensions_in_BIOS.html) in the *Enterprise Linux Virtualization Deployment and Administration Guide* for more information.
2. Enable the IOMMU flag in the kernel by selecting the **Hostdev Passthrough & SR-IOV** check box when adding the host to the Engine or by editing the **grub** configuration file manually.
   - To enable the IOMMU flag from the Administration Portal, see [Adding Standard Hosts to the oVirt Engine](https://ovirt.org/documentation/administration_guide#Adding_standard_hosts_to_the_Manager) and [Kernel Settings Explained](https://ovirt.org/documentation/administration_guide#Kernel_Settings_Explained).
   - To edit the **grub** configuration file manually, see  [Enabling IOMMU Manually](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#Enabling_IOMMU_Manually).
3. For GPU passthrough, you need to run additional configuration steps on both the host and the guest system. See [GPU device passthrough: Assigning a host GPU to a single virtual machine](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/setting_up_an_nvidia_gpu_for_a_virtual_machine_in_red_hat_virtualization/index#proc_nvidia_gpu_passthrough_nvidia_gpu_passthrough) in *Setting up an NVIDIA GPU for a virtual machine in Red Hat Virtualization* for more information.

Enabling IOMMU Manually

1. Enable IOMMU by editing the grub configuration file.

   |      | If you are using IBM POWER8 hardware, skip this step as IOMMU is enabled by default. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   - For Intel, boot the machine, and append `intel_iommu=on` to the end of the `GRUB_CMDLINE_LINUX` line in the **grub** configuration file.

     ```
     # vi /etc/default/grub
     ...
     GRUB_CMDLINE_LINUX="nofb splash=quiet console=tty0 ... intel_iommu=on
     ...
     ```

   - For AMD, boot the machine, and append `amd_iommu=on` to the end of the `GRUB_CMDLINE_LINUX` line in the **grub** configuration file.

     ```
     # vi /etc/default/grub
     …
     GRUB_CMDLINE_LINUX="nofb splash=quiet console=tty0 … amd_iommu=on
     …
     ```

     |      | If `intel_iommu=on` or an AMD IOMMU is detected, you can try adding `iommu=pt`. The `pt` option only enables IOMMU for devices used in passthrough and provides  better host performance. However, the option might not be supported on  all hardware. Revert to the previous option if the `pt` option doesn’t work for your host.  If the passthrough fails because the hardware does not support interrupt remapping, you can consider enabling the `allow_unsafe_interrupts` option if the virtual machines are trusted. The `allow_unsafe_interrupts` is not enabled by default because enabling it potentially exposes the  host to MSI attacks from virtual machines. To enable the option:  `# vi /etc/modprobe.d options vfio_iommu_type1 allow_unsafe_interrupts=1` |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

2. Refresh the **grub.cfg** file and reboot the host for these changes to take effect:

   ```
   # grub2-mkconfig -o /boot/grub2/grub.cfg
   ```

   ```
   # reboot
   ```

## Appendix C: Removing the standalone oVirt Engine

The `engine-cleanup` command removes all components of the oVirt Engine and automatically backs up the following:

- the Grafana database, in `/var/lib/grafana/`
- the Engine database in `/var/lib/ovirt-engine/backups/`
- a compressed archive of the PKI keys and configuration in `/var/lib/ovirt-engine/backups/`

Backup file names include the date and time.

|      | You should use this procedure only on a standalone installation of the oVirt Engine. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Run the following command on the Engine machine:

   ```
   # engine-cleanup
   ```

2. The Engine service must be stopped before proceeding. You are prompted to confirm. Enter `OK` to proceed:

   ```
   During execution engine service will be stopped (OK, Cancel) [OK]:
   ```

3. You are prompted to confirm that you want to remove all Engine components. Enter `OK` to remove all components, or `Cancel` to exit `engine-cleanup`:

   ```
   All the installed ovirt components are about to be removed, data will be lost (OK, Cancel) [Cancel]: OK
   ```

   `engine-cleanup` details the components that are removed, and the location of backup files.

4. Remove the oVirt packages:

   ```
   # dnf remove ovirt-engine* vdsm-bootstrap
   ```

## Appendix D: Preventing kernel modules from loading automatically

You can prevent a kernel module from being loaded automatically,  whether the module is loaded directly, loaded as a dependency from  another module, or during the boot process.

Procedure

1. The module name must be added to a configuration file for the `modprobe` utility.  This file must reside in the configuration directory `/etc/modprobe.d`.

   For more information on this configuration directory, see the man page `modprobe.d`.

2. Ensure the module is not configured to get loaded in any of the following:

   - `/etc/modprobe.conf`
   - `/etc/modprobe.d/*`
   - `/etc/rc.modules`
   - `/etc/sysconfig/modules/*`

   ```
   # modprobe --showconfig <_configuration_file_name_>
   ```

3. If the module appears in the output, ensure it is ignored and not loaded:

   ```
   # modprobe --ignore-install <_module_name_>
   ```

4. Unload the module from the running system, if it is loaded:

   ```
   # modprobe -r <_module_name_>
   ```

5. Prevent the module from being loaded directly by adding the `blacklist` line to a configuration file specific to the system - for example `/etc/modprobe.d/local-dontload.conf`:

   ```
   # echo "blacklist <_module_name_> >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | This step does not prevent a module from loading if it is a required or an optional dependency of another module. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Prevent optional modules from being loading on demand:

   ```
   # echo "install <_module_name_>/bin/false" >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | If the excluded module is required for other hardware, excluding it might cause unexpected side effects. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. Make a backup copy of your `initramfs`:

   ```
   # cp /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.$(date +%m-%d-%H%M%S).bak
   ```

8. If the kernel module is part of the `initramfs`, rebuild your initial `ramdisk` image, omitting the module:

   ```
   # dracut --omit-drivers <_module_name_> -f
   ```

9. Get the current kernel command line parameters:

   ```
   # grub2-editenv - list | grep kernelopts
   ```

10. Append `<_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>` to the generated output:

    ```
    # grub2-editenv - set kernelopts="<> <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

    For example:

    ```
    # grub2-editenv - set kernelopts="root=/dev/mapper/rhel_example-root ro crashkernel=auto resume=/dev/mapper/rhel_example-swap rd.lvm.lv=rhel_example/root rd.lvm.lv=rhel_example/swap <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

11. Make a backup copy of the `kdump initramfs`:

    ```
    # cp /boot/initramfs-$(uname -r)kdump.img /boot/initramfs-$(uname -r)kdump.img.$(date +%m-%d-%H%M%S).bak
    ```

12. Append `rd.driver.blacklist=<_module_name_>` to the `KDUMP_COMMANDLINE_APPEND` setting in `/etc/sysconfig/kdump` to omit it from the `kdump initramfs`:

    ```
    # sed -i '/^KDUMP_COMMANDLINE_APPEND=/s/"$/ rd.driver.blacklist=module_name"/' /etc/sysconfig/kdump
    ```

13. Restart the `kdump` service to pick up the changes to the `kdump initrd`:

    ```
      # kdumpctl restart
    ```

14. Rebuild the `kdump` initial `ramdisk` image:

    ```
      # mkdumprd -f /boot/initramfs-$(uname -r)kdump.img
    ```

15. Reboot the system.

### Removing a module temporarily

You can remove a module temporarily.

Procedure

1. Run `modprobe` to remove any currently-loaded module:

   ```
   # modprobe -r <module name>
   ```

2. If the module cannot be unloaded, a process or another module might  still be using the module. If so, terminate the process and run the `modpole` command written above another time to unload the module.

## Appendix E: Legal notice

Certain portions of this text first appeared in [Red Hat Virtualization 4.4 Installing Red Hat Virtualization as a standalone Manager with local databases](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/installing_red_hat_virtualization_as_a_standalone_manager_with_local_databases/index). Copyright © 2022 Red Hat, Inc. Licensed under a [Creative Commons Attribution-ShareAlike 4.0 Unported License](https://creativecommons.org/licenses/by-sa/4.0/).

## a standalone Engine with remote databases

Standalone Engine installation is manual and customizable. You must  install a Enterprise Linux machine, then run the configuration script (`engine-setup`) and provide information about how you want to configure the oVirt  Engine. Add hosts and storage after the Engine is running. At least two  hosts are required for virtual machine high availability.

To install the Engine with a remote Engine database, manually create the database on the remote machine before running `engine-setup`. To install the Data Warehouse database on a remote machine, run the Data Warehouse configuration script (`ovirt-engine-dwh-setup`) on the remote machine. This script installs the Data Warehouse service  and can create the Data Warehouse database automatically.

See the [*Planning and Prerequisites Guide*](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index) for information on environment options and recommended configuration.

## oVirt Key Components

| Component Name | Description                                                  |
| -------------- | ------------------------------------------------------------ |
| oVirt Engine   | A  service that provides a graphical user interface and a REST API to  manage the resources in the environment. The Engine is installed on a  physical or virtual machine running Enterprise Linux. |
| Hosts          | Enterprise Linux hosts (Enterprise Linux hosts) and oVirt Nodes (image-based  hypervisors) are the two supported types of host. Hosts use Kernel-based Virtual Machine (KVM) technology and provide resources used to run  virtual machines. |
| Shared Storage | A storage service is used to store the data associated with virtual machines. |
| Data Warehouse | A service that collects configuration information and statistical data from the Engine. |

## Standalone Engine Architecture

The oVirt Engine runs on a physical server, or a virtual machine  hosted in a separate virtualization environment. A standalone Engine is  easier to deploy and manage, but requires an additional physical server. The Engine is only highly available when managed externally with a  product such as Red Hat’s High Availability Add-On.

The minimum setup for a standalone Engine environment includes:

- One oVirt Engine machine. The Engine is typically deployed on a  physical server. However, it can also be deployed on a virtual machine,  as long as that virtual machine is hosted in a separate environment. The Engine must run on Enterprise Linux 8.
- A minimum of two hosts for virtual machine high availability. You can use Enterprise Linux hosts or oVirt Nodes (oVirt Node). VDSM (the host  agent) runs on all hosts to facilitate communication with the oVirt  Engine.
- One storage service, which can be hosted locally or on a remote  server, depending on the storage type used. The storage service must be  accessible to all hosts.

![](../../../Image/r/RHV_STANDARD_ARCHITECTURE1.png)

Figure 1. Standalone Engine oVirt Architecture

## 1. Installation Overview

Installing a standalone Engine environment with remote databases involves the following steps:

1. [Install and configure the oVirt Engine:](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Installing_the_Red_Hat_Virtualization_Manager_SM_remoteDB_deploy)
   1. [Install two Enterprise Linux machines: one for the Engine, and one for the databases.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Installing_RHEL_for_RHVM_SM_remoteDB_deploy) The second machine will be referred to as the remote server.
   2. [Enable the oVirt Engine repositories.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Enabling_the_Red_Hat_Virtualization_Manager_Repositories_install_RHVM)
   3. [Manually configure the Engine database on the remote server.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_a_Remote_PostgreSQL_Database_install_RHVM) You can also use this procedure to manually configure the Data  Warehouse database if you do not want the Data Warehouse setup script to configure it automatically.
   4. [Configure the oVirt Engine using `engine-setup`.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Configuring_the_Red_Hat_Virtualization_Manager_install_RHVM)
   5. [Install the Data Warehouse service and database on the remote server.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Installing_and_Configuring_Data_Warehouse_on_a_Separate_Machine_install_RHVM)
   6. [Connect to the Administration Portal to add hosts and storage domains.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Connecting_to_the_Administration_Portal_install_RHVM)
2. [Install hosts to run virtual machines on:](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Installing_Hosts_for_RHV_SM_remoteDB_deploy)
   1. Use either host type, or both:
      - [oVirt Node](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Red_Hat_Virtualization_Hosts_SM_remoteDB_deploy)
      - [Enterprise Linux](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Red_Hat_Enterprise_Linux_hosts_SM_remoteDB_deploy)
   2. [Add the hosts to the Engine.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_standard_hosts_to_the_Manager_SM_remoteDB_deploy)
3. [Prepare storage to use for storage domains.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_Storage_for_RHV_SM_remoteDB_deploy) You can use one of the following storage types:
   - [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_NFS_Storage_SM_remoteDB_deploy)
   - [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_iSCSI_Storage_SM_remoteDB_deploy)
   - [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_FCP_Storage_SM_remoteDB_deploy)
   - [POSIX-compliant file system](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_POSIX_Storage_SM_remoteDB_deploy)
   - [Local storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_Local_Storage_SM_remoteDB_deploy)
   - [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_Red_Hat_Gluster_Storage_SM_remoteDB_deploy)
4. [Add storage domains to the Engine.](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_Storage_Domains_to_RHV_SM_remoteDB_deploy)

|      | Keep the environment up to date. Since bug fixes for known issues are frequently released, use scheduled tasks to update the hosts and the  Engine. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 2. Requirements

### 2.1. oVirt Engine Requirements

#### 2.1.1. Hardware Requirements

The minimum and recommended hardware requirements outlined here are  based on a typical small to medium-sized installation. The exact  requirements vary between deployments based on sizing and load.

The oVirt Engine runs on Enterprise Linux operating systems like [CentOS Linux](https://www.centos.org/) or [Red Hat Enterprise Linux](https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux).

| Resource          | Minimum                                                      | Recommended                                                  |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| CPU               | A dual core x86_64 CPU.                                      | A quad core x86_64 CPU or multiple dual core x86_64 CPUs.    |
| Memory            | 4 GB of available system RAM if Data Warehouse is not installed and if memory is not being consumed by existing processes. | 16 GB of system RAM.                                         |
| Hard Disk         | 25 GB of locally accessible, writable disk space.            | 50 GB of locally accessible, writable disk space. You can use the [RHV Engine History Database Size Calculator](https://access.redhat.com/labs/rhevmhdsc/) to calculate the appropriate disk space for the Engine history database size. |
| Network Interface | 1 Network Interface Card (NIC) with bandwidth of at least 1 Gbps. | 1 Network Interface Card (NIC) with bandwidth of at least 1 Gbps. |

#### 2.1.2. Browser Requirements

The following browser versions and operating systems can be used to access the Administration Portal and the VM Portal.

Browser testing is divided into tiers:

- Tier 1: Browser and operating system combinations that are fully tested.
- Tier 2: Browser and operating system combinations that are partially tested, and are likely to work.
- Tier 3: Browser and operating system combinations that are not tested, but may work.

| Support Tier | Operating System Family | Browser                                                      |
| ------------ | ----------------------- | ------------------------------------------------------------ |
| Tier 1       | Enterprise Linux        | Mozilla Firefox Extended Support Release (ESR) version       |
|              | Any                     | Most recent version of Google Chrome, Mozilla Firefox, or Microsoft Edge |
| Tier 2       |                         |                                                              |
| Tier 3       | Any                     | Earlier versions of Google Chrome or Mozilla Firefox         |
|              | Any                     | Other browsers                                               |

#### 2.1.3. Client Requirements

Virtual machine consoles can only be accessed using supported Remote Viewer (`virt-viewer`) clients on Enterprise Linux and Windows. To install `virt-viewer`, see [Installing Supporting Components on Client Machines](https://ovirt.org/documentation/virtual_machine_management_guide/index#sect-installing_supporting_components) in the *Virtual Machine Management Guide*. Installing `virt-viewer` requires Administrator privileges.

You can access virtual machine consoles using the SPICE, VNC, or RDP  (Windows only) protocols. You can install the QXLDOD graphical driver in the guest operating system to improve the functionality of SPICE. SPICE currently supports a maximum resolution of 2560x1600 pixels.

Client Operating System SPICE Support

Supported QXLDOD drivers are available on Enterprise Linux 7.2 and later, and Windows 10.

|      | SPICE may work with Windows 8 or 8.1 using QXLDOD drivers, but it is neither certified nor tested. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 2.1.4. Operating System Requirements

The oVirt Engine must be installed on a base installation of Enterprise Linux 8.6.

Do not install any additional packages after the base installation,  as they may cause dependency issues when attempting to install the  packages required by the Engine.

Do not enable additional repositories other than those required for the Engine installation.

### 2.2. Host Requirements

To use Enterprise Linux 9 on virtualization hosts, the UEFI Secure Boot option must be disabled due to [Bug 2081648 - dmidecode module fails to decode DMI data](https://bugzilla.redhat.com/show_bug.cgi?id=2081648).

#### 2.2.1. CPU Requirements

All CPUs must have support for the Intel® 64 or AMD64 CPU extensions, and the AMD-V™ or Intel VT® hardware virtualization extensions enabled. Support for the No eXecute flag (NX) is also required.

The following CPU models are supported:

- AMD
  - Opteron G4
  - Opteron G5
  - EPYC
- Intel
  - Nehalem
  - Westmere
  - SandyBridge
  - IvyBridge
  - Haswell
  - Broadwell
  - Skylake Client
  - Skylake Server
  - Cascadelake Server
- IBM
  - POWER8
  - POWER9

Enterprise Linux 9 doesn’t support virtualization for ppc64le: [CentOS Virtualization SIG](https://wiki.centos.org/SpecialInterestGroup/Virtualization) is working on re-introducing virtualization support but it’s not ready yet.

The oVirt project also provides packages for the 64-bit ARM architecture (ARM 64) but only as a Technology Preview.

For each CPU model with security updates, the **CPU Type** lists a basic type and a secure type. For example:

- **Intel Cascadelake Server Family**
- **Secure Intel Cascadelake Server Family**

The Secure CPU type contains the latest updates. For details, see BZ#[1731395](https://bugzilla.redhat.com/1731395)

##### Checking if a Processor Supports the Required Flags

You must enable virtualization in the BIOS. Power off and reboot the  host after this change to ensure that the change is applied.

Procedure

1. At the Enterprise Linux or oVirt Node boot screen, press any key and select the **Boot** or **Boot with serial console** entry from the list.

2. Press `Tab` to edit the kernel parameters for the selected option.

3. Ensure there is a space after the last kernel parameter listed, and append the parameter `rescue`.

4. Press `Enter` to boot into rescue mode.

5. At the prompt, determine that your processor has the required extensions and that they are enabled by running this command:

   ```
   # grep -E 'svm|vmx' /proc/cpuinfo | grep nx
   ```

If any output is shown, the processor is hardware virtualization  capable. If no output is shown, your processor may still support  hardware virtualization; in some circumstances manufacturers disable the virtualization extensions in the BIOS. If you believe this to be the  case, consult the system’s BIOS and the motherboard manual provided by  the manufacturer.

#### 2.2.2. Memory Requirements

The minimum required RAM is 2 GB. For cluster levels 4.2 to 4.5, the  maximum supported RAM per VM in oVirt Node is 6 TB. For cluster levels  4.6 to 4.7, the maximum supported RAM per VM in oVirt Node is 16 TB.

However, the amount of RAM required varies depending on guest  operating system requirements, guest application requirements, and guest memory activity and usage. KVM can also overcommit physical RAM for  virtualized guests, allowing you to provision guests with RAM  requirements greater than what is physically present, on the assumption  that the guests are not all working concurrently at peak load. KVM does  this by only allocating RAM for guests as required and shifting  underutilized guests into swap.

#### 2.2.3. Storage Requirements

Hosts require storage to store configuration, logs, kernel dumps, and for use as swap space. Storage can be local or network-based. oVirt  Node (oVirt Node) can boot with one, some, or all of its default  allocations  in network storage. Booting from network storage can result in a freeze if there is a network disconnect. Adding a drop-in  multipath configuration file can help address losses in network  connectivity. If oVirt Node boots from SAN storage and loses  connectivity, the files become read-only until network connectivity  restores. Using network storage might result in a performance downgrade.

The minimum storage requirements of oVirt Node are documented in this section. The storage requirements for Enterprise Linux hosts vary based on the amount of disk space used by their existing configuration but  are expected to be greater than those of oVirt Node.

The minimum storage requirements for host installation are listed  below. However, use the default allocations, which use more storage  space.

- / (root) - 6 GB
- /home - 1 GB
- /tmp - 1 GB
- /boot - 1 GB
- /var - 5 GB
- /var/crash - 10 GB
- /var/log - 8 GB
- /var/log/audit - 2 GB
- /var/tmp - 10 GB
- swap - 1 GB. See [What is the recommended swap size for Red Hat platforms?](https://access.redhat.com/solutions/15244) for details.
- Anaconda reserves 20% of the thin pool size within the volume group  for future metadata expansion. This is to prevent an out-of-the-box  configuration from running out of space under normal usage conditions.  Overprovisioning of thin pools during installation is also not  supported.
- **Minimum Total - 64 GiB**

If you are also installing the Engine Appliance for self-hosted engine installation, `/var/tmp` must be at least 10 GB.

If you plan to use memory overcommitment, add enough swap space to provide virtual memory for all of virtual machines. See [Memory Optimization](https://ovirt.org/documentation/administration_guide/index#Memory_Optimization).

#### 2.2.4. PCI Device Requirements

Hosts must have at least one network interface with a minimum  bandwidth of 1 Gbps. Each host should have two network interfaces, with  one dedicated to supporting network-intensive activities, such as  virtual machine migration. The performance of such operations is limited by the bandwidth available.

For information about how to use PCI Express and conventional PCI devices with Intel Q35-based virtual machines, see [*Using PCI Express and Conventional PCI Devices with the Q35 Virtual Machine*](https://access.redhat.com/articles/3201152).

#### 2.2.5. Device Assignment Requirements

If you plan to implement device assignment and PCI passthrough so  that a virtual machine can use a specific PCIe device from a host,  ensure the following requirements are met:

- CPU must support IOMMU (for example, VT-d or AMD-Vi). IBM POWER8 supports IOMMU by default.
- Firmware must support IOMMU.
- CPU root ports used must support ACS or ACS-equivalent capability.
- PCIe devices must support ACS or ACS-equivalent capability.
- All PCIe switches and bridges between the PCIe device and the root  port should support ACS. For example, if a switch does not support ACS,  all devices behind that switch share the same IOMMU group, and can only  be assigned to the same virtual machine.
- For GPU support, Enterprise Linux 8 supports PCI device assignment of PCIe-based NVIDIA K-Series Quadro (model 2000 series or higher), GRID,  and Tesla as non-VGA graphics devices. Currently up to two GPUs may be  attached to a virtual machine in addition to one of the standard,  emulated VGA interfaces. The emulated VGA is used for pre-boot and  installation and the NVIDIA GPU takes over when the NVIDIA graphics  drivers are loaded. Note that the NVIDIA Quadro 2000 is not supported,  nor is the Quadro K420 card.

Check vendor specification and datasheets to confirm that your hardware meets these requirements. The `lspci -v` command can be used to print information for PCI devices already installed on a system.

#### 2.2.6. vGPU Requirements

A host must meet the following requirements in order for virtual machines on that host to use a vGPU:

- vGPU-compatible GPU
- GPU-enabled host kernel
- Installed GPU with correct drivers
- Select a vGPU type and the number of instances that you would like to use with this virtual machine using the **Manage vGPU** dialog in the **Administration Portal** **Host Devices** tab of the virtual machine.
- vGPU-capable drivers installed on each host in the cluster
- vGPU-supported virtual machine operating system with vGPU drivers installed

### 2.3. Networking requirements

#### 2.3.1. General requirements

oVirt requires IPv6 to remain enabled on the physical or virtual machine running the Engine. [Do not disable IPv6](https://access.redhat.com/solutions/8709) on the Engine machine, even if your systems do not use it.

#### 2.3.2. Firewall Requirements for DNS, NTP, and IPMI Fencing

The firewall requirements for all of the following topics are special cases that require individual consideration.

DNS and NTP

oVirt does not create a DNS or NTP server, so the firewall does not need to have open ports for incoming traffic.

By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, define  exceptions for requests that are sent to DNS and NTP servers.

|      | The oVirt Engine and all hosts (oVirt Node and Enterprise Linux host) must have a fully qualified domain name and full, perfectly-aligned  forward and reverse name resolution.  Running a DNS service as a virtual machine in the oVirt environment  is not supported. All DNS services the oVirt environment uses must be  hosted outside of the environment.  Use DNS instead of the `/etc/hosts` file for name resolution. Using a hosts file typically requires more work and has a greater chance for errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

IPMI and Other Fencing Mechanisms (optional)

For IPMI (Intelligent Platform Management Interface) and other  fencing mechanisms, the firewall does not need to have open ports for  incoming traffic.

By default, Enterprise Linux allows outbound IPMI traffic to ports on any destination address. If you disable outgoing traffic, make  exceptions for requests being sent to your IPMI or fencing servers.

Each oVirt Node and Enterprise Linux host in the cluster must be able to connect to the fencing devices of all other hosts in the cluster. If the cluster hosts are experiencing an error (network error, storage  error…) and cannot function as hosts, they must be able to connect to  other hosts in the data center.

The specific port number depends on the type of the fence agent you are using and how it is configured.

The firewall requirement tables in the following sections do not represent this option.

#### 2.3.3. oVirt Engine Firewall Requirements

The oVirt Engine requires that a number of ports be opened to allow network traffic through the system’s firewall.

The `engine-setup` script can configure the firewall automatically.

The firewall configuration documented here assumes a default configuration.

| ID   | Port(s) | Protocol | Source                                                       | Destination                                    | Purpose                                                      | Encrypted by default                               |
| ---- | ------- | -------- | ------------------------------------------------------------ | ---------------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------- |
| M1   | -       | ICMP     | oVirt Nodes Enterprise Linux hosts                           | oVirt Engine                                   | Optional. May help in diagnosis.                             | No                                                 |
| M2   | 22      | TCP      | System(s) used for maintenance of the Engine including backend configuration, and software upgrades. | oVirt Engine                                   | Secure Shell (SSH) access. Optional.                         | Yes                                                |
| M3   | 2222    | TCP      | Clients accessing virtual machine serial consoles.           | oVirt Engine                                   | Secure Shell (SSH) access to enable connection to virtual machine serial consoles. | Yes                                                |
| M4   | 80, 443 | TCP      | Administration Portal clients VM Portal clients oVirt Nodes Enterprise Linux hosts REST API clients | oVirt Engine                                   | Provides HTTP (port 80, not encrypted) and HTTPS (port 443, encrypted) access to the Engine. HTTP redirects connections to HTTPS. | Yes                                                |
| M5   | 6100    | TCP      | Administration Portal clients VM Portal clients              | oVirt Engine                                   | Provides websocket proxy access for a web-based console client, `noVNC`, when the websocket proxy is running on the Engine. | No                                                 |
| M6   | 7410    | UDP      | oVirt Nodes Enterprise Linux hosts                           | oVirt Engine                                   | If Kdump is enabled on the hosts, open this port for the fence_kdump listener on the Engine. See [fence_kdump Advanced Configuration](https://ovirt.org/documentation/administration_guide/index#sect-fence_kdump_Advanced_Configuration). `fence_kdump` doesn’t provide a way to encrypt the connection. However, you can  manually configure this port to block access from hosts that are not  eligible. | No                                                 |
| M7   | 54323   | TCP      | Administration Portal clients                                | oVirt Engine (`ovirt-imageio` service)         | Required for communication with the `ovirt-imageo` service.  | Yes                                                |
| M8   | 6642    | TCP      | oVirt Nodes Enterprise Linux hosts                           | Open Virtual Network (OVN) southbound database | Connect to Open Virtual Network (OVN) database               | Yes                                                |
| M9   | 9696    | TCP      | Clients of external network provider for OVN                 | External network provider for OVN              | OpenStack Networking API                                     | Yes, with configuration generated by engine-setup. |
| M10  | 35357   | TCP      | Clients of external network provider for OVN                 | External network provider for OVN              | OpenStack Identity API                                       | Yes, with configuration generated by engine-setup. |
| M11  | 53      | TCP, UDP | oVirt Engine                                                 | DNS Server                                     | DNS lookup requests from ports above 1023 to port 53, and responses. Open by default. | No                                                 |
| M12  | 123     | UDP      | oVirt Engine                                                 | NTP Server                                     | NTP requests from ports above 1023 to port 123, and responses.  Open by default. | No                                                 |

|      | A port for the OVN northbound database (6641) is not listed because,  in the default configuration, the only client for the OVN northbound  database (6641) is `ovirt-provider-ovn`. Because they both run on the same host, their communication is not visible to the network.  By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, make  exceptions for the Engine to send requests to DNS and NTP servers. Other nodes may also require DNS and NTP. In that case, consult the  requirements for those nodes and configure the firewall accordingly. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 2.3.4. Host Firewall Requirements

Enterprise Linux hosts and oVirt Nodes (oVirt Node) require a number  of ports to be opened to allow network traffic through the system’s  firewall. The firewall rules are automatically configured by default  when adding a new host to the Engine, overwriting any pre-existing  firewall configuration.

To disable automatic firewall configuration when adding a new host, clear the **Automatically configure host firewall** check box under **Advanced Parameters**.

| ID   | Port(s)       | Protocol | Source                                          | Destination                        | Purpose                                                      | Encrypted by default                                         |
| ---- | ------------- | -------- | ----------------------------------------------- | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| H1   | 22            | TCP      | oVirt Engine                                    | oVirt Nodes Enterprise Linux hosts | Secure Shell (SSH) access. Optional.                         | Yes                                                          |
| H2   | 2223          | TCP      | oVirt Engine                                    | oVirt Nodes Enterprise Linux hosts | Secure Shell (SSH) access to enable connection to virtual machine serial consoles. | Yes                                                          |
| H3   | 161           | UDP      | oVirt Nodes Enterprise Linux hosts              | oVirt Engine                       | Simple network management protocol (SNMP). Only required if you want Simple  Network Management Protocol traps sent from the host to one or more  external SNMP managers. Optional. | No                                                           |
| H4   | 111           | TCP      | NFS storage server                              | oVirt Nodes Enterprise Linux hosts | NFS connections. Optional.                                   | No                                                           |
| H5   | 5900 - 6923   | TCP      | Administration Portal clients VM Portal clients | oVirt Nodes Enterprise Linux hosts | Remote guest console access via VNC and SPICE. These ports must be open to facilitate client access to virtual machines. | Yes (optional)                                               |
| H6   | 5989          | TCP, UDP | Common Information Model Object Manager (CIMOM) | oVirt Nodes Enterprise Linux hosts | Used by Common Information Model Object Managers (CIMOM) to monitor virtual  machines running on the host. Only required if you want to use a CIMOM  to monitor the virtual machines in your virtualization environment. Optional. | No                                                           |
| H7   | 9090          | TCP      | oVirt Engine Client machines                    | oVirt Nodes Enterprise Linux hosts | Required to access the Cockpit web interface, if installed.  | Yes                                                          |
| H8   | 16514         | TCP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Virtual machine migration using **libvirt**.                 | Yes                                                          |
| H9   | 49152 - 49215 | TCP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Virtual machine migration and fencing using VDSM. These ports must be open to  facilitate both automated and manual migration of virtual machines. | Yes. Depending on agent for fencing, migration is done through libvirt. |
| H10  | 54321         | TCP      | oVirt Engine oVirt Nodes Enterprise Linux hosts | oVirt Nodes Enterprise Linux hosts | VDSM communications with the Engine and other virtualization hosts. | Yes                                                          |
| H11  | 54322         | TCP      | oVirt Engine `ovirt-imageio` service            | oVirt Nodes Enterprise Linux hosts | Required for communication with the `ovirt-imageo` service.  | Yes                                                          |
| H12  | 6081          | UDP      | oVirt Nodes Enterprise Linux hosts              | oVirt Nodes Enterprise Linux hosts | Required, when Open Virtual Network (OVN) is used as a network provider, to allow OVN to create tunnels between hosts. | No                                                           |
| H13  | 53            | TCP, UDP | oVirt Nodes Enterprise Linux hosts              | DNS Server                         | DNS lookup requests from ports above 1023 to port 53, and responses. This port is required and open by default. | No                                                           |
| H14  | 123           | UDP      | oVirt Nodes Enterprise Linux hosts              | NTP Server                         | NTP requests from ports above 1023 to port 123, and responses. This port is required and open by default. |                                                              |
| H15  | 4500          | TCP, UDP | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |
| H16  | 500           | UDP      | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |
| H17  | -             | AH, ESP  | oVirt Nodes                                     | oVirt Nodes                        | Internet Security Protocol (IPSec)                           | Yes                                                          |

|      | By default, Enterprise Linux allows outbound traffic to DNS and NTP  on any destination address. If you disable outgoing traffic, make  exceptions for the oVirt Nodes  Enterprise Linux hosts to send requests to DNS and NTP servers. Other nodes may also require DNS and NTP. In that case, consult the  requirements for those nodes and configure the firewall accordingly. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 2.3.5. Database Server Firewall Requirements

oVirt supports the use of a remote database server for the Engine database (`engine`) and the Data Warehouse database (`ovirt-engine-history`). If you plan to use a remote database server, it must allow connections  from the Engine and the Data Warehouse service (which can be separate  from the Engine).

Similarly, if you plan to access a local or remote Data Warehouse  database from an external system, the database must allow connections  from that system.

|      | Accessing the Engine database from external systems is not supported. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| ID   | Port(s) | Protocol | Source                              | Destination                                                  | Purpose                                           | Encrypted by default                                         |
| ---- | ------- | -------- | ----------------------------------- | ------------------------------------------------------------ | ------------------------------------------------- | ------------------------------------------------------------ |
| D1   | 5432    | TCP, UDP | oVirt Engine Data Warehouse service | Engine (`engine`) database server Data Warehouse (`ovirt-engine-history`) database server | Default port for PostgreSQL database connections. | [No, but can be enabled](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH). |
| D2   | 5432    | TCP, UDP | External systems                    | Data Warehouse (`ovirt-engine-history`) database server      | Default port for PostgreSQL database connections. | Disabled by default. [No, but can be enabled](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Migrating_the_Data_Warehouse_Database_to_a_Separate_Machine_migrate_DWH). |

#### 2.3.6. Maximum Transmission Unit Requirements

The recommended Maximum Transmission Units (MTU) setting for Hosts  during deployment is 1500. It is possible to update this setting after  the environment is set up to a different MTU. For more information on  changing the MTU setting, see [How to change the Hosted Engine VM network MTU](https://access.redhat.com/solutions/4129641).

## 3. Installing the oVirt Engine

### 3.1. Installing the oVirt Engine Machine and the Remote Server

1. The oVirt Engine must run on Enterprise Linux 8. For detailed installation instructions, see [*Performing a standard EL installation*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/performing_a_standard_rhel_installation/index).

   This machine must meet the minimum [Engine hardware requirements](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#hardware-requirements_SM_remoteDB_deploy).

2. Install a second Enterprise Linux machine to use for the databases. This machine will be referred to as the remote server.

### 3.2. Enabling the oVirt Engine Repositories

Ensure the correct repositories are enabled.

For oVirt 4.5: If you are going to install on RHEL or derivatives please follow [Installing on RHEL or derivatives](https://www.ovirt.org/download/install_on_rhel.html) first.

```
# dnf install -y centos-release-ovirt45
```

For oVirt 4.4:

```
# dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

Common procedure valid for both 4.4 and 4.5:

You can check which repositories are currently enabled by running `dnf repolist`.

1. Enable the `javapackages-tools` module.

   ```
   # dnf module -y enable javapackages-tools
   ```

2. Enable the `pki-deps` module.

   ```
   # dnf module -y enable pki-deps
   ```

3. Enable version 12 of the `postgresql` module.

   ```
   # dnf module -y enable postgresql:12
   ```

4. Enable version 2.3 of the `mod_auth_openidc` module.

   ```
   # dnf module -y enable mod_auth_openidc:2.3
   ```

5. Enable version 14 of the `nodejs` module:

   ```
   # dnf module -y enable nodejs:14
   ```

6. Synchronize installed packages to update them to the latest available versions.

   ```
   # dnf distro-sync --nobest
   ```

   Additional resources

   For information on modules and module streams, see the following sections in *Installing, managing, and removing user-space components*

   - [Module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#module-streams_introduction-to-modules)
   - [Selecting a stream before installation of packages](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#selecting-a-stream-before-installation-of-packages_installing-rhel-8-content)
   - [Resetting module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#resetting-module-streams_removing-rhel-8-content)
   - [Switching to a later stream](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#switching-to-a-later-stream_managing-versions-of-appstream-content)

Before configuring the oVirt Engine, you must manually configure the  Engine database on the remote server. You can also use this procedure to manually configure the Data Warehouse database if you do not want the  Data Warehouse setup script to configure it automatically.

### 3.3. Preparing a Remote PostgreSQL Database

In a remote database environment, you must create the Engine database manually before running `engine-setup`.

|      | The `engine-setup` and `engine-backup --mode=restore` commands only support system error messages in the `en_US.UTF8` locale, even if the system locale is different.  The locale settings in the `postgresql.conf` file must be set to `en_US.UTF8`. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | The database name must contain only numbers, underscores, and lowercase letters. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### Enabling the oVirt Engine Repositories

Ensure the correct repositories are enabled.

For oVirt 4.5: If you are going to install on RHEL or derivatives please follow [Installing on RHEL or derivatives](https://www.ovirt.org/download/install_on_rhel.html) first.

```
# dnf install -y centos-release-ovirt45
```

For oVirt 4.4:

```
# dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

Common procedure valid for both 4.4 and 4.5:

You can check which repositories are currently enabled by running `dnf repolist`.

1. Enable the `javapackages-tools` module.

   ```
   # dnf module -y enable javapackages-tools
   ```

2. Enable version 12 of the `postgresql` module.

   ```
   # dnf module -y enable postgresql:12
   ```

3. Enable version 2.3 of the `mod_auth_openidc` module.

   ```
   # dnf module -y enable mod_auth_openidc:2.3
   ```

4. Enable version 14 of the `nodejs` module:

   ```
   # dnf module -y enable nodejs:14
   ```

5. Synchronize installed packages to update them to the latest available versions.

   ```
   # dnf distro-sync --nobest
   ```

   Additional resources

   For information on modules and module streams, see the following sections in *Installing, managing, and removing user-space components*

   - [Module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#module-streams_introduction-to-modules)
   - [Selecting a stream before installation of packages](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#selecting-a-stream-before-installation-of-packages_installing-rhel-8-content)
   - [Resetting module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#resetting-module-streams_removing-rhel-8-content)
   - [Switching to a later stream](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#switching-to-a-later-stream_managing-versions-of-appstream-content)

#### Initializing the PostgreSQL Database

1. Install the PostgreSQL server package:

   ```
   # dnf install postgresql-server postgresql-contrib
   ```

2. Initialize the PostgreSQL database instance:

   ```
   # postgresql-setup --initdb
   ```

3. Enable the `postgresql` service and configure it to start when the machine boots:

   ```
   # systemctl enable postgresql
   # systemctl start postgresql
   ```

4. Connect to the `psql` command line interface as the `postgres` user:

   ```
   # su - postgres -c psql
   ```

5. Create a default user. The Engine’s default user is `engine`:

   ```
   postgres=# create role user_name with login encrypted password 'password';
   ```

6. Create a database. The Engine’s default database name is `engine`:

   ```
   postgres=# create database database_name owner user_name template template0 encoding 'UTF8' lc_collate 'en_US.UTF-8' lc_ctype 'en_US.UTF-8';
   ```

7. Connect to the new database:

   ```
   postgres=# \c database_name
   ```

8. Add the `uuid-ossp` extension:

   ```
   database_name=# CREATE EXTENSION "uuid-ossp";
   ```

9. Add the `plpgsql` language if it does not exist:

   ```
   database_name=# CREATE LANGUAGE plpgsql;
   ```

10. Quit the `psql` interface:

    ```
    database_name=# \q
    ```

11. Edit the `/var/lib/pgsql/data/pg_hba.conf` file to enable  md5 client authentication, so that the engine can access the database  remotely. Add the following line immediately below the line that starts  with `local` at the bottom of the file. Replace `X.X.X.X` with the IP address of the Engine or Data Warehouse machine, and replace `0-32` or `0-128` with the CIDR mask length:

    ```
    host    database_name    user_name    X.X.X.X/0-32    md5
    host    database_name    user_name    X.X.X.X::/0-128   md5
    ```

    For example:

    ```
    # IPv4, 32-bit address:
    host    engine    engine    192.168.12.10/32    md5
    
    # IPv6, 128-bit address:
    host    engine    engine    fe80::7a31:c1ff:0000:0000/96   md5
    ```

12. Allow TCP/IP connections to the database. Edit the `/var/lib/pgsql/data/postgresql.conf` file and add the following line:

    ```
    listen_addresses='*'
    ```

    This example configures the `postgresql` service to listen for connections on all interfaces. You can specify an interface by giving its IP address.

13. Update the PostgreSQL server’s configuration. In the `/var/lib/pgsql/data/postgresql.conf` file, add the following lines to the bottom of the file:

    ```
    autovacuum_vacuum_scale_factor=0.01
    autovacuum_analyze_scale_factor=0.075
    autovacuum_max_workers=6
    maintenance_work_mem=65536
    max_connections=150
    work_mem=8192
    ```

14. Open the default port used for PostgreSQL database connections, and save the updated firewall rules:

    ```
    # firewall-cmd --zone=public --add-service=postgresql
    # firewall-cmd --permanent --zone=public --add-service=postgresql
    ```

15. Restart the `postgresql` service:

    ```
    # systemctl restart postgresql
    ```

16. Optionally, set up [SSL](https://www.postgresql.org/docs/12/ssl-tcp.html#SSL-FILE-USAGE) to secure database connections.

### 3.4. Installing and Configuring the oVirt Engine

Install the package and dependencies for the oVirt Engine, and configure it using the `engine-setup` command. The script asks you a series of questions and, after you  provide the required values for all questions, applies that  configuration and starts the `ovirt-engine` service.

|      | The `engine-setup` command guides you through several  distinct configuration stages, each comprising several steps that  require user input. Suggested configuration defaults are provided in  square brackets; if the suggested value is acceptable for a given step,  press `Enter` to accept that value.  You can run `engine-setup --accept-defaults` to  automatically accept all questions that have default answers. This  option should be used with caution and only if you are familiar with `engine-setup`. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Ensure all packages are up to date:

   ```
   # dnf upgrade --nobest
   ```

   |      | Reboot the machine if any kernel-related packages were updated. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

2. Install the `ovirt-engine` package and dependencies.

   ```
   # dnf install ovirt-engine
   ```

3. Run the `engine-setup` command to begin configuring the oVirt Engine:

   ```
   # engine-setup
   ```

4. Optional: Type **Yes** and press `Enter` to set up Cinderlib integration on this machine:

   ```
   Set up Cinderlib integration
   (Currently in tech preview)
   (Yes, No) [No]:
   ```

   |      | Cinderlib is a Technology Preview feature only. Technology Preview  features are not supported with Red Hat production service level  agreements (SLAs), might not be functionally complete, and Red Hat does  not recommend to use them for production. These features provide early  access to upcoming product features, enabling customers to test  functionality and provide feedback during the development process. For more information on Red Hat Technology Preview features support  scope, see [Red Hat Technology Preview Features Support Scope](https://access.redhat.com/support/offerings/techpreview/). |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

5. Press `Enter` to configure the Engine on this machine:

   ```
   Configure Engine on this host (Yes, No) [Yes]:
   ```

6. Optional: Install Open Virtual Network (OVN). Selecting `Yes` installs an OVN server on the Engine machine and adds it to oVirt as an external network provider. This action also configures the Default  cluster to use OVN as its default network provider.

   |      | Also see the "Next steps" in [Adding Open Virtual Network (OVN) as an External Network Provider](https://ovirt.org/documentation/administration_guide/index#Installing-OVN-next-steps) in the *Administration Guide*. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   ```
   Configuring ovirt-provider-ovn also sets the Default cluster’s default network provider to ovirt-provider-ovn.
   Non-Default clusters may be configured with an OVN after installation.
   Configure ovirt-provider-ovn (Yes, No) [Yes]:
   ```

   For more information on using OVN networks in oVirt, see [Adding Open Virtual Network (OVN) as an External Network Provider](https://ovirt.org/documentation/administration_guide/index#Adding_OVN_as_an_External_Network_Provider) in the *Administration Guide*.

7. Optional: Allow `engine-setup` to configure a WebSocket Proxy server for allowing users to connect to virtual machines through the `noVNC` console:

   ```
   Configure WebSocket Proxy on this machine? (Yes, No) [Yes]:
   ```

8. To configure Data Warehouse on a remote server, answer `No` and see [Installing and Configuring Data Warehouse on a Separate Machine](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Installing_and_Configuring_Data_Warehouse_on_a_Separate_Machine_install_RHVM) after completing the Engine configuration.

   ```
   Please note: Data Warehouse is required for the engine. If you choose to not configure it on this host, you have to configure it on a remote host, and then configure the engine on this host so that it can access the database of the remote Data Warehouse host.
   Configure Data Warehouse on this host (Yes, No) [Yes]:
   ```

   |      | oVirt only supports installing the Data Warehouse database, the Data  Warehouse service, and Grafana all on the same machine as each other. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

9. To configure Grafana on the same machine as the Data Warehouse service, enter `No`:

   ```
   Configure Grafana on this host (Yes, No) [Yes]:
   ```

10. Optional: Allow access to a virtual machine’s serial console from the command line.

    ```
    Configure VM Console Proxy on this host (Yes, No) [Yes]:
    ```

    Additional configuration is required on the client machine to use this feature. See [Opening a Serial Console to a Virtual Machine](https://ovirt.org/documentation/virtual_machine_management_guide/index#Opening_a_Serial_Console_to_a_Virtual_Machine) in the *Virtual Machine Management Guide*.

11. Press `Enter` to accept the automatically detected host name, or enter an alternative host name and press `Enter`. Note that the automatically detected host name may be incorrect if you are using virtual hosts.

    ```
    Host fully qualified DNS name of this server [autodetected host name]:
    ```

12. The `engine-setup` command checks your firewall  configuration and offers to open the ports used by the Engine for  external communication, such as ports 80 and 443. If you do not allow `engine-setup` to modify your firewall configuration, you must manually open the ports used by the Engine. `firewalld` is configured as the firewall manager.

    ```
    Setup can automatically configure the firewall on this system.
    Note: automatic configuration of the firewall may overwrite current settings.
    Do you want Setup to configure the firewall? (Yes, No) [Yes]:
    ```

    If you choose to automatically configure the firewall, and no  firewall managers are active, you are prompted to select your chosen  firewall manager from a list of supported options. Type the name of the  firewall manager and press `Enter`. This applies even in cases where only one option is listed.

13. Specify whether to configure the Engine database on this machine, or on another machine:

    ```
    Where is the Engine database located? (Local, Remote) [Local]:
    ```

    |      | Deployment with a remote engine database is now deprecated. This functionality will be removed in a future release. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    If you select `Remote`, input the following values for the preconfigured remote database server. Replace `localhost` with the ip address or FQDN of the remote database server:

    ```
    Engine database host [localhost]:
    Engine database port [5432]:
    Engine database secured connection (Yes, No) [No]:
    Engine database name [engine]:
    Engine database user [engine]:
    Engine database password:
    ```

14. Set a password for the automatically created administrative user of the oVirt Engine:

    ```
    Engine admin password:
    Confirm engine admin password:
    ```

15. Select **Gluster**, **Virt**, or **Both**:

    ```
    Application mode (Both, Virt, Gluster) [Both]:
    ```

    - **Both** - offers the greatest flexibility. In most cases, select **Both**.
    - **Virt** - allows you to run virtual machines in the environment.
    - **Gluster** - only allows you to manage GlusterFS from the Administration Portal.

16. If you installed the OVN provider, you can choose to use the default credentials, or specify an alternative.

    ```
    Use default credentials (admin@internal) for ovirt-provider-ovn (Yes, No) [Yes]:
    oVirt OVN provider user[admin@internal]:
    oVirt OVN provider password:
    ```

17. Set the default value for the `wipe_after_delete` flag, which wipes the blocks of a virtual disk when the disk is deleted.

    ```
    Default SAN wipe after delete (Yes, No) [No]:
    ```

18. The Engine uses certificates to communicate securely with its hosts.  This certificate can also optionally be used to secure HTTPS  communications with the Engine. Provide the organization name for the  certificate:

    ```
    Organization name for certificate [autodetected domain-based name]:
    ```

19. Optionally allow `engine-setup` to make the landing page of the Engine the default page presented by the Apache web server:

    ```
    Setup can configure the default page of the web server to present the application home page. This may conflict with existing applications.
    Do you wish to set the application as the default web page of the server? (Yes, No) [Yes]:
    ```

20. By default, external SSL (HTTPS) communication with the Engine is  secured with the self-signed certificate created earlier in the  configuration to securely communicate with hosts. Alternatively, choose  another certificate for external HTTPS connections; this does not affect how the Engine communicates with hosts:

    ```
    Setup can configure apache to use SSL using a certificate issued from the internal CA.
    Do you wish Setup to configure that, or prefer to perform that manually? (Automatic, Manual) [Automatic]:
    ```

21. You can specify a unique password for the Grafana admin user, or use same one as the Engine admin password:

    ```
    Use Engine admin password as initial Grafana admin password (Yes, No) [Yes]:
    ```

22. Review the installation settings, and press `Enter` to accept the values and proceed with the installation:

    ```
    Please confirm installation settings (OK, Cancel) [OK]:
    ```

When your environment has been configured, `engine-setup` displays details about how to access your environment.

Next steps

If you chose to manually configure the firewall, `engine-setup` provides a custom list of ports that need to be opened, based on the options selected during setup. `engine-setup` also saves your answers to a file that can be used to reconfigure the  Engine using the same values, and outputs the location of the log file  for the oVirt Engine configuration process.

- If you intend to link your oVirt environment with a directory server, configure the date and time to synchronize with the system clock used  by the directory server to avoid unexpected account expiry issues. See [Synchronizing the System Clock with a Remote Server](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-Configuring_the_Date_and_Time.html#sect-Configuring_the_Date_and_Time-timedatectl-NTP) in the *Enterprise Linux System Administrator’s Guide* for more information.
- Install the certificate authority according to the instructions  provided by your browser. You can get the certificate authority’s  certificate by navigating to `http://<manager-fqdn>/ovirt-engine/services/pki-resource?resource=ca-certificate&format=X509-PEM-CA`, replacing <manager-fqdn> with the FQDN that you provided during the installation.

Install the Data Warehouse service and database on the remote server:

### 3.5. Installing and Configuring Data Warehouse on a Separate Machine

This section describes installing and configuring the Data Warehouse  service on a separate machine from the oVirt Engine. Installing Data  Warehouse on a separate machine helps to reduce the load on the Engine  machine.

|      | oVirt only supports installing the Data Warehouse database, the Data  Warehouse service and Grafana all on the same machine as each other,  even though you can install each of these components on separate  machines from each other. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

- The oVirt Engine is installed on a separate machine.
- A physical server or virtual machine running Enterprise Linux 8.
- The Engine database password.

#### Enabling the oVirt Engine Repositories

Ensure the correct repositories are enabled.

For oVirt 4.5: If you are going to install on RHEL or derivatives please follow [Installing on RHEL or derivatives](https://www.ovirt.org/download/install_on_rhel.html) first.

```
# dnf install -y centos-release-ovirt45
```

For oVirt 4.4:

```
# dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

Common procedure valid for both 4.4 and 4.5:

You can check which repositories are currently enabled by running `dnf repolist`.

1. Enable the `javapackages-tools` module.

   ```
   # dnf module -y enable javapackages-tools
   ```

2. Enable the `pki-deps` module.

   ```
   # dnf module -y enable pki-deps
   ```

3. Enable version 12 of the `postgresql` module.

   ```
   # dnf module -y enable postgresql:12
   ```

4. Enable version 2.3 of the `mod_auth_openidc` module.

   ```
   # dnf module -y enable mod_auth_openidc:2.3
   ```

5. Enable version 14 of the `nodejs` module:

   ```
   # dnf module -y enable nodejs:14
   ```

6. Synchronize installed packages to update them to the latest available versions.

   ```
   # dnf distro-sync --nobest
   ```

   Additional resources

   For information on modules and module streams, see the following sections in *Installing, managing, and removing user-space components*

   - [Module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#module-streams_introduction-to-modules)
   - [Selecting a stream before installation of packages](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#selecting-a-stream-before-installation-of-packages_installing-rhel-8-content)
   - [Resetting module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#resetting-module-streams_removing-rhel-8-content)
   - [Switching to a later stream](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#switching-to-a-later-stream_managing-versions-of-appstream-content)

#### Installing Data Warehouse on a Separate Machine

Procedure

1. Log in to the machine where you want to install the database.

2. Ensure that all packages are up to date:

   ```
   # dnf upgrade --nobest
   ```

3. Install the `ovirt-engine-dwh-setup` package:

   ```
   # dnf install ovirt-engine-dwh-setup
   ```

4. Run the `engine-setup` command to begin the installation:

   ```
   # engine-setup
   ```

5. Answer `Yes` to install Data Warehouse on this machine:

   ```
   Configure Data Warehouse on this host (Yes, No) [Yes]:
   ```

6. Answer `Yes` to install Grafana on this machine:

   ```
   Configure Grafana on this host (Yes, No) [Yes]:
   ```

7. Press `Enter` to accept the automatically-detected host name, or enter an alternative host name and press `Enter`:

   ```
   Host fully qualified DNS name of this server [autodetected hostname]:
   ```

8. Press `Enter` to automatically configure the firewall, or type `No` and press `Enter` to maintain existing settings:

   ```
   Setup can automatically configure the firewall on this system.
   Note: automatic configuration of the firewall may overwrite current settings.
   Do you want Setup to configure the firewall? (Yes, No) [Yes]:
   ```

   If you choose to automatically configure the firewall, and no  firewall managers are active, you are prompted to select your chosen  firewall manager from a list of supported options. Type the name of the  firewall manager and press `Enter`. This applies even in cases where only one option is listed.

9. Enter the fully qualified domain name of the Engine machine, and then press `Enter`:

   ```
   Host fully qualified DNS name of the engine server []:
   ```

10. Press `Enter` to allow setup to sign the certificate on the Engine via SSH:

    ```
    Setup will need to do some actions on the remote engine server. Either automatically, using ssh as root to access it, or you will be prompted to manually perform each such action.
    Please choose one of the following:
    1 - Access remote engine server using ssh as root
    2 - Perform each action manually, use files to copy content around
    (1, 2) [1]:
    ```

11. Press `Enter` to accept the default SSH port, or enter an alternative port number and then press `Enter`:

    ```
    ssh port on remote engine server [22]:
    ```

12. Enter the root password for the Engine machine:

    ```
    root password on remote engine server manager.example.com:
    ```

13. Specify whether to host the Data Warehouse database on this machine (Local), or on another machine (Remote).:

    |      | oVirt only supports installing the Data Warehouse database, the Data  Warehouse service and Grafana all on the same machine as each other,  even though you can install each of these components on separate  machines from each other. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    ```
    Where is the DWH database located? (Local, Remote) [Local]:
    ```

    - If you select `Local`, the `engine-setup`  script can configure your database automatically (including adding a  user and a database), or it can connect to a preconfigured local  database:

      ```
      Setup can configure the local postgresql server automatically for the DWH to run. This may conflict with existing applications.
      Would you like Setup to automatically configure postgresql and create DWH database, or prefer to perform that manually? (Automatic, Manual) [Automatic]:
      ```

      - If you select `Automatic` by pressing `Enter`, no further action is required here.

      - If you select `Manual`, input the following values for the manually-configured local database:

        ```
        DWH database secured connection (Yes, No) [No]:
        DWH database name [ovirt_engine_history]:
        DWH database user [ovirt_engine_history]:
        DWH database password:
        ```

    - If you select `Remote`, you are prompted to provide  details about the remote database host. Input the following values for  the preconfigured remote database host:

      ```
      DWH database host []: dwh-db-fqdn
      DWH database port [5432]:
      DWH database secured connection (Yes, No) [No]:
      DWH database name [ovirt_engine_history]:
      DWH database user [ovirt_engine_history]:
      DWH database password: password
      ```

    - If you select `Remote`, you are prompted to enter the username and password for the Grafana database user:

      ```
      Grafana database user [ovirt_engine_history_grafana]:
      Grafana database password:
      ```

14. Enter the fully qualified domain name and password for the Engine  database machine. If you are installing the Data Warehouse database on  the same machine where the Engine database is installed, use the same  FQDN. Press `Enter` to accept the default values in each other field:

    ```
    Engine database host []: engine-db-fqdn
    Engine database port [5432]:
    Engine database secured connection (Yes, No) [No]:
    Engine database name [engine]:
    Engine database user [engine]:
    Engine database password: password
    ```

15. Choose how long Data Warehouse will retain collected data:

    ```
    Please choose Data Warehouse sampling scale:
    (1) Basic
    (2) Full
    (1, 2)[1]:
    ```

    `Full` uses the default values for the data storage settings listed in [Application Settings for the Data Warehouse service in ovirt-engine-dwhd.conf](https://ovirt.org/documentation/data_warehouse_guide/index#Application_Settings_for_the_Data_Warehouse_service_in_ovirt-engine-dwhd_file) (recommended when Data Warehouse is installed on a remote host).

    `Basic` reduces the values of `DWH_TABLES_KEEP_HOURLY` to `720` and `DWH_TABLES_KEEP_DAILY` to `0`, easing the load on the Engine machine. Use `Basic` when the Engine and Data Warehouse are installed on the same machine.

16. Confirm your installation settings:

    ```
    Please confirm installation settings (OK, Cancel) [OK]:
    ```

17. After the Data Warehouse configuration is complete, on the oVirt Engine, restart the `ovirt-engine` service:

    ```
    # systemctl restart ovirt-engine
    ```

18. Optionally, set up [SSL](https://www.postgresql.org/docs/12/ssl-tcp.html#SSL-FILE-USAGE) to secure database connections.

Log in to the Administration Portal, where you can add hosts and storage to the environment:

### 3.6. Connecting to the Administration Portal

Access the Administration Portal using a web browser.

1. In a web browser, navigate to `https://*manager-fqdn*/ovirt-engine`, replacing *manager-fqdn* with the FQDN that you provided during installation.

   |      | You can access the Administration Portal using alternate host names  or IP addresses. To do so, you need to add a configuration file under **/etc/ovirt-engine/engine.conf.d/**. For example:  `# vi /etc/ovirt-engine/engine.conf.d/99-custom-sso-setup.conf SSO_ALTERNATE_ENGINE_FQDNS="_alias1.example.com alias2.example.com_"`  The list of alternate host names needs to be separated by spaces. You can also add the IP address of the Engine to the list, but using IP  addresses instead of DNS-resolvable host names is not recommended. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

2. Click **Administration Portal**. An SSO login page displays. SSO login enables you to log in to the Administration and VM Portal at the same time.

3. Enter your **User Name** and **Password**. If you are logging in for the first time, use the user name **admin** along with the password that you specified during installation.

4. Select the **Domain** to authenticate against. If you are logging in using the internal **admin** user name, select the **internal** domain.

5. Click **Log In**.

6. You can view the Administration Portal in multiple languages. The  default selection is chosen based on the locale settings of your web  browser. If you want to view the Administration Portal in a language  other than the default, select your preferred language from the  drop-down list on the welcome page.

To log out of the oVirt Administration Portal, click your user name in the header bar and click **Sign Out**. You are logged out of all portals and the Engine welcome screen displays.

## 4. Installing Hosts for oVirt

oVirt supports two types of hosts: [oVirt Nodes (oVirt Node)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Red_Hat_Virtualization_Hosts_SM_remoteDB_deploy) and [Enterprise Linux hosts](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Red_Hat_Enterprise_Linux_hosts_SM_remoteDB_deploy). Depending on your environment, you may want to use one type only, or  both. At least two hosts are required for features such as migration and high availability.

See [Recommended practices for configuring host networks](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Recommended_practices_for_configuring_host_networks_SM_remoteDB_deploy) for networking information.

|      | SELinux is in enforcing mode upon installation. To verify, run `getenforce`. SELinux must be in enforcing mode on all hosts and Managers for your oVirt environment to be supported. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| Host Type                 | Other Names                       | Description                                                  |
| ------------------------- | --------------------------------- | ------------------------------------------------------------ |
| **oVirt Node**            | oVirt Node, thin host             | This is a minimal operating system based on Enterprise Linux. It is  distributed as an ISO file from the Customer Portal and contains only  the packages required for the machine to act as a host. |
| **Enterprise Linux host** | Enterprise Linux host, thick host | Enterprise Linux systems with the appropriate repositories enabled can be used as hosts. |

Host Compatibility

When you create a new data center, you can set the compatibility  version. Select the compatibility version that suits all the hosts in  the data center. Once set, version regression is not allowed. For a  fresh oVirt installation, the latest compatibility version is set in the default data center and default cluster; to use an earlier  compatibility version, you must create additional data centers and  clusters.

### 4.1. oVirt Nodes

#### 4.1.1. Installing oVirt Nodes

oVirt Node (oVirt Node) is a minimal operating system based on  Enterprise Linux that is designed to provide a simple method for setting up a physical machine to act as a hypervisor in a oVirt environment.  The minimal operating system contains only the packages required for the machine to act as a hypervisor, and features a Cockpit web interface  for monitoring the host and performing administrative tasks. See [Running Cockpit](http://cockpit-project.org/running.html) for the minimum browser requirements.

oVirt Node supports NIST 800-53 partitioning requirements to improve  security. oVirt Node uses a NIST 800-53 partition layout by default.

The host must meet the minimum  [host requirements](https://ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Write the oVirt Node Installation ISO disk image to a USB, CD, or DVD.

4. Start the machine on which you are installing oVirt Node, booting from the prepared installation media.

5. From the boot menu, select **Install oVirt Node 4.5** and press `Enter`.

   |      | You can also press the `Tab` key to edit the kernel  parameters. Kernel parameters must be separated by a space, and you can  boot the system using the specified kernel parameters by pressing the `Enter` key. Press the `Esc` key to clear any changes to the kernel parameters and return to the boot menu. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Select a language, and click **Continue**.

7. Select a keyboard layout from the **Keyboard Layout** screen and click **Done**.

8. Select the device on which to install oVirt Node from the **Installation Destination** screen. Optionally, enable encryption. Click **Done**.

   |      | Use the **Automatically configure partitioning** option. |
   | ---- | -------------------------------------------------------- |
   |      |                                                          |

9. Select a time zone from the **Time & Date** screen and click **Done**.

10. Select a network from the **Network & Host Name** screen and click **Configure…** to configure the connection details.

    |      | To use the connection every time the system boots, select the **Connect automatically with priority** check box. For more information, see [Configuring network and host name options](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation#network-hostname_configuring-system-settings) in the *Enterprise Linux 8 Installation Guide*. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    Enter a host name in the **Host Name** field, and click **Done**.

11. Optional: Configure **Security Policy** and **Kdump**. See [Customizing your RHEL installation using the GUI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation) in *Performing a standard RHEL installation* for Enterprise Linux 8 for more information on each of the sections in the **Installation Summary** screen.

12. Click **Begin Installation**.

13. Set a root password and, optionally, create an additional user while oVirt Node installs.

    |      | Do not create untrusted users on oVirt Node, as this can lead to exploitation of local security vulnerabilities. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

14. Click **Reboot** to complete the installation.

    |      | When oVirt Node restarts, `nodectl check` performs a health check on the host and displays the result when you log in on the command line. The message `node status: OK` or `node status: DEGRADED` indicates the health status. Run `nodectl check` to get more information. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    |      | If necessary, you can [ prevent kernel modules from loading automatically](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#proc-Preventing_Kernel_Modules_from_Loading_Automatically_Install_nodes_RHVH). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

#### 4.1.2. Installing a third-party package on oVirt-node

If you need a package that is not included in the oVirt provided  repository, you need to provide the repository before you can install  the package.

Prerequisites

- The path to the repository that includes the package you want to install.
- You are logged in to the host with root permissions.

Procedure

1. Open an existing `.repo` file or create a new one in `/etc/yum.repos.d/`.

2. Add an entry to the `.repo` file. For example, to install `sssd-ldap`, add the following entry to a new `.repo` file name `third-party.repo`:

   ```
   # imgbased: set-enabled
   [custom-sssd-ldap]
   name = Provides sssd-ldap
   mirrorlist=http://mirrorlist.centos.org/?release=$stream&arch=$basearch&repo=BaseOS&infra=$infra
   #baseurl=http://mirror.centos.org/$contentdir/$stream/BaseOS/$basearch/os/
   gpgcheck=1
   enabled=1
   gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial
   includepkgs = sssd-ldap
   ```

3. Install ` sssd-ldap`:

   ```
   # dnf install sssd-ldap
   ```

#### 4.1.3. Advanced Installation

##### Custom Partitioning

Custom partitioning on oVirt Node (oVirt Node) is not recommended. Use the **Automatically configure partitioning** option in the **Installation Destination** window.

If your installation requires custom partitioning, select the `I will configure partitioning` option during the installation, and note that the following restrictions apply:

- Ensure the default **LVM Thin Provisioning** option is selected in the **Manual Partitioning** window.

- The following directories are required and must be on thin provisioned logical volumes:

  - root (`/`)

  - `/home`

  - `/tmp`

  - `/var`

  - `/var/crash`

  - `/var/log`

  - `/var/log/audit`

    |      | Do not create a separate partition for `/usr`. Doing so will cause the installation to fail.  `/usr` must be on a logical volume that is able to change versions along with oVirt Node, and therefore should be left on root (`/`). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    For information about the required storage sizes for each partition, see [Storage Requirements](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Storage_Requirements_SM_remoteDB_deploy).

- The `/boot` directory should be defined as a standard partition.

- The `/var` directory must be on a separate volume or disk.

- Only XFS or Ext4 file systems are supported.

**Configuring Manual Partitioning in a Kickstart File**

The following example demonstrates how to configure manual partitioning in a Kickstart file.

```
clearpart --all
part /boot --fstype xfs --size=1000 --ondisk=sda
part pv.01 --size=42000 --grow
volgroup HostVG pv.01 --reserved-percent=20
logvol swap --vgname=HostVG --name=swap --fstype=swap --recommended
logvol none --vgname=HostVG --name=HostPool --thinpool --size=40000 --grow
logvol / --vgname=HostVG --name=root --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=6000 --grow
logvol /var --vgname=HostVG --name=var --thin --fstype=ext4 --poolname=HostPool
--fsoptions="defaults,discard" --size=15000
logvol /var/crash --vgname=HostVG --name=var_crash --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=10000
logvol /var/log --vgname=HostVG --name=var_log --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=8000
logvol /var/log/audit --vgname=HostVG --name=var_audit --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=2000
logvol /home --vgname=HostVG --name=home --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=1000
logvol /tmp --vgname=HostVG --name=tmp --thin --fstype=ext4 --poolname=HostPool --fsoptions="defaults,discard" --size=1000
```

|      | If you use `logvol --thinpool --grow`, you must also include `volgroup --reserved-space` or `volgroup --reserved-percent` to reserve space in the volume group for the thin pool to grow. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

##### Installing a DUD driver on a host without installer support

There are times when installing oVirt Node (oVirt Node) requires a  Driver Update Disk (DUD), such as when using a hardware RAID device that is not supported by the default configuration of oVirt Node. In  contrast with Enterprise Linux hosts, oVirt Node does not fully support  using a DUD. Subsequently the host fails to boot normally after  installation because it does not see RAID. Instead it boots into  emergency mode.

Example output:

```
Warning: /dev/test/rhvh-4.4-20210202.0+1 does not exist
Warning: /dev/test/swap does not exist
Entering emergency mode. Exit the shell to continue.
```

In such a case you can manually add the drivers before finishing the installation.

Prerequisites

- A machine onto which you are installing oVirt Node.
- A DUD.
- If you are using a USB drive for the DUD and oVirt Node, you must have at least two available USB ports.

Procedure

1. Load the DUD on the host machine.

   You can search for DUDs or modules for CentOS Stream at the following locations:

   - [DUDs at the ELRepo Project](https://elrepo.org/linux/dud/el8/x86_64/)
   - [Kmods Special Interest Group](https://wiki.centos.org/SpecialInterestGroup/Kmods)

2. Install oVirt Node. See [Installing oVirt Nodes](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#Installing_Red_Hat_Virtualization_Hosts_SHE_cli_deploy) in *Installing oVirt as a self-hosted engine using the command line*.

   |      | When installation completes, do not reboot the system. |
   | ---- | ------------------------------------------------------ |
   |      |                                                        |

   |      | If you want to access the DUD using SSH, do the following:   Add the string **` inst.sshd`** to the kernel command line:  `<*kernel_command_line*> inst.sshd`   Enable networking during the installation. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

3. Enter the console mode, by pressing Ctrl + Alt + F3. Alternatively you can connect to it using SSH.

4. Mount the DUD:

   ```
   # mkdir /mnt/dud
   # mount -r /dev/<dud_device> /mnt/dud
   ```

5. Copy the RPM file inside the DUD to the target machine’s disk:

   ```
   # cp /mnt/dud/rpms/<path>/<rpm_file>.rpm /mnt/sysroot/root/
   ```

   For example:

   ```
   # cp /mnt/dud/rpms/x86_64/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm /mnt/sysroot/root/
   ```

6. Change the root directory to `/mnt/sysroot`:

   ```
   # chroot /mnt/sysroot
   ```

7. Back up the current initrd images. For example:

   ```
   # cp -p /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
   # cp -p /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
   ```

8. Install the RPM file for the driver from the copy you made earlier.

   For example:

   ```
   # dnf install /root/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm
   ```

   |      | This package is not visible on the system after you reboot into the  installed environment, so if you need it, for example, to rebuild the `initramfs`, you need to install that package once again, after which the package remains.  If you update the host using `dnf`, the driver update persists, so you do not need to repeat this process. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   |      | If you do not have an internet connection, use the `rpm` command instead of `dnf`:  `# rpm -ivh /root/kmod-3w-9xxx-2.26.02.014-5.el8_3.elrepo.x86_64.rpm` |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

9. Create a new image, forcefully adding the driver:

   ```
   # dracut --force --add-drivers <module_name> --kver <kernel_version>
   ```

   For example:

   ```
   # dracut --force --add-drivers 3w-9xxx --kver 4.18.0-240.15.1.el8_3.x86_64
   ```

10. Check the results. The new image should be larger, and include the  driver. For example, compare the sizes of the original, backed-up image  file and the new image file.

    In this example, the new image file is 88739013 bytes, larger than the original 88717417 bytes:

    ```
    # ls -ltr /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img*
    -rw-------. 1 root root 88717417 Jun  2 14:29 /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img.bck1
    -rw-------. 1 root root 88739013 Jun  2 17:47 /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img
    ```

    The new drivers should be part of the image file. For example, the 3w-9xxx module should be included:

    ```
    # lsinitrd /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img | grep 3w-9xxx
    drwxr-xr-x   2 root     root            0 Feb 22 15:57 usr/lib/modules/4.18.0-240.15.1.el8_3.x86_64/weak-updates/3w-9xxx
    lrwxrwxrwx   1 root     root           55 Feb 22 15:57 usr/lib/modules/4.18.0-240.15.1.el8_3.x86_64/weak-updates/3w-9xxx/3w-9xxx.ko-../../../4.18.0-240.el8.x86_64/extra/3w-9xxx/3w-9xxx.ko
    drwxr-xr-x   2 root     root            0 Feb 22 15:57 usr/lib/modules/4.18.0-240.el8.x86_64/extra/3w-9xxx
    -rw-r--r--   1 root     root        80121 Nov 10  2020 usr/lib/modules/4.18.0-240.el8.x86_64/extra/3w-9xxx/3w-9xxx.ko
    ```

11. Copy the image to the the directory under `/boot` that contains the kernel to be used in the layer being installed, for example:

    ```
    # cp -p /boot/initramfs-4.18.0-240.15.1.el8_3.x86_64.img /boot/ovirt-node-ng-4.4.5.1-0.20210323.0+1/initramfs-4.18.0-240.15.1.el8_3.x86_64.img
    ```

12. Exit chroot.

13. Exit the shell.

14. If you used Ctrl + Alt + F3 to access a virtual terminal, then move back to the installer by pressing Ctrl + Alt + F_<n>_, usually F1 or F5

15. At the installer screen, reboot.

Verification

The machine should reboot successfully.

##### Automating oVirt Node deployment

You can install oVirt Node (oVirt Node) without a physical media  device by booting from a PXE server over the network with a Kickstart  file that contains the answers to the installation questions.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

General instructions for installing from a PXE server with a Kickstart file are available in the [*Enterprise Linux Installation Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/chap-kickstart-installations), as oVirt Node is installed in much the same way as Enterprise Linux.  oVirt Node-specific instructions, with examples for deploying oVirt Node with Red Hat Satellite, are described below.

The automated oVirt Node deployment has 3 stages:

- [Preparing the Installation Environment](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_the_Installation_Environment)
- [Configuring the PXE Server and the Boot Loader](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Configuring_the_PXE_Server_and_the_Boot_Loader)
- [Creating and Running a Kickstart File](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Creating_and_Running_a_Kickstart_File)

###### Preparing the installation environment

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Make the oVirt Node ISO image available over the network. See [Installation Source on a Network](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Installation_Guide/sect-making-media-additional-sources.html#sect-making-media-sources-network) in the *Enterprise Linux Installation Guide*.

4. Extract the **squashfs.img** hypervisor image file from the oVirt Node ISO:

   ```
   # mount -o loop /path/to/oVirt Node-ISO /mnt/rhvh
   # cp /mnt/rhvh/Packages/redhat-virtualization-host-image-update* /tmp
   # cd /tmp
   # rpm2cpio redhat-virtualization-host-image-update* | cpio -idmv
   ```

   |      | This **squashfs.img** file, located in the `/tmp/usr/share/redhat-virtualization-host/image/` directory, is called **redhat-virtualization-host-\*version_number\*_version.squashfs.img**. It contains the hypervisor image for installation on the physical machine. It should not be confused with the **/LiveOS/squashfs.img** file, which is used by the Anaconda `inst.stage2` option. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

###### Configuring the PXE server and the boot loader

1. Configure the PXE server. See [Preparing for a Network Installation](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-installation-server-setup.html) in the *Enterprise Linux Installation Guide*.

2. Copy the oVirt Node boot images to the `/tftpboot` directory:

   ```
   # cp mnt/rhvh/images/pxeboot/{vmlinuz,initrd.img} /var/lib/tftpboot/pxelinux/
   ```

3. Create a `rhvh` label specifying the oVirt Node boot images in the boot loader configuration:

   ```
   LABEL rhvh
   MENU LABEL Install oVirt Node
   KERNEL /var/lib/tftpboot/pxelinux/vmlinuz
   APPEND initrd=/var/lib/tftpboot/pxelinux/initrd.img inst.stage2=URL/to/oVirt Node-ISO
   ```

   oVirt Node Boot loader configuration example for Red Hat Satellite

   If you are using information from Red Hat Satellite to provision the  host, you must create a global or host group level parameter called `rhvh_image` and populate it with the directory URL where the ISO is mounted or extracted:

   ```
   <%#
   kind: PXELinux
   name: oVirt Node PXELinux
   %>
   # Created for booting new hosts
   #
   
   DEFAULT rhvh
   
   LABEL rhvh
   KERNEL <%= @kernel %>
   APPEND initrd=<%= @initrd %> inst.ks=<%= foreman_url("provision") %> inst.stage2=<%= @host.params["rhvh_image"] %> intel_iommu=on console=tty0 console=ttyS1,115200n8 ssh_pwauth=1 local_boot_trigger=<%= foreman_url("built") %>
   IPAPPEND 2
   ```

4. Make the content of the oVirt Node ISO locally available and export it to the network, for example, using an HTTPD server:

   ```
   # cp -a /mnt/rhvh/ /var/www/html/rhvh-install
   # curl URL/to/oVirt Node-ISO/rhvh-install
   ```

###### Creating and running a Kickstart file

1. Create a Kickstart file and make it available over the network. See [Kickstart Installations](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-kickstart-installations.html) in the *Enterprise Linux Installation Guide*.

2. Ensure that the Kickstart file meets the following oVirt-specific requirements:

   - The `%packages` section is not required for oVirt Node. Instead, use the `liveimg` option and specify the **redhat-virtualization-host-\*version_number\*_version.squashfs.img** file from the oVirt Node ISO image:

     ```
     liveimg --url=example.com/tmp/usr/share/redhat-virtualization-host/image/redhat-virtualization-host-version_number_version.squashfs.img
     ```

   - Autopartitioning is highly recommended, but use caution: ensure that the local disk is detected first, include the `ignoredisk` command, and specify the local disk to ignore, such as `sda`.  To ensure that a particular drive is used, oVirt recommends using `ignoredisk --only-use=/dev/disk/<*path*>` or `ignoredisk --only-use=/dev/disk/<*ID*>`:

     ```
     autopart --type=thinp
     ignoredisk --only-use=sda
     ignoredisk --only-use=/dev/disk/<path>
     ignoredisk --only-use=/dev/disk/<ID>
     ```

     |      | Autopartitioning requires thin provisioning.  The `--no-home` option does not work in oVirt Node because `/home` is a required directory. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     If your installation requires manual partitioning, see [Custom Partitioning](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Custom_Partitioning_SM_remoteDB_deploy) for a list of limitations that apply to partitions and an example of manual partitioning in a Kickstart file.

   - A `%post` section that calls the `nodectl init` command is required:

     ```
     %post
     nodectl init
     %end
     ```

     |      | Ensure that the `nodectl init` command is at the very end of the `%post` section but before the reboot code, if any. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     Kickstart example for deploying oVirt Node on its own

     This Kickstart example shows you how to deploy oVirt Node. You can include additional commands and options as required.

     |      | This example assumes that all disks are empty and can be initialized. If you have attached disks with data, either remove them or add them to the `ignoredisks` property. |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

     ```
     liveimg --url=http://FQDN/tmp/usr/share/redhat-virtualization-host/image/redhat-virtualization-host-version_number_version.squashfs.img
     clearpart --all
     autopart --type=thinp
     rootpw --plaintext ovirt
     timezone --utc America/Phoenix
     zerombr
     text
     
     reboot
     
     %post --erroronfail
     nodectl init
     %end
     ```

3. Add the Kickstart file location to the boot loader configuration file on the PXE server:

   ```
   APPEND initrd=/var/tftpboot/pxelinux/initrd.img inst.stage2=URL/to/oVirt Node-ISO inst.ks=URL/to/oVirt Node-ks.cfg
   ```

4. Install oVirt Node following the instructions in [Booting from the Network Using PXE](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Installation_Guide/chap-booting-installer-x86.html#sect-booting-from-pxe-x86) in the *Enterprise Linux Installation Guide*.

### 4.2. Enterprise Linux hosts

#### 4.2.1. Installing Enterprise Linux hosts

A Enterprise Linux host is based on a standard basic installation of Enterprise Linux 8 on a physical server, with the `Enterprise Linux Server` and `oVirt` repositories enabled.

The oVirt project also provides packages for Enterprise Linux 9 but only as a Technology Preview.

For detailed installation instructions, see the [*Performing a standard EL installation*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index.html).

The host must meet the minimum [host requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Virtualization must be enabled in your host’s BIOS settings. For  information on changing your host’s BIOS settings, refer to your host’s  hardware documentation. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Do not install third-party watchdogs on Enterprise Linux hosts. They can interfere with the watchdog daemon provided by VDSM. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

#### 4.2.2. Installing Cockpit on Enterprise Linux hosts

You can install Cockpit for monitoring the host’s resources and performing administrative tasks.

Procedure

1. Install the dashboard packages:

   ```
   # dnf install cockpit-ovirt-dashboard
   ```

2. Enable and start the `cockpit.socket` service:

   ```
   # systemctl enable cockpit.socket
   # systemctl start cockpit.socket
   ```

3. Check if Cockpit is an active service in the firewall:

   ```
   # firewall-cmd --list-services
   ```

   You should see `cockpit` listed. If it is not, enter the following with root permissions to add `cockpit` as a service to your firewall:

   ```
   # firewall-cmd --permanent --add-service=cockpit
   ```

   The `--permanent` option keeps the `cockpit` service active after rebooting.

You can log in to the Cockpit web interface at `https://*HostFQDNorIP*:9090`.

### 4.3. Recommended Practices for Configuring Host Networks

|      | Always use the oVirt Engine to modify the network configuration of hosts in your clusters. Otherwise, you might create an unsupported  configuration. For details, see [Network Manager Stateful Configuration (nmstate)](https://ovirt.org/documentation/administration_guide/index#con-Network-Manager-Stateful-Configuration-nmstate). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

If your network environment is complex, you may need to configure a  host network manually before adding the host to the oVirt Engine.

Consider the following practices for configuring a host network:

- Configure the network with Cockpit. Alternatively, you can use `nmtui` or `nmcli`.

- If a network is not required for a self-hosted engine deployment or  for adding a host to the Engine, configure the network in the  Administration Portal after adding the host to the Engine. See [Creating a New Logical Network in a Data Center or Cluster](https://ovirt.org/documentation/administration_guide/index#Creating_a_new_logical_network_in_a_data_center_or_cluster).

- Use the following naming conventions:

  - VLAN devices: `*VLAN_NAME_TYPE_RAW_PLUS_VID_NO_PAD*`
  - VLAN interfaces: `*physical_device*.*VLAN_ID*` (for example, `eth0.23`, `eth1.128`, `enp3s0.50`)
  - Bond interfaces: `bond*number*` (for example, `bond0`, `bond1`)
  - VLANs on bond interfaces: `bond*number*.*VLAN_ID*` (for example, `bond0.50`, `bond1.128`)

- Use [network bonding](https://ovirt.org/documentation/administration_guide/index#sect-Network_Bonding). Network teaming is not supported in oVirt and will cause errors if the  host is used to deploy a self-hosted engine or added to the Engine.

- Use recommended bonding modes:

  - If the `ovirtmgmt` network is not used by virtual machines, the network may use any supported bonding mode.
  - If the `ovirtmgmt` network is used by virtual machines, see [*Which bonding modes work when used with a bridge that virtual machine guests or containers connect to?*](https://access.redhat.com/solutions/67546).
  - oVirt’s default bonding mode is `(Mode 4) Dynamic Link Aggregation`. If your switch does not support Link Aggregation Control Protocol (LACP), use `(Mode 1) Active-Backup`. See [Bonding Modes](https://ovirt.org/documentation/administration_guide/index#Bonding_Modes) for details.

- Configure a VLAN on a physical NIC as in the following example (although `nmcli` is used, you can use any tool):

  ```
  # nmcli connection add type vlan con-name vlan50 ifname eth0.50 dev eth0 id 50
  # nmcli con mod vlan50 +ipv4.dns 8.8.8.8 +ipv4.addresses 123.123.0.1/24 +ipv4.gateway 123.123.0.254
  ```

- Configure a VLAN on a bond as in the following example (although `nmcli` is used, you can use any tool):

  ```
  # nmcli connection add type bond con-name bond0 ifname bond0 bond.options "mode=active-backup,miimon=100" ipv4.method disabled ipv6.method ignore
  # nmcli connection add type ethernet con-name eth0 ifname eth0 master bond0 slave-type bond
  # nmcli connection add type ethernet con-name eth1 ifname eth1 master bond0 slave-type bond
  # nmcli connection add type vlan con-name vlan50 ifname bond0.50 dev bond0 id 50
  # nmcli con mod vlan50 +ipv4.dns 8.8.8.8 +ipv4.addresses 123.123.0.1/24 +ipv4.gateway 123.123.0.254
  ```

- Do not disable `firewalld`.

- Customize the firewall rules in the Administration Portal after adding the host to the Engine. See [Configuring Host Firewall Rules](https://ovirt.org/documentation/administration_guide/index#Configuring_Host_Firewall_Rules).

### 4.4. Adding Standard Hosts to the oVirt Engine

|      | Always use the oVirt Engine to modify the network configuration of hosts in your clusters. Otherwise, you might create an unsupported  configuration. For details, see [Network Manager Stateful Configuration (nmstate)](https://ovirt.org/documentation/administration_guide/index#con-Network-Manager-Stateful-Configuration-nmstate). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Adding a host to your oVirt environment can take some time, as the  following steps are completed by the platform: virtualization checks,  installation of packages, and creation of a bridge.

Procedure

1. From the Administration Portal, click **Compute** **Hosts**.
2. Click **New**.
3. Use the drop-down list to select the **Data Center** and **Host Cluster** for the new host.
4. Enter the **Name** and the **Address** of the new host. The standard SSH port, port 22, is auto-filled in the **SSH Port** field.
5. Select an authentication method to use for the Engine to access the host.
   - Enter the root user’s password to use password authentication.
   - Alternatively, copy the key displayed in the **SSH PublicKey** field to **/root/.ssh/authorized_keys** on the host to use public key authentication.
6. Optionally, click the **Advanced Parameters** button to change the following advanced host settings:
   - Disable automatic firewall configuration.
   - Add a host SSH fingerprint to increase security. You can add it manually, or fetch it automatically.
7. Optionally configure power management, where the host has a supported power management card. For information on power management  configuration, see [Host Power Management Settings Explained](https://ovirt.org/documentation/administration_guide/index#Host_Power_Management_settings_explained) in the *Administration Guide*.
8. Click **OK**.

The new host displays in the list of hosts with a status of `Installing`, and you can view the progress of the installation in the **Events** section of the **Notification Drawer** (![EventsIcon](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/common/images/EventsIcon.png)). After a brief delay the host status changes to `Up`.

## 5. Preparing Storage for oVirt

You need to prepare storage to be used for storage domains in the new environment. A oVirt environment must have at least one data storage  domain, but adding more is recommended.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

A data domain holds the virtual hard disks and OVF files of all the  virtual machines and templates in a data center, and cannot be shared  across data centers while active (but can be migrated between data  centers). Data domains of multiple storage types can be added to the  same data center, provided they are all shared, rather than local,  domains.

You can use one of the following storage types:

- [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_NFS_Storage_SM_remoteDB_deploy)
- [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_iSCSI_Storage_SM_remoteDB_deploy)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_FCP_Storage_SM_remoteDB_deploy)

- [POSIX-compliant file system](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_POSIX_Storage_SM_remoteDB_deploy)
- [Local storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_Local_Storage_SM_remoteDB_deploy)
- [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Preparing_Red_Hat_Gluster_Storage_SM_remoteDB_deploy)

### 5.1. Preparing NFS Storage

Set up NFS shares on your file storage or remote server to serve as  storage domains on Red Hat Enterprise Virtualization Host systems. After exporting the shares on the remote storage and configuring them in the  Red Hat Virtualization Manager, the shares will be automatically  imported on the Red Hat Virtualization hosts.

For information on setting up, configuring, mounting and exporting NFS, see [*Managing file systems*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_file_systems/index) for Red Hat Enterprise Linux 8.

Specific system user accounts and system user groups are required by  oVirt so the Engine can store data in the storage domains represented by the exported directories. The following procedure sets the permissions  for one directory. You must repeat the `chown` and `chmod` steps for all of the directories you intend to use as storage domains in oVirt.

Prerequisites

1. Install the NFS `utils` package.

   ```
   # dnf install nfs-utils -y
   ```

2. To check the enabled versions:

   ```
   # cat /proc/fs/nfsd/versions
   ```

3. Enable the following services:

   ```
   # systemctl enable nfs-server
   # systemctl enable rpcbind
   ```

Procedure

1. Create the group `kvm`:

   ```
   # groupadd kvm -g 36
   ```

2. Create the user `vdsm` in the group `kvm`:

   ```
   # useradd vdsm -u 36 -g kvm
   ```

3. Create the `storage` directory and modify the access rights.

   ```
   # mkdir /storage
   # chmod 0755 /storage
   # chown 36:36 /storage/
   ```

4. Add the `storage` directory to `/etc/exports` with the relevant permissions.

   ```
   # vi /etc/exports
   # cat /etc/exports
    /storage *(rw)
   ```

5. Restart the following services:

   ```
   # systemctl restart rpcbind
   # systemctl restart nfs-server
   ```

6. To see which export are available for a specific IP address:

   ```
   # exportfs
    /nfs_server/srv
                  10.46.11.3/24
    /nfs_server       <world>
   ```

|      | If changes in `/etc/exports` have been made after starting the services, the `exportfs -ra` command can be used to reload the changes. After performing all the above stages, the exports directory should be  ready and can be tested on a different host to check that it is usable. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.2. Preparing iSCSI Storage

oVirt supports iSCSI storage, which is a storage domain created from a volume group made up of LUNs. Volume groups and LUNs cannot be attached to more than one storage domain at a time.

For information on setting up and configuring iSCSI storage, see [Getting started with iSCSI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_storage_devices/index#getting-started-with-iscsi_managing-storage-devices) in *Managing storage devices* for Red Hat Enterprise Linux 8.

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.3. Preparing FCP Storage

oVirt supports SAN storage by creating a storage domain from a volume group made of pre-existing LUNs. Neither volume groups nor LUNs can be  attached to more than one storage domain at a time.

oVirt system administrators need a working knowledge of Storage Area  Networks (SAN) concepts. SAN usually uses Fibre Channel Protocol (FCP)  for traffic between hosts and shared external storage. For this reason,  SAN may occasionally be referred to as FCP storage.

For information on setting up and configuring FCP or multipathing on Enterprise Linux, see the [*Storage Administration Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Storage_Administration_Guide/index.html) and [*DM Multipath Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/DM_Multipath/index.html).

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }  }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.4. Preparing POSIX-compliant File System Storage

POSIX file system support allows you to mount file systems using the  same mount options that you would normally use when mounting them  manually from the command line. This functionality is intended to allow  access to storage not exposed using NFS, iSCSI, or FCP.

Any POSIX-compliant file system used as a storage domain in oVirt  must be a clustered file system, such as Global File System 2 (GFS2),  and must support sparse files and direct I/O. The Common Internet File  System (CIFS), for example, does not support direct I/O, making it  incompatible with oVirt.

For information on setting up and configuring POSIX-compliant file system storage, see [*Enterprise Linux Global File System 2*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Global_File_System_2/index.html).

|      | Do **not** mount NFS storage by creating a POSIX-compliant file system storage domain. Always create an NFS storage domain instead. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 5.5. Preparing local storage

On oVirt Node (oVirt Node), local storage should always be defined on a file system that is separate from `/` (root). Use a separate logical volume or disk, to prevent possible loss of data during upgrades.

Procedure for Enterprise Linux hosts

1. On the host, create the directory to be used for the local storage:

   ```
   # mkdir -p /data/images
   ```

2. Ensure that the directory has permissions allowing read/write access to the **vdsm** user (UID 36) and **kvm** group (GID 36):

   ```
   # chown 36:36 /data /data/images
   # chmod 0755 /data /data/images
   ```

Procedure for oVirt Nodes

Create the local storage on a logical volume:

1. Create a local storage directory:

   ```
   # mkdir /data
   # lvcreate -L $SIZE rhvh -n data
   # mkfs.ext4 /dev/mapper/rhvh-data
   # echo "/dev/mapper/rhvh-data /data ext4 defaults,discard 1 2" >> /etc/fstab
   # mount /data
   ```

2. Mount the new local storage:

   ```
   # mount -a
   ```

3. Ensure that the directory has permissions allowing read/write access to the **vdsm** user (UID 36) and **kvm** group (GID 36):

   ```
   # chown 36:36 /data /rhvh-data
   # chmod 0755 /data /rhvh-data
   ```

### 5.6. Preparing Gluster Storage

For information on setting up and configuring Gluster Storage, see the [*Gluster Storage Installation Guide*](https://docs.gluster.org/en/latest/Install-Guide/Overview/).

### 5.7. Customizing Multipath Configurations for SAN Vendors

If your RHV environment is configured to use multipath connections  with SANs, you can customize the multipath configuration settings to  meet requirements specified by your storage vendor. These customizations can override both the default settings and settings that are specified  in `/etc/multipath.conf`.

To override the multipath settings, do not customize `/etc/multipath.conf`. Because VDSM owns `/etc/multipath.conf`, installing or upgrading VDSM or oVirt can overwrite this file including any customizations it contains. This overwriting can cause severe  storage failures.

Instead, you create a file in the `/etc/multipath/conf.d` directory that contains the settings you want to customize or override.

VDSM executes the files in `/etc/multipath/conf.d` in  alphabetical order. So, to control the order of execution, you begin the filename with a number that makes it come last. For example, `/etc/multipath/conf.d/90-myfile.conf`.

To avoid causing severe storage failures, follow these guidelines:

- Do not modify `/etc/multipath.conf`. If the file contains user modifications, and the file is overwritten, it can cause unexpected storage problems.
- Do not override the `user_friendly_names` and `find_multipaths` settings. For details, see [Recommended Settings for Multipath.conf](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#ref-Recommended_Settings_for_Multipath_conf_SM_remoteDB_deploy).
- Avoid overriding the `no_path_retry` and `polling_interval` settings unless a storage vendor specifically requires you to do so. For details, see [Recommended Settings for Multipath.conf](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#ref-Recommended_Settings_for_Multipath_conf_SM_remoteDB_deploy).

|      | Not following these guidelines can cause catastrophic storage errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

- VDSM is configured to use the multipath module. To verify this, enter:

  ```
  # vdsm-tool is-configured --module multipath
  ```

Procedure

1. Create a new configuration file in the `/etc/multipath/conf.d` directory.

2. Copy the individual setting you want to override from `/etc/multipath.conf` to the new configuration file in `/etc/multipath/conf.d/<my_device>.conf`. Remove any comment marks, edit the setting values, and save your changes.

3. Apply the new configuration settings by entering:

   ```
   # systemctl reload multipathd
   ```

   |      | Do not restart the multipathd service. Doing so generates errors in the VDSM logs. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

Verification steps

1. Test that the new configuration performs as expected on a  non-production cluster in a variety of failure scenarios. For example,  disable all of the storage connections.
2. Enable one connection at a time and verify that doing so makes the storage domain reachable.

Additional resources

- [Recommended Settings for Multipath.conf](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#ref-Recommended_Settings_for_Multipath_conf_SHE_cli_deploy)
- [*Enterprise Linux DM Multipath*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/dm_multipath/)
- [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing)
- [How do I customize /etc/multipath.conf on my RHVH hypervisors? What values must not change and why?](https://access.redhat.com/solutions/3234761)

### 5.8. Recommended Settings for Multipath.conf

Do not override the following settings:

- user_friendly_names  no

  Device names must be consistent across all hypervisors. For example, `/dev/mapper/{WWID}`. The default value of this setting, `no`, prevents the assignment of arbitrary and inconsistent device names such as `/dev/mapper/mpath{N}` on various hypervisors, which can lead to unpredictable system behavior.    Do not change this setting to `user_friendly_names  yes`. User-friendly names are likely to cause unpredictable system behavior or failures, and are not supported.

- `find_multipaths	no`

  This setting controls whether oVirt Node tries to access devices  through multipath only if more than one path is available. The current  value, `no`, allows oVirt to access devices through multipath even if only one path is available.    Do not override this setting.

Avoid overriding the following settings unless required by the storage system vendor:

- `no_path_retry	4`

  This setting controls the number of polling attempts to retry when no paths are available. Before oVirt version 4.2, the value of `no_path_retry` was `fail` because QEMU had trouble with the I/O queuing when no paths were available. The `fail` value made it fail quickly and paused the virtual machine. oVirt version 4.2 changed this value to `4` so when multipathd detects the last path has failed, it checks all of  the paths four more times. Assuming the default 5-second polling  interval, checking the paths takes 20 seconds. If no path is up,  multipathd tells the kernel to stop queuing and fails all outstanding  and future I/O until a path is restored. When a path is restored, the  20-second delay is reset for the next time all paths fail. For more  details, see [the commit that changed this setting](https://gerrit.ovirt.org/#/c/88082/).

- `polling_interval	5`

  This setting determines the number of seconds between polling  attempts to detect whether a path is open or has failed. Unless the  vendor provides a clear reason for increasing the value, keep the  VDSM-generated default so the system responds to path failures sooner.

## 6. Adding Storage for oVirt

Add storage as data domains in the new environment. A oVirt  environment must have at least one data domain, but adding more is  recommended.

Add the storage you prepared earlier:

- [NFS](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_NFS_Storage_SM_remoteDB_deploy)
- [iSCSI](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_iSCSI_Storage_SM_remoteDB_deploy)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_FCP_Storage_SM_remoteDB_deploy)

- [POSIX-compliant file system](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_POSIX_Storage_SM_remoteDB_deploy)
- [Local storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_Local_Storage_SM_remoteDB_deploy)
- [Gluster Storage](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Adding_Red_Hat_Gluster_Storage_SM_remoteDB_deploy)

### 6.1. Adding NFS Storage

This procedure shows you how to attach existing NFS storage to your oVirt environment as a data domain.

If you require an ISO or export domain, use this procedure, but select **ISO** or **Export** from the **Domain Function** list.

Procedure

1. In the Administration Portal, click **Storage** **Domains**.
2. Click **New Domain**.
3. Enter a **Name** for the storage domain.
4. Accept the default values for the **Data Center**, **Domain Function**, **Storage Type**, **Format**, and **Host** lists.
5. Enter the **Export Path** to be used for the storage domain. The export path should be in the format of *123.123.0.10:/data* (for IPv4), *[2001:0:0:0:0:0:0:5db1]:/data* (for IPv6), or *domain.example.com:/data*.
6. Optionally, you can configure the advanced parameters:
   1. Click **Advanced Parameters**.
   2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
   3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
   4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
7. Click **OK**.

The new NFS data domain has a status of `Locked` until the disk is prepared. The data domain is then automatically attached to the data center.

### 6.2. Adding iSCSI Storage

This procedure shows you how to attach existing iSCSI storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** of the new storage domain.

4. Select a **Data Center** from the drop-down list.

5. Select **Data** as the **Domain Function** and **iSCSI** as the **Storage Type**.

6. Select an active host as the **Host**.

   |      | Communication to the storage domain is from the selected host and not directly from the Engine. Therefore, all hosts must have access to the  storage device before the storage domain can be configured. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. The Engine can map iSCSI targets to LUNs or LUNs to iSCSI targets. The **New Domain** window automatically displays known targets with unused LUNs when the  iSCSI storage type is selected. If the target that you are using to add  storage does not appear, you can use target discovery to find it;  otherwise proceed to the next step.

   1. Click **Discover Targets** to enable target discovery options. When targets have been discovered and logged in to, the **New Domain** window automatically displays targets with LUNs unused by the environment.

      |      | LUNs used externally for the environment are also displayed. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      You can use the **Discover Targets** options to add LUNs on many targets or multiple paths to the same LUNs.

      |      | If you use the REST API method `discoveriscsi` to discover the iscsi targets, you can use an FQDN or an IP address, but you must  use the iscsi details from the discovered targets results to log in  using the REST API method `iscsilogin`. See [discoveriscsi](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-host-methods-discover_iscsi) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   2. Enter the FQDN or IP address of the iSCSI host in the **Address** field.

   3. Enter the port with which to connect to the host when browsing for targets in the **Port** field. The default is `3260`.

   4. If CHAP is used to secure the storage, select the **User Authentication** check box. Enter the **CHAP user name** and **CHAP password**.

      |      | You can define credentials for an iSCSI target for a specific host with the REST API. See [StorageServerConnectionExtensions: add](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-storage_server_connection_extensions-methods-add) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

   5. Click **Discover**.

   6. Select one or more targets from the discovery results and click **Login** for one target or **Login All** for multiple targets.

      |      | If more than one path access is required, you must discover and log  in to the target through all the required paths. Modifying a storage  domain to add additional paths is currently not supported. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

      |      | When using the REST API `iscsilogin` method to log in, you must use the iscsi details from the discovered targets results in the `discoveriscsi` method. See [iscsilogin](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/rest_api_guide/index#services-host-methods-iscsi_login) in the *REST API Guide* for more information. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

8. Click the **+** button next to the desired target. This expands the entry and displays all unused LUNs attached to the target.

9. Select the check box for each LUN that you are using to create the storage domain.

10. Optionally, you can configure the advanced parameters:

    1. Click **Advanced Parameters**.
    2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
    3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
    4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
    5. Select the **Discard After Delete** check box to enable  the discard after delete option. This option can be edited after the  domain is created. This option is only available to block storage  domains.

11. Click **OK**.

If you have configured multiple storage connection paths to the same target, follow the procedure in [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing) to complete iSCSI bonding.

If you want to migrate your current storage network to an iSCSI bond, see [Migrating a Logical Network to an iSCSI Bond](https://ovirt.org/documentation/administration_guide/index#Migrating_a_logical_network_to_an_iscsi_bond).

### 6.3. Adding FCP Storage

This procedure shows you how to attach existing FCP storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** of the storage domain.

4. Select an FCP **Data Center** from the drop-down list.

   If you do not yet have an appropriate FCP data center, select `(none)`.

5. Select the **Domain Function** and the **Storage Type** from the drop-down lists. The storage domain types that are not compatible with the chosen data center are not available.

6. Select an active host in the **Host** field. If this is not the first data domain in a data center, you must select the data center’s SPM host.

   |      | All communication to the storage domain is through the selected host  and not directly from the oVirt Engine. At least one active host must  exist in the system and be attached to the chosen data center. All hosts must have access to the storage device before the storage domain can be configured. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. The **New Domain** window automatically displays known targets with unused LUNs when **Fibre Channel** is selected as the storage type. Select the **LUN ID** check box to select all of the available LUNs.

8. Optionally, you can configure the advanced parameters.

   1. Click **Advanced Parameters**.
   2. Enter a percentage value into the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
   3. Enter a GB value into the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
   4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.
   5. Select the **Discard After Delete** check box to enable  the discard after delete option. This option can be edited after the  domain is created. This option is only available to block storage  domains.

9. Click **OK**.

The new FCP data domain remains in a `Locked` status while it is being prepared for use. When ready, it is automatically attached to the data center.

### 6.4. Adding POSIX-compliant File System Storage

This procedure shows you how to attach existing POSIX-compliant file system storage to your oVirt environment as a data domain.

Procedure

1. Click **Storage** **Domains**.

2. Click **New Domain**.

3. Enter the **Name** for the storage domain.

4. Select the **Data Center** to be associated with the storage domain. The data center selected must be of type **POSIX (POSIX compliant FS)**. Alternatively, select `(none)`.

5. Select `Data` from the **Domain Function** drop-down list, and `POSIX compliant FS` from the **Storage Type** drop-down list.

   If applicable, select the **Format** from the drop-down menu.

6. Select a host from the **Host** drop-down list.

7. Enter the **Path** to the POSIX file system, as you would normally provide it to the `mount` command.

8. Enter the **VFS Type**, as you would normally provide it to the `mount` command using the `-t` argument. See `man mount` for a list of valid VFS types.

9. Enter additional **Mount Options**, as you would normally provide them to the `mount` command using the `-o` argument. The mount options should be provided in a comma-separated list. See `man mount` for a list of valid mount options.

10. Optionally, you can configure the advanced parameters.

    1. Click **Advanced Parameters**.
    2. Enter a percentage value in the **Warning Low Space Indicator** field. If the free space available on the storage domain is below this  percentage, warning messages are displayed to the user and logged.
    3. Enter a GB value in the **Critical Space Action Blocker** field. If the free space available on the storage domain is below this  value, error messages are displayed to the user and logged, and any new  action that consumes space, even temporarily, will be blocked.
    4. Select the **Wipe After Delete** check box to enable the wipe after delete option. This option can be edited after the domain is created, but doing so will not change the wipe after delete property of disks that already exist.

11. Click **OK**.

### 6.5. Adding a local storage domain

When adding a local storage domain to a host, setting the path to the local storage directory automatically creates and places the host in a  local data center, local cluster, and local storage domain.

Procedure

1. Click **Compute** **Hosts** and select the host.
2. Click **Management** **Maintenance** and **OK**. The host’s status changes to **Maintenance**.
3. Click **Management** **Configure Local Storage**.
4. Click the **Edit** buttons next to the **Data Center**, **Cluster**, and **Storage** fields to configure and name the local storage domain.
5. Set the path to your local storage in the text entry field.
6. If applicable, click the **Optimization** tab to configure the memory optimization policy for the new local storage cluster.
7. Click **OK**.

The Engine sets up the local data center with a local cluster, local storage domain. It also changes the host’s status to **Up**.

Verification

1. Click **Storage** **Domains**.
2. Locate the local storage domain you just added.

The domain’s status should be **Active** (![status active icon](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/common/images/status_active_icon.png)), and the value in the **Storage Type** column should be **Local on Host**.

You can now upload a disk image in the new local storage domain.

### 6.6. Adding Gluster Storage

To use Gluster Storage with oVirt, see [*Configuring oVirt with Gluster Storage*](https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.4/html/configuring_red_hat_virtualization_with_red_hat_gluster_storage/).

For the Gluster Storage versions that are supported with oVirt, see [Red Hat Gluster Storage Version Compatibility and Support](https://access.redhat.com/articles/2356261).

## Appendix A: Configuring a Host for PCI Passthrough

|      | This is one in a series of topics that show how to set up and configure SR-IOV on oVirt. For more information, see [Setting Up and Configuring SR-IOV](https://ovirt.org/documentation/administration_guide/index#setting-up-and-configuring-sr-iov) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Enabling PCI passthrough allows a virtual machine to use a host  device as if the device were directly attached to the virtual machine.  To enable the PCI passthrough function, you must enable virtualization  extensions and the IOMMU function. The following procedure requires you  to reboot the host. If the host is attached to the Engine already,  ensure you place the host into maintenance mode first.

Prerequisites

- Ensure that the host hardware meets the requirements for PCI device passthrough and assignment. See [PCI Device Requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#PCI_Device_Requirements_RHV_planning) for more information.

Configuring a Host for PCI Passthrough

1. Enable the virtualization extension and IOMMU extension in the BIOS. See [Enabling Intel VT-x and AMD-V virtualization hardware extensions in BIOS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Virtualization_Deployment_and_Administration_Guide/sect-Troubleshooting-Enabling_Intel_VT_x_and_AMD_V_virtualization_hardware_extensions_in_BIOS.html) in the *Enterprise Linux Virtualization Deployment and Administration Guide* for more information.
2. Enable the IOMMU flag in the kernel by selecting the **Hostdev Passthrough & SR-IOV** check box when adding the host to the Engine or by editing the **grub** configuration file manually.
   - To enable the IOMMU flag from the Administration Portal, see [Adding Standard Hosts to the oVirt Engine](https://ovirt.org/documentation/administration_guide#Adding_standard_hosts_to_the_Manager) and [Kernel Settings Explained](https://ovirt.org/documentation/administration_guide#Kernel_Settings_Explained).
   - To edit the **grub** configuration file manually, see  [Enabling IOMMU Manually](https://www.ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_remote_databases/index.html#Enabling_IOMMU_Manually).
3. For GPU passthrough, you need to run additional configuration steps on both the host and the guest system. See [GPU device passthrough: Assigning a host GPU to a single virtual machine](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/setting_up_an_nvidia_gpu_for_a_virtual_machine_in_red_hat_virtualization/index#proc_nvidia_gpu_passthrough_nvidia_gpu_passthrough) in *Setting up an NVIDIA GPU for a virtual machine in Red Hat Virtualization* for more information.

Enabling IOMMU Manually

1. Enable IOMMU by editing the grub configuration file.

   |      | If you are using IBM POWER8 hardware, skip this step as IOMMU is enabled by default. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   - For Intel, boot the machine, and append `intel_iommu=on` to the end of the `GRUB_CMDLINE_LINUX` line in the **grub** configuration file.

     ```
     # vi /etc/default/grub
     ...
     GRUB_CMDLINE_LINUX="nofb splash=quiet console=tty0 ... intel_iommu=on
     ...
     ```

   - For AMD, boot the machine, and append `amd_iommu=on` to the end of the `GRUB_CMDLINE_LINUX` line in the **grub** configuration file.

     ```
     # vi /etc/default/grub
     …
     GRUB_CMDLINE_LINUX="nofb splash=quiet console=tty0 … amd_iommu=on
     …
     ```

     |      | If `intel_iommu=on` or an AMD IOMMU is detected, you can try adding `iommu=pt`. The `pt` option only enables IOMMU for devices used in passthrough and provides  better host performance. However, the option might not be supported on  all hardware. Revert to the previous option if the `pt` option doesn’t work for your host.  If the passthrough fails because the hardware does not support interrupt remapping, you can consider enabling the `allow_unsafe_interrupts` option if the virtual machines are trusted. The `allow_unsafe_interrupts` is not enabled by default because enabling it potentially exposes the  host to MSI attacks from virtual machines. To enable the option:  `# vi /etc/modprobe.d options vfio_iommu_type1 allow_unsafe_interrupts=1` |
     | ---- | ------------------------------------------------------------ |
     |      |                                                              |

2. Refresh the **grub.cfg** file and reboot the host for these changes to take effect:

   ```
   # grub2-mkconfig -o /boot/grub2/grub.cfg
   ```

   ```
   # reboot
   ```

## Appendix B: Removing the standalone oVirt Engine

The `engine-cleanup` command removes all components of the oVirt Engine and automatically backs up the following:

- the Grafana database, in `/var/lib/grafana/`
- the Engine database in `/var/lib/ovirt-engine/backups/`
- a compressed archive of the PKI keys and configuration in `/var/lib/ovirt-engine/backups/`

Backup file names include the date and time.

|      | You should use this procedure only on a standalone installation of the oVirt Engine. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Run the following command on the Engine machine:

   ```
   # engine-cleanup
   ```

2. The Engine service must be stopped before proceeding. You are prompted to confirm. Enter `OK` to proceed:

   ```
   During execution engine service will be stopped (OK, Cancel) [OK]:
   ```

3. You are prompted to confirm that you want to remove all Engine components. Enter `OK` to remove all components, or `Cancel` to exit `engine-cleanup`:

   ```
   All the installed ovirt components are about to be removed, data will be lost (OK, Cancel) [Cancel]: OK
   ```

   `engine-cleanup` details the components that are removed, and the location of backup files.

4. Remove the oVirt packages:

   ```
   # dnf remove ovirt-engine* vdsm-bootstrap
   ```

## Appendix C: Preventing kernel modules from loading automatically

You can prevent a kernel module from being loaded automatically,  whether the module is loaded directly, loaded as a dependency from  another module, or during the boot process.

Procedure

1. The module name must be added to a configuration file for the `modprobe` utility.  This file must reside in the configuration directory `/etc/modprobe.d`.

   For more information on this configuration directory, see the man page `modprobe.d`.

2. Ensure the module is not configured to get loaded in any of the following:

   - `/etc/modprobe.conf`
   - `/etc/modprobe.d/*`
   - `/etc/rc.modules`
   - `/etc/sysconfig/modules/*`

   ```
   # modprobe --showconfig <_configuration_file_name_>
   ```

3. If the module appears in the output, ensure it is ignored and not loaded:

   ```
   # modprobe --ignore-install <_module_name_>
   ```

4. Unload the module from the running system, if it is loaded:

   ```
   # modprobe -r <_module_name_>
   ```

5. Prevent the module from being loaded directly by adding the `blacklist` line to a configuration file specific to the system - for example `/etc/modprobe.d/local-dontload.conf`:

   ```
   # echo "blacklist <_module_name_> >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | This step does not prevent a module from loading if it is a required or an optional dependency of another module. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Prevent optional modules from being loading on demand:

   ```
   # echo "install <_module_name_>/bin/false" >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | If the excluded module is required for other hardware, excluding it might cause unexpected side effects. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. Make a backup copy of your `initramfs`:

   ```
   # cp /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.$(date +%m-%d-%H%M%S).bak
   ```

8. If the kernel module is part of the `initramfs`, rebuild your initial `ramdisk` image, omitting the module:

   ```
   # dracut --omit-drivers <_module_name_> -f
   ```

9. Get the current kernel command line parameters:

   ```
   # grub2-editenv - list | grep kernelopts
   ```

10. Append `<_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>` to the generated output:

    ```
    # grub2-editenv - set kernelopts="<> <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

    For example:

    ```
    # grub2-editenv - set kernelopts="root=/dev/mapper/rhel_example-root ro crashkernel=auto resume=/dev/mapper/rhel_example-swap rd.lvm.lv=rhel_example/root rd.lvm.lv=rhel_example/swap <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

11. Make a backup copy of the `kdump initramfs`:

    ```
    # cp /boot/initramfs-$(uname -r)kdump.img /boot/initramfs-$(uname -r)kdump.img.$(date +%m-%d-%H%M%S).bak
    ```

12. Append `rd.driver.blacklist=<_module_name_>` to the `KDUMP_COMMANDLINE_APPEND` setting in `/etc/sysconfig/kdump` to omit it from the `kdump initramfs`:

    ```
    # sed -i '/^KDUMP_COMMANDLINE_APPEND=/s/"$/ rd.driver.blacklist=module_name"/' /etc/sysconfig/kdump
    ```

13. Restart the `kdump` service to pick up the changes to the `kdump initrd`:

    ```
      # kdumpctl restart
    ```

14. Rebuild the `kdump` initial `ramdisk` image:

    ```
      # mkdumprd -f /boot/initramfs-$(uname -r)kdump.img
    ```

15. Reboot the system.

### Removing a module temporarily

You can remove a module temporarily.

Procedure

1. Run `modprobe` to remove any currently-loaded module:

   ```
   # modprobe -r <module name>
   ```

2. If the module cannot be unloaded, a process or another module might  still be using the module. If so, terminate the process and run the `modpole` command written above another time to unload the module.

## Appendix D: Legal notice

Certain portions of this text first appeared in [Red Hat Virtualization 4.4 Installing Red Hat Virtualization as a standalone Manager with remote databases](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/installing_red_hat_virtualization_as_a_standalone_manager_with_remote_databases/index). Copyright © 2022 Red Hat, Inc. Licensed under a [Creative Commons Attribution-ShareAlike 4.0 Unported License](https://creativecommons.org/licenses/by-sa/4.0/).

## Migrating from a standalone Engine to a self-hosted engine

You can convert a standalone oVirt Engine to a self-hosted engine by  backing up the standalone Engine and restoring it in a new self-hosted  environment.

The difference between the two environment types is explained below:

### Standalone Engine Architecture

The oVirt Engine runs on a physical server, or a virtual machine  hosted in a separate virtualization environment. A standalone Engine is  easier to deploy and manage, but requires an additional physical server. The Engine is only highly available when managed externally with a  product such as Red Hat’s High Availability Add-On.

The minimum setup for a standalone Engine environment includes:

- One oVirt Engine machine. The Engine is typically deployed on a  physical server. However, it can also be deployed on a virtual machine,  as long as that virtual machine is hosted in a separate environment. The Engine must run on Enterprise Linux 8.
- A minimum of two hosts for virtual machine high availability. You can use Enterprise Linux hosts or oVirt Nodes (oVirt Node). VDSM (the host  agent) runs on all hosts to facilitate communication with the oVirt  Engine.
- One storage service, which can be hosted locally or on a remote  server, depending on the storage type used. The storage service must be  accessible to all hosts.

![Standalone Architecture](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/common/images/RHV_STANDARD_ARCHITECTURE1.png)

Figure 1. Standalone Engine oVirt Architecture

### Self-Hosted Engine Architecture

The oVirt Engine runs as a virtual machine on self-hosted engine  nodes (specialized hosts) in the same environment it manages. A  self-hosted engine environment requires one less physical server, but  requires more administrative overhead to deploy and manage. The Engine  is highly available without external HA management.

The minimum setup of a self-hosted engine environment includes:

- One oVirt Engine virtual machine that is hosted on the self-hosted  engine nodes. The Engine Appliance is used to automate the installation  of a Enterprise Linux 8 virtual machine, and the Engine on that virtual  machine.
- A minimum of two self-hosted engine nodes for virtual machine high  availability. You can use Enterprise Linux hosts or oVirt Nodes (oVirt  Node). VDSM (the host agent) runs on all hosts to facilitate  communication with the oVirt Engine. The HA services run on all  self-hosted engine nodes to manage the high availability of the Engine  virtual machine.
- One storage service, which can be hosted locally or on a remote  server, depending on the storage type used. The storage service must be  accessible to all hosts.

![Self-Hosted Engine Architecture](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/common/images/RHV_SHE_ARCHITECTURE1.png)

Figure 2. Self-Hosted Engine oVirt Architecture

## 1. Migration Overview

When you specify a backup file during self-hosted engine deployment,  the Engine backup is restored on a new virtual machine, with a dedicated self-hosted engine storage domain. Deploying on a fresh host is highly  recommended; if the host used for deployment existed in the backed up  environment, it will be removed from the restored database to avoid  conflicts in the new environment. If you deploy on a new host, you must  assign a unique name to the host. Reusing the name of an existing host  included in the backup can cause conflicts in the new environment.

At least two self-hosted engine nodes are required for the Engine  virtual machine to be highly available. You can add new nodes, or  convert existing hosts.

The migration involves the following key steps:

1. [Install a new host to deploy the self-hosted engine on.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Installing_the_self-hosted_engine_deployment_host_migrating_to_SHE) You can use either host type:
   - [oVirt Node](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Installing_Red_Hat_Virtualization_Hosts_SHE_deployment_host)
   - [Enterprise Linux](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Installing_Red_Hat_Enterprise_Linux_Hosts_SHE_deployment_host)
2. [Prepare storage for the self-hosted engine storage domain.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_Storage_for_RHV_migrating_to_SHE) You can use one of the following storage types:
   - [NFS](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_NFS_Storage_migrating_to_SHE)
   - [iSCSI](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_iSCSI_Storage_migrating_to_SHE)
   - [Fibre Channel (FCP)](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_FCP_Storage_migrating_to_SHE)
   - [Gluster Storage](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_Red_Hat_Gluster_Storage_migrating_to_SHE)
3. [Update the original Engine to the latest minor version before you back it up.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Updating_the_Red_Hat_Virtualization_Manager_migrating_to_SHE)
4. [Back up the original Engine using the `engine-backup` tool.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Backing_up_the_Original_Manager_migrating_to_SHE)
5. [Deploy a new self-hosted engine and restore the backup.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Restoring_the_Backup_on_a_New_Self-hosted_Engine_migrating_to_SHE)
6. [Enable the Engine repositories on the new Engine virtual machine.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Enabling_the_Red_Hat_Virtualization_Manager_Repositories_migrating_to_SHE)
7. [Convert regular hosts to self-hosted engine nodes that can host the new Engine.](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Reinstalling_an_Existing_Host_as_a_Self-Hosted_Engine_Node_migrating_to_SHE)

This procedure assumes that you have access and can make changes to the original Engine.

Prerequisites

- FQDNs prepared for your Engine and the deployment host. Forward and  reverse lookup records must both be set in the DNS. The new Engine must  have the same FQDN as the original Engine.
- The management network (**ovirtmgmt** by default) must be configured as a **VM network**, so that it can manage the Engine virtual machine.

## 2. Installing the Self-hosted Engine Deployment Host

A self-hosted engine can be deployed from a [oVirt Node](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Installing_Red_Hat_Virtualization_Hosts_SHE_deployment_host) or a [Enterprise Linux host](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Installing_Red_Hat_Enterprise_Linux_Hosts_SHE_deployment_host).

|      | If you plan to use bonded interfaces for high availability or VLANs  to separate different types of traffic (for example, for storage or  management connections), you should configure them on the host before  beginning the self-hosted engine deployment. See [Networking Recommendations](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#networking-recommendations) in the *Planning and Prerequisites Guide*. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.1. Installing oVirt Nodes

oVirt Node (oVirt Node) is a minimal operating system based on  Enterprise Linux that is designed to provide a simple method for setting up a physical machine to act as a hypervisor in a oVirt environment.  The minimal operating system contains only the packages required for the machine to act as a hypervisor, and features a Cockpit web interface  for monitoring the host and performing administrative tasks. See [Running Cockpit](http://cockpit-project.org/running.html) for the minimum browser requirements.

oVirt Node supports NIST 800-53 partitioning requirements to improve  security. oVirt Node uses a NIST 800-53 partition layout by default.

The host must meet the minimum  [host requirements](https://ovirt.org/documentation/installing_ovirt_as_a_standalone_manager_with_local_databases/index.html#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Visit the [oVirt Node Download](https://ovirt.org/download/node.html) page.

2. Choose the version of **oVirt Node** to download and click its **Installation ISO** link.

3. Write the oVirt Node Installation ISO disk image to a USB, CD, or DVD.

4. Start the machine on which you are installing oVirt Node, booting from the prepared installation media.

5. From the boot menu, select **Install oVirt Node 4.5** and press `Enter`.

   |      | You can also press the `Tab` key to edit the kernel  parameters. Kernel parameters must be separated by a space, and you can  boot the system using the specified kernel parameters by pressing the `Enter` key. Press the `Esc` key to clear any changes to the kernel parameters and return to the boot menu. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Select a language, and click **Continue**.

7. Select a keyboard layout from the **Keyboard Layout** screen and click **Done**.

8. Select the device on which to install oVirt Node from the **Installation Destination** screen. Optionally, enable encryption. Click **Done**.

   |      | Use the **Automatically configure partitioning** option. |
   | ---- | -------------------------------------------------------- |
   |      |                                                          |

9. Select a time zone from the **Time & Date** screen and click **Done**.

10. Select a network from the **Network & Host Name** screen and click **Configure…** to configure the connection details.

    |      | To use the connection every time the system boots, select the **Connect automatically with priority** check box. For more information, see [Configuring network and host name options](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation#network-hostname_configuring-system-settings) in the *Enterprise Linux 8 Installation Guide*. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    Enter a host name in the **Host Name** field, and click **Done**.

11. Optional: Configure **Security Policy** and **Kdump**. See [Customizing your RHEL installation using the GUI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/graphical-installation_graphical-installation) in *Performing a standard RHEL installation* for Enterprise Linux 8 for more information on each of the sections in the **Installation Summary** screen.

12. Click **Begin Installation**.

13. Set a root password and, optionally, create an additional user while oVirt Node installs.

    |      | Do not create untrusted users on oVirt Node, as this can lead to exploitation of local security vulnerabilities. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

14. Click **Reboot** to complete the installation.

    |      | When oVirt Node restarts, `nodectl check` performs a health check on the host and displays the result when you log in on the command line. The message `node status: OK` or `node status: DEGRADED` indicates the health status. Run `nodectl check` to get more information. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

    |      | If necessary, you can [ prevent kernel modules from loading automatically](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#proc-Preventing_Kernel_Modules_from_Loading_Automatically_Install_nodes_RHVH). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

### 2.2. Installing Enterprise Linux hosts

A Enterprise Linux host is based on a standard basic installation of Enterprise Linux 8 on a physical server, with the `Enterprise Linux Server` and `oVirt` repositories enabled.

The oVirt project also provides packages for Enterprise Linux 9 but only as a Technology Preview.

For detailed installation instructions, see the [*Performing a standard EL installation*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index.html).

The host must meet the minimum [host requirements](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/planning_and_prerequisites_guide/index#host-requirements).

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Virtualization must be enabled in your host’s BIOS settings. For  information on changing your host’s BIOS settings, refer to your host’s  hardware documentation. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | Do not install third-party watchdogs on Enterprise Linux hosts. They can interfere with the watchdog daemon provided by VDSM. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Although the existing storage domains will be migrated from the  standalone Engine, you must prepare additional storage for a self-hosted engine storage domain that is dedicated to the Engine virtual machine.

## 3. Preparing Storage for oVirt

You need to prepare storage to be used for storage domains in the new environment. A oVirt environment must have at least one data storage  domain, but adding more is recommended.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

A data domain holds the virtual hard disks and OVF files of all the  virtual machines and templates in a data center, and cannot be shared  across data centers while active (but can be migrated between data  centers). Data domains of multiple storage types can be added to the  same data center, provided they are all shared, rather than local,  domains.

You can use one of the following storage types:

- [NFS](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_NFS_Storage_migrating_to_SHE)
- [iSCSI](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_iSCSI_Storage_migrating_to_SHE)
- [Fibre Channel (FCP)](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_FCP_Storage_migrating_to_SHE)
- [Gluster Storage](https://www.ovirt.org/documentation/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index.html#Preparing_Red_Hat_Gluster_Storage_migrating_to_SHE)

Prerequisites

- Self-hosted engines must have an additional data domain with at least 74 GiB dedicated to the Engine virtual machine. The self-hosted engine  installer creates this domain. Prepare the storage for this domain  before installation.

  |      | Extending or otherwise changing the self-hosted engine storage domain after deployment of the self-hosted engine is not supported. Any such  change might prevent the self-hosted engine from booting. |
  | ---- | ------------------------------------------------------------ |
  |      |                                                              |

- When using a block storage domain, either FCP or iSCSI, a single  target LUN is the only supported setup for a self-hosted engine.

- If you use iSCSI storage, the self-hosted engine storage domain must  use a dedicated iSCSI target. Any additional storage domains must use a  different iSCSI target.

- It is strongly recommended to create additional data storage domains  in the same data center as the self-hosted engine storage domain. If you deploy the self-hosted engine in a data center with only one active  data storage domain, and that storage domain is corrupted, you cannot  add new storage domains or remove the corrupted storage domain. You must redeploy the self-hosted engine.

### 3.1. Preparing NFS Storage

Set up NFS shares on your file storage or remote server to serve as  storage domains on Red Hat Enterprise Virtualization Host systems. After exporting the shares on the remote storage and configuring them in the  Red Hat Virtualization Manager, the shares will be automatically  imported on the Red Hat Virtualization hosts.

For information on setting up, configuring, mounting and exporting NFS, see [*Managing file systems*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_file_systems/index) for Red Hat Enterprise Linux 8.

Specific system user accounts and system user groups are required by  oVirt so the Engine can store data in the storage domains represented by the exported directories. The following procedure sets the permissions  for one directory. You must repeat the `chown` and `chmod` steps for all of the directories you intend to use as storage domains in oVirt.

Prerequisites

1. Install the NFS `utils` package.

   ```
   # dnf install nfs-utils -y
   ```

2. To check the enabled versions:

   ```
   # cat /proc/fs/nfsd/versions
   ```

3. Enable the following services:

   ```
   # systemctl enable nfs-server
   # systemctl enable rpcbind
   ```

Procedure

1. Create the group `kvm`:

   ```
   # groupadd kvm -g 36
   ```

2. Create the user `vdsm` in the group `kvm`:

   ```
   # useradd vdsm -u 36 -g kvm
   ```

3. Create the `storage` directory and modify the access rights.

   ```
   # mkdir /storage
   # chmod 0755 /storage
   # chown 36:36 /storage/
   ```

4. Add the `storage` directory to `/etc/exports` with the relevant permissions.

   ```
   # vi /etc/exports
   # cat /etc/exports
    /storage *(rw)
   ```

5. Restart the following services:

   ```
   # systemctl restart rpcbind
   # systemctl restart nfs-server
   ```

6. To see which export are available for a specific IP address:

   ```
   # exportfs
    /nfs_server/srv
                  10.46.11.3/24
    /nfs_server       <world>
   ```

|      | If changes in `/etc/exports` have been made after starting the services, the `exportfs -ra` command can be used to reload the changes. After performing all the above stages, the exports directory should be  ready and can be tested on a different host to check that it is usable. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 3.2. Preparing iSCSI Storage

oVirt supports iSCSI storage, which is a storage domain created from a volume group made up of LUNs. Volume groups and LUNs cannot be attached to more than one storage domain at a time.

For information on setting up and configuring iSCSI storage, see [Getting started with iSCSI](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/managing_storage_devices/index#getting-started-with-iscsi_managing-storage-devices) in *Managing storage devices* for Red Hat Enterprise Linux 8.

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 3.3. Preparing FCP Storage

oVirt supports SAN storage by creating a storage domain from a volume group made of pre-existing LUNs. Neither volume groups nor LUNs can be  attached to more than one storage domain at a time.

oVirt system administrators need a working knowledge of Storage Area  Networks (SAN) concepts. SAN usually uses Fibre Channel Protocol (FCP)  for traffic between hosts and shared external storage. For this reason,  SAN may occasionally be referred to as FCP storage.

For information on setting up and configuring FCP or multipathing on Enterprise Linux, see the [*Storage Administration Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/Storage_Administration_Guide/index.html) and [*DM Multipath Guide*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/DM_Multipath/index.html).

|      | If you are using block storage and intend to deploy virtual machines  on raw devices or direct LUNs and manage them with the Logical Volume  Manager (LVM), you must create a filter to hide guest logical volumes.  This will prevent guest logical volumes from being activated when the  host is booted, a situation that could lead to stale logical volumes and cause data corruption. Use the `vdsm-tool config-lvm-filter` command to create filters for the LVM. See [Creating an LVM filter](https://ovirt.org/documentation/administration_guide/index#Creating_LVM_filter_storage_admin) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | oVirt currently does not support block storage with a block size of  4K. You must configure block storage in legacy (512b block) mode. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

|      | If your host is booting from SAN storage and loses connectivity to  the storage, the storage file systems become read-only and remain in  this state after connectivity is restored.  To prevent this situation, add a drop-in multipath configuration file on the root file system of the SAN for the boot LUN to ensure that it  is queued when there is a connection:  `# cat /etc/multipath/conf.d/host.conf multipaths {    multipath {        wwid *boot_LUN_wwid*        no_path_retry queue    }  }` |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 3.4. Preparing Gluster Storage

For information on setting up and configuring Gluster Storage, see the [*Gluster Storage Installation Guide*](https://docs.gluster.org/en/latest/Install-Guide/Overview/).

### 3.5. Customizing Multipath Configurations for SAN Vendors

If your RHV environment is configured to use multipath connections  with SANs, you can customize the multipath configuration settings to  meet requirements specified by your storage vendor. These customizations can override both the default settings and settings that are specified  in `/etc/multipath.conf`.

To override the multipath settings, do not customize `/etc/multipath.conf`. Because VDSM owns `/etc/multipath.conf`, installing or upgrading VDSM or oVirt can overwrite this file including any customizations it contains. This overwriting can cause severe  storage failures.

Instead, you create a file in the `/etc/multipath/conf.d` directory that contains the settings you want to customize or override.

VDSM executes the files in `/etc/multipath/conf.d` in  alphabetical order. So, to control the order of execution, you begin the filename with a number that makes it come last. For example, `/etc/multipath/conf.d/90-myfile.conf`.

To avoid causing severe storage failures, follow these guidelines:

- Do not modify `/etc/multipath.conf`. If the file contains user modifications, and the file is overwritten, it can cause unexpected storage problems.

|      | Not following these guidelines can cause catastrophic storage errors. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Prerequisites

- VDSM is configured to use the multipath module. To verify this, enter:

  ```
  # vdsm-tool is-configured --module multipath
  ```

Procedure

1. Create a new configuration file in the `/etc/multipath/conf.d` directory.

2. Copy the individual setting you want to override from `/etc/multipath.conf` to the new configuration file in `/etc/multipath/conf.d/<my_device>.conf`. Remove any comment marks, edit the setting values, and save your changes.

3. Apply the new configuration settings by entering:

   ```
   # systemctl reload multipathd
   ```

   |      | Do not restart the multipathd service. Doing so generates errors in the VDSM logs. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

Verification steps

1. Test that the new configuration performs as expected on a  non-production cluster in a variety of failure scenarios. For example,  disable all of the storage connections.
2. Enable one connection at a time and verify that doing so makes the storage domain reachable.

Additional resources

- [Recommended Settings for Multipath.conf](https://ovirt.org/documentation/installing_ovirt_as_a_self-hosted_engine_using_the_command_line/index#ref-Recommended_Settings_for_Multipath_conf_SHE_cli_deploy)
- [*Enterprise Linux DM Multipath*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/dm_multipath/)
- [Configuring iSCSI Multipathing](https://ovirt.org/documentation/administration_guide/index#Configuring_iSCSI_Multipathing)
- [How do I customize /etc/multipath.conf on my RHVH hypervisors? What values must not change and why?](https://access.redhat.com/solutions/3234761)

### 3.6. Recommended Settings for Multipath.conf

Do not override the following settings:

- user_friendly_names  no

  Device names must be consistent across all hypervisors. For example, `/dev/mapper/{WWID}`. The default value of this setting, `no`, prevents the assignment of arbitrary and inconsistent device names such as `/dev/mapper/mpath{N}` on various hypervisors, which can lead to unpredictable system behavior.    Do not change this setting to `user_friendly_names  yes`. User-friendly names are likely to cause unpredictable system behavior or failures, and are not supported.

- `find_multipaths	no`

  This setting controls whether oVirt Node tries to access devices  through multipath only if more than one path is available. The current  value, `no`, allows oVirt to access devices through multipath even if only one path is available.    Do not override this setting.

Avoid overriding the following settings unless required by the storage system vendor:

- `no_path_retry	4`

  This setting controls the number of polling attempts to retry when no paths are available. Before oVirt version 4.2, the value of `no_path_retry` was `fail` because QEMU had trouble with the I/O queuing when no paths were available. The `fail` value made it fail quickly and paused the virtual machine. oVirt version 4.2 changed this value to `4` so when multipathd detects the last path has failed, it checks all of  the paths four more times. Assuming the default 5-second polling  interval, checking the paths takes 20 seconds. If no path is up,  multipathd tells the kernel to stop queuing and fails all outstanding  and future I/O until a path is restored. When a path is restored, the  20-second delay is reset for the next time all paths fail. For more  details, see [the commit that changed this setting](https://gerrit.ovirt.org/#/c/88082/).

- `polling_interval	5`

  This setting determines the number of seconds between polling  attempts to detect whether a path is open or has failed. Unless the  vendor provides a clear reason for increasing the value, keep the  VDSM-generated default so the system responds to path failures sooner.

Before backing up the Engine, ensure it is updated to the latest  minor version. The Engine version in the backup file must match the  version of the new Engine.

## 4. Updating the oVirt Engine

Prerequisites

- The data center compatibility level must be set to the latest version to ensure compatibility with the updated storage version.

Procedure

1. On the Engine machine, check if updated packages are available:

   ```
   # engine-upgrade-check
   ```

2. Update the setup packages:

   ```
   # dnf update ovirt\*setup\*
   ```

3. Update the oVirt Engine with the `engine-setup` script. The `engine-setup` script prompts you with some configuration questions, then stops the `ovirt-engine` service, downloads and installs the updated packages, backs up and  updates the database, performs post-installation configuration, and  starts the `ovirt-engine` service.

   ```
   # engine-setup
   ```

   When the script completes successfully, the following message appears:

   ```
   Execution of setup completed successfully
   ```

   |      | The `engine-setup` script is also used during the oVirt  Engine installation process, and it stores the configuration values  supplied. During an update, the stored values are displayed when  previewing the configuration, and might not be up to date if `engine-config` was used to update configuration after installation. For example, if `engine-config` was used to update `SANWipeAfterDelete` to `true` after installation, `engine-setup` will output "Default SAN wipe after delete: False" in the configuration preview. However, the updated values will not be overwritten by `engine-setup`. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   |      | The update process might take some time. Do not stop the process before it completes. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

4. Update the base operating system and any optional packages installed on the Engine:

   ```
   # yum update --nobest
   ```

   |      | If you encounter a required Ansible package conflict during the update, see [Cannot perform yum update on my RHV manager (ansible conflict)](https://access.redhat.com/solutions/5480561). |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

   |      | If any kernel packages were updated, reboot the machine to complete the update. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

## 5. Backing up the Original Engine

Back up the original Engine using the `engine-backup` command, and copy the backup file to a separate location so that it can be accessed at any point during the process.

For more information about `engine-backup --mode=backup` options, see [Backing Up and Restoring the oVirt Engine](https://ovirt.org/documentation/administration_guide/index#sect-Backing_Up_and_Restoring_the_Red_Hat_Enterprise_Virtualization_Manager) in the *Administration Guide*.

Procedure

1. Log in to the original Engine and stop the `ovirt-engine` service:

   ```
   # systemctl stop ovirt-engine
   # systemctl disable ovirt-engine
   ```

   |      | Though stopping the original Engine from running is not obligatory,  it is recommended as it ensures no changes are made to the environment  after the backup is created. Additionally, it prevents the original  Engine and the new Engine from simultaneously managing existing  resources. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

2. Run the `engine-backup` command, specifying the name of the backup file to create, and the name of the log file to create to store the backup log:

   ```
   # engine-backup --mode=backup --file=file_name --log=log_file_name
   ```

3. Copy the files to an external server. In the following example, `storage.example.com` is the fully qualified domain name of a network storage server that will store the backup until it is needed, and `/backup/` is any designated folder or path.

   ```
   # scp -p file_name log_file_name storage.example.com:/backup/
   ```

After backing up the Engine, deploy a new self-hosted engine and restore the backup on the new virtual machine.

## 6. Restoring the Backup on a New Self-Hosted Engine

Run the `hosted-engine` script on a new host, and use the `--restore-from-file=*path/to/file_name*` option to restore the Engine backup during the deployment.

|      | If you are using iSCSI storage, and your iSCSI target filters  connections according to the initiator’s ACL, the deployment may fail  with a `STORAGE_DOMAIN_UNREACHABLE` error. To prevent this, you must update your iSCSI configuration before beginning the self-hosted engine deployment:   If you are redeploying on an existing host, you must update the host’s iSCSI initiator settings in `/etc/iscsi/initiatorname.iscsi`. The initiator IQN must be the same as was previously mapped on the iSCSI target, or updated to a new IQN, if applicable.  If you are deploying on a fresh host, you must update the iSCSI target configuration to accept connections from that host.   Note that the IQN can be updated on the host side (iSCSI initiator), or on the storage side (iSCSI target). |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Copy the backup file to the new host. In the following example, `host.example.com` is the FQDN for the host, and `/backup/` is any designated folder or path.

   ```
   # scp -p file_name host.example.com:/backup/
   ```

2. Log in to the new host.

3. If you are deploying on oVirt Node, `ovirt-hosted-engine-setup` is already installed, so skip this step. If you are deploying on Enterprise Linux, install the `ovirt-hosted-engine-setup` package:

   ```
   # dnf install ovirt-hosted-engine-setup
   ```

4. Use the `tmux` window manager to run the script to avoid losing the session in case of network or terminal disruption.

   Install and run `tmux`:

   ```
   # dnf -y install tmux
   # tmux
   ```

5. Run the `hosted-engine` script, specifying the path to the backup file:

   ```
   # hosted-engine --deploy --restore-from-file=backup/file_name
   ```

   To escape the script at any time, use CTRL+D to abort deployment.

6. Select **Yes** to begin the deployment.

7. Configure the network. The script detects possible NICs to use as a management bridge for the environment.

8. If you want to use a custom appliance for the virtual machine  installation, enter the path to the OVA archive. Otherwise, leave this  field empty to use the Engine Appliance.

9. Enter the root password for the Engine.

10. Enter an SSH public key that will allow you to log in to the Engine  as the root user, and specify whether to enable SSH access for the root  user.

11. Enter the virtual machine’s CPU and memory configuration.

    |      | The virtual machine must have the same amount of RAM as the physical  machine from which the Engine is being migrated. If you must migrate to a virtual machine that has less RAM than the physical machine from which  the Engine is migrated, see [Configuring the amount of RAM in Red Hat Virtualization Hosted Engine](https://access.redhat.com/articles/2705841). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

12. Enter a MAC address for the Engine virtual machine, or accept a  randomly generated one. If you want to provide the Engine virtual  machine with an IP address via DHCP, ensure that you have a valid DHCP  reservation for this MAC address. The deployment script will not  configure the DHCP server for you.

13. Enter the virtual machine’s networking details. If you specify **Static**, enter the IP address of the Engine.

    |      | The static IP address must belong to the same subnet as the host. For example, if the host is in 10.1.1.0/24, the Engine virtual machine’s IP must be in the same subnet range (10.1.1.1-254/24). |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

14. Specify whether to add entries for the Engine virtual machine and the base host to the virtual machine’s `/etc/hosts` file. You must ensure that the host names are resolvable.

15. Provide the name and TCP port number of the SMTP server, the email  address used to send email notifications, and a comma-separated list of  email addresses to receive these notifications:

16. Enter a password for the `admin@internal` user to access the Administration Portal.

    The script creates the virtual machine. This can take some time if the Engine Appliance needs to be installed.

    |      | If the host becomes non operational, due to a missing required  network or a similar problem, the deployment pauses and a message such  as the following is displayed:  `[ INFO  ] You can now connect to https://<host name>:6900/ovirt-engine/ and check the status of this host and eventually remediate it, please continue only when the host is listed as 'up' [ INFO  ] TASK [ovirt.ovirt.hosted_engine_setup : include_tasks] [ INFO  ] ok: [localhost] [ INFO  ] TASK [ovirt.ovirt.hosted_engine_setup : Create temporary lock file] [ INFO  ] changed: [localhost] [ INFO  ] TASK [ovirt.ovirt.hosted_engine_setup : Pause execution until /tmp/ansible.<random>_he_setup_lock is removed, delete it once ready to proceed]`  Pausing the process allows you to:   Connect to the Administration Portal using the provided URL.  Assess the situation, find out why the host is non operational, and fix whatever is needed. For example, if this deployment was restored from a backup, and the backup included *required networks* for the host cluster, configure the networks, attaching the relevant host NICs to these networks.  Once everything looks OK, and the host status is *Up*, remove the lock file presented in the message above. The deployment continues. |
    | ---- | ------------------------------------------------------------ |
    |      |                                                              |

17. Select the type of storage to use:

    - For NFS, enter the version, full address and path to the storage, and any mount options.

    - For iSCSI, enter the portal details and select a target and LUN from  the auto-detected lists. You can only select one iSCSI target during the deployment, but multipathing is supported to connect all portals of the same portal group.

      |      | To specify more than one iSCSI target, you must enable multipathing before deploying the self-hosted engine. See [*Enterprise Linux DM Multipath*](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/dm_multipath/) for details. There is also a [Multipath Helper](https://access.redhat.com/labs/multipathhelper/#/) tool that generates a script to install and configure multipath with different options. |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

    - For Gluster storage, enter the full address and path to the storage, and any mount options.

      |      | Only replica 1 and replica 3 Gluster storage are supported. Ensure you configure the volume as follows:  `gluster volume set *VOLUME_NAME* group virt gluster volume set *VOLUME_NAME* performance.strict-o-direct on gluster volume set *VOLUME_NAME* network.remote-dio off gluster volume set *VOLUME_NAME* storage.owner-uid 36 gluster volume set *VOLUME_NAME* storage.owner-gid 36 gluster volume set *VOLUME_NAME* network.ping-timeout 30` |
      | ---- | ------------------------------------------------------------ |
      |      |                                                              |

    - For Fibre Channel, select a LUN from the auto-detected list. The host bus adapters must be configured and connected, and the LUN must not  contain any existing data. To reuse an existing LUN, see [Reusing LUNs](https://ovirt.org/documentation/administration_guide/index#Reusing_LUNs) in the *Administration Guide*.

18. Enter the Engine disk size.

    The script continues until the deployment is complete.

19. The deployment process changes the Engine’s SSH keys. To allow client machines to access the new Engine without SSH errors, remove the  original Engine’s entry from the `.ssh/known_hosts` file on any client machines that accessed the original Engine.

When the deployment is complete, log in to the new Engine virtual machine and enable the required repositories.

## 7. Enabling the oVirt Engine Repositories

Ensure the correct repositories are enabled.

For oVirt 4.5: If you are going to install on RHEL or derivatives please follow [Installing on RHEL or derivatives](https://www.ovirt.org/download/install_on_rhel.html) first.

```
# dnf install -y centos-release-ovirt45
```

For oVirt 4.4:

```
# dnf install https://resources.ovirt.org/pub/yum-repo/ovirt-release44.rpm
```

Common procedure valid for both 4.4 and 4.5:

You can check which repositories are currently enabled by running `dnf repolist`.

1. Enable the `javapackages-tools` module.

   ```
   # dnf module -y enable javapackages-tools
   ```

2. Enable the `pki-deps` module.

   ```
   # dnf module -y enable pki-deps
   ```

3. Enable version 12 of the `postgresql` module.

   ```
   # dnf module -y enable postgresql:12
   ```

4. Enable version 2.3 of the `mod_auth_openidc` module.

   ```
   # dnf module -y enable mod_auth_openidc:2.3
   ```

5. Enable version 14 of the `nodejs` module:

   ```
   # dnf module -y enable nodejs:14
   ```

6. Synchronize installed packages to update them to the latest available versions.

   ```
   # dnf distro-sync --nobest
   ```

   Additional resources

   For information on modules and module streams, see the following sections in *Installing, managing, and removing user-space components*

   - [Module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#module-streams_introduction-to-modules)
   - [Selecting a stream before installation of packages](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#selecting-a-stream-before-installation-of-packages_installing-rhel-8-content)
   - [Resetting module streams](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#resetting-module-streams_removing-rhel-8-content)
   - [Switching to a later stream](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/installing_managing_and_removing_user-space_components/index#switching-to-a-later-stream_managing-versions-of-appstream-content)

The oVirt Engine has been migrated to a self-hosted engine setup. The Engine is now operating on a virtual machine on the new self-hosted  engine node.

The hosts will be running in the new environment, but cannot host the Engine virtual machine. You can convert some or all of these hosts to  self-hosted engine nodes.

## 8. Reinstalling an Existing Host as a Self-Hosted Engine Node

You can convert an existing, standard host in a self-hosted engine  environment to a self-hosted engine node capable of hosting the Engine  virtual machine.

|      | When installing or reinstalling the host’s operating system, oVirt  strongly recommends that you first detach any existing non-OS storage  that is attached to the host to avoid accidental initialization of these disks, and with that, potential data loss. |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

Procedure

1. Click **Compute** **Hosts** and select the host.
2. Click **Management** **Maintenance** and **OK**.
3. Click **Installation** **Reinstall**.
4. Click the **Hosted Engine** tab and select **DEPLOY** from the drop-down list.
5. Click **OK**.

The host is reinstalled with self-hosted engine configuration, and is flagged with a crown icon in the Administration Portal.

After reinstalling the hosts as self-hosted engine nodes, you can  check the status of the new environment by running the following command on one of the nodes:

```
# hosted-engine --vm-status
```

If the new environment is running without issue, you can decommission the original Engine machine.

## Appendix A: Preventing kernel modules from loading automatically

You can prevent a kernel module from being loaded automatically,  whether the module is loaded directly, loaded as a dependency from  another module, or during the boot process.

Procedure

1. The module name must be added to a configuration file for the `modprobe` utility.  This file must reside in the configuration directory `/etc/modprobe.d`.

   For more information on this configuration directory, see the man page `modprobe.d`.

2. Ensure the module is not configured to get loaded in any of the following:

   - `/etc/modprobe.conf`
   - `/etc/modprobe.d/*`
   - `/etc/rc.modules`
   - `/etc/sysconfig/modules/*`

   ```
   # modprobe --showconfig <_configuration_file_name_>
   ```

3. If the module appears in the output, ensure it is ignored and not loaded:

   ```
   # modprobe --ignore-install <_module_name_>
   ```

4. Unload the module from the running system, if it is loaded:

   ```
   # modprobe -r <_module_name_>
   ```

5. Prevent the module from being loaded directly by adding the `blacklist` line to a configuration file specific to the system - for example `/etc/modprobe.d/local-dontload.conf`:

   ```
   # echo "blacklist <_module_name_> >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | This step does not prevent a module from loading if it is a required or an optional dependency of another module. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

6. Prevent optional modules from being loading on demand:

   ```
   # echo "install <_module_name_>/bin/false" >> /etc/modprobe.d/local-dontload.conf
   ```

   |      | If the excluded module is required for other hardware, excluding it might cause unexpected side effects. |
   | ---- | ------------------------------------------------------------ |
   |      |                                                              |

7. Make a backup copy of your `initramfs`:

   ```
   # cp /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.$(date +%m-%d-%H%M%S).bak
   ```

8. If the kernel module is part of the `initramfs`, rebuild your initial `ramdisk` image, omitting the module:

   ```
   # dracut --omit-drivers <_module_name_> -f
   ```

9. Get the current kernel command line parameters:

   ```
   # grub2-editenv - list | grep kernelopts
   ```

10. Append `<_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>` to the generated output:

    ```
    # grub2-editenv - set kernelopts="<> <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

    For example:

    ```
    # grub2-editenv - set kernelopts="root=/dev/mapper/rhel_example-root ro crashkernel=auto resume=/dev/mapper/rhel_example-swap rd.lvm.lv=rhel_example/root rd.lvm.lv=rhel_example/swap <_module_name_>.blacklist=1 rd.driver.blacklist=<_module_name_>"
    ```

11. Make a backup copy of the `kdump initramfs`:

    ```
    # cp /boot/initramfs-$(uname -r)kdump.img /boot/initramfs-$(uname -r)kdump.img.$(date +%m-%d-%H%M%S).bak
    ```

12. Append `rd.driver.blacklist=<_module_name_>` to the `KDUMP_COMMANDLINE_APPEND` setting in `/etc/sysconfig/kdump` to omit it from the `kdump initramfs`:

    ```
    # sed -i '/^KDUMP_COMMANDLINE_APPEND=/s/"$/ rd.driver.blacklist=module_name"/' /etc/sysconfig/kdump
    ```

13. Restart the `kdump` service to pick up the changes to the `kdump initrd`:

    ```
      # kdumpctl restart
    ```

14. Rebuild the `kdump` initial `ramdisk` image:

    ```
      # mkdumprd -f /boot/initramfs-$(uname -r)kdump.img
    ```

15. Reboot the system.

### A.1. Removing a module temporarily

You can remove a module temporarily.

Procedure

1. Run `modprobe` to remove any currently-loaded module:

   ```
   # modprobe -r <module name>
   ```

2. If the module cannot be unloaded, a process or another module might  still be using the module. If so, terminate the process and run the `modpole` command written above another time to unload the module.

## Appendix B: Legal notice

Certain portions of this text first appeared in [Red Hat Virtualization 4.4 Migrating from a standalone Manager to a self-hosted engine](https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/migrating_from_a_standalone_manager_to_a_self-hosted_engine/index). Copyright © 2022 Red Hat, Inc. Licensed under a [Creative Commons Attribution-ShareAlike 4.0 Unported License](https://creativecommons.org/licenses/by-sa/4.0/).