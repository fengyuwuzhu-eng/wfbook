# 配置

[TOC]

## MicroK8s 服务

MicroK8s 将 Kubernetes 作为许多不同的服务通过 `systemd` 运行。这些服务的配置是从存储在 $SNAP_DATA/args 目录中的文件读取的，该目录通常指向 `/var/snap/microk8s/current/args` .

要重新配置服务，您需要编辑相应的文件，然后重新启动 MicroK8s。例如，要将 `debug` 级别日志记录添加到 containerd：

```bash
echo '-l=debug' | sudo tee -a /var/snap/microk8s/current/args/containerd
microk8s stop
microk8s start
```

以下 systemd 服务将由 MicroK8s 运行。从 1.21 版本开始，下面列出的许多单独服务被整合到一个 kubelite 服务中。

### `snap.microk8s.daemon-apiserver`

The Kubernetes API server validates and configures data for the API objects which include pods, services, replication controllers, and others. The  API Server services REST operations and provides the frontend to the  cluster’s shared state through which all other components interact.
Kubernetes API 服务器验证和配置 API 对象（包括 Pod、服务、复制控制器等）的数据。API 服务器为 REST作提供服务，并为集群的共享状态提供前端，所有其他组件都通过该状态进行交互。

从 1.21 版本开始，daemon-apiserver 被合并到 daemon-kubelite 中。

The apiserver daemon is started using the arguments in `${SNAP_DATA}/args/kube-apiserver`. The service configuration is described in full in the upstream [kube-apiserver documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/).
apiserver 守护进程使用 中的 `${SNAP_DATA}/args/kube-apiserver` 参数启动 。上游 [kube-apiserver 文档中](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/)对服务配置有完整的描述。

### `snap.microk8s.daemon-controller-manager`

The Kubernetes controller manager is a daemon that embeds the core control  loops shipped with Kubernetes. In Kubernetes, a controller is a control  loop which watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the  desired state.
Kubernetes 控制器管理器是一个守护程序，它嵌入了 Kubernetes 附带的核心控制循环。在 Kubernetes 中，控制器是一个控制回路，它通过 apiserver 监视集群的共享状态，并进行更改以尝试将当前状态移动到所需状态。

从 1.21 版本开始，daemon-controller-manager 被合并到 daemon-kubelite 中。

The kube-controller-manager daemon is started using the arguments in `${SNAP_DATA}/args/kube-controller-manager`. For more detail on these arguments, see the upstream [kube-controller-manager documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/).
kube-controller-manager 守护进程使用 中的 `${SNAP_DATA}/args/kube-controller-manager` 参数启动 。有关这些参数的更多详细信息，请参阅上游 [kube-controller-manager 文档](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/)。

### `snap.microk8s.daemon-proxy` 

The Kubernetes network proxy runs on each node. This reflects services as  defined in the Kubernetes API on each node and can do simple TCP, UDP,  and SCTP stream forwarding or round robin TCP, UDP, and SCTP forwarding  across a set of backends.
Kubernetes 网络代理在每个节点上运行。这反映了每个节点上 Kubernetes API 中定义的服务，并且可以在一组后端之间进行简单的 TCP、UDP 和 SCTP 流转发或循环 TCP、UDP 和 SCTP 转发。

从版本 1.21 开始，daemon-proxy 被合并到 daemon-kubelite 中。

The kube-proxy daemon is started using the arguments in
kube-proxy 守护进程使用
 `${SNAP_DATA}/args/kube-proxy`. For more details see the upstream [kube-proxy documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/).
`${SNAP_DATA}/args/kube-proxy` 的有关更多详细信息，请参阅上游 [kube-proxy 文档](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/)。

### `snap.microk8s.daemon-scheduler`

The Kubernetes scheduler is a workload-specific function which takes into  account individual and collective resource requirements, quality of  service requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload  interference, deadlines, and so on. Workload-specific requirements will  be exposed through the API as necessary.
Kubernetes 调度器是一项特定于工作负载的功能，它考虑了单个和集体的资源需求、服务质量要求、硬件/软件/策略约束、关联性和反关联性规范、数据局部性、工作负载间干扰、截止日期等。特定于工作负载的要求将根据需要通过 API 公开。

从 1.21 版本开始，daemon-scheduler 被合并到 daemon-kubelite 中。

The kube-scheduler daemon started using the arguments in `${SNAP_DATA}/args/kube-scheduler`. These are explained fully in the upstream [kube-scheduler documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/).
kube-scheduler 守护进程开始使用 中的 `${SNAP_DATA}/args/kube-scheduler` 参数。这些在上游 [kube-scheduler 文档中](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/)有完整的解释。

### `snap.microk8s.daemon-kubelet`

The *kubelet* is the primary “node agent” that runs on each node. The kubelet takes a set of PodSpecs(a YAML or JSON object that describes a pod) that are  provided and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn’t manage containers which were  not
*kubelet* 是在每个节点上运行的主要 “节点代理”。kubelet 采用提供的一组 PodSpecs（描述 Pod 的 YAML 或 JSON 对象），并确保这些 PodSpec 中描述的容器正在运行且运行正常。kubelet 不管理不是
 created by Kubernetes. 由 Kubernetes 创建。

从 1.21 版本开始，daemon-kubelet 被合并到 daemon-kubelite 中。

The kubelet daemon is started using the arguments in `${SNAP_DATA}/args/kubelet`. These are fully documented in the upstream
kubelet 守护进程使用 `${SNAP_DATA}/args/kubelet` 中的参数启动。这些在上游中都有完整的记录
 [kubelet documentation](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/).
[kubelet 文档](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/)。

### `snap.microk8s.daemon-kubelite`

The kubelite daemon runs as subprocesses the scheduler, controller, proxy, kubelet, and apiserver services. Each of  these individual services can be configured using arguments in the  matching ${SNAP_DATA}/args/ directory:
在版本 1.21 及更高版本中使用。kubelite 守护进程作为调度器、控制器、代理、kubelet 和 apiserver 服务的子进程运行。这些单独的服务中的每一个都可以使用匹配的 ${SNAP_DATA}/args/ 目录中的参数进行配置：

- scheduler ${SNAP_DATA}/args/kube-scheduler
  调度器 ${SNAP_DATA}/args/kube-scheduler
- controller ${SNAP_DATA}/args/kube-controller-manager
  控制器 ${SNAP_DATA}/args/kube-controller-manager
- proxy ${SNAP_DATA}/args/kube-proxy
  代理 ${SNAP_DATA}/args/kube-proxy
- kubelet ${SNAP_DATA}/args/kubelet
- apiserver ${SNAP_DATA}/args/kube-apiserver

### `snap.microk8s.daemon-containerd`

[Containerd](https://containerd.io/) is the container runtime used by MicroK8s to manage images and execute containers.
[Containerd](https://containerd.io/) 是 MicroK8s 用于管理镜像和执行容器的容器运行时。

The containerd daemon started using the configuration in
containerd 守护进程开始使用
 `${SNAP_DATA}/args/containerd` and `${SNAP_DATA}/args/containerd-template.toml`.
`${SNAP_DATA}/args/containerd` 和 `${SNAP_DATA}/args/containerd-template.toml` .

### `snap.microk8s.daemon-k8s-dqlite`

The k8s-dqlite daemon runs the dqlite datastore that is used to store the  state of Kubernetes. In clusters with more than three control plane  nodes this daemon ensures the high availability of the datastore.
k8s-dqlite 守护程序运行用于存储 Kubernetes 状态的 dqlite 数据存储。在具有三个以上 control plane 节点的集群中，此守护进程可确保数据存储的高可用性。

The k8s-dqlite daemon is started using the arguments in `${SNAP_DATA}/args/k8s-dqlite`
k8s-dqlite 守护程序使用 `${SNAP_DATA}/args/k8s-dqlite` 中的参数启动

### `snap.microk8s.daemon-etcd`

Etcd 是一个键/值数据存储，用于支持 Kubernetes 的组件。

如果禁用了 ha-cluster，则 etcd 将运行。如果启用了 ha-cluster，则运行 dqlite 而不是 etcd。

The etcd daemon is started using the arguments in `${SNAP_DATA}/args/etcd`. For more information on the configuration, see the [etcd documentation](https://etcd.io/docs/v3.4.0/op-guide/configuration/). Note that different channels of MicroK8s may use different versions of etcd.
etcd 守护进程使用 `${SNAP_DATA}/args/etcd` 中的参数启动。有关配置的更多信息，请参阅 [etcd 文档](https://etcd.io/docs/v3.4.0/op-guide/configuration/)。注意，MicroK8s 的不同通道可能会使用不同版本的 etcd。

### `snap.microk8s.daemon-flanneld`

Flannel is a CNI which gives a subnet to each host for use with container runtimes.
Flannel 是一个 CNI，它为每个主机提供一个子网，用于容器运行时。

如果未启用 ha-cluster，则 Flanneld 将运行。如果启用了 ha-cluster，则改为运行 calico。

The flannel daemon is started using the arguments in `${SNAP_DATA}/args/flanneld`. For more information on the configuration, see the  [flannel documentation](https://github.com/flannel-io/flannel).
flannel 守护程序使用 `${SNAP_DATA}/args/flanneld` 中的参数启动。有关配置的更多信息，请参阅 [Flannel 文档](https://github.com/flannel-io/flannel)。

### `calico-node`

Calico is a CNI which provides networking services. 
Calico 是一个提供网络服务的 CNI。Calico 在每个节点上运行。calico-node 不受 systemd 管理。

如果启用了 ha-cluster，则 Calico 将运行。如果未启用 ha-cluster，则改为 Flannel 运行。

### `snap.microk8s.daemon-traefik` 和 `snap.microk8s.daemon-apiserver-proxy` 

The traefik and apiserver-proxy daemons are used in worker nodes to as a  proxy to all API server control plane endpoints. The traefik daemon was  replaced by the apiserver proxy in 1.25+ releases.
traefik 和 apiserver-proxy 守护进程在 worker 节点中用作所有 API 服务器控制平面端点的代理。traefik 守护进程在 1.25+ 版本中被 apiserver 代理取代。

The most significant configuration option for both daemons is the API server endpoints found in `${SNAP_DATA}/args/traefik/provider.yaml`. For apiserver-proxy daemon (1.25+ on wards) the refresh frequency of the available control plane endpoints can be set in `${SNAP_DATA}/args/apiserver-proxy` via the `--refresh-interval` parameter.
这两个守护进程最重要的配置选项是 中的 API 服务器端点 `${SNAP_DATA}/args/traefik/provider.yaml` 。对于 apiserver-proxy 守护进程（wards为 1.25+），可以通过 `--refresh-interval` 参数设置 `${SNAP_DATA}/args/apiserver-proxy` 可用控制平面端点的刷新频率。

## 配置 MicroK8s 使用的主机接口

默认情况下，MicroK8s 将对所有控制平面（例如 `kube-apiserver`）和数据平面（例如 Calico vxlan 网络）服务使用默认的主机接口。对于具有多个接口或指定 VLAN 的生产部署，集群管理员可能希望配置 MicroK8s 正在使用的主机接口。

> *NOTE*: For the rest of this document, the term **default interface** refers to the host interface that includes a default gateway route.
> *注意：*对于本文档的其余部分，术语 **default interface** 是指包含默认网关路由的主机接口。

### Control Plane

#### kube-apiserver

`kube-apiserver` will bind to all host interfaces and advertise the default interface. You can configure it by editing `/var/snap/microk8s/current/args/kube-apiserver` and setting the following arguments:
默认情况下，`kube-apiserver` 将绑定到所有 host 接口并公布默认接口。您可以通过编辑 `/var/snap/microk8s/current/args/kube-apiserver` 和设置以下参数来配置它：

```bash
# /var/snap/microk8s/current/args/kube-apiserver
--advertise-address=10.10.10.10
--bind-address=0.0.0.0
--secure-port=16443
```

使用以下方式应用更改：

```bash
sudo snap restart microk8s
```

> *NOTE*: MicroK8s assumes that `kube-apiserver` is accessible from the local interface `127.0.0.1` for some of its default configuration. If you change the `bind-address`, you may want to update the address of the `apiserver` in the kubeconfig files in `/var/snap/microk8s/current/credentials` accordingly.
> *注意*：MicroK8s 假设 `kube-apiserver` 的某些默认配置可以从本地接口 `127.0.0.1` 访问。如果你更改了 `bind-address`，你可能希望 `/var/snap/microk8s/current/credentials` 相应地更新 kubeconfig 文件中 `apiserver` 的地址。

#### kube-controller-manager

By default, `kube-controller-manager` binds to the default host interface. You can configure it by editing `/var/snap/microk8s/current/args/kube-controller-manager` and setting the following arguments:
默认情况下，`kube-controller-manager` 绑定到默认的主机接口。您可以通过编辑 `/var/snap/microk8s/current/args/kube-controller-manager` 和设置以下参数来配置它：

```bash
# /var/snap/microk8s/current/args/kube-controller-manager
--bind-address=0.0.0.0
--secure-port=10257
```

使用以下方式应用更改：

```bash
sudo snap restart microk8s
```

> *NOTE*: The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients.
> *注意*：集群的其余部分以及 CLI/Web 客户端必须能够访问关联的接口。

#### kube-scheduler

`kube-scheduler` binds to the default host interface. You can configure it by editing `/var/snap/microk8s/current/args/kube-scheduler` and setting the following arguments:
默认情况下，`kube-scheduler` 绑定到默认的主机接口。您可以通过编辑 `/var/snap/microk8s/current/args/kube-scheduler` 和设置以下参数来配置它：

```bash
# /var/snap/microk8s/current/args/kube-scheduler
--bind-address=0.0.0.0
--secure-port=10259
```

使用以下方式应用更改：

```bash
sudo snap restart microk8s
```

> *NOTE*: The associated interface(s) must be reachable by the rest of the cluster, and by CLI/web clients.
> *注意*：集群的其余部分以及 CLI/Web 客户端必须能够访问关联的接口。

#### kube-proxy

By default, `kube-proxy` binds to localhost for its health endpoint, and binds NodePort services to all host interfaces. You can configure it by editing `/var/snap/microk8s/current/args/kube-proxy` and setting the following arguments:
默认情况下，`kube-proxy` 会绑定到 localhost 作为其健康端点，并将 NodePort 服务绑定到所有主机接口。您可以通过编辑 `/var/snap/microk8s/current/args/kube-proxy` 和设置以下参数来配置它：

```bash
# /var/snap/microk8s/current/args/kube-proxy
--bind-address=0.0.0.0
--healthz-bind-address=127.0.0.1
```

使用以下方式应用更改：

```bash
sudo snap restart microk8s
```

#### kubelet

`kubelet` binds to all host interfaces and advertises the default host interface. You can configure it by editing `/var/snap/microk8s/current/args/kubelet` and setting the following arguments:
默认情况下，`kubelet` 绑定到所有主机接口并公布默认主机接口。您可以通过编辑 `/var/snap/microk8s/current/args/kubelet` 和设置以下参数来配置它：

```bash
# /var/snap/microk8s/current/args/kubelet
--address=0.0.0.0
--node-ip=10.10.10.10
--healthz-bind-address=127.0.0.1
```

使用以下方式应用更改：

```bash
sudo snap restart microk8s
```

> *NOTE*: The address must be reachable by the rest of the cluster.
> *注意：*集群的其余部分必须可以访问该地址。

> *NOTE*: The address set in `--node-ip` is used as the InternalIP of the node, as shown in `microk8s kubectl get node -o wide`
> *注意*：`--node-ip` 中设置的地址作为节点的 InternalIP，如 `microk8s kubectl get node -o wide` 

#### dqlite

dqlite will bind to localhost (127.0.0.1). When forming a  MicroK8s cluster, dqlite will be updated to use the address that was  used in the `microk8s join` command.
默认情况下，dqlite 将绑定到 localhost （127.0.0.1）。在形成 MicroK8s 集群时，dqlite 将被更新为使用 `microk8s join` 命令中使用的地址。

### Data Plane

#### Calico VXLAN interface

Calico is the default CNI for MicroK8s, and VXLAN overlay networks are used to configure pod networking. By default, Calico uses the default host  interface for pod networking(`IP_AUTODETECTION_METHOD=first-found`). When forming a MicroK8s cluster, Calico is updated to use address that was used in the `microk8s join` command (`IP_AUTODETECTION_METHOD=can-reach=10.10.10.10`). You can configure it by editing `/var/snap/microk8s/current/args/cni-network/cni.yaml` and setting the following configuration parameters:
Calico 是 MicroK8s 的默认 CNI，VXLAN 叠加网络用于配置 Pod 网络。默认情况下，Calico 使用 pod networking（ `IP_AUTODETECTION_METHOD=first-found` ） 的默认主机接口。在形成 MicroK8s 群集时，Calico 将更新为使用 `microk8s join` 命令 （） 中使用的地址 `IP_AUTODETECTION_METHOD=can-reach=10.10.10.10` 。您可以通过编辑 `/var/snap/microk8s/current/args/cni-network/cni.yaml` 和设置以下配置参数来配置它：

```yaml
     - name: IP_AUTODETECTION_METHOD
       value: "first-found"
```

使用以下方式应用更改：

```bash
microk8s kubectl apply -f /var/snap/microk8s/current/args/cni-network/cni.yaml
```

#### NodePort services

`kube-proxy` will bind NodePort services to all host interfaces. This can be configured by editing `/var/snap/microk8s/current/args/kube-proxy` and setting the following arguments:
默认情况下，`kube-proxy` 会将 NodePort 服务绑定到所有主机接口。这可以通过编辑 `/var/snap/microk8s/current/args/kube-proxy` 和设置以下参数来配置：

```yaml
--nodeport-addresses=10.10.10.10
```

使用以下方式应用更改：

```bash
sudo snap restart microk8s
```

## MicroK8s CNI 配置

### Common

Kubernetes 需要 CNI 来配置 Pod 的网络命名空间和接口。CNI 配置包括两部分：

- One (or more) `conflist` files (in JSON format) describing the CNI configuration. This configuration is typically found at `/etc/cni/net.d`. In a MicroK8s cluster, the configuration path is instead `/var/snap/microk8s/current/args/cni-network`.
  描述 CNI 配置的一个或多个 `conflist` 文件（JSON 格式）。此配置通常位于 `/etc/cni/net.d` 中。在 MicroK8s 集群中，配置路径为 `/var/snap/microk8s/current/args/cni-network` 。
- CNI binaries, which implement the CNI specification. These binaries are typically found at `/opt/cni/bin`. In a MicroK8s cluster, the binaries are instead installed under `/var/snap/microk8s/current/opt/cni/bin`.
  CNI 二进制文件，用于实现 CNI 规范。这些二进制文件通常位于 `/opt/cni/bin`。在 MicroK8s 群集中，二进制文件安装在 `/var/snap/microk8s/current/opt/cni/bin` .

Pod 的默认 CIDR 为 `10.1.0.0/16`。所有 Pod 都分配了该范围内的 IP 地址。

默认服务 CIDR 为 `10.152.183.0/24` 。`10.152.183.1` 通常保留给 Kubernetes API ，而 `10.152.183.10` 将由 CoreDNS 使用。

### Calico

从版本 1.19 开始，MicroK8s 集群默认使用 Calico CNI，并使用 vxlan 后端进行配置。Calico 本身在集群内的 Pod 中运行。部署了两个组件：

- `daemonset/calico-node` which is the calico node process
  `daemonset/calico-node`，即 Calico 节点进程
- `deployment/calico-kube-controllers` which is a supporting service
   `deployment/calico-kube-controllers` 这是一项支持服务

The manifest that is used to deploy Calico is placed under `/var/snap/microk8s/current/args/cni-network/cni.yaml`. You can see the source manifest for the latest MicroK8s version [on GitHub](https://github.com/canonical/microk8s/blob/1.26/upgrade-scripts/000-switch-to-calico/resources/calico.yaml), or for a specific version, e.g. 1.26 [from the respective branch](https://github.com/canonical/microk8s/blob/1.26/upgrade-scripts/000-switch-to-calico/resources/calico.yaml)
用于部署 Calico 的清单位于 `/var/snap/microk8s/current/args/cni-network/cni.yaml` 下。可以在 [GitHub 上](https://github.com/canonical/microk8s/blob/1.26/upgrade-scripts/000-switch-to-calico/resources/calico.yaml)查看最新 MicroK8s 版本的源清单，也可以查看相应[分支](https://github.com/canonical/microk8s/blob/1.26/upgrade-scripts/000-switch-to-calico/resources/calico.yaml)中的特定版本（例如 1.26）的源清单

The manifest contains RBAC rules, ServiceAccounts, CustomResourceDefinitions, the `calico-node` daemonset and the `calico-kube-controllers` deployment.
该清单包含 RBAC 规则、ServiceAccounts、CustomResourceDefinitions、`calico-node` 守护进程集和 `calico-kube-controllers` 部署。

#### 配置 Calico

对于以下大多数情况，更改 Calico 配置的方法是修补已部署的 `cni.yaml`，然后使用以下命令将其重新应用于集群：

```bash
microk8s kubectl apply -f /var/snap/microk8s/current/args/cni-network/cni.yaml
```

对于 MicroK8s 集群，建议在所有集群节点上更新该文件。

#### 配置 Calico IP 自动检测方法

编辑 `/var/snap/microk8s/current/args/cni-network/cni.yaml` 并更改以下部分：

```yaml
# in daemonset/calico-node/containers[calico-node]/env
            - name: IP_AUTODETECTION_METHOD
              value: "first-found"          # change "first-found" to desired value
```

Example values for the autodetection method may be. Refer to the [Calico docs](https://docs.tigera.io/calico/latest/networking/ipam/ip-autodetection#change-the-autodetection-method) for more details:
autodetection 方法的示例值可能是。有关更多详细信息，请参阅 [Calico 文档](https://docs.tigera.io/calico/latest/networking/ipam/ip-autodetection#change-the-autodetection-method)：

- `interface=eth0`
- `kubernetes-internal-ip`
- `can-reach=10.10.10.10`

When you create a cluster with `microk8s add-node` and `microk8s join` and the IP autodetection method is `first-found`, MicroK8s will automatically change it to `can-reach=$ipaddress`, where `$ipaddress` is the IP address used in the `microk8s join` command.
当您使用 `microk8s add-node` 和 `microk8s join` 创建集群并`首先找到` IP 自动检测方法时，MicroK8s 会自动将其更改为 `can-reach=$ipaddress`，其中 `$ipaddress` 是 `microk8s join` 命令中使用的 IP 地址。

#### 在 BGP 模式下配置 Calico

编辑 `/var/snap/microk8s/current/args/cni-network/cni.yaml` 和更改以下部分：

```yaml
# in configmap/calico-config/data
    calico_backend: "vxlan"                         # Change "vxlan" to "bird"

# in daemonset/calico-node/containers[calico-node]/env
            - name: CALICO_IPV4POOL_VXLAN           # Change "CALICO_IPV4POOL_VXLAN" to "CALICO_IPV4POOL_IPIP"
              value: "Always"

# in daemonset/calico-node/livenessProbe/command
              - -felix-live                         # Change "-felix-live" to "-bird-live"

# in daemonset/calico-node/readinessProbe/command
              - -felix-ready                        # Change "-felix-ready" to "-bird-ready"
```

Then re-apply the Calico manifest. If Calico has already started and created a default IPPool, you might have to delete it with:
然后重新应用 Calico 清单。如果 Calico 已经启动并创建了默认 IPPool，您可能必须使用以下命令将其删除：

```bash
microk8s kubectl delete ippools default-ipv4-ippool
microk8s kubectl rollout restart daemonset/calico-node
```

See also [Calico docs](https://docs.tigera.io/calico/latest/networking/ipam/migrate-pools#migrate-from-one-ip-pool-to-another-1) for migrating to a different IP pool.
另请参阅 [Calico 文档](https://docs.tigera.io/calico/latest/networking/ipam/migrate-pools#migrate-from-one-ip-pool-to-another-1)以迁移到其他 IP 池。

#### 配置 Pod CIDR

The default pod CIDR is `10.1.0.0/16`. Assuming we want to change that to `10.100.0.0/16`, the following steps are needed
默认 Pod CIDR 为 `10.1.0.0/16`。假设我们想将其更改为 `10.100.0.0/16`，则需要执行以下步骤

Edit `/var/snap/microk8s/current/args/cni-network/cni.yaml` and change the following sections:
编辑 `/var/snap/microk8s/current/args/cni-network/cni.yaml` 和更改以下部分：

```yaml
# in daemonset/calico-node/containers[calico-node]/env
            - name: CALICO_IPV4POOL_CIDR
              value: "10.1.0.0/16"                     # Change "10.1.0.0/16" to "10.100.0.0/16"
```

Also, edit `/var/snap/microk8s/current/args/kube-proxy` and set the `--cluster-cidr` accordingly:
此外，请相应地编辑 `/var/snap/microk8s/current/args/kube-proxy` 并设置 `--cluster-cidr`：

```bash
--cluster-cidr=10.1.0.0/16                            # Change "10.1.0.0/16" to "10.100.0.0/16"
```

Then, restart MicroK8s and re-apply Calico with:
然后，重新启动 MicroK8s 并使用以下命令重新应用 Calico：

```bash
microk8s kubectl apply -f /var/snap/microk8s/current/args/cni-network/cni.yaml
sudo snap restart microk8s
```

If Calico has already started and created a default IPPool, you might have to delete it with:
如果 Calico 已经启动并创建了默认 IPPool，您可能必须使用以下命令将其删除：

```bash
microk8s kubectl delete ippools default-ipv4-pool
microk8s kubectl rollout restart daemonset/calico-node
```

#### 升级 Calico

升级 MicroK8s 集群*不会*升级集群上部署的 Calico 版本。这是设计使然，因此可以防止可能不需要的中断。目前，升级 Calico 的过程如下所述。

> *Note*: Executing the commands below in one of the cluster nodes will suffice, but it is recommended that you update `cni.yaml` on all nodes to prevent drifts in the configuration between the nodes.
> *注意*：在其中一个集群节点中执行以下命令就足够了，但建议您在所有节点上更新 `cni.yaml`，以防止节点之间的配置出现偏差。

```bash
# keep a backup of the existing cni.yaml
cp /var/snap/microk8s/current/args/cni-network/cni.yaml /var/snap/microk8s/current/args/cni-network/cni.yaml.backup

# copy calico manifest from snap
cp /snap/microk8s/current/upgrade-scripts/000-switch-to-calico/resources/calico.yaml /var/snap/microk8s/current/args/cni-network/cni.yaml

# (manual step), you can often skip if you have not done any changes
# compare cni.yaml and cni.yaml.backup, and make sure to carry any changes from cni.yaml.backup (e.g. ip autodetection method)
vim /var/snap/microk8s/current/args/cni-network/cni.yaml

# upgrade to the new Calico version
microk8s kubectl apply -f /var/snap/microk8s/current/args/cni-network/cni.yaml
```

### Flannel

Flannel is included in MicroK8s and is historically used for non-HA clusters.  In this scenario, the flanneld daemon is running on each node as a  system service. If you see that Calico has a negative impact on the  performance or memory usage of your cluster, you can switch to flannel  with the following command:
Flannel 包含在 MicroK8s 中，历史上用于非 HA 集群。在此方案中，flanneld 守护程序作为系统服务在每个节点上运行。如果您发现 Calico 对集群的性能或内存使用有负面影响，则可以使用以下命令切换到 flannel：

> *Warning*: This is a destructive operation, as it will also delete **all** resources from your cluster. You will have to re-deploy any  applications you were running and re-enable all addons you had  previously configured. This action is supposed to be performed in new  clusters only.
> *警告*：这是一个破坏性作，因为它还会删除集群**中的所有**资源。您必须重新部署您正在运行的任何应用程序并重新启用您之前配置的所有插件。此作应仅在新集群中执行。

```bash
microk8s disable ha-cluster --force
```

In this setup, flannel is using the etcd data store from the single control plane node in the cluster.
在此设置中，flannel 使用来自集群中单个 control plane 节点的 etcd 数据存储。

#### Flannel 与 Kubernetes 存储

Starting from MicroK8s 1.27, it is possible to configure the flanneld service  with the Kubernetes data store instead (Kubernetes Subnet Manager). This allows having HA MicroK8s clusters that are using the flannel CNI  instead of Calico.
从 MicroK8s 1.27 开始，可以使用 Kubernetes 数据存储（Kubernetes 子网管理器）配置 flanneld 服务。这允许使用 flannel CNI 而不是 Calico 的 HA MicroK8s 集群。

For this to work, configuration for a number of services must be adjusted.  The configuration you need for each node is shown below. Change `10.1.0.0/16` to the pod CIDR you want to use:
为此，必须调整许多服务的配置。每个节点所需的配置如下所示。将 `10.1.0.0/16` 更改为要使用的 Pod CIDR：

```bash
# configure kubernetes to allocate podCIDR per node
echo '
--cluster-cidr=10.1.0.0/16
--allocate-node-cidrs=true
' | sudo tee -a /var/snap/microk8s/current/args/kube-controller-manager

# restart kubelite for changes to take effect
sudo snap restart microk8s.daemon-kubelite

# configure Flannel arguments and environment
echo 'NODE_NAME=$(hostname)' | sudo tee /var/snap/microk8s/current/args/flanneld-env
echo '{"Network": "10.1.0.0/16", "Backend": {"Type": "vxlan"}}' | sudo tee /var/snap/microk8s/current/args/flannel-network-mgr-config
echo '
--iface=""
--subnet-file=$SNAP_COMMON/run/flannel/subnet.env
--ip-masq=true
--kube-subnet-mgr=true
--kubeconfig-file=$SNAP_DATA/credentials/kubelet.config
--net-config-path=$SNAP_DATA/args/flannel-network-mgr-config
' | sudo tee /var/snap/microk8s/current/args/flanneld

# remove calico
touch /var/snap/microk8s/current/var/lock/cni-loaded
sudo microk8s kubectl delete -f /var/snap/microk8s/current/args/cni-network/cni.yaml
rm /var/snap/microk8s/current/args/cni-network/*

# enable flanneld and restart
rm $SNAP_DATA/var/lock/no-flanneld
sudo snap restart microk8s.daemon-containerd microk8s.daemon-flanneld
```

#### 升级 Flannel

Flannel 和 flanneld 包含在 MicroK8s 中，并在快照刷新时自动更新。

### Kube OVN

从 MicroK8s 1.25 开始，可以使用 `kube-ovn` 插件将 Calico 替换为 KubeOVN：

```bash
microk8s enable kube-ovn --force
```

## How to use launch configurations 如何使用启动配置                                                 

In this HowTo we present the three ways launch configurations can be applied on a local MicroK8s node.  In this guide, we will use  the following configuration file, which deploys a MicroK8s node and  enables the `dns`, `ingress`, `rbac`, `hostpath-storage` and `registry` addons automatically. See the [launch configurations reference](https://microk8s.io/docs/ref-launch-config) for a reference of all possible configuration options as well as examples.
在本 HowTo 中，我们介绍了在本地 MicroK8s 节点上应用启动配置的三种方法。在本指南中，我们将使用以下配置文件，它部署了一个 MicroK8s 节点并自动启用 `dns`、`ingress`、`rbac`、`hostpath-storage` 和 `registry` 插件。请参阅 [启动配置参考](https://microk8s.io/docs/ref-launch-config) 有关所有可能的配置选项和示例的参考。

```yaml
# microk8s-config.yaml
---
version: 0.1.0
addons:
  - name: dns
  - name: rbac
  - name: ingress
  - name: hostpath-storage
  - name: registry

# The extra arguments listed here are not required, as they would be set by the 'rbac' and the 'dns'
# addons respectively. However, we set them to save time by not having to restart the Kubernetes
# services a few times during cluster bootstrap.
extraKubeAPIServerArgs:
  --authorization-mode: RBAC,Node
extraKubeletArgs:
  --cluster-dns: 10.152.183.10
  --cluster-domain: cluster.local
```

### 部署集群时

Starting with version 1.27, MicroK8s contains a [default launch configuration](https://github.com/canonical/microk8s/blob/master/microk8s-resources/microk8s.default.yaml) that automatically enables the `dns` addon for newly deployed clusters. It is possible to override the default launch configuration by creating `/var/snap/microk8s/common/.microk8s.yaml` like this:
从版本 1.27 开始，MicroK8s 包含一个[默认启动配置](https://github.com/canonical/microk8s/blob/master/microk8s-resources/microk8s.default.yaml)，该配置会自动为新部署的集群启用 `dns` 插件。可以通过像这样创建 `/var/snap/microk8s/common/.microk8s.yaml` 来覆盖默认启动配置：

```bash
sudo mkdir -p /var/snap/microk8s/common/
sudo cp microk8s-config.yaml /var/snap/microk8s/common/.microk8s.yaml
```

Similarly, to automatically sideload OCI images into the container runtime, add any `.tar` archives to `/var/snap/microk8s/common/sideload/<image>.tar`:
同样，要自动将 OCI 映像旁加载到容器运行时，请将任何 `.tar` 存档添加到 `/var/snap/microk8s/common/sideload/<image>.tar` ：

```bash
sudo mkdir -p /var/snap/microk8s/common/sideload
sudo cp *.tar /var/snap/microk8s/common/sideload/
```

The configuration file will be picked up automatically during the `snap install microk8s` command. After creating the launch configuration file, install MicroK8s as you normally would:
在 `snap install microk8s` 命令期间，将自动获取配置文件。创建启动配置文件后，像往常一样安装 MicroK8s：

```bash
sudo snap install microk8s --classic --channel 1.27
```

And then wait for the cluster to come up. After a while, list the running pods with `sudo microk8s kubectl get pod -A` and you should see:
然后等待集群启动。片刻之后，列出正在运行的 Pod，您应该 `sudo microk8s kubectl get pod -A` 会看到：

```bash
NAMESPACE            NAME                                      READY   STATUS    RESTARTS   AGE
kube-system          calico-node-wtjk2                         1/1     Running   0          2m25s
kube-system          coredns-f58c55f4c-xw87l                   1/1     Running   0          2m37s
kube-system          hostpath-provisioner-69cd9ff5b8-njtpc     1/1     Running   0          2m33s
kube-system          calico-kube-controllers-57b57c56f-wcj57   1/1     Running   0          2m25s
container-registry   registry-77c7575667-66kvf                 1/1     Running   0          2m20s
ingress              nginx-ingress-microk8s-controller-s9wft   1/1     Running   0          2m13s
```

> *NOTE*: When installing MicroK8s with an invalid launch configuration file, the `snap install microk8s` command will fail.
> *注意：*使用无效的启动配置文件安装 MicroK8s 时，`snap install microk8s` 命令将失败。

### 使用 `snap set`

It is also possible to set a launch configuration on an existing cluster using the `snap set microk8s config=<contents>` command.
还可以使用该 `snap set microk8s config=<contents>` 命令在现有集群上设置启动配置。

First, install MicroK8s: 首先，安装 MicroK8s：

```bash
sudo snap install microk8s --classic --channel 1.27
```

Then, apply the launch configuration by setting the `config` config option. The option value must be the file contents, not the path:
然后，通过设置 `config` config 选项来应用启动配置。option 值必须是文件内容，而不是路径：

```bash
sudo snap set microk8s config="$(cat microk8s-config.yaml)"
```

After a while, the configuration is applied to the local node.
一段时间后，配置将应用于本地节点。

### Using a content snap 3. 使用内容快照

This method is only supported when running MicroK8s with [strict-confinement](https://microk8s.io/docs/install-strict). This method makes sense in large scale deployments where the operator  wants to control the launch configuration of a large fleet of machines,  and requires the development and installation of an extra “content  snap”. The content snap is built with the custom launch configuration of the operator, and the configuration is applied by connecting the  content snap to MicroK8s.
仅当在严格[限制](https://microk8s.io/docs/install-strict)下运行 MicroK8s  时，才支持此方法。这种方法在大规模部署中是有意义的，因为作员希望控制大量机器的启动配置，并且需要开发和安装额外的“内容快照”。内容快照是使用  Operator 的自定义启动配置构建的，并且通过将内容快照连接到 MicroK8s 来应用配置。

See [microk8s-content-demo-snap](https://github.com/canonical/microk8s-content-demo-snap) for an example content snap. You can use it to scaffold your own content snaps. The [readme](https://github.com/canonical/microk8s-content-demo-snap#readme) contains instructions for editing and building the content snap.
有关示例内容快照，请参阅 [microk8s-content-demo-snap](https://github.com/canonical/microk8s-content-demo-snap)。您可以使用它来搭建自己的内容快照。[自述文件](https://github.com/canonical/microk8s-content-demo-snap#readme)包含有关编辑和构建内容快照的说明。

When using a content snap, the installation process looks like this:
使用内容贴靠时，安装过程如下所示：

First, install the content snap on the host:
首先，在主机上安装 content snap：

```bash
# install content snap from the store...
sudo snap install content-demo-microk8s

# ... or from a local file
sudo snap install ./content-demo-microk8s.snap --dangerous
```

Then, install MicroK8s with strict-confinement:
然后，安装具有严格限制的 MicroK8s：

```bash
sudo snap install microk8s --channel 1.27-strict
```

Finally, connect the content snap (replace `content-demo-microk8s` with your own content snap name):
最后，连接内容快照（将 `content-demo-microk8s` 替换为您自己的内容快照名称）：

```bash
sudo snap connect content-demo-microk8s:configuration microk8s
```

After a while, the configuration is applied to the local node.
一段时间后，配置将应用于本地节点。

## How to configure network Dual-stack 如何配置网络双栈

To ensure that pods and services do not end up on unintended network  subnets, it is crucial to perform network configuration at the early  stages of cluster or node installation.
为了确保 Pod 和服务不会最终出现在意外的网络子网上，在集群或节点安装的早期阶段执行网络配置至关重要。

From  version 1.28, MicroK8s introduced [launch configuration](https://microk8s.io/docs/explain-launch-config) options which allow the configuration of network CIDRs for the cluster  in both single and dual-stack.Defining the desired network settings in  this way prevents any inadvertent leakage of pods or services into  inappropriate subnets. By addressing network configuration from the  start, you can establish a secure and well-organised networking  environment for your Kubernetes cluster.
从版本 1.28 开始，MicroK8s 引入了[启动配置](https://microk8s.io/docs/explain-launch-config)选项，允许在单堆栈和双堆栈中为集群配置网络 CIDR。以这种方式定义所需的网络设置可以防止 Pod 或服务无意中泄漏到不适当的子网中。通过从一开始就解决网络配置问题，您可以为 Kubernetes 集群建立一个安全且组织良好的网络环境。

**Note:** On pre-1.28 MicroK8s deployments please try the manual configuration steps described in the [MicroK8s IPv6 DualStack HOW-TO](https://discuss.kubernetes.io/t/microk8s-ipv6-dualstack-how-to/14507).
**注意：**在 1.28 之前的 MicroK8s 部署中，请尝试 [MicroK8s IPv6 DualStack HOW-TO](https://discuss.kubernetes.io/t/microk8s-ipv6-dualstack-how-to/14507)中描述的手动配置步骤。

### Customise the network in your deployment 在部署中自定义网络](https://microk8s.io/docs/how-to-dual-stack#customise-the-network-in-your-deployment)

To configure your network customise the following launch configuration to match your needs:
要配置您的网络，请自定义以下启动配置以满足您的需求：

```auto
---
version: 0.1.0
extraCNIEnv:
  IPv4_SUPPORT: true
  IPv4_CLUSTER_CIDR: 10.3.0.0/16
  IPv4_SERVICE_CIDR: 10.153.183.0/24
  IPv6_SUPPORT: true
  IPv6_CLUSTER_CIDR: fd02::/64
  IPv6_SERVICE_CIDR: fd99::/108
extraSANs:
  - 10.153.183.1
```

Most of the fields are self explanatory:
大多数字段都是不言自明的：
 `IPv4_SUPPORT` and `IPv6_SUPPORT` mark the support for IPv4 and IPv6 respectively. `IPv4_CLUSTER_CIDR` and `IPv6_CLUSTER_CIDR` are the CIDRs from where pods will get their IPs.
`IPv4_SUPPORT` 和 `IPv6_SUPPORT` 分别表示对 IPv4 和 IPv6 的支持。`IPv4_CLUSTER_CIDR` 和 `IPv6_CLUSTER_CIDR` 是 Pod 将从中获取 IP 的 CIDR。
 `IPv4_SERVICE_CIDR` and `IPv6_SERVICE_CIDR` is where the services will get their IPs.
`IPv4_SERVICE_CIDR` 和 `IPv6_SERVICE_CIDR` 是服务获取其 IP 的位置。
 In the `extraSANs` section you want to have the IP of the Kubernetes service itself which is going to be the first IP in the `IPv4_CLUSTER_CIDR` range.
在 `extraSANs` 部分中，您希望拥有 Kubernetes 服务本身的 IP，该 IP 将成为 `IPv4_CLUSTER_CIDR` 范围内的第一个 IP。

Place the launch configuration in a location where it can be [picked up by MicroK8s during installation](https://microk8s.io/docs/add-launch-config). Here we assume the `/var/tmp/lc.yaml` includes your network customisation:
将启动配置放置在 [MicroK8s 在安装过程中可以拾取](https://microk8s.io/docs/add-launch-config)的位置。这里我们假设 `/var/tmp/lc.yaml` 包含您的网络自定义：

```auto
sudo mkdir -p /var/snap/microk8s/common/
sudo cp /var/tmp/lc.yaml /var/snap/microk8s/common/.microk8s.yaml
```

Install MicroK8s from a channel newer or equal to 1.28:
从高于 1.28 的通道安装 MicroK8s：

```auto
sudo snap install microk8s --classic --channel 1.28
```

**Note:** All nodes joining a cluster need to be pre-configured with the same  network configuration. The network configuration process above needs to  be repeated on every node joining the cluster.
**注意：**加入集群的所有节点都需要使用相同的网络配置进行预配置。需要在加入集群的每个节点上重复上述网络配置过程。

### [Verify dual-stack is configured correctly 验证双堆栈配置是否正确](https://microk8s.io/docs/how-to-dual-stack#verify-dual-stack-is-configured-correctly)

To test that the cluster is configured with dual-stack, apply the following manifest that creates a service with `ipFamilyPolicy: RequireDualStack`:
要测试集群是否配置了双堆栈，请应用以下清单，该清单使用 `ipFamilyPolicy: RequireDualStack` ：

```auto
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginxdualstack
spec:
  selector:
    matchLabels:
      run: nginxdualstack
  replicas: 1
  template:
    metadata:
      labels:
        run: nginxdualstack
    spec:
      containers:
      - name: nginxdualstack
        image: rocks.canonical.com/cdk/diverdane/nginxdualstack:1.0.0
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx6
  labels:
    run: nginxdualstack
spec:
  type: NodePort
  ipFamilies:
  - IPv6
  ipFamilyPolicy: RequireDualStack
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: nginxdualstack
```

Get the `nginx6` service IPv6 endpoint with `microk8s kubectl get svc` and query it with something similar to:
使用 `microk8s kubectl get svc` 获取 `nginx6` 服务 IPv6 终端节点，并使用类似于以下内容进行查询：

```auto
curl http://[fd99::d4ce]/ 
```

The curl output should look like:
curl 输出应如下所示：

```auto
<!DOCTYPE html>
<html>
<head>
<title>Kubernetes IPv6 nginx</title> 
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx on <span style="color:  #C70039">IPv6</span> Kubernetes!</h1>
<p>Pod: nginxdualstack-56fccfd475-hccqq</p>
</body>
</html>
```

## 带外部 LMA 的 MicroK8s

本页介绍如何配置 MicroK8s 以将日志和指标数据发送到外部可观测性堆栈，以进行日志记录、监控和警报。此示例中使用的 Observability 堆栈包括：

- Prometheus
- Elasticsearch
- Alertmanager

What’s covered in this documentation:
本文档涵盖的内容：

- What metrics endpoints are available
  哪些指标终端节点可用
- Any configuration needed to access metrics endpoints
  访问指标终端节点所需的任何配置
- The role of the metrics server
  度量服务器的角色
- Location of logs for log forwarding
  日志转发的日志位置

What isn’t covered: 不包括的内容：

- How to setup and configure Grafana, Prometheus, Alertmanager or any other  mentioned tools - please refer to the documentation for these products
  如何设置和配置 Grafana、Prometheus、Alertmanager 或任何其他提到的工具 - 请参阅这些产品的文档
- Recommendation of which logging exporter to use
  推荐使用哪个日志记录导出器

### Low level building blocks for monitoring & logging 用于监控和日志记录的低级构建块

#### Metrics

Kubernetes cluster [metrics](https://github.com/kubernetes/metrics) are available through designated API endpoints from each respective  service. Therefore it is important to be aware of the hosts each service runs on.
Kubernetes 集群[指标](https://github.com/kubernetes/metrics)可通过每个相应服务的指定 API 终端节点获得。因此，了解每个服务运行的主机非常重要。

- *kube-apiserver* runs on all hosts but serves the same content on any host
  *kube-apiserver* 在所有主机上运行，但在任何主机上提供相同的内容
- *kube-proxy* runs on every host
  *kube-proxy* 在每个主机上运行
- *kubelet* runs on every host
  *kubelet* 在每个主机上运行
- *kube-scheduler* runs on the three hosts (at most) which are dqlite voters
  *kube-scheduler* 运行在（最多）三台主机上，这些主机是 Dqlite 投票者
- *kube-controller-manager* runs on the three hosts (at most) which are dqlite voters
  *kube-controller-manager* 在三台主机（最多）上运行，这些主机是 Dqlite 投票者

Metrics [endpoints](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/):
指标[终端节点](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/)：

- On 

  kube-controller-manager, kube-proxy, kube-apiserver, kube-scheduler

  :

  
  在 *kube-controller-manager、kube-proxy、kube-apiserver、kube-scheduler* 上：

  - `/metrics` `/指标`

- On 

  kubelet

  :

   在 *kubelet* 上：

  - `/metrics` `/指标`
  - `/metrics/cadvisor`
    `/指标/cadvisor`
  - `/metrics/resource`
    `' metrics/resource'`
  - `/metrics/probes`

Access to the metrics endpoints is tuned with the service startup arguments. Configuration arguments are added to the files in `/var/snap/microk8s/current/args/`.
对指标终端节点的访问使用服务启动参数进行优化。配置参数将添加到 中的 `/var/snap/microk8s/current/args/` 文件中。

After updating the configuration file, MicroK8s should be restarted with:
更新配置文件后，应使用以下命令重新启动 MicroK8s：

```auto
microk8s.stop
microk8s.start
```

#### kube-apiserver

(edit the file `/var/snap/microk8s/current/args/kube-apiserver`)
（编辑文件 `/var/snap/microk8s/current/args/kube-apiserver` ）

| option 选择                         | type 类型     | default 违约 |
| ----------------------------------- | ------------- | ------------ |
| `--show-hidden-metrics-for-version` | string 字符串 |              |
| `--bind-address` `--绑定地址`       | ip IP 地址    | 0.0.0.0      |
| `--secure-port`                     | int           | 16443        |

#### kube-controller-manager

(edit the file `/var/snap/microk8s/current/args/kube-controller-manager`)
（编辑文件 `/var/snap/microk8s/current/args/kube-controller-manager` ）

| option 选择                         | type 类型     | default 违约 |
| ----------------------------------- | ------------- | ------------ |
| `--show-hidden-metrics-for-version` | string 字符串 |              |
| `--secure-port`                     | int           | 10257        |
| ` --bind-address` ` --绑定地址`     | ip IP 地址    | 0.0.0.0      |

#### kube-proxy

(edit the file `/var/snap/microk8s/current/args/kube-proxy`)
（编辑文件 `/var/snap/microk8s/current/args/kube-proxy` ）

| option 选择                                                  | type 类型        | default 违约    |
| ------------------------------------------------------------ | ---------------- | --------------- |
| `--show-hidden-metrics-for-version`                          | string 字符串    |                 |
| `--metrics-bind-address` `--metrics-bind-address（指标绑定地址）` | ip:port IP：端口 | 127.0.0.1:10249 |

#### kube-scheduler

(edit the file `/var/snap/microk8s/current/args/kube-scheduler`)
（编辑文件 `/var/snap/microk8s/current/args/kube-scheduler` ）

| option 选择                         | type 类型     | default 违约 |
| ----------------------------------- | ------------- | ------------ |
| `--show-hidden-metrics-for-version` | string 字符串 |              |
| `--bind-address` `--绑定地址`       | ip IP 地址    | 0.0.0.0      |
| `--port` `--港口`                   | int           | 10251        |

#### kubelet

(edit the file `/var/snap/microk8s/current/args/kubelet`)
（编辑文件 `/var/snap/microk8s/current/args/kubelet` ）

| option 选择          | type 类型  | default 违约 |
| -------------------- | ---------- | ------------ |
| `--address` `--地址` | ip IP 地址 | 0.0.0.0      |
| `--port` `--港口`    | int        | 10250        |

The `--show-hidden-metrics-for-version` argument allows you to indicate the previous version for which you want to show hidden metrics. Only the previous minor version is meaningful,  other values will not be allowed. The format is `<major>.<minor>`, e.g.: ‘1.16’. The purpose of this format is to make sure you have the  opportunity to notice if the next release hides additional metrics,  rather than being surprised when they are permanently removed in the  release after that.
该 `--show-hidden-metrics-for-version` 参数允许您指示要显示隐藏指标的先前版本。只有之前的 minor 版本才有意义，不允许使用其他值。格式为 `<major>.<minor>`，例如：'1.16'。此格式的目的是确保您有机会注意到下一个版本是否隐藏了其他量度，而不是在之后的版本中永久删除这些量度时感到惊讶。

The API endpoints above are subject to RBAC. Make sure you configure RBAC according to your needs (see the [example in the Kubernetes docs](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/)).
上述 API 端点受 RBAC 的约束。确保根据需要配置 RBAC（请参阅 [Kubernetes 文档中的示例](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/)）。

#### Logs

Pod logs to be imported to elasticsearch are found in `/var/log/containers/` in links to files in `/var/log/pods/` on all nodes as all MicroK8s nodes run kubelet.
要导入 Elasticsearch 的 Pod 日志位于 `/var/log/containers/` 中指向所有节点上 `/var/log/pods/` 中文件的链接中，因为所有 MicroK8s 节点都运行 kubelet。

To gather logs for the Kubernetes services you should be aware that all services are systemd services:
要收集 Kubernetes 服务的日志，您应该知道所有服务都是 systemd 服务：

- `snap.microk8s.daemon-cluster-agent`
- `snap.microk8s.daemon-containerd`
- `snap.microk8s.daemon-apiserver`
- `snap.microk8s.daemon-apiserver-kicker`
- `snap.microk8s.daemon-proxy`
  `snap.microk8s.守护程序代理`
- `snap.microk8s.daemon-kubelet`
- `snap.microk8s.daemon-scheduler`
- `snap.microk8s.daemon-controller-manager`

### 用于监控、日志记录和警报的高级工具

#### Metrics Server

[Metrics server](https://github.com/kubernetes-sigs/metrics-server) collects resource metrics from kubelets and exposes them in Kubernetes apiserver through the [Metrics API](https://github.com/kubernetes/metrics). Metrics server is not meant for non-autoscaling purposes.
[Metrics Server](https://github.com/kubernetes-sigs/metrics-server) 从 kubelets 中收集资源指标，并通过 [Metrics API](https://github.com/kubernetes/metrics) 将其暴露在 Kubernetes apiserver 中。Metrics Server 不适用于非自动扩展目的。

To get the metrics server running in MicroK8s, run the following:
要在 MicroK8s 中运行指标服务器，请运行以下命令：

```bash
microk8s enable metrics-server
```

Visit the [metric project’s docs](https://github.com/kubernetes-sigs/metrics-server) for alternative installation methods.
有关其他安装方法，请访问 [metric project 的文档](https://github.com/kubernetes-sigs/metrics-server)。

The focus of the metrics server is on CPU and memory as these metrics are  used by the Horizontal and Vertical Pod Autoscalers. As a user you can  view the metrics gathered with the `microk8s kubectl top` command.
指标服务器的重点是 CPU 和内存，因为这些指标由 Horizontal 和 Vertical Pod Autoscaler 使用。作为用户，您可以查看使用 `microk8s kubectl top` 命令收集的指标。

The metrics server will not give you accurate readings of resource usage metrics.
指标服务器不会为您提供资源使用情况指标的准确读数。

#### Prometheus

[Prometheus](https://github.com/prometheus/prometheus) collects metrics from configured targets at given intervals, evaluates  rule expressions, displays the results, and can trigger alerts when  specific conditions are observed.
[Prometheus](https://github.com/prometheus/prometheus) 按给定的时间间隔从配置的目标收集指标，评估规则表达式，显示结果，并可以在观察到特定条件时触发警报。

Prometheus gathers metrics from the Kubernetes endpoints discussed in the previous sections. Prometheus is closely associated with Alertmanager.
Prometheus 从前面部分讨论的 Kubernetes 终端节点收集指标。Prometheus 与 Alertmanager 密切相关。

Describing the deployment steps of Prometheus is outside the scope of this  document. However, you should be aware of which of the few deployment  layouts is at hand. The use case we expect to have is a number of  MicroK8s clusters all sending metrics to a central Prometheus  installation. A few ways to achieve this layout are:
描述 Prometheus 的部署步骤不在本文档的讨论范围之内。但是，您应该了解手头的少数部署布局中的哪一种。我们预期拥有的使用案例是许多 MicroK8s 集群，所有集群都将指标发送到中央 Prometheus 安装。实现此布局的几种方法是：

- **Scrape remote k8s clusters**: Run the [prometheus node-exporter](https://github.com/prometheus/node_exporter) and the [prometheus adapter for kubernetes metrics APIs](https://github.com/DirectXMan12/k8s-prometheus-adapter) (or any other exporter) to gather information from each MicroK8s cluster to a central Prometheus installation.
  **抓取远程 k8s 集群**：运行 [prometheus node-exporter](https://github.com/prometheus/node_exporter) 和 [prometheus adapter for kubernetes metrics API](https://github.com/DirectXMan12/k8s-prometheus-adapter)（或任何其他导出器），将信息从每个 MicroK8s 集群收集到中央 Prometheus 安装。
- **Remote Prometheus as Grafana data sources**: Run the entire Prometheus on each cluster and have a central Grafana  that would view each Prometheus as a different data source. In this case the Prometheus service needs to be exposed and be reachable outside the K8s cluster.
  **远程 Prometheus 作为 Grafana 数据源**：在每个集群上运行整个 Prometheus，并有一个中央 Grafana，它将每个 Prometheus 视为不同的数据源。在这种情况下，Prometheus 服务需要公开并可在 K8s 集群外部访问。
- **Federation**: With [federation](https://prometheus.io/docs/prometheus/latest/federation/) you can consolidate selected metrics from multiple k8s clusters.
  **联合**：通过[联合，](https://prometheus.io/docs/prometheus/latest/federation/)您可以整合来自多个 k8s 集群的选定指标。

For the setup 2 and 3, where Prometheus needs to be installed on each  cluster, you can make use of the Prometheus addon by running the  command:
对于设置 2 和 3，需要在每个集群上安装 Prometheus，您可以通过运行命令来使用 Prometheus 插件：

```bash
microk8s enable prometheus
```

Alternatively, visit the [Prometheus documentation](https://github.com/prometheus/prometheus) to select an alternative installation method.
或者，访问 [Prometheus 文档](https://github.com/prometheus/prometheus)以选择其他安装方法。

Based on the metrics gathered you may want to import respective Grafana dashboards. You can find some predefined dashboards [online](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/grafana/dashboards-1.14).
根据收集的指标，您可能需要导入相应的 Grafana 控制面板。您可以[在线](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/grafana/dashboards-1.14)找到一些预定义的仪表板。

#### Alertmanager

The [Alertmanager](https://github.com/prometheus/alertmanager) handles alerts sent by client applications such as the Prometheus  server. It takes care of deduplicating, grouping, and routing them to  the correct receiver integration such as email, PagerDuty, or OpsGenie.
[Alertmanager](https://github.com/prometheus/alertmanager) 处理客户端应用程序（如 Prometheus 服务器）发送的警报。它负责重复数据删除、分组以及将它们路由到正确的接收器集成，例如电子邮件、PagerDuty 或 OpsGenie。

For installation, please refer to the [Alertmanager documentation](https://github.com/prometheus/alertmanager).
有关安装，请参阅 [Alertmanager 文档](https://github.com/prometheus/alertmanager)。

A wide range of pre-defined alert rules are available online, for example, in the [Prometheus community GitHub repo](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14).
在线提供了各种预定义的警报规则，例如，在 [Prometheus 社区 GitHub 存储库](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/prometheus/rules-1.14)中。

# Using an external etcd with MicroK8s 将外部 etcd 与 MicroK8s 结合使用                                        

This guide will take you through the process of using an external etcd as  the underlying data store for MicroK8s. This makes sense depending on  your use case, for example:
本指南将引导您完成使用外部 etcd 作为 MicroK8s 的底层数据存储的过程。根据您的使用案例，这是有意义的，例如：

- In production environments, you want your data store to run in designated nodes (for example, nodes with SSD disks).
  在生产环境中，您希望数据存储在指定的节点（例如，具有 SSD 磁盘的节点）中运行。
- You want to manage the data store yourself, instead of depending on MicroK8s for backup and restore functionality.
  您希望自己管理数据存储，而不是依赖 MicroK8s 进行备份和恢复功能。

It is important to note that **while migrating a running MicroK8s cluster from dqlite to an external etcd  should be theoretically possible, it is highly discouraged. The  instructions below are meant to be followed in a fresh MicroK8s  installation**.
需要注意的是，**虽然理论上可以将正在运行的 MicroK8s 集群从 dqlite 迁移到外部 etcd 是可行的，但强烈建议不要这样做。以下说明适用于全新 MicroK8s 安装**。

Further, any existing resources and workloads that might be running in the cluster will be lost.
此外，集群中可能运行的任何现有资源和工作负载都将丢失。

### [Deploy an external etcd cluster 部署外部 etcd 集群](https://microk8s.io/docs/external-etcd#deploy-an-external-etcd-cluster)

The first step is to deploy the etcd cluster you wish to use. If you are  installing it manually or using an existing installation, the section  below details the information you will need to gather from the cluster.  Alternatively, you can use Juju to deploy etcd, also detailed below.
第一步是部署你想要使用的 etcd 集群。如果您手动安装或使用现有安装，则以下部分详细介绍了您需要从集群收集的信息。或者，你可以使用 Juju 来部署 etcd，下面也详细说明了。

#### Deploy manually 手动部署

Deploy your etcd cluster using your method of choice. There are no limitations other than making sure that your etcd members are reachable from  MicroK8s.
使用您选择的方法部署 etcd 集群。除了确保可以从 MicroK8s 访问您的 etcd 成员之外，没有任何限制。

After deploying, you will need the following:
部署后，您将需要以下内容：

- List of addresses of the etcd members, for example `https://10.10.0.1:2379,https://10.10.0.2:2379,https://10.10.0.3:2379`.
  etcd 成员的地址列表，例如 `https://10.10.0.1:2379,https://10.10.0.2:2379,https://10.10.0.3:2379` .
- CA certificate for the etcd cluster, we will refer to this as `ca.crt`.
  etcd 集群的 CA 证书，我们将其称为 `ca.crt`。
- Client certificate for connecting to the etcd cluster, we will refer to this as `client.crt`.
  连接到 etcd 集群的客户端证书，我们将此称为 `client.crt`。
- Client key for connecting to the etcd cluster, we will refer to this as `client.key`.
  Client key 连接到 etcd 集群，我们将此称为 `client.key`。

#### Deploy etcd in local LXD containers using Juju 使用 Juju 在本地 LXD 容器中部署 etcd

Use the following commands to [deploy a simple etcd cluster using Juju](https://ubuntu.com/kubernetes/docs/charm-etcd), running in LXD containers in a local machine:
使用以下命令[使用 Juju 部署一个简单的 etcd 集群](https://ubuntu.com/kubernetes/docs/charm-etcd)，在本地机器的 LXD 容器中运行：

If not already installed, you should install and configure LXD:
如果尚未安装，则应安装并配置 LXD：

```bash
sudo snap install lxd
sudo lxd init --auto
lxc network set lxdbr0 ipv6.address none
```

Similarly, install Juju via snap if it isn’t already installed:
同样，如果尚未安装 Juju，请通过 snap 安装 Juju：

```bash
sudo snap install juju --classic
```

Juju can then be used to deploy an etcd cluster
然后可以使用 Juju 来部署 etcd 集群

```bash
juju bootstrap lxd
juju deploy containers-easyrsa
juju deploy etcd -n 3
juju integrate easyrsa etcd
```

You can then check the status of the install using:
然后，您可以使用以下方法检查安装状态：

```bash
watch -d -c juju status etcd --color
```

After etcd is deployed successfully, create client credentials for use in MicroK8s:
成功部署 etcd 后，创建用于 MicroK8s 的客户端凭证：

```bash
juju run etcd/leader package-client-credentials --wait=2m
juju scp etcd/leader:etcd_credentials.tar.gz ./
tar xvzf etcd_credentials.tar.gz
```

This leaves us with a directory named `etcd_credentials`, with the following contents:
这给我们留下了一个名为 `etcd_credentials` 的目录，其中包含以下内容：

```auto
etcd_credentials/
etcd_credentials/client.key
etcd_credentials/client.crt
etcd_credentials/ca.crt
etcd_credentials/README.txt
```

Also, note the IPs of the etcd cluster members. These can be found with the `juju status etcd` command. For example, in the output below …
此外，请注意 etcd 集群成员的 IP。这些可以通过 `juju status etcd` 命令找到。例如，在下面的输出中...

```bash
Unit     Workload  Agent  Machine  Public address  Ports     Message
etcd/0*  active    idle   1        10.139.219.7    2379/tcp  Healthy with 3 known peers
etcd/1   active    idle   2        10.139.219.148  2379/tcp  Healthy with 3 known peers
etcd/2   active    idle   3        10.139.219.100  2379/tcp  Healthy with 3 known peers
```

… the IP addresses of the etcd cluster would be `10.139.219.7`, `10.139.219.148`, and `10.139.219.100`.
…etcd 集群的 IP 地址为 `10.139.219.7`、`10.139.219.148` 和 `10.139.219.100`。

### [Install MicroK8s 安装 MicroK8s](https://microk8s.io/docs/external-etcd#install-microk8s)

Install a new MicroK8s cluster with:
使用以下命令安装新的 MicroK8s 集群：

```auto
sudo snap install microk8s --classic
```

Do not add any extra nodes at this point.
此时不要添加任何额外的节点。

### [Switch MicroK8s to external etcd 将 MicroK8s 切换到外部 etcd](https://microk8s.io/docs/external-etcd#switch-microk8s-to-external-etcd)

1. Copy the etcd certificates from the previous step to the machine where MicroK8s is running if required (e.g. via `scp`). Move the cert files into the MicroK8s certificates directory. **Make sure to prefix them with `etcd-` to avoid overwriting existing files**:
   如果需要（例如，通过 `scp`），将上一步中的 etcd 证书复制到运行 MicroK8s 的计算机。将证书文件移动到 MicroK8s 证书目录中。**确保为它们加上 `etcd-` 的前缀，以避免覆盖现有文件**：

   ```bash
   mv etcd_credentials/client.key /var/snap/microk8s/current/certs/etcd-client.key
   mv etcd_credentials/client.crt /var/snap/microk8s/current/certs/etcd-client.crt
   mv etcd_credentials/ca.crt /var/snap/microk8s/current/certs/etcd-ca.crt
   ```

2. Edit `/var/snap/microk8s/current/args/kube-apiserver` to **remove** any existing `--etcd-servers` arguments (for MicroK8s 1.23+), or `--storage-backend=dqlite` (MicroK8s 1.22 and earlier).
   编辑 `/var/snap/microk8s/current/args/kube-apiserver` **以删除**任何现有的 `--etcd-servers` 参数（适用于 MicroK8s 1.23+）或 `--storage-backend=dqlite`（MicroK8s 1.22 及更早版本）。

3. Append the following lines in `/var/snap/microk8s/current/args/kube-apiserver`. Make sure to replace the IP addresses with the IP addresses of your etcd cluster members.
   在 中附加以下行 `/var/snap/microk8s/current/args/kube-apiserver` 。确保将 IP 地址替换为 etcd 集群成员的 IP 地址。

   ```bash
   --etcd-servers=https://10.139.219.7:2379,https://10.139.219.148:2379,https://10.139.219.100:2379
   --etcd-cafile=${SNAP_DATA}/certs/etcd-ca.crt
   --etcd-certfile=${SNAP_DATA}/certs/etcd-client.crt
   --etcd-keyfile=${SNAP_DATA}/certs/etcd-client.key
   ```

4. Restart MicroK8s 重启 MicroK8s

   ```bash
   sudo snap restart microk8s
   ```

5. Re-apply the Calico CNI: 重新应用 Calico CNI：

   ```bash
   microk8s kubectl apply -f /var/snap/microk8s/current/args/cni-network/cni.yaml
   ```

### [That’s all! 就这样！](https://microk8s.io/docs/external-etcd#thats-all)

Congratulations! Your MicroK8s installation is now running using your external etcd as a data store!
祝贺！您的 MicroK8s 安装现在正在使用外部 etcd 作为数据存储运行！

### [Notes for MicroK8s cluster MicroK8s 集群说明](https://microk8s.io/docs/external-etcd#notes-for-microk8s-cluster)

#### Adding worker only nodes (MicroK8s 1.23+) 添加仅限 worker 的节点 （MicroK8s 1.23+）

When using an external data store, adding new worker-only nodes works as is:
使用外部数据存储时，添加新的仅限 worker 的节点按原样工作：

1. In the existing MicroK8s node, run the `add-node` command:
   在已有的 MicroK8s 节点中，执行 `add-node` 命令：

   ```bash
   microk8s add-node
   ```

2. In the new node, join as worker:
   在新节点中，以 worker 身份加入：

   ```bash
   microk8s join 10.0.1.1:25000/XXXXXXXXXXXXXXXXXXXXXXXXXXX/YYYYYYYYYY --worker
   ```

#### Adding more control plane nodes 添加更多 control plane 节点

When adding new control plane nodes to the cluster, an extra manual step is required:
在向集群添加新的 control plane 节点时，需要一个额外的手动步骤：

1. In an existing MicroK8s node, run the `add-node` command:
   在现有的 MicroK8s 节点中，运行 `add-node` 命令：

   ```bash
   microk8s add-node
   ```

2. In the new node, join the cluster:
   在新节点中，加入集群：

   ```bash
   microk8s join 10.0.1.1:25000/XXXXXXXXXXXXXXXXXXXXXXXXXXX/YYYYYYYYYY
   ```

3. Copy the etcd certificates to the new MicroK8s node (e.g. via `scp`). Then, move them into the MicroK8s certificates directory. Make sure to prefix them with `etcd-` to avoid overwriting existing files:
   将 etcd 证书复制到新的 MicroK8s 节点（例如通过 `scp`）。然后，将它们移动到 MicroK8s 证书目录中。确保为它们加上 `etcd-` 的前缀，以避免覆盖现有文件：

   ```bash
   mv etcd_credentials/client.key /var/snap/microk8s/current/certs/etcd-client.key
   mv etcd_credentials/client.crt /var/snap/microk8s/current/certs/etcd-client.crt
   mv etcd_credentials/ca.crt /var/snap/microk8s/current/certs/etcd-ca.crt
   ```

4. Edit `/var/snap/microk8s/current/args/kube-apiserver` and **remove** any existing `--etcd-servers` arguments (for MicroK8s 1.23+), or `--storage-backend=dqlite` (MicroK8s 1.22 and earlier).
   编辑 `/var/snap/microk8s/current/args/kube-apiserver` 并**删除**任何现有的 `--etcd-servers` 参数（适用于 MicroK8s 1.23+）或 `--storage-backend=dqlite`（MicroK8s 1.22 及更早版本）。

5. Append the following lines in `/var/snap/microk8s/current/args/kube-apiserver`. Make sure to replace the IP addresses with the IP addresses of your etcd cluster members.
   在 中附加以下行 `/var/snap/microk8s/current/args/kube-apiserver` 。确保将 IP 地址替换为 etcd 集群成员的 IP 地址。

   ```bash
   --etcd-servers=https://10.139.219.7:2379,https://10.139.219.148:2379,https://10.139.219.100:2379
   --etcd-cafile=${SNAP_DATA}/certs/etcd-ca.crt
   --etcd-certfile=${SNAP_DATA}/certs/etcd-client.crt
   --etcd-keyfile=${SNAP_DATA}/certs/etcd-client.key
   ```

6. Restart MicroK8s 重启 MicroK8s

   ```bash
   sudo snap restart microk8s
   ```

# Multi-user MicroK8s 多用户 MicroK8s

MicroK8s is inherently multi-user capable in the sense that any user added to
MicroK8s 本质上具有多用户功能，从某种意义上说，任何添加到
 the `microk8s` group can run commands against the cluster.
`microk8s` 组可以针对集群运行命令。

In some circumstances, it may be desirable to have a degree of  user-isolation, e.g. when multiple users are accessing a MicroK8s  cluster. MicroK8s is a full implementation of Kubernetes, and therefore  any existing strategy for handling multiple users can be applied. There  is extensive upstream [documentation](https://kubernetes.io/docs/reference/access-authn-authz/authentication/) relating to managing users.
在某些情况下，可能需要一定程度的用户隔离，例如，当多个用户正在访问 MicroK8s 集群时。MicroK8s 是 Kubernetes 的完整实现，因此可以应用任何处理多个用户的现有策略。有大量与管理用户相关的上游[文档](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)。

> Note: If you are using the built-in dashboard addon, see the following docs to configure your user access: [dashboard/docs/user/access-control/creating-sample-user.md at master · kubernetes/dashboard · GitHub](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)
> 注意：如果你正在使用内置的 dashboard 插件，请参阅以下文档来配置你的用户访问权限： [dashboard/docs/user/access-control/creating-sample-user.md 在 master · kubernetes/dashboard ·GitHub的](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md)

As a guide though, the following steps are recommended.
不过，作为指导，建议执行以下步骤。

1. Enable Role Based Access Control (RBAC):
   启用基于角色的访问控制 （RBAC）：

```bash
microk8s enable rbac
```

1. If required, create a specific namespace for the user (in this case,  ‘alice’) by generating and applying a namespace object such as:
   如果需要，通过生成和应用命名空间对象（在本例中为 'alice'）为用户创建特定命名空间，例如：

namespace.json: namespace.json：

```json
{
  "apiVersion": "v1",
  "kind": "Namespace",
  "metadata": {
    "name": "alice",
    "labels": {
      "name": "alice"
    }
  }
}
microk8s kubectl apply -f namespace.json
```

1. Create and apply a rolebinding
   创建并应用 rolebinding

RBAC uses roles to control what aspects of a namespace can be viewed and/or modified. (see upstream [rbac documentation](https://kubernetes.io/docs/reference/access-authn-authz/rbac/))
RBAC 使用角色来控制可以查看和/或修改命名空间的哪些方面。（参见上游 [RBAC 文档](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)）

E.g to access pods: 例如，要访问 Pod：

```auto
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: alice
  name: alice-pods
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

To bind role this role to the user, run:
要将 role 此角色绑定到用户，请运行：

```auto
kubectl create rolebinding rolebindingname --role alice-pods --user alice
```

1. Install `kubectl` 安装 `kubectl`

```bash
sudo snap install kubectl
```

This installs a standalone version of the `kubectl` command, which can be used
这将安装 `kubectl` 命令的独立版本，该命令可以使用
 instead of the built-in MicroK8s version of kubectl.
而不是内置的 MicroK8s 版本的 kubectl。

1. Authenticate the user. 对用户进行身份验证。

There are different ways of authenticating users for Kubernetes. x509  certificates are recommended. You can read the documentation for  supported methods in the [upstream documentation](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)
有多种方法可以对 Kubernetes 的用户进行身份验证。建议使用 x509 证书。您可以在[上游文档中](https://kubernetes.io/docs/reference/access-authn-authz/authentication/)阅读有关支持方法的文档

1. Create a local kubectl config
   创建本地 kubectl 配置

You can run the command:
您可以运行以下命令：

```bash
microk8s config
```

…to output the contents of the configuration file used by MicroK8s. This  can be used as the basis for a user config file - bear in mind that the  user information and the authentication should be matched to the user  and the authentication method used.
…输出 MicroK8s 使用的配置文件内容。这可以用作用户配置文件的基础 - 请记住，用户信息和身份验证应与用户和所使用的身份验证方法相匹配。

# Configure OIDC with Dex for a MicroK8s cluster 使用 Dex 为 MicroK8s 集群配置 OIDC

#### On this page 本页内容

​                                                [Overview 概述](https://microk8s.io/docs/oidc-dex#overview)                                                      [Install MicroK8s 安装 MicroK8s](https://microk8s.io/docs/oidc-dex#install-microk8s)                                                      [Generate self-signed CA and certificates for Dex
为 Dex 生成自签名 CA 和证书](https://microk8s.io/docs/oidc-dex#generate-self-signed-ca-and-certificates-for-dex)                                                      [Deploy Dex 部署 Dex](https://microk8s.io/docs/oidc-dex#deploy-dex)                                                      [Configure MicroK8s API server to connect to Dex
配置 MicroK8s API 服务器以连接到 Dex](https://microk8s.io/docs/oidc-dex#configure-microk8s-api-server-to-connect-to-dex)                                                                                            [Important notes 重要说明](https://microk8s.io/docs/oidc-dex#important-notes)                                                                                                    [Generate a kubeconfig file for clients authenticating via OIDC
为通过 OIDC 进行身份验证的客户端生成 kubeconfig 文件](https://microk8s.io/docs/oidc-dex#generate-a-kubeconfig-file-for-clients-authenticating-via-oidc)                                                      [Onboard a new client 载入新客户端](https://microk8s.io/docs/oidc-dex#onboard-a-new-client)                                                      [That’s all! 就这样！](https://microk8s.io/docs/oidc-dex#thats-all)                                                                                            [Configure RBAC (optional)
配置 RBAC（可选）](https://microk8s.io/docs/oidc-dex#configure-rbac-optional)                                                                                                            

## [Overview 概述](https://microk8s.io/docs/oidc-dex#overview)

This guide is intended for development purposes only, please refer to the official [Dex documentation](https://github.com/dexidp/dex) for guidance on how to secure the deployment and configure additional connectors/identity providers.
本指南仅用于开发目的，请参阅官方 [Dex 文档](https://github.com/dexidp/dex)，以获取有关如何保护部署和配置其他连接器/身份提供商的指导。

Dex will be deployed on top of the MicroK8s cluster, and exposed as a simple NodePort service.
Dex 将部署在 MicroK8s 集群之上，并作为简单的 NodePort 服务公开。

This how-to will guide you through the following steps:
本作方法将指导您完成以下步骤：

- Install MicroK8s 安装 MicroK8s
- Generate a self-signed certificate for Dex
  为 Dex 生成自签名证书
- Deploy Dex on MicroK8s 在 MicroK8s 上部署 Dex
- Configure MicroK8s API server to connect to Dex
  配置 MicroK8s API 服务器以连接到 Dex
- Generate a kubeconfig file for clients authenticating via OIDC
  为通过 OIDC 进行身份验证的客户端生成 kubeconfig 文件
- Onboard a new client 载入新客户端
- Configure RBAC (Optional)
  配置 RBAC（可选）

## [Install MicroK8s 安装 MicroK8s](https://microk8s.io/docs/oidc-dex#install-microk8s)

Install the latest version of MicroK8s with the following command:
使用以下命令安装最新版本的 MicroK8s：

```bash
sudo snap install microk8s --classic
sudo usermod -a -G microk8s $USER
newgrp -
```

(it may be necessary to restart your session for the user to be added to the group).
（可能需要重新启动会话，才能将用户添加到组中）。

## [Generate self-signed CA and certificates for Dex 为 Dex 生成自签名 CA 和证书](https://microk8s.io/docs/oidc-dex#generate-self-signed-ca-and-certificates-for-dex)

Dex can be served over HTTP or HTTPS, but `kube-apiserver` only accepts issuers that are served over HTTPS for security reasons.  This means that we will need a CA certificate, as well as certificates  for our Dex server.
Dex 可以通过 HTTP 或 HTTPS 提供，但出于安全原因，`kube-apiserver` 只接受通过 HTTPS 提供的颁发者。这意味着我们需要 CA 证书，以及 Dex 服务器的证书。

We will use the following script for this purpose. Replace `andromeda` and `10.10.10.142` with the IP address and/or the hostname of your MicroK8s cluster. Save it as `certificates.sh`
为此，我们将使用以下脚本。将 `andromeda` 和 `10.10.10.142` 替换为 MicroK8s 集群的 IP 地址和/或主机名。另存为 `certificates.sh`

> **NOTE**: The IP address will be used in a few places. If you are testing and MicroK8s is running locally, you can stick to `localhost` instead.
> **注意**：IP 地址将在几个地方使用。如果您正在测试并且 MicroK8s 在本地运行，则可以改用 `localhost`。

> **NOTE**: For production systems, consider using Cert Manager and Let’s Encrypt certificates instead.
> **注意**：对于生产系统，请考虑改用 Cert Manager 和 Let's Encrypt 证书。

```bash
#!/bin/bash

mkdir -p ssl

cat << EOF > ssl/req.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name

[req_distinguished_name]

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[alt_names]
DNS.1 = andromeda
IP.1 = 10.10.10.142
EOF

openssl genrsa -out ssl/ca.key 4096
openssl req -x509 -new -nodes -key ssl/ca.key -days 3650 -out ssl/ca.crt -subj "/CN=kube-ca"

openssl genrsa -out ssl/tls.key 4096
openssl req -new -key ssl/tls.key -out ssl/tls.csr -subj "/CN=kube-ca" -config ssl/req.cnf
openssl x509 -req -in ssl/tls.csr -CA ssl/ca.crt -CAkey ssl/ca.key -CAcreateserial -out ssl/tls.crt -days 3650 -extensions v3_req -extfile ssl/req.cnf
```

Then, build your certificates with:
然后，使用以下方法构建您的证书：

```bash
chmod +x ./certificates.sh
./certificates.sh
```

The following 4 files will be created:
将创建以下 4 个文件：

- **ssl/ca.key**: This is the private key of our CA. It is used to sign new certificates. We will not need it for the rest of this tutorial, but make sure to  keep it in a safe place.
  **ssl/ca.key**：这是我们 CA 的私钥。它用于对新证书进行签名。在本教程的其余部分，我们不需要它，但请确保将其保存在安全的地方。
- **ssl/ca.crt**: This is the root CA certificate. `kube-apiserver` will use this when connecting to Dex.
  **ssl/ca.crt**：这是根 CA 证书。`kube-apiserver` 在连接到 Dex 时将使用它。
- **ssl/tls.key**: This is the private key that will be used by Dex to serve HTTPS traffic.
  **ssl/tls.key**：这是 Dex 将用于提供 HTTPS 流量的私钥。
- **ssl/tls.crt**: This is the certificate that will be used by Dex to serve HTTPS traffic.
  **ssl/tls.crt**：这是 Dex 将用于提供 HTTPS 流量的证书。

Next, we will create a Kubernetes TLS secret named `dex-certs`, containing the certificate and key for Dex:
接下来，我们将创建一个名为 `dex-certs` 的 Kubernetes TLS 密钥，其中包含 Dex 的证书和密钥：

```bash
microk8s kubectl create secret tls dex-certs --cert=ssl/tls.crt --key=ssl/tls.key
```

## [Deploy Dex 部署 Dex](https://microk8s.io/docs/oidc-dex#deploy-dex)

As mentioned in the beginning, we will run Dex as a simple Deployment on  our MicroK8s cluster, using the official Helm Chart. Refer to the [Dex documentation](https://dexidp.io/docs/getting-started/) for more details on deploying Dex.
如开头所述，我们将使用官方 Helm Chart 在我们的 MicroK8s 集群上以简单的 Deployment 形式运行 Dex。请参阅 [Dex 文档 有关部署 Dex](https://dexidp.io/docs/getting-started/) 的更多详细信息。

Make sure to replace `10.10.10.142` with the IP address of your MicroK8s cluster,  as in the previous step when generating the certificates.
确保将 `10.10.10.142` 替换为 MicroK8s 集群的 IP 地址，如上一步生成证书时所示。

> **NOTE**: The dex configuration below only defines a static user `admin@example.com` with password `password`. Adding more connectors (e.g. via LDAP, Keystone, etc) is outside the  scope of this guide. Please refer to the Dex documentation instead.
> **注意**：下面的 dex 配置仅定义了一个具有密码 `password 的`静态用户 `admin@example.com`。添加更多连接器（例如通过 LDAP、Keystone 等）不在本指南的讨论范围之内。请参阅 Dex 文档。

```yaml
# config.yaml
---
volumes:
- name: certs
  secret:
    secretName: dex-certs
volumeMounts:
- name: certs
  readOnly: true
  mountPath: /certs
https:
  enabled: true
service:
  type: NodePort
  ports:
    https:
      nodePort: 31000

# Dex configuration
config:
  issuer: https://10.10.10.142:31000/dex
  storage:
    type: memory
  web:
    https: 0.0.0.0:5554
    tlsCert: /certs/tls.crt
    tlsKey: /certs/tls.key
  staticClients:
  - name: Kubernetes
    id: kubernetes
    secret: super-safe-client-secret
    redirectURIs:
    - http://localhost:8000  # for kubelogin
  enablePasswordDB: true
  staticPasswords:
  - email: "admin@example.com"
    # bcrypt hash of the string "password": $(echo password | htpasswd -BinC 10 admin | cut -d: -f2)
    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
    username: "admin"
    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"
```

Deploy Dex with: 使用 Dex 部署 Dex：

```bash
microk8s enable helm3
microk8s.helm3 repo add dex https://charts.dexidp.io
microk8s.helm3 repo update
microk8s.helm3 install dex dex/dex -f config.yaml
```

Wait for dex to deploy, then verify that the CA cert can be used to trust the Dex certificate:
等待 dex 部署，然后验证 CA 证书是否可用于信任 Dex 证书：

```bash
curl https://10.10.10.142:31000/dex/auth --cacert ssl/ca.crt
```

If this prints some HTML in the terminal, and **NOT** a warning related to a missing issuer certificate, then you should be good to go.
如果这在终端中打印了一些 HTML，**而不是**与缺少颁发者证书相关的警告，那么您应该很高兴。

## [Configure MicroK8s API server to connect to Dex 配置 MicroK8s API 服务器以连接到 Dex](https://microk8s.io/docs/oidc-dex#configure-microk8s-api-server-to-connect-to-dex)

1. Copy the Dex CA `ssl/ca.crt` in a place where the MicroK8s snap can access it:
   将 Dex CA `ssl/ca.crt` 复制到 MicroK8s 快照可以访问它的地方：

   ```bash
   cp ssl/ca.crt /var/snap/microk8s/current/certs/dex-ca.crt
   ```

2. Edit `/var/snap/microk8s/current/args/kube-apiserver` and append the following lines. Make sure to replace `10.10.10.142` with the IP address of your MicroK8s host.
   编辑 `/var/snap/microk8s/current/args/kube-apiserver` 并附加以下行。确保将 `10.10.10.142` 替换为 MicroK8s 主机的 IP 地址。

   ```bash
   --oidc-issuer-url=https://10.10.10.142:31000/dex
   --oidc-ca-file=${SNAP_DATA}/certs/dex-ca.crt
   --oidc-client-id=kubernetes
   --oidc-username-claim=name
   --oidc-username-prefix=oidc:
   ```

3. Restart MicroK8s: 重启 MicroK8s：

   ```bash
   sudo snap restart microk8s
   ```

If you are running a MicroK8s cluster, you should repeat this process for all control plane nodes.
如果您运行的是 MicroK8s 集群，则应对所有 control plane 节点重复此过程。

### [Important notes 重要说明](https://microk8s.io/docs/oidc-dex#important-notes)

- The client ID that we set in `kube-apiserver` is the one from the `staticClients` section of the dex config file. The client secret **IS NOT** needed in the `kube-apiserver`.
  我们在 `kube-apiserver` 中设置的客户端 ID 是 dex 配置文件的 `staticClients` 部分中的 ID 。`kube-apiserver` 中**不需要**客户端 secret。
- With `--oidc-username-claim=name --oidc-username-prefix=oidc:`, Dex users will authenticate to the Kubernetes cluster as `oidc:$username`. This is useful for managing RBAC rules for users.
  使用 `--oidc-username-claim=name --oidc-username-prefix=oidc:` ，Dex 用户将作为 `oidc：$username` 向 Kubernetes 集群进行身份验证。这对于管理用户的 RBAC 规则非常有用。

## [Generate a kubeconfig file for clients authenticating via OIDC 为通过 OIDC 进行身份验证的客户端生成 kubeconfig 文件](https://microk8s.io/docs/oidc-dex#generate-a-kubeconfig-file-for-clients-authenticating-via-oidc)

Next, we will create `oidc-kubeconfig`, a config file which authenticates to our cluster via Dex:
接下来，我们将创建 `oidc-kubeconfig`，这是一个通过 Dex 对我们的集群进行身份验证的配置文件：

```bash
microk8s config > oidc-kubeconfig
```

Remove the admin user: 删除 admin 用户：

```bash
kubectl --kubeconfig=./oidc-kubeconfig config delete-user admin
```

Now, configure the oidc user (dynamically retrieves OIDC tokens using `kubectl oidc-login get-token`). You should remove any scopes you do not need.Also:
现在，配置 oidc 用户（使用 `kubectl oidc-login get-token` 动态检索 OIDC 令牌）。您应该删除任何不需要的 Scope。也：

- Make sure to replace “https://10.10.10.142:31000/dex” according to your needs…
  请务必根据您的需要更换“https://10.10.10.142:31000/dex”...
- make sure the client id and client secret match the ones in the dex `staticClients` section.
  确保 Client ID （客户端 ID） 和 Client Secret （客户端密钥） 与 Dex `StaticClients` 部分中的 ID 和 Client Secret （客户端密钥） 匹配。

```bash
kubectl --kubeconfig=./oidc-kubeconfig config set-credentials oidc \
    --exec-api-version=client.authentication.k8s.io/v1beta1 \
    --exec-command=kubectl \
    --exec-arg=oidc-login \
    --exec-arg=get-token \
    --exec-arg=--certificate-authority=./dex-ca.crt \
    --exec-arg=--oidc-issuer-url=https://10.10.10.142:31000/dex \
    --exec-arg=--oidc-client-id=kubernetes \
    --exec-arg=--oidc-client-secret=super-safe-client-secret \
    --exec-arg=--oidc-extra-scope=email \
    --exec-arg=--oidc-extra-scope=profile
```

Now use the oidc user by default:
现在默认使用 oidc 用户：

```bash
kubectl --kubeconfig=./oidc-kubeconfig config set-context --current --user=oidc
```

In order to onboard new clients, you will need the `oidc-kubeconfig` file we just created, as well as the Dex CA file (`ssl/ca.crt`).
为了加入新客户端，你需要我们刚刚创建的 `oidc-kubeconfig` 文件，以及 Dex CA 文件 （`ssl/ca.crt`）。

## [Onboard a new client 载入新客户端](https://microk8s.io/docs/oidc-dex#onboard-a-new-client)

In this section, we will configure our local machine to connect to the MicroK8s cluster, using Dex for authentication.
在本节中，我们将配置本地计算机以连接到 MicroK8s 集群，使用 Dex 进行身份验证。

1. Install `kubectl`:
   安装 `kubectl`：

   ```bash
   sudo snap install kubectl --classic
   ```

2. Download [kubelogin](https://github.com/int128/kubelogin/releases) and install in `PATH` as `kubectl-oidc_login`. This makes the plugin system of `kubectl` automatically recognize it and make it available via the `kubectl oidc-login` command.
   下载 [kubelogin](https://github.com/int128/kubelogin/releases) 并以 `kubectl-oidc_login` 的 `PATH` 形式安装。这使得 `kubectl` 的插件系统会自动识别它，并通过 `kubectl oidc-login` 命令使其可用。

   ```bash
   curl -fsSL https://github.com/int128/kubelogin/releases/download/v1.25.0/kubelogin_linux_amd64.zip > kubelogin.zip
   unzip kubelogin.zip
   sudo install -c ./kubelogin /usr/local/bin/kubectl-oidc_login
   ```

3. Retrieve the `oidc-kubeconfig` you created previously (e.g. using `scp`), then install in `~/.kube/config`. Also, retrieve `ssl/ca.crt` and rename it to `dex-ca.crt`:
   检索你之前创建的 `oidc-kubeconfig`（例如使用 `scp`），然后在 `~/.kube/config` 中安装。此外，检索 `ssl/ca.crt` 并将其重命名为 `dex-ca.crt`：

   ```bash
   mkdir -p ~/.kube
   cp /path/to/oidc-kubeconfig ~/.kube/config
   chmod 0600 ~/.kube/config
   
   cp /path/to/ca.crt ./dex-ca.crt
   ```

4. Run any `kubectl` command. `kubelogin` will open a new browser window. Login via Dex (the username is `admin@example.com` and password is `password` from the configuration we uploaded previously).
   运行任何 `kubectl` 命令。`kubelogin` 将打开一个新的浏览器窗口。通过 Dex 登录（用户名是 `admin@example.com` 密码是我们之前上传的配置中的`密码`）。

   ```bash
   kubectl get pod
   ```

   After authenticating successfully, you can close the window, and you will get a response:
   身份验证成功后，您可以关闭窗口，您将收到响应：

   ```bash
   NAME                  READY   STATUS    RESTARTS   AGE
   dex-78d687897-v85c9   2/2     Running   0          30m52s
   ```

## [That’s all! 就这样！](https://microk8s.io/docs/oidc-dex#thats-all)

That’s all! It is now possible to access the Kubernetes cluster using `kubectl` commands as normal. The first time, a browser window will open for the user to login through Dex.
就这样！现在可以正常使用 `kubectl` 命令访问 Kubernetes 集群。第一次，将打开一个浏览器窗口，供用户通过 Dex 登录。

### [Configure RBAC (optional) 配置 RBAC（可选）](https://microk8s.io/docs/oidc-dex#configure-rbac-optional)

Configuring RBAC for your Kubernetes cluster is useful in cases when multiple  people (with multiple roles) need to access the cluster. For example,  you may have operators with admin access, multiple developer teams with  access limited to a single namespace, or monitoring roles with read-only access to very specific resources. For such scenarios, it is heavily  recommended to configure **Role-Based Access Control** (RBAC) for your cluster.
当多个人（具有多个角色）需要访问集群时，为 Kubernetes 集群配置 RBAC  非常有用。例如，您可能拥有具有管理员访问权限的作员、访问权限仅限于单个命名空间的多个开发人员团队，或者具有对非常特定资源的只读访问权限的监控角色。对于此类情况，强烈建议为集群配置**基于角色的访问控制** （RBAC）。

In a MicroK8s cluster, enable RBAC with the following command (on any control plane node):
在 MicroK8s 集群中，使用以下命令启用 RBAC（在任何控制平面节点上）：

```bash
microk8s enable rbac
```

After enabling RBAC, trying to run any `kubectl` command with OIDC…
启用 RBAC 后，尝试使用 OIDC 运行任何 `kubectl` 命令...

```bash
kubectl get pod
```

… which should return an error like the following:
…，它应该返回如下错误：

```bash
Error from server (Forbidden): pods is forbidden: User "oidc:admin" cannot list resource "pods" in API group "" in the namespace "default"
```

This is because our user (`oidc:admin`) has authenticated properly, but is not authorized to perform any  actions in the cluster. We can authorize our user by creating a `RoleBinding` (for a single namespace) or a `ClusterRoleBinding` (for all namespaces in the cluster). Below you can see some examples. `cluster-admin` and `view` are ClusterRoles that exist by default in a Kubernetes cluster. The commands below need to run from the MicroK8s node (the `microk8s kubectl` command runs as admin in the cluster).
这是因为我们的用户 （`oidc：admin`） 已正确进行身份验证，但无权在集群中执行任何作。我们可以通过创建 `RoleBinding`（针对单个命名空间）或 `ClusterRoleBinding`（针对集群中的所有命名空间）来授权我们的用户。下面是一些示例。`cluster-admin` 和 `view` 是 Kubernetes 集群中默认存在的 ClusterRole。以下命令需要从 MicroK8s 节点运行（`microk8s kubectl` 命令在集群中以 admin 身份运行）。
 Note: this assumes “–oidc-username-prefix=oidc: --oidc-username-claim=name” in the kube-apiserver args)
注意：这假设 kube-apiserver 参数中为 “–oidc-username-prefix=oidc： --oidc-username-claim=name”）

```bash
microk8s kubectl create clusterrolebinding oidc-admin --user=oidc:admin --clusterrole=cluster-admin
```

For this example, we will give the `oidc:admin` user read-only access to the cluster:
在此示例中，我们将为 `oidc：admin` 用户提供对集群的只读访问权限：

First revert previous ClusterRoleBinding:
首先还原之前的 ClusterRoleBinding：

```bash
microk8s kubectl delete clusterrolebinding oidc-admin
```

… now give view access to the `oidc:admin` user:
…现在向 `OIDC：admin` 用户授予查看访问权限：

```auto
microk8s kubectl create clusterrolebinding oidc-view --user=oidc:admin --clusterrole=view
```

This can be tested on the local machine:
这可以在本地计算机上进行测试：

```bash
kubectl auth can-i get pods             # yes
kubectl auth can-i get deployments      # yes
kubectl auth can-i create deployments   # no
```