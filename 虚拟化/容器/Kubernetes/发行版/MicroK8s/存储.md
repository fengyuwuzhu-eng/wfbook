# 存储

[TOC]

# HowTo setup MicroK8s with (Micro)Ceph storage 如何使用 （Micro）Ceph 存储设置 MicroK8s

#### On this page 本页内容

​                                                [Install MicroCeph 安装 MicroCeph](https://microk8s.io/docs/how-to-ceph#install-microceph)                                                      [Add virtual disks 添加虚拟磁盘](https://microk8s.io/docs/how-to-ceph#add-virtual-disks)                                                      [Connect MicroCeph to MicroK8s
将 MicroCeph 连接到 MicroK8s](https://microk8s.io/docs/how-to-ceph#connect-microceph-to-microk8s)                                                      [Further reading 延伸阅读](https://microk8s.io/docs/how-to-ceph#further-reading)                                                              

With the 1.28 release, we introduced a new rook-ceph addon that allows users to easily setup, import, and manage [Ceph](https://ceph.io/) deployments via [rook](https://rook.io/).
在 1.28 版本中，我们引入了一个新的 rook-ceph 插件，允许用户通过 [rook](https://rook.io/) 轻松设置、导入和管理 [Ceph](https://ceph.io/) 部署。

In this guide we show how to setup a Ceph cluster with [MicroCeph](https://canonical-microceph.readthedocs-hosted.com/en/latest/), give it three virtual disks backed up by local files, and import the Ceph cluster in MicroK8s using the `rook-ceph` addon.
在本指南中，我们将展示如何使用 [MicroCeph](https://canonical-microceph.readthedocs-hosted.com/en/latest/) 设置 Ceph 集群，为其提供三个由本地文件备份的虚拟磁盘，并使用 `rook-ceph` 插件在 MicroK8s 中导入 Ceph 集群。

## [Install MicroCeph 安装 MicroCeph](https://microk8s.io/docs/how-to-ceph#install-microceph)

MicroCeph is a lightweight way of deploying a Ceph cluster with a focus on  reduced ops. It is distributed as a snap and thus it gets deployed with:
MicroCeph 是一种部署 Ceph 集群的轻量级方式，专注于减少作。它以快照的形式分发，因此可以通过以下方式进行部署：

```auto
sudo snap install microceph --channel=latest/edge
```

First, we need to bootstrap the Ceph cluster:
首先，我们需要引导 Ceph 集群：

```auto
sudo microceph cluster bootstrap
```

In this guide, we do not cluster multiple nodes. The interested reader can look into the [official docs](https://canonical-microceph.readthedocs-hosted.com/en/latest/tutorial/initial_setup/#initial-setup) on how to form a multinode Ceph cluster with MicroCeph.
在本指南中，我们不会对多个节点进行集群。感兴趣的读者可以查看有关如何使用 MicroCeph 形成多节点 Ceph 集群[的官方文档](https://canonical-microceph.readthedocs-hosted.com/en/latest/tutorial/initial_setup/#initial-setup)。

At this point we can check the status of the cluster and query the list of available disks that should be empty. The disk status is queried with:
此时我们可以检查集群的状态，并查询应为空的可用磁盘列表。磁盘状态通过以下方式查询：

```auto
sudo microceph.ceph status                                                                                                                                                                                        
```

Its output should look like:
其输出应如下所示：

```auto
  cluster:                                                                                                                                                                                                                                  
    id:     b5205159-8092-4be4-9f26-8176c397c929                                                                                                                                                                                            
    health: HEALTH_OK                                                                                                                                                                                                                       
                                                                                                                                                                                                                                            
  services:                                                                                                                                                                                                                                 
    mon: 1 daemons, quorum ip-172-31-3-156 (age 22s)                                                                                                                                                                                        
    mgr: ip-172-31-3-156(active, since 14s)                                                                                                                                                                                                 
    osd: 0 osds: 0 up, 0 in                                                                                                                                                                                                                 
                                                                                                                                                                                                                                            
  data:                                                                                                                                                                                                                                     
    pools:   0 pools, 0 pgs                                                                                                                                                                                                                 
    objects: 0 objects, 0 B                                                                                                                                                                                                                 
    usage:   0 B used, 0 B / 0 B avail                                                                                                                                                                                                      
    pgs:                                                                                                                                                                                                                                    
```

The disk list is shown with:
磁盘列表显示为：

```auto
sudo microceph disk list                                                                    
```

In our empty cluster the disks list should be:
在我们的空集群中，磁盘列表应该是：

```auto
Disks configured in MicroCeph:                                                                                        
+-----+----------+------+                                  
| OSD | LOCATION | PATH |                                                                                             
+-----+----------+------+                                                                                             
                                                                                                                      
Available unpartitioned disks on this system:                                                                         
+-------+----------+------+------+                                                                                                                                                                                                          
| MODEL | CAPACITY | TYPE | PATH |                                                                                    
+-------+----------+------+------+                                            
```

## [Add virtual disks 添加虚拟磁盘](https://microk8s.io/docs/how-to-ceph#add-virtual-disks)

The following loop creates three files under `/mnt` that will back respective loop devices. Each Virtual disk is then added as an OSD to Ceph:
以下循环在 `/mnt` 下创建三个文件，这些文件将支持相应的循环设备。然后，每个虚拟磁盘将作为 OSD 添加到 Ceph：

```auto
for l in a b c; do
  loop_file="$(sudo mktemp -p /mnt XXXX.img)"
  sudo truncate -s 1G "${loop_file}"
  loop_dev="$(sudo losetup --show -f "${loop_file}")"
  # the block-devices plug doesn't allow accessing /dev/loopX
  # devices so we make those same devices available under alternate
  # names (/dev/sdiY)
  minor="${loop_dev##/dev/loop}"
  sudo mknod -m 0660 "/dev/sdi${l}" b 7 "${minor}"
  sudo microceph disk add --wipe "/dev/sdi${l}"
done
```

At this point the disks show show up in the `sudo microceph.ceph status` command:
此时，disks show 显示在 `sudo microceph.ceph status` 命令中：

```auto
  cluster:
    id:     b5205159-8092-4be4-9f26-8176c397c929
    health: HEALTH_OK
  
  services:
    mon: 1 daemons, quorum ip-172-31-3-156 (age 115s)
    mgr: ip-172-31-3-156(active, since 107s)
    osd: 3 osds: 3 up (since 25s), 3 in (since 29s)
  
  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 449 KiB
    usage:   25 MiB used, 3.0 GiB / 3 GiB avail
    pgs:     1 active+clean
```

And the `sudo microceph disk list`:
以及 `sudo microceph 磁盘列表`：

```auto
Disks configured in MicroCeph:
+-----+-----------------+-----------+
| OSD |    LOCATION     |   PATH    |
+-----+-----------------+-----------+
| 0   | ip-172-31-3-156 | /dev/sdia |
+-----+-----------------+-----------+
| 1   | ip-172-31-3-156 | /dev/sdib |
+-----+-----------------+-----------+
| 2   | ip-172-31-3-156 | /dev/sdic |
+-----+-----------------+-----------+

Available unpartitioned disks on this system:
+-------+----------+------+------+
| MODEL | CAPACITY | TYPE | PATH |
+-------+----------+------+------+
```

It is worth looking into customizing your Ceph setup at this point. Here,  as this cluster is a local one and is going to be used by a local  MicroK8s deployment we set the replica count to be 2, we disable manager redirects, and we set the bucket type to use for chooseleaf in a CRUSH  rule to 0:
此时，值得考虑自定义 Ceph 设置。在这里，由于此集群是本地集群，并且将由本地 MicroK8s 部署使用，因此我们将副本计数设置为 2，禁用管理器重定向，并将 CRUSH 规则中用于 chooseleaf 的存储桶类型设置为 0：

```auto
sudo microceph.ceph config set global osd_pool_default_size 2                               
sudo microceph.ceph config set mgr mgr_standby_modules false                                                                                                                                                      
sudo microceph.ceph config set osd osd_crush_chooseleaf_type 0
```

Refer to the [Ceph docs](https://docs.ceph.com/en/) to shape the cluster according to your needs.
请参阅 [Ceph 文档](https://docs.ceph.com/en/) 以根据您的需求调整集群。

## [Connect MicroCeph to MicroK8s 将 MicroCeph 连接到 MicroK8s](https://microk8s.io/docs/how-to-ceph#connect-microceph-to-microk8s)

The `rook-ceph` addon first appeared with the 1.28 release, so we should select a MicroK8s deployment channel greater or equal to 1.28:
`rook-ceph` 插件首次出现在 1.28 版本中，因此我们应该选择大于或等于 1.28 的 MicroK8s 部署频道：

```auto
sudo snap install microk8s --channel=1.28/stable
sudo microk8s status --wait-ready
```

**Note:**  Before enabling the `rook-ceph` addon on a strictly confined MicroK8s, make sure the `rbd` kernel module is loaded with `sudo modprobe rbd`.
**注意：**在严格受限的 MicroK8s 上启用 `rook-ceph` 插件之前，请确保 `rbd` 内核模块加载了 `sudo modprobe rbd`。

The output message of enabling the addon, `sudo microk8s enable rook-ceph`, describes what the next steps should be to import a Ceph cluster:
启用插件的输出消息 `sudo microk8s enable rook-ceph` 描述了导入 Ceph 集群的后续步骤：

```auto
Infer repository core for addon rook-ceph                                                                                                                                                                                                   
Add Rook Helm repository https://charts.rook.io/release                                                                                                                                                                                     
"rook-release" has been added to your repositories                                                                                                                                                                                          
...
=================================================

Rook Ceph operator v1.11.9 is now deployed in your MicroK8s cluster and
will shortly be available for use.

As a next step, you can either deploy Ceph on MicroK8s, or connect MicroK8s with an
existing Ceph cluster.

To connect MicroK8s with an existing Ceph cluster, you can use the helper command
'microk8s connect-external-ceph'. If you are running MicroCeph on the same node, then
you can use the following command:

    sudo microk8s connect-external-ceph

Alternatively, you can connect MicroK8s with any external Ceph cluster using:

    sudo microk8s connect-external-ceph \
        --ceph-conf /path/to/cluster/ceph.conf \
        --keyring /path/to/cluster/ceph.keyring \
        --rbd-pool microk8s-rbd

For a list of all supported options, use 'microk8s connect-external-ceph --help'.

To deploy Ceph on the MicroK8s cluster using storage from your Kubernetes nodes, refer
to https://rook.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/
```

As we have already setup MicroCeph having it managed by rook is done with just:
由于我们已经设置了 MicroCeph，因此只需通过 rook 来管理它即可完成：

```auto
sudo microk8s connect-external-ceph
```

At the end of this process you should have a storage class ready to use:
在此过程结束时，您应该有一个可供使用的存储类：

```auto
NAME       PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
ceph-rbd   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   3h38m
```

# Use NFS for Persistent Volumes on MicroK8s 将 NFS 用于 MicroK8s 上的持久卷

#### On this page 本页内容

​                                                [Setup an NFS server 设置 NFS 服务器](https://microk8s.io/docs/how-to-nfs#setup-an-nfs-server)                                                      [Install the CSI driver for NFS
安装适用于 NFS 的 CSI 驱动程序](https://microk8s.io/docs/how-to-nfs#install-the-csi-driver-for-nfs)                                                      [Create a StorageClass for NFS
为 NFS 创建 StorageClass](https://microk8s.io/docs/how-to-nfs#create-a-storageclass-for-nfs)                                                      [Create a new PVC 创建新的 PVC](https://microk8s.io/docs/how-to-nfs#create-a-new-pvc)                                                      [Common Issues 常见问题](https://microk8s.io/docs/how-to-nfs#common-issues)                                                              

In this how-to we will explain how to provision NFS mounts as Kubernetes  Persistent Volumes on MicroK8s. If you run into difficulties, please see the troubleshooting section at the end!
在本作指南中，我们将介绍如何在 MicroK8s 上将 NFS 挂载配置为 Kubernetes 持久卷。如果您遇到困难，请参阅最后的故障排除部分！

## [Setup an NFS server 设置 NFS 服务器](https://microk8s.io/docs/how-to-nfs#setup-an-nfs-server)

> **Caution**: This section will show you how to configure a simple NFS server on  Ubuntu for the purpose of this tutorial. This is not a production-grade  NFS setup.
> **注意**：本节将向您展示如何在本教程中在 Ubuntu 上配置一个简单的 NFS 服务器。这不是生产级 NFS 设置。

If you don’t have a suitable NFS server already, you can simply create one on a local machine with the following commands on Ubuntu:
如果您还没有合适的 NFS 服务器，只需在 Ubuntu 上使用以下命令在本地计算机上创建一个：

```bash
sudo apt-get install nfs-kernel-server
```

Create a directory to be used for NFS:
创建用于 NFS 的目录：

```bash
sudo mkdir -p /srv/nfs
sudo chown nobody:nogroup /srv/nfs
sudo chmod 0777 /srv/nfs
```

Edit the `/etc/exports` file. Make sure that the IP addresses of all your MicroK8s nodes are  able to mount this share. For example, to allow all IP addresses in the `10.0.0.0/24` subnet:
编辑 `/etc/exports` 文件。确保所有 MicroK8s 节点的 IP 地址都能够挂载此共享。例如，要允许 `10.0.0.0/24` 子网中的所有 IP 地址，请执行以下作：

```bash
sudo mv /etc/exports /etc/exports.bak
echo '/srv/nfs 10.0.0.0/24(rw,sync,no_subtree_check)' | sudo tee /etc/exports
```

Finally, restart the NFS server:
最后，重新启动 NFS 服务器：

```bash
sudo systemctl restart nfs-kernel-server
```

For other OSes, follow specific documentation.
对于其他作系统，请遵循特定文档。

## [Install the CSI driver for NFS 安装适用于 NFS 的 CSI 驱动程序](https://microk8s.io/docs/how-to-nfs#install-the-csi-driver-for-nfs)

We will use the [upstream NFS CSI driver](https://github.com/kubernetes-csi/csi-driver-nfs). First, we will deploy the NFS provisioner using the official Helm chart.
我们将使用[上游 NFS CSI 驱动程序](https://github.com/kubernetes-csi/csi-driver-nfs)。首先，我们将使用官方 Helm 图表部署 NFS 预置程序。

Enable the Helm3 addon (if not already enabled) and add the repository for the NFS CSI driver:
启用 Helm3 插件（如果尚未启用）并添加 NFS CSI 驱动程序的存储库：

```bash
microk8s enable helm3
microk8s helm3 repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts
microk8s helm3 repo update
```

Then, install the Helm chart under the `kube-system` namespace with:
然后，在 `kube-system` 命名空间下安装 Helm chart：

```bash
microk8s helm3 install csi-driver-nfs csi-driver-nfs/csi-driver-nfs \
    --namespace kube-system \
    --set kubeletDir=/var/snap/microk8s/common/var/lib/kubelet
```

After deploying the Helm chart, wait for the CSI controller and node pods to come up using the following `kubectl` command …
部署 Helm Chart 后，使用以下 `kubectl` 命令等待 CSI 控制器和节点 Pod 出现...

```bash
microk8s kubectl wait pod --selector app.kubernetes.io/name=csi-driver-nfs --for condition=ready --namespace kube-system
```

… which, once successful, will produce output similar to:
…一旦成功，将产生类似于以下内容的输出：

```bash
pod/csi-nfs-controller-67bd588cc6-7vvn7 condition met
pod/csi-nfs-node-qw8rg condition met
```

At this point, you should also be able to list the available CSI drivers in your Kubernetes cluster …
此时，您还应该能够列出 Kubernetes 集群中可用的 CSI 驱动程序......

```bash
microk8s kubectl get csidrivers
```

… and see `nfs.csi.k8s.io` in the list:
…，并在列表中查看 `nfs.csi.k8s.io`：

```bash
NAME             ATTACHREQUIRED   PODINFOONMOUNT   STORAGECAPACITY   TOKENREQUESTS   REQUIRESREPUBLISH   MODES        AGE
nfs.csi.k8s.io   false            false            false             <unset>         false               Persistent   39m
```

## [Create a StorageClass for NFS 为 NFS 创建 StorageClass](https://microk8s.io/docs/how-to-nfs#create-a-storageclass-for-nfs)

Next, we will need to create a Kubernetes Storage Class that uses the `nfs.csi.k8s.io` CSI driver. Assuming you have configured an NFS share `/srv/nfs` and the address of your NFS server is  `10.0.0.42`, create the following file:
接下来，我们需要创建一个使用 `nfs.csi.k8s.io` CSI 驱动程序的 Kubernetes 存储类。假设您已配置 NFS 共享 `/srv/nfs`，并且 NFS 服务器的地址为 `10.0.0.42`，请创建以下文件：

```yaml
# sc-nfs.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-csi
provisioner: nfs.csi.k8s.io
parameters:
  server: 10.0.0.42
  share: /srv/nfs
reclaimPolicy: Delete
volumeBindingMode: Immediate
mountOptions:
  - hard
  - nfsvers=4.1
```

> **Note**: The last line of the above YAML indicates a specific version of NFS.  This should match the version of the NFS server being used - if you are  using an existing service please check which version it uses and adjust  accordingly.
> **注意**：上述 YAML 的最后一行表示 NFS 的特定版本。这应该与正在使用的 NFS 服务器的版本相匹配 - 如果您使用的是现有服务，请检查它使用的版本并进行相应调整。

Then apply it on your MicroK8s cluster with:
然后，使用以下命令将其应用于您的 MicroK8s 集群：

```bash
microk8s kubectl apply -f - < sc-nfs.yaml
```

## [Create a new PVC 创建新的 PVC](https://microk8s.io/docs/how-to-nfs#create-a-new-pvc)

The final step is to create a new PersistentVolumeClaim using the `nfs-csi` storage class. This is as simple as specifying `storageClassName: nfs-csi` in the PVC definition, for example:
最后一步是使用 `nfs-csi` 存储类创建新的 PersistentVolumeClaim。这就像在 PVC 定义中指定 `storageClassName： nfs-csi` 一样简单，例如：

```yaml
# pvc-nfs.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: nfs-csi
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 5Gi
```

Then create the PVC with:
然后使用以下命令创建 PVC：

```bash
microk8s kubectl apply -f - < pvc-nfs.yaml
```

If everything has been configured correctly, you should be able to check the PVC…
如果一切都已正确配置，您应该能够检查 PVC...

```bash
microk8s kubectl describe pvc my-pvc
```

… and see that a volume was provisioned successfully:
…并查看卷已成功预置：

```bash
Name:          my-pvc
Namespace:     default
StorageClass:  nfs-csi
Status:        Bound
Volume:        pvc-5676d353-4d46-49a2-b7ff-bdd4603d2c06
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-provisioner: nfs.csi.k8s.io
               volume.kubernetes.io/storage-provisioner: nfs.csi.k8s.io
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      5Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       <none>
Events:
  Type     Reason                 Age                    From                                                           Message
  ----     ------                 ----                   ----                                                           -------
  Normal   ExternalProvisioning   2m59s (x2 over 2m59s)  persistentvolume-controller                                    waiting for a volume to be created, either by external provisioner "nfs.csi.k8s.io" or manually created by system administrator
  Normal   Provisioning           2m58s (x2 over 2m59s)  nfs.csi.k8s.io_andromeda_61e4b876-324d-4f52-a5c3-f26047fbbc97  External provisioner is provisioning volume for claim "default/my-pvc"
  Normal   ProvisioningSucceeded  2m58s                  nfs.csi.k8s.io_andromeda_61e4b876-324d-4f52-a5c3-f26047fbbc97  Successfully provisioned volume pvc-5676d353-4d46-49a2-b7ff-bdd4603d2c06
```

That’s it! You can now use this PVC to run stateful workloads on your MicroK8s cluster.
就是这样！您现在可以使用此 PVC 在 MicroK8s 集群上运行有状态工作负载。

## [Common Issues 常见问题](https://microk8s.io/docs/how-to-nfs#common-issues)

NFS CSI 控制器和节点 Pod 卡在 “ 待处理 ” 状态

Make sure that you specify --set kubeletDir=/var/snap/microk8s/common/var/lib/kubelet when installing the Helm chart.
确保在安装 Helm Chart 时指定 --set kubeletDir=/var/snap/microk8s/common/var/lib/kubelet 。



我创建了存储类，但无法预置卷

Double-check that you have specified the NFS server IP address and share path correctly. Also, make sure that your MicroK8s node can mount NFS shares. If you are running a cluster, all MicroK8s nodes should be allowed to mount NFS shares.
仔细检查您是否已正确指定 NFS 服务器 IP 地址和共享路径。此外，请确保您的 MicroK8s 节点可以挂载 NFS 共享。如果您运行的是集群，则应允许所有 MicroK8s 节点挂载 NFS 共享。

Provisioning new volumes fails, but I've done everything else correctly
预置新卷失败，但我已经正确地完成了其他所有作

Check the logs of the
检查

nfs

containers in the controller and node pods, using the following commands:
容器中，使用以下命令：

microk8s kubectl logs --selector app=csi-nfs-controller -n kube-system -c nfs
microk8s kubectl logs --selector app=csi-nfs-node -n kube-system -c nfs

The logs should help with debugging any issues.
日志应有助于调试任何问题。