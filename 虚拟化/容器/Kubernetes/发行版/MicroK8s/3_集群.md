# MicroK8s 集群

[TOC]

## 创建 MicroK8s 集群

尽管 MicroK8s 被设计为 Kubernetes 的超轻量级实现，但创建 MicroK8s 集群仍然是可能的，并且很有用。

**Note:**  Each node on a MicroK8s cluster requires its own environment to work  in, whether that is a separate VM or container on a single machine or a  different machine on the same network. Note that, as with almost all  networked services, it is also important that these instances have the  correct time (e.g. updated from an ntp server) for inter-node  communication to work.
**注意：**MicroK8s 集群上的每个节点都需要自己的工作环境，无论是单台机器上的单独 VM 或容器，还是同一网络上的不同机器。请注意，与几乎所有联网服务一样，这些实例具有正确的时间（例如从 ntp 服务器更新）以使节点间通信正常工作也很重要。

### 添加节点

要从两个或多个已在运行的 MicroK8s 实例中创建集群，请使用 `microk8s add-node` 命令。运行此命令的 MicroK8s 实例将成为集群的主实例，并将托管 Kubernetes 控制平面：

```bash
microk8s add-node
```

This will return some joining instructions which should be executed **on the MicroK8s instance that you wish to join to the cluster (NOT THE NODE YOU RAN `add-node` FROM)**
这将返回一些加入指令，这些指令应该**在您希望加入集群的 MicroK8s 实例上执行（不是您运行 `add-node` 的节点）**

```bash
From the node you wish to join to this cluster, run the following:
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05

Use the '--worker' flag to join a node as a worker not running the control plane, eg:
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05 --worker

If the node you are adding is not reachable through the default interface you can use one of the following:
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05
microk8s join 10.23.209.1:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05
microk8s join 172.17.0.1:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05
```

将节点加入集群只需几秒钟。之后应该能够看到节点已加入：

```bash
microk8s kubectl get no
```

将返回类似于以下内容的输出：

```bash
NAME               STATUS   ROLES    AGE   VERSION
10.22.254.79       Ready    <none>   27s   v1.15.3
ip-172-31-20-243   Ready    <none>   53s   v1.15.3
```

### 删除节点

首先，在要删除的节点上，运行 `microk8s leave` 。离开节点上的 MicroK8s 将重新启动自己的 control plane 并作为完整的单节点集群恢复运作：

```bash
microk8s leave
```

To complete the node removal, call `microk8s remove-node` from the remaining nodes to
要完成节点删除，请从剩余节点调用 `microk8s remove-node` 到
 indicate that the departing (unreachable now) node should be removed permanently:
指示应永久删除 Departing （Unreachable Now） 节点：

```bash
microk8s remove-node 10.22.254.79
```

### 存储

If you are using the simple storage provided by the hostpath storage  add-on, note that this will only be available to the nodes it has been  enabled on. For clustered storage, you should set up alternative  storage. For example, [see the  guide on using NFS](https://microk8s.io/docs/how-to-nfs).
如果您使用的是 hostpath storage 附加组件提供的简单存储，请注意，这仅适用于已启用它的节点。对于群集存储，应设置备用存储。例如，[请参阅有关使用 NFS 的指南](https://microk8s.io/docs/how-to-nfs)。

### 高可用性

从 MicroK8s 的 1.19 版本开始，HA 默认处于启用状态。如果集群由三个或更多节点组成，则数据存储将在节点之间复制，并且它将能够灵活应对单个故障（如果一个节点出现问题，工作负载将继续运行而不会中断）。

`microk8s status` 现在包含有关 HA 状态的信息。例如：

```bash
microk8s is running
high-availability: yes
  datastore master nodes: 10.128.63.86:19001 10.128.63.166:19001 10.128.63.43:19001
  datastore standby nodes: none
```

### Worker 节点

从 1.23 版本开始，节点可以作为 Worker 节点加入集群。Worker 节点能够托管工作负载，但它们不运行 Kubernetes  控制平面，因此它们不会增加集群的可用性 （HA）。Worker 节点非常适合低端设备，因为它们消耗的资源更少。它们在具有足够 control  plane 节点以确保 HA 的大型集群中也很有意义。要添加 worker 节点，请在运行 `microk8s join` 命令时使用 `--worker` 标志：

```bash
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05 --worker
```

A worker node runs a local API server proxy that takes care of the  communication between the local services (kubelet, kube-proxy) and the  API servers running on multiple control plane nodes. When adding a  worker node, MicroK8s attempts to detect all API server endpoints in the cluster and configure the new node accordingly. The list of API servers is stored in`/var/snap/microk8s/current/args/traefik/provider.yaml`.
Worker 节点运行本地 API 服务器代理，该代理负责本地服务（kubelet、kube-proxy）与在多个控制平面节点上运行的 API  服务器之间的通信。添加 worker 节点时，MicroK8s 会尝试检测集群中的所有 API 服务器终端节点并相应地配置新节点。API  服务器列表存储在 `/var/snap/microk8s/current/args/traefik/provider.yaml` 中。

The API server proxy will automatically check for updates when the control  plane nodes of the cluster are changed (e.g. a new control plane node is added, an old one is removed) and update the list of known API server  endpoints.
当集群的控制平面节点发生更改时（例如，添加新的控制平面节点，删除旧的控制平面节点），API 服务器代理将自动检查更新，并更新已知 API 服务器端点的列表。

If you already have a load balancer in front of the API server, you can configure the load balancer address manually in `/var/snap/microk8s/current/args/traefik/provider.yaml`. In this case, make sure to also disable the automatic refresh of the control plane endpoints by setting `--refresh-interval 0` in  `/var/snap/microk8s/current/args/apiserver-proxy`.
如果 API 服务器前面已经有负载均衡器，则可以在 中 `/var/snap/microk8s/current/args/traefik/provider.yaml` 手动配置负载均衡器地址。在这种情况下，请确保还通过在 中设置 `--refresh-interval 0` 来禁用控制平面端点的自动刷新 `/var/snap/microk8s/current/args/apiserver-proxy` 。

## 升级 MicroK8s 集群                                                              

概述了将 3 节点 MicroK8s 集群从版本 `1.22/stable` 升级到 `1.23/candidate` 的过程。

### 一般说明

以下是升级 Kubernetes 集群时要遵循的一般建议：

- 对于生产集群，在开始之前，请始终确保您有 Kubernetes 集群数据库的工作备份。
- 为了最大限度地减少误差幅度，并确保始终可以回滚到工作状态，一次只升级一个 Kubernetes 节点。
- 从 Kubernetes 控制面板节点开始。升级所有 control plane 节点后，继续升级 Kubernetes Worker 节点，逐个升级它们（或者，对于较大的集群，也以小批量方式升级）。
- 确保一次更新一个次要版本。在继续进行任何升级之前，请参阅 Kubernetes 发行说明，了解任何重大更改、已删除和/或弃用的 API，并确保它们不会影响您的集群。
- 在升级之前封锁和排空任何节点，并在升级后恢复它们，以确保 Kubernetes 集群中托管的应用程序工作负载不受影响。

> *NOTE*: For finer-grained control over MicroK8s revision upgrades in clusters running in production, consider using a [Snap Store Proxy](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy).
> *注意*：要对生产中运行的集群中的 MicroK8s 修订版升级进行更精细的控制，请考虑使用 [Snap Store 代理](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy)。

### 升级 3 节点集群

For our example, we have the following cluster, running on `k8s-1`, `k8s-2` and `k8s-3`. Two services are running in the workload (an `nginx` and a `microbot` deployment):
在示例中，有以下集群，在 `k8s-1`、`k8s-2` 和 `k8s-3` 上运行。工作负载中运行了两项服务（一个 `nginx` 和一个 `microbot` 部署）：

```bash
microk8s kubectl get node
microk8s kubectl get pod -o wide
```

输出如下所示（nginx 有 3 个 pod，microbot 有 10 个）：

```bash
NAME    STATUS   ROLES    AGE   VERSION
k8s-3   Ready    <none>   19d   v1.22.3-3+9ec7c40ec93c73
k8s-2   Ready    <none>   19d   v1.22.3-3+9ec7c40ec93c73
k8s-1   Ready    <none>   19d   v1.22.3-3+9ec7c40ec93c73

NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES
nginx-7848d4b86f-xwhcp     1/1     Running   0          5m41s   10.1.200.196   k8s-2   <none>           <none>
nginx-7848d4b86f-kxxjv     1/1     Running   0          4m51s   10.1.200.197   k8s-2   <none>           <none>
nginx-7848d4b86f-wsdws     1/1     Running   0          4m51s   10.1.13.71     k8s-3   <none>           <none>
microbot-fdcc4594f-mlqr7   1/1     Running   0          2m34s   10.1.13.73     k8s-3   <none>           <none>
microbot-fdcc4594f-kjcjq   1/1     Running   0          2m34s   10.1.200.199   k8s-2   <none>           <none>
microbot-fdcc4594f-4vsrd   1/1     Running   0          2m27s   10.1.231.202   k8s-1   <none>           <none>
microbot-fdcc4594f-hkqrw   1/1     Running   0          2m26s   10.1.231.203   k8s-1   <none>           <none>
microbot-fdcc4594f-qmjhq   1/1     Running   0          16s     10.1.200.200   k8s-2   <none>           <none>
microbot-fdcc4594f-nxx9j   1/1     Running   0          16s     10.1.13.74     k8s-3   <none>           <none>
microbot-fdcc4594f-pbndr   1/1     Running   0          8s      10.1.200.202   k8s-2   <none>           <none>
microbot-fdcc4594f-f2jmm   1/1     Running   0          16s     10.1.13.75     k8s-3   <none>           <none>
microbot-fdcc4594f-jtfdf   1/1     Running   0          8s      10.1.200.201   k8s-2   <none>           <none>
microbot-fdcc4594f-zl2sl   1/1     Running   0          8s      10.1.13.76     k8s-3   <none>           <none>
```

### 升级第一个节点

将从 `k8s-1` 开始集群升级。

1. 运行 `kubectl drain k8s-1`。此命令将封锁节点（用 `NoSchedule` 污点标记它，这样就不会在其上调度新的工作负载），并将所有正在运行的 Pod 驱逐到其他节点：
   
   ```bash
   microk8s kubectl drain k8s-1 --ignore-daemonsets
   ```
   
   输出应如下所示：
   
   ```bash
   node/k8s-1 cordoned
   WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-mhbqw, ingress/nginx-ingress-microk8s-controller-gtb8p
   evicting pod default/microbot-fdcc4594f-hkqrw
   evicting pod kube-system/hostpath-provisioner-5c65fbdb4f-gfdpj
   evicting pod kube-system/coredns-7f9c69c78c-nfd4b
   evicting pod default/microbot-fdcc4594f-4vsrd
   pod/hostpath-provisioner-5c65fbdb4f-gfdpj evicted
   pod/coredns-7f9c69c78c-nfd4b evicted
   pod/microbot-fdcc4594f-hkqrw evicted
   pod/microbot-fdcc4594f-4vsrd evicted
   node/k8s-1 evicted
   ```
   
2. 验证之前在 `k8s-1` 上运行的所有 Pod 都已删除，并且新 Pod 已部署到其他集群节点上。此外，请确保节点已标记为 `SchedulingDisabled`：
   
   ```bash
   microk8s kubectl get node
   microk8s kubectl get pod -o wide
   ```
   
   请注意，在 `k8s-1` 上没有看到 Pod 运行：
   
   ```bash
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready,SchedulingDisabled   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   
   NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES
   nginx-7848d4b86f-xwhcp     1/1     Running   0          14m     10.1.200.196   k8s-2   <none>           <none>
   nginx-7848d4b86f-kxxjv     1/1     Running   0          13m     10.1.200.197   k8s-2   <none>           <none>
   nginx-7848d4b86f-wsdws     1/1     Running   0          13m     10.1.13.71     k8s-3   <none>           <none>
   microbot-fdcc4594f-mlqr7   1/1     Running   0          11m     10.1.13.73     k8s-3   <none>           <none>
   microbot-fdcc4594f-kjcjq   1/1     Running   0          11m     10.1.200.199   k8s-2   <none>           <none>
   microbot-fdcc4594f-qmjhq   1/1     Running   0          9m      10.1.200.200   k8s-2   <none>           <none>
   microbot-fdcc4594f-nxx9j   1/1     Running   0          9m      10.1.13.74     k8s-3   <none>           <none>
   microbot-fdcc4594f-f2jmm   1/1     Running   0          9m      10.1.13.75     k8s-3   <none>           <none>
   microbot-fdcc4594f-jtfdf   1/1     Running   0          8m52s   10.1.200.201   k8s-2   <none>           <none>
   microbot-fdcc4594f-zl2sl   1/1     Running   0          8m52s   10.1.13.76     k8s-3   <none>           <none>
   microbot-fdcc4594f-pbndr   1/1     Running   0          8m52s   10.1.200.202   k8s-2   <none>           <none>
   microbot-fdcc4594f-nrqh9   1/1     Running   0          8m18s   10.1.200.204   k8s-2   <none>           <none>
   microbot-fdcc4594f-dx2pk   1/1     Running   0          8m17s   10.1.13.78     k8s-3   <none>           <none>
   ```
   
3. 刷新 MicroK8s 快照以跟踪 `1.23/candidate` 通道（在撰写本文时，`1.23/stable` 尚未发布）。此命令需要在 `k8s-1` 上运行。
   
   ```bash
   sudo snap refresh microk8s --channel 1.23/candidate
   ```
   
   输出应如下所示：
   
   ```bash
   microk8s (1.23/candidate) v1.23.0-rc.0 from Canonical✓ refreshed
   ```
   
4. 不久之后，我们可以看到 `k8s-1` 现在运行的是 `1.23.0` 版本：
   
   ```bash
   microk8s kubectl get node
   
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-2   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready,SchedulingDisabled   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   ```
   
5. 最后一步是解除对节点的封锁，以便集群可以开始在其上调度新的工作负载：
   
   ```bash
   microk8s kubectl uncordon k8s-1
   
   node/k8s-1 uncordoned
   
   microk8s kubectl get node
   
   NAME    STATUS  ROLES    AGE   VERSION
   k8s-1   Ready   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   k8s-3   Ready   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   ```
   

#### 失败时回滚

此时，我们假设发生了一个假设的错误，并且我们观察到我们的 Kubernetes 集群没有按预期运行（例如，连接问题、Pod 进入错误状态、升级后的节点上的错误日志数量增加等）。在这种情况下，可能需要将节点回滚到以前的版本（例如，`1.22`）。

使用 MicroK8s，这就像运行 `sudo snap revert microk8s` 一样简单：

1. 如果节点有任何新的工作负载，请确保在进行任何更改之前 drain ：
   
   ```bash
   microk8s kubectl drain k8s-1
   ```
   
2. 恢复到以前的 MicroK8s 版本。这将重新安装之前的 snap 修订版，并恢复 control plane 服务的所有配置文件：
   
   ```bash
   sudo snap revert microk8s
   
   microk8s reverted to v1.22.3
   
   microk8s kubectl get node
   
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready,SchedulingDisabled   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   ```
   

### [Upgrade second node 升级第二个节点](https://microk8s.io/docs/upgrade-cluster#upgrade-second-node)

按照与之前相同的步骤进行作。请注意，所有 `kubectl` 命令都可以从集群中的任何节点运行。

1. 排空并封锁节点
   
   ```bash
   microk8s kubectl drain k8s-2 --ignore-daemonsets
   ```
   
2. 确保所有工作负载都已移动到其他集群节点：
   
   ```bash
   microk8s kubectl get node
   microk8s kubectl get pod -o wide
   ```
   
   请注意输出，显示 `k8s-2` 的 `SchedulingDisabled`，并且没有 Pod 运行。
   
   ```bash
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready,SchedulingDisabled   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready                      <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f0
   
   NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
   nginx-7848d4b86f-wsdws     1/1     Running   0          96m   10.1.13.71     k8s-3   <none>           <none>
   microbot-fdcc4594f-mlqr7   1/1     Running   0          94m   10.1.13.73     k8s-3   <none>           <none>
   microbot-fdcc4594f-nxx9j   1/1     Running   0          92m   10.1.13.74     k8s-3   <none>           <none>
   microbot-fdcc4594f-f2jmm   1/1     Running   0          92m   10.1.13.75     k8s-3   <none>           <none>
   microbot-fdcc4594f-zl2sl   1/1     Running   0          91m   10.1.13.76     k8s-3   <none>           <none>
   microbot-fdcc4594f-dx2pk   1/1     Running   0          91m   10.1.13.78     k8s-3   <none>           <none>
   nginx-7848d4b86f-lsjq6     1/1     Running   0          33m   10.1.231.204   k8s-1   <none>           <none>
   microbot-fdcc4594f-h9cjg   1/1     Running   0          33m   10.1.231.205   k8s-1   <none>           <none>
   microbot-fdcc4594f-98vnj   1/1     Running   0          33m   10.1.231.207   k8s-1   <none>           <none>
   microbot-fdcc4594f-glvcm   1/1     Running   0          33m   10.1.231.208   k8s-1   <none>           <none>
   microbot-fdcc4594f-m5wzj   1/1     Running   0          33m   10.1.231.209   k8s-1   <none>           <none>
   microbot-fdcc4594f-7n5k5   1/1     Running   0          33m   10.1.231.210   k8s-1   <none>           <none>
   nginx-7848d4b86f-skshj     1/1     Running   0          33m   10.1.231.211   k8s-1   <none>           <none>
   ```
   
3. 升级。此命令必须在 `k8s-2` 上运行：
   
   ```bash
   sudo snap refresh microk8s --channel 1.23/candidate
   ```
   
4. 验证 `k8s-2` 现在是否也在 `1.23.0` 上运行：
   
   ```bash
   microk8s kubectl uncordon k8s-2
   
   node/k8s-2 uncordoned
   
   microk8s kubectl get node
   
   NAME    STATUS  ROLES    AGE   VERSION
   k8s-1   Ready   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   k8s-3   Ready   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   ```
   

### 升级第三个节点

The process is exactly the same as with the previous two nodes:
该过程与前两个节点完全相同。

### 通过部署新工作负载进行验证

将通过创建新的 `microbot-2` 测试部署来验证我们的集群是否仍在按预期工作：

```bash
microk8s kubectl create deploy --image dontrebootme/microbot:v1 microbot-2
microk8s kubectl scale deploy microbot-2 --replicas 4
microk8s kubectl expose deploy microbot-2 --port 80 --type NodePort
deployment.apps/microbot-2 created
deployment.apps/microbot-2 scaled
service/microbot-2 exposed
```

部署完成后，服务应如下所示：

```bash
microk8s kubectl get pod -l app=microbot-2 -o wide
microk8s kubectl get svc microbot-2
NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
microbot-2-5484459568-g299z   1/1     Running   0          43m   10.1.13.83     k8s-3   <none>           <none>
microbot-2-5484459568-2dj7z   1/1     Running   0          43m   10.1.200.214   k8s-2   <none>           <none>
microbot-2-5484459568-52cgn   1/1     Running   0          43m   10.1.231.212   k8s-1   <none>           <none>
microbot-2-5484459568-nb52k   1/1     Running   0          43m   10.1.13.84     k8s-3   <none>           <none>

NAME         TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
microbot-2   NodePort   10.152.183.221   <none>        80:30032/TCP   42m
```

最后，使用刚刚从所有 3 个集群节点创建的 `NodePort` 服务，并验证对 4 个已部署的 Pod 的负载均衡是否正常工作：

```bash
curl --silent k8s-1:30032 | grep hostname
curl --silent k8s-1:30032 | grep hostname
curl --silent k8s-2:30032 | grep hostname
curl --silent k8s-2:30032 | grep hostname
curl --silent k8s-3:30032 | grep hostname
```

在输出中，可以看到不同的 Pod 在每个 `curl` 上响应。

```bash
<p class="centered">Container hostname: microbot-2-5484459568-g299z</p>
<p class="centered">Container hostname: microbot-2-5484459568-nb52k</p>
<p class="centered">Container hostname: microbot-2-5484459568-52cgn</p>
<p class="centered">Container hostname: microbot-2-5484459568-2dj7z</p>
<p class="centered">Container hostname: microbot-2-5484459568-2dj7z</p>
```

# Add a Windows worker node to MicroK8s 向 MicroK8s 添加 Windows 工作节点

#### On this page 本页内容

​                                                [Requirements 要求](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-requirements)                                                      [Prepare the MicroK8s cluster
准备 MicroK8s 集群](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-prepare-the-microk8s-cluster)                                                      [Prepare the Windows node 准备 Windows 节点](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-prepare-the-windows-node)                                                      [Install Calico 安装 Calico](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-install-calico)                                                      [Verify the Windows node 验证 Windows 节点](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-verify-the-windows-node)                                                      [Uninstall the Windows worker node
卸载 Windows Worker 节点](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-uninstall-the-windows-worker-node)                                                      [Upgrade 升级](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-upgrade)                                                      [Further reading 延伸阅读](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-further-reading)                                                              

This guide explains how to add Windows worker nodes to MicroK8s. This how-to guide explains how to join a Windows Server 2022 to an existing  MicroK8s cluster running Calico.
本指南介绍如何将 Windows Worker 节点添加到 MicroK8s。本作指南介绍了如何将 Windows Server 2022 加入运行 Calico 的现有 MicroK8s 集群。

> Note: Currently Windows nodes may be affected by a bug in the Windows  container code for Server 2022/July update. Please see this issue for  more details and a workaround:[Container  networking on Kubernetes broken after Server 2022 July 2024 / KB5040437  (OS Build 20348.2582) update · Issue #516 ·  microsoft/Windows-Containers · GitHub](https://github.com/microsoft/Windows-Containers/issues/516)
> 注意：目前，Windows 节点可能受到 Server 2022/7 月更新的 Windows 容器代码中错误的影响。有关更多详细信息和解决方法，请参阅此问题：[Server 2022 July 2024 / KB5040437（OS Build 20348.2582）更新后 Kubernetes 上的容器网络中断 ·问题 #516 ·microsoft/Windows 容器 ·GitHub的](https://github.com/microsoft/Windows-Containers/issues/516)

## [Requirements 要求](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-requirements)

- A node running Windows (tested with Windows Server 2022).
  运行 Windows 的节点（使用 Windows Server 2022 测试）。
- A MicroK8s cluster using the Calico CNI (enabled by default since 1.19).
  使用 Calico CNI 的 MicroK8s 集群（从 1.19 开始默认启用）。

## [Prepare the MicroK8s cluster 准备 MicroK8s 集群](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-prepare-the-microk8s-cluster)

1. Determine the exact version of Kubernetes running in the cluster, e.g. `1.27.1`. You can use the following command:
   确定集群中运行的 Kubernetes 的确切版本，例如 `1.27.1`。您可以使用以下命令：

   ```bash
   microk8s kubectl get node -o wide
   ```

2. Determine the exact version of Calico running in the MicroK8s cluster, e.g. `3.25.0`. For this, you can inspect the image used by the `calico-node` containers:
   确定在 MicroK8s 集群中运行的 Calico 的确切版本，例如 `3.25.0`。为此，您可以检查 `calico-node` 容器使用的镜像：

   ```bash
   microk8s kubectl get ds/calico-node -n kube-system -o jsonpath='{.spec.template.spec.containers[?(.name=="calico-node")].image}{"\n"}'
   ```

3. Generate a `kubeconfig` file for the MicroK8s cluster. You will need this to run `calicoctl` commands, and later copy it to the Windows node.
   为 MicroK8s 集群生成 `kubeconfig` 文件。您将需要此密钥来运行 `calicoctl` 命令，然后将其复制到 Windows 节点。

   ```bash
   mkdir -p ~/.kube
   microk8s config > ~/.kube/config
   ```

4. In order for Windows pods to schedule, strict affinity must be set to  true. This is required to prevent Linux nodes from borrowing IP  addresses from Windows nodes. This can be set with the calicoctl binary. Install the `calicoctl` binary with:
   为了让 Windows Pod 进行调度，必须将 strict affinity 设置为 true。这是防止 Linux 节点从 Windows 节点借用 IP 地址所必需的。这可以使用 calicoctl 二进制文件进行设置。使用以下命令安装 `calicoctl` 二进制文件：

   ```bash
   CALICO_VERSION="3.25.0"
   curl -L https://github.com/projectcalico/calico/releases/download/v$CALICO_VERSION/calicoctl-linux-amd64 -o calicoctl
   chmod +x ./calicoctl
   ```

   (set the environment variable to the appropriate version of Calico for your system)
   （将环境变量设置为适合您系统的 Calico 版本）
    Then, set strict affinity to true with the following command:
   然后，使用以下命令将 strict affinity 设置为 true：

   ```bash
   ./calicoctl ipam configure --strictaffinity=true --allow-version-mismatch
   ```

## [Prepare the Windows node 准备 Windows 节点](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-prepare-the-windows-node)

All the commands shown next have to be run in a PowerShell window as Administrator.
接下来显示的所有命令都必须以管理员身份在 PowerShell 窗口中运行。

1. Install the `containerd` container runtime. The machine might restart during this operation.
   安装 `containerd` 容器运行时。在此作期间，计算机可能会重新启动。

   ```powershell
   Invoke-WebRequest -UseBasicParsing "https://raw.githubusercontent.com/microsoft/Windows-Containers/Main/helpful_tools/Install-ContainerdRuntime/install-containerd-runtime.ps1" -o install-containerd-runtime.ps1
   
   .\install-containerd-runtime.ps1
   ```

2. Create directory `c:\k`, create the kubeconfig file `c:\k\config`. After creating, open the file with notepad, paste the contents of the `kubeconfig` file of the MicroK8s cluster and save.
   创建目录 `c：\k`，创建 kubeconfig 文件 `c：\k\config`。创建完成后，用记事本打开文件，粘贴 MicroK8s 集群的 `kubeconfig` 文件内容并保存。

   ```powershell
   mkdir c:\k
   New-Item c:\k\config
   
   notepad c:\k\config
   ```

## [Install Calico 安装 Calico](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-install-calico)

1. Retrieve the `install-calico-windows.ps1` script from the Calico GitHub releases page. **NOTE**: do not worry about the calico version in the URL, we will pick the Calico version to install later.
   从 Calico GitHub 版本页面检索 `install-calico-windows.ps1` 脚本。**注意**：不用担心 URL 中的 calico 版本，我们稍后会选择 Calico 版本进行安装。

   ```powershell
   Invoke-WebRequest -Uri https://github.com/projectcalico/calico/releases/download/v3.25.1/install-calico-windows.ps1 -OutFile c:\k\install-calico-windows.ps1
   ```

2. Download Calico and Kubernetes binaries using the following command (replace `1.27.1` with the Kubernetes version and `3.25.0` with the Calico version running in the cluster):
   使用以下命令下载 Calico 和 Kubernetes 二进制文件（将 `1.27.1` 替换为 Kubernetes 版本，将 `3.25.0` 替换为集群中运行的 Calico 版本）：

   ```powershell
   c:\k\install-calico-windows.ps1 -ReleaseBaseURL "https://github.com/projectcalico/calico/releases/download/v3.25.0" -ReleaseFile "calico-windows-v3.25.0.zip" -KubeVersion "1.27.1" -DownloadOnly "yes" -ServiceCidr "10.152.183.0/24" -DNSServerIPs "10.152.183.10"
   ```

3. Configure the CNI bin and configuration directories and then install the Calico  services. If the vSwitch is not yet created, this will temporarily  affect network connectivity for a few seconds.
   配置 CNI bin 和配置目录，然后安装 Calico 服务。如果尚未创建 vSwitch，这将暂时影响网络连接几秒钟。

   ```powershell
   $ENV:CNI_BIN_DIR="c:\program files\containerd\cni\bin"
   $ENV:CNI_CONF_DIR="c:\program files\containerd\cni\conf"
   c:\calicowindows\install-calico.ps1
   c:\calicowindows\start-calico.ps1
   ```

   If successful, the output should look like this:
   如果成功，输出应如下所示：

   ```powershell
   Starting Calico...
   This may take several seconds if the vSwitch needs to be created.
   Waiting for Calico initialisation to finish...
   Waiting for Calico initialisation to finish...StoredLastBootTime , CurrentLastBootTime 5/21/2023 8:21:24 AM
   Waiting for Calico initialisation to finish...StoredLastBootTime , CurrentLastBootTime 5/21/2023 8:21:24 AM
   Calico initialisation finished.
   Done, the Calico services are running:
   
   Status   Name               DisplayName
   ------   ----               -----------
   Running  CalicoFelix        Calico Windows Agent
   Running  CalicoNode         Calico Windows Startup
   ```

4. Install Kubernetes services (kubelet and kube-proxy):
   安装 Kubernetes 服务（kubelet 和 kube-proxy）：

   ```powershell
   c:\calicowindows\kubernetes\install-kube-services.ps1
   ```

5. Add a firewall rule for incoming connections to the Windows kubelet node service. This is for `kubectl logs` and `kubectl exec` commands to work with pods running in Windows nodes:
   为到 Windows kubelet 节点服务的传入连接添加防火墙规则。这是为了让 `kubectl logs` 和 `kubectl exec` 命令与 Windows 节点中运行的 Pod 一起使用：

   ```powershell
   New-NetFirewallRule -Name 'Kubelet-In-TCP' -DisplayName 'Kubelet (node)' -Enabled True -Direction Inbound -Protocol TCP -Action Allow -LocalPort 10250
   ```

6. Configure the network adapter name expected by the kubelet service.
   配置 kubelet 服务所需的网络适配器名称。

   Run `ipconfig` to retrieve the IP configuration of the machine’s network adapters. An  example output is shown below, where the network adapter name is `vEthernet (tapeb67a82f-a1)`:
   运行 `ipconfig` 以检索计算机网络适配器的 IP 配置。示例输出如下所示，其中网络适配器名称为 `vEthernet （tapeb67a82f-a1）：`

   ```powershell
   Windows IP Configuration
   
   
   Ethernet adapter vEthernet (tapeb67a82f-a1):
   
   Connection-specific DNS Suffix  . : teststack.internal
   Link-local IPv6 Address . . . . . : fe80::d17b:a2e:c02c:1163%8
   IPv4 Address. . . . . . . . . . . : 172.16.1.177
   Subnet Mask . . . . . . . . . . . : 255.255.255.0
   Default Gateway . . . . . . . . . : 172.16.1.1
   ```

   Then, open `c:\calicowindows\kubernetes\kubelet-service.ps1` with notepad and edit the default value of the `InterfaceName` parameter to match the one above.
   然后，使用记事本打开 `c:\calicowindows\kubernetes\kubelet-service.ps1` 并编辑 `InterfaceName` 参数的默认值以匹配上面的值。

   ```powershell
   # c:\calicowindows\kubernetes\kubelet-service.ps1
   Param(
       [string]$NodeIp="",
       [string]$InterfaceName="vEthernet (tapeb67a82f-a1)"
   )
   ```

   Save the file and exit notepad.
   保存文件并退出记事本。

7. Clean service arguments for kubelet. Open `c:\calicowindows\kubernetes\kubelet-service.ps1` with notepad and ensure that:
   清理 kubelet 的服务参数。使用记事本打开 `c:\calicowindows\kubernetes\kubelet-service.ps1` 并确保：

   - For MicroK8s version 1.26 or newer, remove the argument `--logtostderr=true`
     对于 MicroK8s 版本 1.26 或更高版本，请删除参数 `--logtostderr=true`
   - For MicroK8s version 1.27 or newer, remove the argument `--container-runtime=remote`
     对于 MicroK8s 版本 1.27 或更高版本，请删除参数 `--container-runtime=remote`

   Save the file and exit notepad.
   保存文件并退出记事本。

8. Start Kubernetes services
   启动 Kubernetes 服务

   ```powershell
   Start-Service kubelet
   Start-Service kube-proxy
   ```

## [Verify the Windows node 验证 Windows 节点](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-verify-the-windows-node)

1. Ensure that the Windows node appears in the output of `microk8s kubectl get nodes -o wide`, for example:
   确保 Windows 节点显示在 的输出中 `microk8s kubectl get nodes -o wide` ，例如：

   ```bash
   NAME   STATUS   ROLES    AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                                    KERNEL-VERSION      CONTAINER-RUNTIME
   u1     Ready    <none>   3h3m   v1.27.1   172.16.1.49    <none>        Ubuntu 20.04.5 LTS                          5.4.0-139-generic   containerd://1.6.15
   w1     Ready    <none>   105m   v1.27.1   172.16.1.177   <none>        Windows Server 2022 Datacenter Evaluation   10.0.20348.587      containerd://1.6.6
   ```

2. Start a `nanoserver` pod on the Windows node and wait for it to come up.
   在 Windows 节点上启动 `nanoserver` pod 并等待它出现。

   ```bash
   microk8s kubectl run test-pod -it --image mcr.microsoft.com/windows/nanoserver:ltsc2022 --overrides '{"spec": {"nodeSelector": {"kubernetes.io/os": "windows"}}}'
   ```

   The default timeout for the pod to come up is 60 seconds, which might not  be enough for the nanoserver image to be fetched and the pod to be  started. In that case, simply wait for the pod to start and then attach  using `microk8s kubectl attach -it test-pod`
   Pod 启动的默认超时为 60 秒，这可能不足以获取 nanoserver 镜像和启动 Pod。在这种情况下，只需等待 Pod 启动，然后使用 `microk8s kubectl attach -it test-pod` 

3. Ensure that you can curl the `kubernetes` service, e.g. with `curl https://kubernetes --insecure`
   确保你可以卷曲 `kubernetes` 服务，例如使用 `curl https://kubernetes --insecure` 

   ```shell
   Microsoft Windows [Version 10.0.20348.1726]
   (c) Microsoft Corporation. All rights reserved.
   
   C:\>curl https://kubernetes --insecure
   {
   "kind": "Status",
   "apiVersion": "v1",
   "metadata": {},
   "status": "Failure",
   "message": "Unauthorized",
   "reason": "Unauthorized",
   "code": 401
   }
   ```

## [Uninstall the Windows worker node 卸载 Windows Worker 节点](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-uninstall-the-windows-worker-node)

On the Windows machine: 在 Windows 计算机上：

1. Stop and uninstall Kubernetes services:
   停止和卸载 Kubernetes 服务：

   ```powershell
   c:\calicowindows\kubernetes\uninstall-kube-services.ps1
   ```

2. Stop and uninstall Calico services:
   停止并卸载 Calico 服务：

   ```powershell
   c:\calicowindows\uninstall-calico.ps1
   ```

On the MicroK8s machine: 在 MicroK8s 机器上：

1. Remove the node from MicroK8s (replace `w1` with the name of the Windows node):
   从 MicroK8s 中删除节点（将 `w1` 替换为 Windows 节点的名称）：

   ```bash
   microk8s kubectl delete node w1
   ```

## [Upgrade 升级](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-upgrade)

There is no specific upgrade path for a Windows node. If you have upgraded  MicroK8s to a later version, and wish the Windows node to remain  compatible, you should first uninstall the node components (see above),  then re-install the newer version.
Windows 节点没有特定的升级路径。如果您已将 MicroK8s 升级到更高版本，并希望 Windows 节点保持兼容，则应首先卸载节点组件（见上文），然后重新安装较新版本。

## [Further reading 延伸阅读](https://microk8s.io/docs/add-a-windows-worker-node-to-microk8s#p-23956-further-reading)

- https://docs.tigera.io/calico/latest/getting-started/kubernetes/windows-calico/quickstart
- [Prepare Windows operating system containers | Microsoft Learn
  准备 Windows作系统容器 |Microsoft 学习](https://learn.microsoft.com/en-us/virtualization/windowscontainers/quick-start/set-up-environment?tabs=containerd#windows-server-1)

# Upgrading MicroK8s 升级 MicroK8s

#### On this page 本页内容

​                                                [Holding upgrades 持有升舱](https://microk8s.io/docs/upgrading#holding-upgrades)                                                      [Cluster upgrades 集群升级](https://microk8s.io/docs/upgrading#cluster-upgrades)                                                                                            [Upgrade a single node cluster
升级单节点集群](https://microk8s.io/docs/upgrading#upgrade-a-single-node-cluster)                                                  [Upgrade a multi-node cluster
升级多节点集群](https://microk8s.io/docs/upgrading#upgrade-a-multi-node-cluster)                                                  [Revert a failed upgrade attempt
还原失败的升级尝试](https://microk8s.io/docs/upgrading#revert-a-failed-upgrade-attempt)                                                                                                            

As detailed in the [documentation for selecting a channel](https://microk8s.io/docs/setting-snap-channel), patch release updates (e.g 1.20.x to 1.20.x+1) happen automatically for the installed version of MicroK8s. This page covers intentionally  upgrading to a new minor version (e.g. 1.20 to 1.21).
如[选择通道的文档](https://microk8s.io/docs/setting-snap-channel)中所述，已安装的 MicroK8s 版本会自动进行补丁版本更新（例如 1.20.x 到 1.20.x+1）。本页介绍了有意升级到新的次要版本（例如 1.20 到 1.21）的情况。

MicroK8s makes use of snap [channels](https://microk8s.io/docs/setting-snap-channel). This restricts automatic updates to new versions published *in that channel*, providing users with a way of making sure version upgrades only occur when the user asks for them.
MicroK8s 利用了捕捉[通道](https://microk8s.io/docs/setting-snap-channel)。这会将自动更新限制为*在该通道中*发布的新版本，从而为用户提供一种方法，确保仅在用户要求时进行版本升级。

MicroK8s channels follow the upstream release versions of Kubernetes. This makes it easy to specify a version of Kubernetes to use when installing  MicroK8s, and to restrict updates to non-breaking changes within the  same minor version. To upgrade to a new version, it is necessary to  refresh the snap to point to a different channel, but some additional  steps may be required on a running cluster to minimise disruption (as  detailed below).
MicroK8s 通道遵循 Kubernetes 的上游发布版本。这使得在安装 MicroK8s 时指定要使用的 Kubernetes  版本变得容易，并将更新限制为同一次要版本中的非重大更改。要升级到新版本，必须刷新快照以指向不同的通道，但可能需要在正在运行的集群上执行一些额外的步骤，以最大限度地减少中断（如下所述）。

**IMPORTANT NOTE:** Workloads running in the cluster, unless specified otherwise in the documentation, will **NOT** be upgraded as part of a MicroK8s upgrade. This includes both enabled  add-ons and the CNI. Currently, the most effective way to upgrade  add-ons is to use `microk8s disable <add-on>` and then re-enable them. Please make sure to read the [release notes](https://microk8s.io/docs/release-notes) for specific details referring to add-ons before upgrading. The latest manifest of the default CNI (calico) is always under `/snap/microk8s/current/upgrade-scripts/000-switch-to-calico/resources/calico.yaml`. Before applying it, please make sure you patch it with any customizations needed in your current setup.
**重要提示：**除非文档中另有说明，否则在集群中运行的工作负载**不会**作为 MicroK8s 升级的一部分进行升级。这包括已启用的附加组件和 CNI。目前，升级附加组件的最有效方法是使用 `microk8s disable <add-on>`，然后重新启用它们。在升级之前，请务必阅读[发行说明](https://microk8s.io/docs/release-notes)，了解涉及附加组件的具体详细信息。默认 CNI （calico） 的最新清单始终位于 `/snap/microk8s/current/upgrade-scripts/000-switch-to-calico/resources/calico.yaml` .在应用它之前，请确保使用当前设置中所需的任何自定义来修补它。

## [Holding upgrades 持有升舱](https://microk8s.io/docs/upgrading#holding-upgrades)

By default, upgrades are delivered by the `snapd` daemon, which checks the store regularly to see if an updated version  of the snap is available (for the currently installed channel). This is  an important part of the security mechanism of delivering MicroK8s as a  snap - making sure users get timely updates to resolve security issues  and bugs.
默认情况下，升级由 `snapd` 守护进程提供，该守护进程会定期检查存储以查看是否有可用的快照更新版本（适用于当前安装的通道）。这是快速交付 MicroK8s 的安全机制的重要组成部分 - 确保用户及时获得更新以解决安全问题和错误。
 However, it can be the case that you may not want a running unit or  cluster of MicroK8s to upgrade on the default schedule, to mitigate  against unexpected downtime.
但是，您可能不希望正在运行的 MicroK8s 单元或集群按默认计划进行升级，以缓解意外停机。
 For this reason, snap upgrades can be held to specific times/dates:
因此，Snap 升级可以保留到特定时间/日期：

To delay any refreshes for a specified period…
要在指定时间段内延迟任何刷新...

```no-highlight
sudo snap refresh --hold=24hr  microk8s
```

…to set a specific date…
…以设置特定日期...

```no-highlight
sudo snap refresh --hold=2023-02-18T15:22:04+00:00
```

…or simply to stop updates altogether…
…或者干脆完全停止更新......

```no-highlight
sudo snap refresh --hold microk8s
```

More specifics on setting the refresh parameters, including more detailed examples of using `--hold` can be found in the [Snap documentation](https://snapcraft.io/docs/keeping-snaps-up-to-date#heading--refresh-hold).
有关设置刷新参数的更多详细信息，包括使用 `--hold` 的更详细示例，请参阅 [Snap 文档](https://snapcraft.io/docs/keeping-snaps-up-to-date#heading--refresh-hold)。

If you hold or turn off snap refreshes, it is important to remember that  the software you are running may no longer be the most up to date  patched version available.
如果您按住或关闭快照刷新，请务必记住，您正在运行的软件可能不再是可用的最新修补版本。

## [Cluster upgrades 集群升级](https://microk8s.io/docs/upgrading#cluster-upgrades)

The Kubenetes project releases strive to be API backwards compatible, so  upgrading a cluster should be possible. However, as a cluster  administrator you should be aware of changes introduced on every release before attempting one. There are often important changes that affect  the behavior of the cluster and the workloads it can serve. Please,  consult the [upstream release notes](https://kubernetes.io/docs/setup/release/notes) of the release you are targeting. Pay particular attention to the deprecation notices for a release you intend to upgrade to
Kubenetes 项目版本努力实现 API 向后兼容，因此应该可以升级集群。但是，作为集群管理员，您应该在尝试每个版本之前了解每个版本中引入的更改。通常有一些重要的更改会影响集群的行为及其可以服务的工作负载。请查阅您目标版本的[上游发行说明](https://kubernetes.io/docs/setup/release/notes)。请特别注意要升级到的版本的弃用通知

Be also aware of the following constraints set by MicroK8s during an upgrade:
还要注意 MicroK8s 在升级期间设置的以下约束：

- “skip-level” updates are **NOT** tested. Only upgrade through one minor release (e.g. 1.19 to 1.20) at a time.
  “skip-level” 更新**不**进行测试。一次只能升级到一个次要版本（例如 1.19 到 1.20）。
- Downgrading (e.g. 1.20 to 1.19) is not tested or supported.
  降级（例如 1.20 到 1.19）未经测试或支持。
- Any configuration changes must be migrated incrementally across multiple versons to ensure the configuration is retained.
  任何配置更改都必须在多个版本之间增量迁移，以确保保留配置。
- Customisation (e.g. changes to the arguments for kubernetes services) **WILL** be carried over to upgraded version, but please be aware that version  changes may make these customisations incompatible with the updated  cluster.
  自定义（例如对 kubernetes 服务参数的更改）**将**转移到升级版本，但请注意，版本更改可能会使这些自定义与更新的集群不兼容。

If an upgrade is not possible it is possible to re-install MicroK8s targetting the desired version.
如果无法升级，则可以重新安装针对所需版本的 MicroK8s。

### [Upgrade a single node cluster 升级单节点集群](https://microk8s.io/docs/upgrading#upgrade-a-single-node-cluster)

Refreshing the MicroK8s snap to the desired channel effectively upgrades the  cluster. For instance, to upgrade a v1.20 cluster to v1.21 simply run `snap refresh` with the new channel name:
将 MicroK8s 快照刷新到所需的通道可以有效地升级集群。例如，要将 v1.20 集群升级到 v1.21，只需使用新的通道名称运行 `snap refresh`：

```bash
sudo snap refresh microk8s --channel=1.21/stable
```

### [Upgrade a multi-node cluster 升级多节点集群](https://microk8s.io/docs/upgrading#upgrade-a-multi-node-cluster)

Kubernetes allows for some degree of [service skew](https://kubernetes.io/docs/setup/release/version-skew-policy/) This allows a multi-node cluster to be upgraded one at a time.
Kubernetes 允许一定程度的服务[倾斜](https://kubernetes.io/docs/setup/release/version-skew-policy/)这允许一次升级一个多节点集群。

For instance, if you have an existing cluster on v1.19 and you want to  upgrade to v1.20, you should perform the following on each node:
例如，如果您在 v1.19 上有一个现有集群，并且想要升级到 v1.20，则应在每个节点上执行以下作：

```bash
microk8s kubectl drain <node> --ignore-daemonsets
```

At this point the node you drained should have no workload pods. Running the command:
此时，您排空的节点应该没有工作负载 Pod。运行命令：

```bash
microk8s kubectl get po -A -o wide
```

…should only show daemon set pods.
…应该只显示守护进程集 pods。

To upgrade the node, run:
要升级节点，请运行：

```bash
sudo snap refresh microk8s --channel=1.21/stable
```

After the new version has been fetched and the snap is updated, the node should register with the new version:
获取新版本并更新 snap 后，节点应注册到新版本：

```bash
microk8s.kubectl get no
```

The last step is to resume pod scheduling on the upgraded node with:
最后一步是在升级后的节点上恢复 Pod 调度：

```bash
microk8s kubectl uncordon <node>
```

These steps should be repeated on all the nodes in the cluster.
应在集群中的所有节点上重复这些步骤。

A [detailed example of upgrading a three node cluster](https://microk8s.io/docs/upgrade-cluster) is available in our How to section.
有关[升级三节点集群的详细示例](https://microk8s.io/docs/upgrade-cluster)，请参阅我们的作方法 部分。

### [Revert a failed upgrade attempt 还原失败的升级尝试](https://microk8s.io/docs/upgrading#revert-a-failed-upgrade-attempt)

If for any reason an upgrade does not result in a working cluster, you can revert the node to its state before the latest refresh with:
如果由于任何原因升级没有导致集群正常工作，您可以通过以下方式将节点恢复到最近一次刷新之前的状态：

```bash
sudo snap revert microk8s
```

For diagnostic purposes, you may wish to run:
出于诊断目的，您可能希望运行：

```bash
microk8s inspect
```

…before reverting the upgrade. This collects a tarball of information about the running cluster/node.
…在还原升级之前。这将收集有关正在运行的集群/节点的信息 tarball。

# How to control MicroK8s upgrades using a Snap Store Proxy 如何使用 Snap Store 代理控制 MicroK8s 升级

#### On this page 本页内容

​                                                [Details 详](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#details)                                                      [Requirements 要求](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#requirements)                                                                                            [Installation 安装](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#installation)                                                  [Connect to the proxy 连接到代理](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#connect-to-the-proxy)                                                  [Install MicroK8s 安装 MicroK8s](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#install-microk8s)                                                  [Control upgrades 控制升级](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#control-upgrades)                                                                                                            

## [ Details 详](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#details)

- Control revisions and upgrade rollouts.
  控制修订和升级推出。
- Cache snaps in local network, limiting external bandwidth requirements.
  在本地网络中缓存快照，从而限制外部带宽要求。

## [ Requirements 要求](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#requirements)

- PostgreSQL database. This guide includes instructions to deploy and configure a  single-node PostgreSQL instance. For production environments, a  PostgreSQL cluster is recommended.
  PostgreSQL 数据库。本指南包括部署和配置单节点 PostgreSQL 实例的说明。对于生产环境，建议使用 PostgreSQL 集群。
- An Ubuntu 18.04 or 20.04 instance, where the snap store proxy will be deployed.
  Ubuntu 18.04 或 20.04 实例，将在其中部署 snap store 代理。
- An Ubuntu SSO account. Ubuntu SSO 帐户。

### [ Installation 安装](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#installation)

These instructions will install a single-node Snap Store Proxy in your own  infrastructure. For a more complete set of instructions, see the [official Snap Store Proxy documentation](https://docs.ubuntu.com/snap-store-proxy/en/).
这些说明将在您自己的基础设施中安装单节点 Snap Store 代理。有关更完整的说明集，请参阅[官方 Snap Store Proxy 文档](https://docs.ubuntu.com/snap-store-proxy/en/)。

1. Start in a fresh Ubuntu 18.04 or 20.04 instance, where the Snap Store Proxy is going to be installed.
   从新的 Ubuntu 18.04 或 20.04 实例开始，其中将安装 Snap Store 代理。

2. (If not using an external PostgreSQL database), setup a single-node PostgreSQL instance:
   （如果不使用外部 PostgreSQL 数据库），设置一个单节点 PostgreSQL 实例：

   ```bash
   sudo apt-get update
   sudo apt-get install postgresql
   ```

3. Create a user and a database for the Snap Store proxy.
   为 Snap Store 代理创建用户和数据库。

   ```bash
   echo "
   CREATE ROLE \"snapproxy-user\" LOGIN CREATEROLE PASSWORD 'snapproxy-password';
   CREATE DATABASE \"snapproxy-db\" OWNER \"snapproxy-user\";
   \connect \"snapproxy-db\"
   CREATE EXTENSION \"btree_gist\";
   " | sudo -u postgres psql
   ```

4. Install snap store proxy.
   安装 snap store 代理。

   ```bash
   sudo snap refresh snapd
   sudo snap install snap-store-proxy
   ```

5. Connect to the database. Replace `postgres://snapstore-username:snapstore-password@localhost:5432/snapproxy-db` if using an external PostgreSQL database.
   连接到数据库。如果使用外部 PostgreSQL 数据库，则为 replace `postgres://snapstore-username:snapstore-password@localhost:5432/snapproxy-db` 。

   ```bash
   export POSTGRESQL_CONNECTION_STRING="postgresql://snapproxy-user:snapproxy-password@localhost:5432/snapproxy-db"
   sudo snap-proxy config proxy.db.connection="${POSTGRESQL_CONNECTION_STRING}"
   ```

6. Ensure network connectivity.
   确保网络连接。

   ```bash
   sudo snap-proxy check-connections
   ```

   This should produce output similar to:
   这应该会产生类似于以下内容的输出：

   ```auto
   http: https://dashboard.snapcraft.io: OK
   http: https://login.ubuntu.com: OK
   http: https://api.snapcraft.io: OK
   postgres: localhost: OK
   All connections appear to be accessible
   ```

7. Configure a domain name for the Snap Store Proxy. The proxy should be reachable as `http://proxy.internal`.
   为 Snap Store 代理配置域名。代理应可`作为 http://proxy.internal` 访问。

   ```bash
   sudo snap-proxy config proxy.domain=proxy.internal
   ```

8. Configure the maximum size of the proxy cache, in MBs. The default value is 2GB,  and it should be enough for most cases. We will increase this to 10GB.
   配置代理缓存的最大大小（以 MB 为单位）。默认值为 2GB，对于大多数情况来说应该足够了。我们会将其增加到 10GB。
    Running the config command without specifying a value…
   在不指定值的情况下运行 config 命令...

   ```bash
   sudo snap-proxy config proxy.cache.size
   ```

   … will return the current config setting:
   …将返回当前的配置设置：

   ```bash
   2048
   ```

   You can set the cache size by specifting a value:
   您可以通过指定一个值来设置缓存大小：

   ```bash
   sudo snap-proxy config proxy.cache.size=10240
   ```

9. Register the Snap Store Proxy. You will be asked to log in with your Ubuntu SSO account and answer a few simple questions.
   注册 Snap Store 代理。您将被要求使用您的 Ubuntu SSO 帐户登录并回答几个简单的问题。

   ```bash
   sudo snap-proxy generate-keys
   sudo snap-proxy register
   ```

10. Verify the Snap Store Proxy has been configured properly by running the command:
    通过运行以下命令验证 Snap Store 代理是否已正确配置：

    ```bash
    sudo snap-proxy status
    ```

    This should produce output like:
    这应该会产生如下输出：

    ```bash
     Store ID: YKNdvTH4IZfIFGtyaS7DSn6QCwgpgNfh
     Status: pending
     Connected Devices (updated daily): 0
     Device Limit: None
     Internal Service Status:
       memcached: running
       nginx: running
       snapauth: running
       snapdevicegw: running
       snapdevicegw-local: running
       snapproxy: running
       snaprevs: running
    ```

### [ Connect to the proxy 连接到代理](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#connect-to-the-proxy)

Connecting instances such as one or more MicroK8s nodes is as simple as pointing  snap to the Snap store proxy instance we just deployed. The `$STORE_ID` can be retrieved from the output of the `sudo snap-proxy status` command, shown just above. This step is required for each server that needs to use our snap store proxy.
连接一个或多个 MicroK8s 节点等实例就像将 snap 指向我们刚刚部署的 Snap store 代理实例一样简单。可以从 `sudo snap-proxy status` 命令的输出中检索 `$STORE_ID`，如上所示。每个需要使用我们的 snap store 代理的服务器都需要此步骤。

```bash
ubuntu@my-server:~$ export STORE_ID="YKNdvTH4IZfIFGtyaS7DSn6QCwgpgNfh"
ubuntu@my-server:~$ curl http://proxy.internal/v2/auth/store/assertions | sudo snap ack /dev/stdin
ubuntu@my-server:~$ sudo snap set core proxy.store="${STORE_ID}"
```

### [ Install MicroK8s 安装 MicroK8s](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#install-microk8s)

After configuring your server to use your snap store proxy instance, you can proceed with installing MicroK8s.
将服务器配置为使用 snap store 代理实例后，您可以继续安装 MicroK8s。

```bash
sudo snap install microk8s --classic
```

The first time, the MicroK8s snap will be fetched from the snap store.  Subsequent installations (e.g. from other servers) will be much faster,  since the snap is cached on the snap store proxy instance.
第一次，将从 snap 存储中获取 MicroK8s snap。后续安装（例如，从其他服务器安装）将快得多，因为快照缓存在 snap store 代理实例上。

### [ Control upgrades 控制升级](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy#control-upgrades)

When using a snap store proxy, it is possible to pin the snap revision that  will be installed by each channel. This allows infrastructure  administrators to gracefully control how (and when) upgrades are  performed.
使用贴靠存储代理时，可以固定每个通道将安装的贴靠修订版。这使基础架构管理员能够正常控制执行升级的方式（和时间）。

1. List available channels and the revision that each channel installs. We see that the `1.22/stable` channel installs (at the time of this writing) snap revision **2645**.
   列出可用渠道以及每个渠道安装的修订版。我们看到 `1.22/stable` 频道安装了（在撰写本文时）快照修订版 **2645**。

   ```bash
   ubuntu@proxy:~$ sudo snap list microk8s
   ...
   channels:
     1.22/stable:      v1.22.3         2021-11-14 (2645) 194MB classic
     1.22/candidate:   v1.22.4         2021-11-22 (2695) 194MB classic
     1.22/beta:        v1.22.4         2021-11-22 (2695) 194MB classic
     1.22/edge:        v1.22.4         2021-11-17 (2695) 194MB classic
     latest/stable:    v1.22.3         2021-11-15 (2645) 194MB classic
     latest/candidate: v1.22.4         2021-11-18 (2693) 198MB classic
     latest/beta:      v1.22.4         2021-11-18 (2693) 198MB classic
     latest/edge:      v1.22.4         2021-11-23 (2727) 217MB classic
   ...
   ```

2. Pin `latest/stable` to revision `2645` with the following command. As an example to verify that everything is working as it should, we also pin the `latest/edge` and `1.22/edge` channels.
   使用以下命令将 `latest/stable` 固定到修订版 `2645`。作为验证一切是否正常工作的示例，我们还固定了 `latest/edge` 和 `1.22/edge` 通道。

   ```bash
   ubuntu@proxy:~$ sudo snap-proxy override microk8s stable=2645
   microk8s stable amd64 2645
   ubuntu@proxy:~$ sudo snap-proxy override microk8s edge=2645
   microk8s edge amd64 2645
   ubuntu@proxy:~$ sudo snap-proxy override microk8s 1.22/edge=2645
   microk8s 1.22/edge amd64 2645
   
   ubuntu@proxy:~$ sudo snap-proxy list-overrides microk8s
   microk8s stable amd64 2645 (upstream 2645)
   microk8s edge amd64 2645 (upstream 2727)
   microk8s 1.22/edge amd64 2645 (upstream 2695)
   ```

3. From your server that uses the snap store proxy, verify that the revisions  have been pinned as they should (note the difference with the output we  received above):
   在使用 snap store 代理的服务器中，验证修订是否已按应有的方式固定（请注意与我们上面收到的输出不同）：

   ```bash
   ubuntu@my-server:~$ sudo snap info microk8s
   ...
   channels:
     1.22/stable:      v1.22.3         2021-11-14 (2645) 194MB classic
     1.22/candidate:   v1.22.4         2021-11-22 (2695) 194MB classic
     1.22/beta:        v1.22.4         2021-11-22 (2695) 194MB classic
     1.22/edge:        v1.22.3         2021-11-24 (2645) 194MB classic
     latest/stable:    v1.22.3         2021-11-24 (2645) 194MB classic
     latest/candidate: v1.22.4         2021-11-18 (2693) 198MB classic
     latest/beta:      v1.22.4         2021-11-18 (2693) 198MB classic
     latest/edge:      v1.22.3         2021-11-24 (2645) 194MB classic
   ...
   ```

4. Proceed with installing MicroK8s from the `edge` channel. We see that revision **2645** is installed, instead of the upstream version (**2727**).
   继续从`边缘`通道安装 MicroK8s。我们看到安装了修订版 **2645**，而不是上游版本 （**2727**）。

   ```bash
   ubuntu@my-server:~$ sudo snap install microk8s --classic --channel=edge
   microk8s (edge) v1.22.3 from Canonical✓ installed
   ubuntu@my-server:~$ snap info microk8s | grep installed
   installed:          v1.22.3                    (2645) 194MB classic
   ```

5. Deleting an override is also possible using a single command:
   也可以使用单个命令删除覆盖：

   ```bash
   ubuntu@proxy:~$ sudo snap-proxy delete-override microk8s 1.22/edge
   microk8s 1.22/edge amd64 is tracking upstream (revision 2695)
   ```

   

## 高可用性 （HA）                                       

> 对于具有三个或更多节点的集群，MicroK8s 上会自动启用高可用性。

高可用性 Kubernetes 集群是指可以承受其任何一个组件发生故障并继续为工作负载提供服务而不会中断的集群。高可用性 Kubernetes 集群需要三个组件：

1. 任何时候都必须有多个节点可用。
2. 控制平面必须在多个节点上运行，这样丢失单个节点就不会导致集群无法运行。
3. 集群状态必须位于本身具有高可用性的数据存储中。

### 为 MicroK8s 设置 HA

要实现 HA，需要：

1. 安装 MicroK8s 的 1.19+ 版本。

2. 至少 3 个节点。

#### 安装第一个节点

在 **Linux** 上，可以通过指定通道来安装任何 1.19+：

```bash
sudo snap install microk8s --classic --channel=1.19/stable
```

或者使用以下命令更新现有安装：

```bash
sudo snap refresh microk8s --classic --channel=1.19/stable
```

对于 **Windows** 和 **macOS** ，可以使用以下命令更新安装：

```bash
multipass exec microk8s -- sudo snap refresh microk8s --classic --channel=1.19/stable
```

#### 添加至少两个其他节点

和以前一样，在至少两台额外的计算机（或 LXD 容器）上安装 1.19+ 版本的 MicroK8s。

在初始节点上，运行：

```bash
microk8s add-node
```

这将输出一个带有生成令牌的命令，例如 `microk8s join 10.128.63.86:25000/567a21bdfc9a64738ef4b3286b2b8a69` 。复制此命令并从下一个节点运行它。成功加入可能需要几分钟时间。

对第三个节点和任何其他节点重复此过程（生成令牌，从加入节点运行它）。

#### 设置故障域

> 适用于 1.20+

To make MicroK8s failure domain aware associate an integer to each failure domain and update the `/var/snap/microk8s/current/args/ha-conf` with it. A restart of MicroK8s in the updated nodes is required (`microk8s.stop; microk8s.start`). For example:
要使 MicroK8s 故障域感知，请将一个整数关联到每个故障域并使用它更新 。 `/var/snap/microk8s/current/args/ha-conf` 需要在更新的节点中重新启动 MicroK8s （`microk8s.stop;microk8s.start`）。例如：

```auto
echo "failure-domain=42" > /var/snap/microk8s/current/args/ha-conf
microk8s.stop
microk8s.start
```

#### 检查状态

运行 status 命令：

```bash
microk8s status
```

从 MicroK8s 版本 1.19 开始，现在将通知您 HA 状态以及其他节点的地址和角色。例如：

```bash
microk8s is running
high-availability: yes
  datastore master nodes: 10.128.63.86:19001 10.128.63.166:19001 10.128.63.43:19001
  datastore standby nodes: none
```

### 使用 HA

Database maintenance involves a voting process through which a leader is elected. Apart from the voting nodes there are non-voting  nodes silently keeping a copy of the database. These nodes are on  standby to take over the position of a departing voter. 
HA 集群的所有节点都运行主控制平面。集群节点的子集（至少三个）维护 Kubernetes [dqlite](https://dqlite.io/) 数据库的副本。数据库维护涉及一个投票过程，通过该过程选举领导者。除了投票节点之外，还有非投票节点静默保留数据库的副本。这些节点处于待命状态，以接管离职选民的位置。最后，有些节点既不投票也不复制数据库。这些节点称为 `spare` 节点。综上所述，这三个节点角色是：

* **voters**            复制数据库，参与领袖选举
* **standby**         复制数据库，不参与 leader 选举
* **spare**              不复制数据库，不参与 leader 选举

集群形成、数据库同步、选民和领导者选举对管理员都是透明的。

HA 集群的当前状态状态显示为：

```bash
microk8s status
```

HA 检查报告的输出：

- 是否达到 HA 。
- voter 和 stand-by 节点。

由于 HA 集群的所有节点都运行主控制平面，因此 `microk8s *` 命令现在随处可用。如果其中一个节点崩溃，可以移动到任何其他节点并继续工作，而不会造成太大干扰。

几乎所有的 HA 集群管理对管理员都是透明的，并且需要最少的配置。管理员只能添加或删除节点。为了确保集群的运行状况，应考虑以下时间：

- 如果 leader 节点被不正常地“删除”，例如它崩溃并且再也没有回来，则集群最多需要 5 秒才能选出新的 leader 。
- 将非投票者提升为投票者最多需要 30 秒。当新节点进入集群或选民崩溃时，将进行此提升。

要正常删除节点，请首先在离开的节点上运行 `leave` 命令：

```bash
microk8s leave
```

该节点将在 Kubernetes 中标记为 'NotReady' （无法访问）。要完成对离开节点的删除，请在其余任何节点上发出以下命令：

```bash
microk8s remove-node <node>
```

In the case we are not able to call `microk8s leave` from the departing node, e.g. due to a node crash, we need to call `microk8s remove-node` with the `--force` flag:
如果我们无法从离开节点调用 `microk8s leave`，例如由于节点崩溃，我们需要使用 `--force` 标志调用 `microk8s remove-node`：

```auto
microk8s remove-node <node> --force
```

#### HA 集群上的附加组件

Certain add-ons download and “install” client binaries. These binaries will be  available only on the node the add-on was enabled from. For example, the helm client that gets installed with `microk8s enable helm` will be available only on the node the user issued the `microk8s enable` command.
某些附加组件会下载并 “install” 客户端二进制文件。这些二进制文件将仅在启用附加组件的节点上可用。例如，使用 `microk8s enable helm` 安装的 helm 客户端将仅在用户发出 `microk8s enable` 命令的节点上可用。

### 升级现有集群

如果有一个现有集群，则必须将所有节点刷新到至少 v1.19，例如：

```bash
sudo snap refresh microk8s --channel=1.19/stable
```

然后，需要在主节点上启用 HA 集群：

```bash
microk8s enable ha-cluster
```

为了建立 HA ，任何已经是集群中节点的计算机都需要退出并重新加入。

为此，循环遍历节点以排出、删除和重新加入它们：

```bash
microk8s kubectl drain <node> --ignore-daemonsets
```

在节点计算机上，强制它离开集群：

```bash
microk8s leave
```

然后使用 `microk8s enable ha-cluster` 启用 HA，并分别在主节点和节点上发出 `microk8s add-node` 和 `microk8s join` 将节点重新加入集群。

### 基于 etcd 的 HA 怎么样？

MicroK8s ships the upstream Kubernetes so an etcd HA setup is also possible, see the upstream documentation on how this can be achieved: [Kubernetes HA topology docs](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/)
MicroK8s 附带上游 Kubernetes，因此也可以进行 etcd HA 设置，有关如何实现此目的，请参阅上游文档：Kubernetes HA 拓扑文档
 The etcd approach is more involved and outside the scope of this  document. Overall you will need to maintain your own etcd HA cluster.  You will then need to configure the API server and flannel to point to  that etcd. Finally you will need to provide a load balancer in front of  the nodes acting as masters and configure the workers to reach the  masters through the load-balanced endpoint.
etcd 方法涉及更多，超出了本文档的范围。总的来说，需要维护自己的 etcd HA 集群。然后，您需要配置 API 服务器和 flannel  以指向该 etcd。最后，您需要在充当主节点的节点前面提供负载均衡器，并将工作线程配置为通过负载均衡终端节点访问主节点。

## 从丢失的仲裁中恢复                                                         

默认情况下，三节点 MicroK8s 集群会自动变为高可用性 （HA）。在 HA 模式下，默认数据存储 （dqlite） 实现基于 [Raft](https://raft.github.io/)  的协议，其中当选的领导者持有数据库的最终副本。在正常情况下，数据库的副本由另外两个节点维护。如果您永久丢失了用作数据库节点的大多数集群成员（例如，如果您有一个三节点集群，但丢失了其中两个节点），则该集群将变得不可用。但是，如果至少有一个数据库节点幸存下来，您将能够通过以下手动步骤恢复集群。

**Note:**This process does not  recover any data you have in PVs on the lost nodes.
**注意：**以下恢复过程仅适用于使用 MicroK8s 的默认 （dqlite） 数据存储的集群。此过程不会恢复丢失节点上 PV 中的任何数据。

### 在所有节点上停止 dqlite

停止 MicroK8s 的命令如下：

```bash
microk8s stop
```

You must also make sure the lost nodes that used to form the cluster will  not come back alive again. Any lost nodes that can be reinstated will  have to re-join the cluster with the `microk8s add-node` and `microk8s join` process (see the [documentation on clusters](https://microk8s.io/docs/clustering)).
还必须确保用于形成集群的丢失节点不会再次恢复活动状态。任何可以恢复的丢失节点都必须使用 `microk8s add-node` 和 `microk8s join` 过程重新加入集群（请参阅[集群文档](https://microk8s.io/docs/clustering)）。

### 备份数据库

Dqlite 将数据和配置文件存储在 `/var/snap/microk8s/current/var/kubernetes/backend/` 下。要制作当前状态的安全副本，请登录到一个幸存的节点并创建 dqlite 目录的 tarball：

```bash
tar -cvf backup.tar /var/snap/microk8s/current/var/kubernetes/backend
```

### 设置数据库集群的新状态

Under `/var/snap/microk8s/current/var/kubernetes/backend` the file `cluster.yaml` reflects the state of the cluster as dqlite sees it.
在文件下 `/var/snap/microk8s/current/var/kubernetes/backend` ，`cluster.yaml` 反映了 dqlite 所看到的集群状态。编辑此文件以删除丢失的节点，只留下可用的节点。例如，假设一个具有节点 `10.211.205.12`、`10.211.205.253` 和 `10.211.205.221` 的三节点集群，其中 `10.211.205.12` 和 `10.211.205.221` 丢失。在这种情况下，`cluster.yaml` 将如下所示：

```yaml
- Address: 10.211.205.122:19001
  ID: 3297041220608546238
  Role: 0
- Address: 10.211.205.253:19001
  ID: 9373968242441247628
  Role: 0
- Address: 10.211.205.221:19001
  ID: 3349965773726029294
  Role: 0
```

通过删除丢失的节点 `10.211.205.122` 和 `10.211.205.221` ，`cluster.yaml` 应只留下 `10.211.205.253` 条目。在此示例中，我们转换为单个节点。您可以选择在新集群中包含更多节点。

```yaml
- Address: 10.211.205.253:19001
  ID: 9373968242441247628
  Role: 0
```

### 重新配置 dqlite

MicroK8s 附带一个用于节点重新配置的 dqlite 客户端实用程序。

要运行的命令是：

```bash
sudo /snap/microk8s/current/bin/dqlite \
  -s 127.0.0.1:19001 \
  -c /var/snap/microk8s/current/var/kubernetes/backend/cluster.crt \
  -k /var/snap/microk8s/current/var/kubernetes/backend/cluster.key \
  k8s ".reconfigure /var/snap/microk8s/current/var/kubernetes/backend/ /var/snap/microk8s/current/var/kubernetes/backend/cluster.yaml"
```

该 `/snap/microk8s/current/bin/dqlite` 实用程序需要使用 `sudo` 调用，并采用以下参数：

- the endpoint to the (now stopped) dqlite service. We have used `-s 127.0.0.1:19001` for this endpoint in the example above.
  （现已停止）Dqlite 服务的终端节点。在上面的示例中，我们为此终端节点使用了 `-s 127.0.0.1：19001`。
- 访问数据库所需的私钥和公钥。这些键通过 `-c` 和 `-k` 参数传递，位于 dqlite 保存数据库的目录中。
- 数据库的名称。对于 MicroK8s，数据库为 `k8s`。
- 在这种情况下要执行的作是 “reconfigure”
- the path to the database we want to reconfigure is the current database under `/var/snap/microk8s/current/var/kubernetes/backend`
  我们要重新配置的数据库的路径是当前数据库 `/var/snap/microk8s/current/var/kubernetes/backend` 
- the end cluster configuration we want to recreate is reflected in the `cluster.yaml` we edited in the previous step.
  我们想要重新创建的 End Cluster 配置反映在我们上一步编辑的 `cluster.yaml` 中。

### 更新其余的集群节点

Copy the `cluster.yaml`, `snapshot-abc-abc-abc`, `snapshot-abc-abc-abc.meta` and segment files `00000abcxx-00000abcxx`) from the node where you ran the `reconfigure` command in the previous step on all other nodes mentioned in the `cluster.yaml` file.
从您在上一步中运行 `reconfigure` 命令的节点复制 `cluster.yaml`、`snapshot-abc-abc-abc`、`snapshot-abc-abc-abc.meta` 和分段文件 `00000abcxx-00000abcxx`） 在 `cluster.yaml` 文件中提到的所有其他节点上。

Further, on all nodes create an `info.yaml` that is in line with the `info.yaml` file you created previously.
此外，在所有节点上创建一个与您之前创建的 `info.yaml` 文件一致的 `info.yaml`。

> *WARNING*: Make sure to delete any leftover `snapshot-abc-abc-abc`, `snapshot-abc-abc-abc.meta`, segment (`00000abcxx-000000abcxx`, `open-abc`) and `metadata{1,2}` files that it contains. This is important otherwise the nodes will fail to cleanly rejoin the node.
> *警告*：确保删除它包含的所有剩余`快照-abc-abc-abc、``snapshot-abc-abc-abc.meta`、分段（`00000abcxx-000000abcxx`、`open-abc`）和`元数据{1,2}`文件。这一点很重要，否则节点将无法完全重新加入节点。

### 重启 MicroK8s 服务

现在应该可以通过以下方式使集群重新联机：

```bash
microk8s start
```

### 从 Kubernetes 中清理丢失的节点

丢失的节点已在 Kubernetes 中注册，但应在以下位置报告为 `NotReady`：

```bash
microk8s kubectl get no
```

要删除丢失的节点，请使用：

```bash
microk8s remove-node <node name>
```

### 恢复 HA

当 MicroK8s 集群中有三个或更多节点时，将重新获得高可用性。如果已恢复原始故障节点或创建新节点，则可以将这些节点加入集群以恢复高可用性。