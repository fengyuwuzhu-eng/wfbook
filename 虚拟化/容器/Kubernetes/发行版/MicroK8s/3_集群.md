# MicroK8s 集群

[TOC]

## 创建 MicroK8s 集群

尽管 MicroK8s 被设计为 Kubernetes 的超轻量级实现，但创建 MicroK8s 集群仍然是可能的，并且很有用。

**Note:**  Each node on a MicroK8s cluster requires its own environment to work  in, whether that is a separate VM or container on a single machine or a  different machine on the same network. Note that, as with almost all  networked services, it is also important that these instances have the  correct time (e.g. updated from an ntp server) for inter-node  communication to work.
**注意：**MicroK8s 集群上的每个节点都需要自己的工作环境，无论是单台机器上的单独 VM 或容器，还是同一网络上的不同机器。请注意，与几乎所有联网服务一样，这些实例具有正确的时间（例如从 ntp 服务器更新）以使节点间通信正常工作也很重要。

### 添加节点

要从两个或多个已在运行的 MicroK8s 实例中创建集群，请使用 `microk8s add-node` 命令。运行此命令的 MicroK8s 实例将成为集群的主实例，并将托管 Kubernetes 控制平面：

```bash
microk8s add-node
```

This will return some joining instructions which should be executed **on the MicroK8s instance that you wish to join to the cluster (NOT THE NODE YOU RAN `add-node` FROM)**
这将返回一些加入指令，这些指令应该**在您希望加入集群的 MicroK8s 实例上执行（不是您运行 `add-node` 的节点）**

```bash
From the node you wish to join to this cluster, run the following:
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05

Use the '--worker' flag to join a node as a worker not running the control plane, eg:
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05 --worker

If the node you are adding is not reachable through the default interface you can use one of the following:
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05
microk8s join 10.23.209.1:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05
microk8s join 172.17.0.1:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05
```

将节点加入集群只需几秒钟。之后应该能够看到节点已加入：

```bash
microk8s kubectl get no
```

将返回类似于以下内容的输出：

```bash
NAME               STATUS   ROLES    AGE   VERSION
10.22.254.79       Ready    <none>   27s   v1.15.3
ip-172-31-20-243   Ready    <none>   53s   v1.15.3
```

### 删除节点

首先，在要删除的节点上，运行 `microk8s leave` 。离开节点上的 MicroK8s 将重新启动自己的 control plane 并作为完整的单节点集群恢复运作：

```bash
microk8s leave
```

To complete the node removal, call `microk8s remove-node` from the remaining nodes to
要完成节点删除，请从剩余节点调用 `microk8s remove-node` 到
 indicate that the departing (unreachable now) node should be removed permanently:
指示应永久删除 Departing （Unreachable Now） 节点：

```bash
microk8s remove-node 10.22.254.79
```

### 存储

If you are using the simple storage provided by the hostpath storage  add-on, note that this will only be available to the nodes it has been  enabled on. For clustered storage, you should set up alternative  storage. For example, [see the  guide on using NFS](https://microk8s.io/docs/how-to-nfs).
如果您使用的是 hostpath storage 附加组件提供的简单存储，请注意，这仅适用于已启用它的节点。对于群集存储，应设置备用存储。例如，[请参阅有关使用 NFS 的指南](https://microk8s.io/docs/how-to-nfs)。

### 高可用性

从 MicroK8s 的 1.19 版本开始，HA 默认处于启用状态。如果集群由三个或更多节点组成，则数据存储将在节点之间复制，并且它将能够灵活应对单个故障（如果一个节点出现问题，工作负载将继续运行而不会中断）。

`microk8s status` 现在包含有关 HA 状态的信息。例如：

```bash
microk8s is running
high-availability: yes
  datastore master nodes: 10.128.63.86:19001 10.128.63.166:19001 10.128.63.43:19001
  datastore standby nodes: none
```

### Worker 节点

从 1.23 版本开始，节点可以作为 Worker 节点加入集群。Worker 节点能够托管工作负载，但它们不运行 Kubernetes  控制平面，因此它们不会增加集群的可用性 （HA）。Worker 节点非常适合低端设备，因为它们消耗的资源更少。它们在具有足够 control  plane 节点以确保 HA 的大型集群中也很有意义。要添加 worker 节点，请在运行 `microk8s join` 命令时使用 `--worker` 标志：

```bash
microk8s join 192.168.1.230:25000/92b2db237428470dc4fcfc4ebbd9dc81/2c0cb3284b05 --worker
```

A worker node runs a local API server proxy that takes care of the  communication between the local services (kubelet, kube-proxy) and the  API servers running on multiple control plane nodes. When adding a  worker node, MicroK8s attempts to detect all API server endpoints in the cluster and configure the new node accordingly. The list of API servers is stored in`/var/snap/microk8s/current/args/traefik/provider.yaml`.
Worker 节点运行本地 API 服务器代理，该代理负责本地服务（kubelet、kube-proxy）与在多个控制平面节点上运行的 API  服务器之间的通信。添加 worker 节点时，MicroK8s 会尝试检测集群中的所有 API 服务器终端节点并相应地配置新节点。API  服务器列表存储在 `/var/snap/microk8s/current/args/traefik/provider.yaml` 中。

The API server proxy will automatically check for updates when the control  plane nodes of the cluster are changed (e.g. a new control plane node is added, an old one is removed) and update the list of known API server  endpoints.
当集群的控制平面节点发生更改时（例如，添加新的控制平面节点，删除旧的控制平面节点），API 服务器代理将自动检查更新，并更新已知 API 服务器端点的列表。

If you already have a load balancer in front of the API server, you can configure the load balancer address manually in `/var/snap/microk8s/current/args/traefik/provider.yaml`. In this case, make sure to also disable the automatic refresh of the control plane endpoints by setting `--refresh-interval 0` in  `/var/snap/microk8s/current/args/apiserver-proxy`.
如果 API 服务器前面已经有负载均衡器，则可以在 中 `/var/snap/microk8s/current/args/traefik/provider.yaml` 手动配置负载均衡器地址。在这种情况下，请确保还通过在 中设置 `--refresh-interval 0` 来禁用控制平面端点的自动刷新 `/var/snap/microk8s/current/args/apiserver-proxy` 。

## 升级 MicroK8s 集群                                                              

概述了将 3 节点 MicroK8s 集群从版本 `1.22/stable` 升级到 `1.23/candidate` 的过程。

### 一般说明

以下是升级 Kubernetes 集群时要遵循的一般建议：

- 对于生产集群，在开始之前，请始终确保您有 Kubernetes 集群数据库的工作备份。
- 为了最大限度地减少误差幅度，并确保始终可以回滚到工作状态，一次只升级一个 Kubernetes 节点。
- 从 Kubernetes 控制面板节点开始。升级所有 control plane 节点后，继续升级 Kubernetes Worker 节点，逐个升级它们（或者，对于较大的集群，也以小批量方式升级）。
- 确保一次更新一个次要版本。在继续进行任何升级之前，请参阅 Kubernetes 发行说明，了解任何重大更改、已删除和/或弃用的 API，并确保它们不会影响您的集群。
- 在升级之前封锁和排空任何节点，并在升级后恢复它们，以确保 Kubernetes 集群中托管的应用程序工作负载不受影响。

> *NOTE*: For finer-grained control over MicroK8s revision upgrades in clusters running in production, consider using a [Snap Store Proxy](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy).
> *注意*：要对生产中运行的集群中的 MicroK8s 修订版升级进行更精细的控制，请考虑使用 [Snap Store 代理](https://microk8s.io/docs/manage-upgrades-with-a-snap-store-proxy)。

### 升级 3 节点集群

For our example, we have the following cluster, running on `k8s-1`, `k8s-2` and `k8s-3`. Two services are running in the workload (an `nginx` and a `microbot` deployment):
在示例中，有以下集群，在 `k8s-1`、`k8s-2` 和 `k8s-3` 上运行。工作负载中运行了两项服务（一个 `nginx` 和一个 `microbot` 部署）：

```bash
microk8s kubectl get node
microk8s kubectl get pod -o wide
```

输出如下所示（nginx 有 3 个 pod，microbot 有 10 个）：

```bash
NAME    STATUS   ROLES    AGE   VERSION
k8s-3   Ready    <none>   19d   v1.22.3-3+9ec7c40ec93c73
k8s-2   Ready    <none>   19d   v1.22.3-3+9ec7c40ec93c73
k8s-1   Ready    <none>   19d   v1.22.3-3+9ec7c40ec93c73

NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES
nginx-7848d4b86f-xwhcp     1/1     Running   0          5m41s   10.1.200.196   k8s-2   <none>           <none>
nginx-7848d4b86f-kxxjv     1/1     Running   0          4m51s   10.1.200.197   k8s-2   <none>           <none>
nginx-7848d4b86f-wsdws     1/1     Running   0          4m51s   10.1.13.71     k8s-3   <none>           <none>
microbot-fdcc4594f-mlqr7   1/1     Running   0          2m34s   10.1.13.73     k8s-3   <none>           <none>
microbot-fdcc4594f-kjcjq   1/1     Running   0          2m34s   10.1.200.199   k8s-2   <none>           <none>
microbot-fdcc4594f-4vsrd   1/1     Running   0          2m27s   10.1.231.202   k8s-1   <none>           <none>
microbot-fdcc4594f-hkqrw   1/1     Running   0          2m26s   10.1.231.203   k8s-1   <none>           <none>
microbot-fdcc4594f-qmjhq   1/1     Running   0          16s     10.1.200.200   k8s-2   <none>           <none>
microbot-fdcc4594f-nxx9j   1/1     Running   0          16s     10.1.13.74     k8s-3   <none>           <none>
microbot-fdcc4594f-pbndr   1/1     Running   0          8s      10.1.200.202   k8s-2   <none>           <none>
microbot-fdcc4594f-f2jmm   1/1     Running   0          16s     10.1.13.75     k8s-3   <none>           <none>
microbot-fdcc4594f-jtfdf   1/1     Running   0          8s      10.1.200.201   k8s-2   <none>           <none>
microbot-fdcc4594f-zl2sl   1/1     Running   0          8s      10.1.13.76     k8s-3   <none>           <none>
```

### 升级第一个节点

将从 `k8s-1` 开始集群升级。

1. 运行 `kubectl drain k8s-1`。此命令将封锁节点（用 `NoSchedule` 污点标记它，这样就不会在其上调度新的工作负载），并将所有正在运行的 Pod 驱逐到其他节点：
   
   ```bash
   microk8s kubectl drain k8s-1 --ignore-daemonsets
   ```
   
   输出应如下所示：
   
   ```bash
   node/k8s-1 cordoned
   WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-mhbqw, ingress/nginx-ingress-microk8s-controller-gtb8p
   evicting pod default/microbot-fdcc4594f-hkqrw
   evicting pod kube-system/hostpath-provisioner-5c65fbdb4f-gfdpj
   evicting pod kube-system/coredns-7f9c69c78c-nfd4b
   evicting pod default/microbot-fdcc4594f-4vsrd
   pod/hostpath-provisioner-5c65fbdb4f-gfdpj evicted
   pod/coredns-7f9c69c78c-nfd4b evicted
   pod/microbot-fdcc4594f-hkqrw evicted
   pod/microbot-fdcc4594f-4vsrd evicted
   node/k8s-1 evicted
   ```
   
2. 验证之前在 `k8s-1` 上运行的所有 Pod 都已删除，并且新 Pod 已部署到其他集群节点上。此外，请确保节点已标记为 `SchedulingDisabled`：
   
   ```bash
   microk8s kubectl get node
   microk8s kubectl get pod -o wide
   ```
   
   请注意，在 `k8s-1` 上没有看到 Pod 运行：
   
   ```bash
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready,SchedulingDisabled   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   
   NAME                       READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES
   nginx-7848d4b86f-xwhcp     1/1     Running   0          14m     10.1.200.196   k8s-2   <none>           <none>
   nginx-7848d4b86f-kxxjv     1/1     Running   0          13m     10.1.200.197   k8s-2   <none>           <none>
   nginx-7848d4b86f-wsdws     1/1     Running   0          13m     10.1.13.71     k8s-3   <none>           <none>
   microbot-fdcc4594f-mlqr7   1/1     Running   0          11m     10.1.13.73     k8s-3   <none>           <none>
   microbot-fdcc4594f-kjcjq   1/1     Running   0          11m     10.1.200.199   k8s-2   <none>           <none>
   microbot-fdcc4594f-qmjhq   1/1     Running   0          9m      10.1.200.200   k8s-2   <none>           <none>
   microbot-fdcc4594f-nxx9j   1/1     Running   0          9m      10.1.13.74     k8s-3   <none>           <none>
   microbot-fdcc4594f-f2jmm   1/1     Running   0          9m      10.1.13.75     k8s-3   <none>           <none>
   microbot-fdcc4594f-jtfdf   1/1     Running   0          8m52s   10.1.200.201   k8s-2   <none>           <none>
   microbot-fdcc4594f-zl2sl   1/1     Running   0          8m52s   10.1.13.76     k8s-3   <none>           <none>
   microbot-fdcc4594f-pbndr   1/1     Running   0          8m52s   10.1.200.202   k8s-2   <none>           <none>
   microbot-fdcc4594f-nrqh9   1/1     Running   0          8m18s   10.1.200.204   k8s-2   <none>           <none>
   microbot-fdcc4594f-dx2pk   1/1     Running   0          8m17s   10.1.13.78     k8s-3   <none>           <none>
   ```
   
3. 刷新 MicroK8s 快照以跟踪 `1.23/candidate` 通道（在撰写本文时，`1.23/stable` 尚未发布）。此命令需要在 `k8s-1` 上运行。
   
   ```bash
   sudo snap refresh microk8s --channel 1.23/candidate
   ```
   
   输出应如下所示：
   
   ```bash
   microk8s (1.23/candidate) v1.23.0-rc.0 from Canonical✓ refreshed
   ```
   
4. 不久之后，我们可以看到 `k8s-1` 现在运行的是 `1.23.0` 版本：
   
   ```bash
   microk8s kubectl get node
   
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-2   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready,SchedulingDisabled   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   ```
   
5. 最后一步是解除对节点的封锁，以便集群可以开始在其上调度新的工作负载：
   
   ```bash
   microk8s kubectl uncordon k8s-1
   
   node/k8s-1 uncordoned
   
   microk8s kubectl get node
   
   NAME    STATUS  ROLES    AGE   VERSION
   k8s-1   Ready   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   k8s-3   Ready   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   ```
   

#### 失败时回滚

此时，我们假设发生了一个假设的错误，并且我们观察到我们的 Kubernetes 集群没有按预期运行（例如，连接问题、Pod 进入错误状态、升级后的节点上的错误日志数量增加等）。在这种情况下，可能需要将节点回滚到以前的版本（例如，`1.22`）。

使用 MicroK8s，这就像运行 `sudo snap revert microk8s` 一样简单：

1. 如果节点有任何新的工作负载，请确保在进行任何更改之前 drain ：
   
   ```bash
   microk8s kubectl drain k8s-1
   ```
   
2. 恢复到以前的 MicroK8s 版本。这将重新安装之前的 snap 修订版，并恢复 control plane 服务的所有配置文件：
   
   ```bash
   sudo snap revert microk8s
   
   microk8s reverted to v1.22.3
   
   microk8s kubectl get node
   
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready,SchedulingDisabled   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   ```
   

### [Upgrade second node 升级第二个节点](https://microk8s.io/docs/upgrade-cluster#upgrade-second-node)

按照与之前相同的步骤进行作。请注意，所有 `kubectl` 命令都可以从集群中的任何节点运行。

1. 排空并封锁节点
   
   ```bash
   microk8s kubectl drain k8s-2 --ignore-daemonsets
   ```
   
2. 确保所有工作负载都已移动到其他集群节点：
   
   ```bash
   microk8s kubectl get node
   microk8s kubectl get pod -o wide
   ```
   
   请注意输出，显示 `k8s-2` 的 `SchedulingDisabled`，并且没有 Pod 运行。
   
   ```bash
   NAME    STATUS                     ROLES    AGE   VERSION
   k8s-3   Ready                      <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready,SchedulingDisabled   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-1   Ready                      <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f0
   
   NAME                       READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
   nginx-7848d4b86f-wsdws     1/1     Running   0          96m   10.1.13.71     k8s-3   <none>           <none>
   microbot-fdcc4594f-mlqr7   1/1     Running   0          94m   10.1.13.73     k8s-3   <none>           <none>
   microbot-fdcc4594f-nxx9j   1/1     Running   0          92m   10.1.13.74     k8s-3   <none>           <none>
   microbot-fdcc4594f-f2jmm   1/1     Running   0          92m   10.1.13.75     k8s-3   <none>           <none>
   microbot-fdcc4594f-zl2sl   1/1     Running   0          91m   10.1.13.76     k8s-3   <none>           <none>
   microbot-fdcc4594f-dx2pk   1/1     Running   0          91m   10.1.13.78     k8s-3   <none>           <none>
   nginx-7848d4b86f-lsjq6     1/1     Running   0          33m   10.1.231.204   k8s-1   <none>           <none>
   microbot-fdcc4594f-h9cjg   1/1     Running   0          33m   10.1.231.205   k8s-1   <none>           <none>
   microbot-fdcc4594f-98vnj   1/1     Running   0          33m   10.1.231.207   k8s-1   <none>           <none>
   microbot-fdcc4594f-glvcm   1/1     Running   0          33m   10.1.231.208   k8s-1   <none>           <none>
   microbot-fdcc4594f-m5wzj   1/1     Running   0          33m   10.1.231.209   k8s-1   <none>           <none>
   microbot-fdcc4594f-7n5k5   1/1     Running   0          33m   10.1.231.210   k8s-1   <none>           <none>
   nginx-7848d4b86f-skshj     1/1     Running   0          33m   10.1.231.211   k8s-1   <none>           <none>
   ```
   
3. 升级。此命令必须在 `k8s-2` 上运行：
   
   ```bash
   sudo snap refresh microk8s --channel 1.23/candidate
   ```
   
4. 验证 `k8s-2` 现在是否也在 `1.23.0` 上运行：
   
   ```bash
   microk8s kubectl uncordon k8s-2
   
   node/k8s-2 uncordoned
   
   microk8s kubectl get node
   
   NAME    STATUS  ROLES    AGE   VERSION
   k8s-1   Ready   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   k8s-3   Ready   <none>   19d   v1.22.3-3+9ec7c40ec93c73
   k8s-2   Ready   <none>   19d   v1.23.0-rc.0.2+f4d3c97c512f07
   ```
   

### 升级第三个节点

The process is exactly the same as with the previous two nodes:
该过程与前两个节点完全相同。

### 通过部署新工作负载进行验证

将通过创建新的 `microbot-2` 测试部署来验证我们的集群是否仍在按预期工作：

```bash
microk8s kubectl create deploy --image dontrebootme/microbot:v1 microbot-2
microk8s kubectl scale deploy microbot-2 --replicas 4
microk8s kubectl expose deploy microbot-2 --port 80 --type NodePort
deployment.apps/microbot-2 created
deployment.apps/microbot-2 scaled
service/microbot-2 exposed
```

部署完成后，服务应如下所示：

```bash
microk8s kubectl get pod -l app=microbot-2 -o wide
microk8s kubectl get svc microbot-2
NAME                          READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
microbot-2-5484459568-g299z   1/1     Running   0          43m   10.1.13.83     k8s-3   <none>           <none>
microbot-2-5484459568-2dj7z   1/1     Running   0          43m   10.1.200.214   k8s-2   <none>           <none>
microbot-2-5484459568-52cgn   1/1     Running   0          43m   10.1.231.212   k8s-1   <none>           <none>
microbot-2-5484459568-nb52k   1/1     Running   0          43m   10.1.13.84     k8s-3   <none>           <none>

NAME         TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
microbot-2   NodePort   10.152.183.221   <none>        80:30032/TCP   42m
```

最后，使用刚刚从所有 3 个集群节点创建的 `NodePort` 服务，并验证对 4 个已部署的 Pod 的负载均衡是否正常工作：

```bash
curl --silent k8s-1:30032 | grep hostname
curl --silent k8s-1:30032 | grep hostname
curl --silent k8s-2:30032 | grep hostname
curl --silent k8s-2:30032 | grep hostname
curl --silent k8s-3:30032 | grep hostname
```

在输出中，可以看到不同的 Pod 在每个 `curl` 上响应。

```bash
<p class="centered">Container hostname: microbot-2-5484459568-g299z</p>
<p class="centered">Container hostname: microbot-2-5484459568-nb52k</p>
<p class="centered">Container hostname: microbot-2-5484459568-52cgn</p>
<p class="centered">Container hostname: microbot-2-5484459568-2dj7z</p>
<p class="centered">Container hostname: microbot-2-5484459568-2dj7z</p>
```

## 高可用性 （HA）                                       

> 对于具有三个或更多节点的集群，MicroK8s 上会自动启用高可用性。

高可用性 Kubernetes 集群是指可以承受其任何一个组件发生故障并继续为工作负载提供服务而不会中断的集群。高可用性 Kubernetes 集群需要三个组件：

1. 任何时候都必须有多个节点可用。
2. 控制平面必须在多个节点上运行，这样丢失单个节点就不会导致集群无法运行。
3. 集群状态必须位于本身具有高可用性的数据存储中。

### 为 MicroK8s 设置 HA

要实现 HA，需要：

1. 安装 MicroK8s 的 1.19+ 版本。

2. 至少 3 个节点。

#### 安装第一个节点

在 **Linux** 上，可以通过指定通道来安装任何 1.19+：

```bash
sudo snap install microk8s --classic --channel=1.19/stable
```

或者使用以下命令更新现有安装：

```bash
sudo snap refresh microk8s --classic --channel=1.19/stable
```

对于 **Windows** 和 **macOS** ，可以使用以下命令更新安装：

```bash
multipass exec microk8s -- sudo snap refresh microk8s --classic --channel=1.19/stable
```

#### 添加至少两个其他节点

和以前一样，在至少两台额外的计算机（或 LXD 容器）上安装 1.19+ 版本的 MicroK8s。

在初始节点上，运行：

```bash
microk8s add-node
```

这将输出一个带有生成令牌的命令，例如 `microk8s join 10.128.63.86:25000/567a21bdfc9a64738ef4b3286b2b8a69` 。复制此命令并从下一个节点运行它。成功加入可能需要几分钟时间。

对第三个节点和任何其他节点重复此过程（生成令牌，从加入节点运行它）。

#### 设置故障域

> 适用于 1.20+

To make MicroK8s failure domain aware associate an integer to each failure domain and update the `/var/snap/microk8s/current/args/ha-conf` with it. A restart of MicroK8s in the updated nodes is required (`microk8s.stop; microk8s.start`). For example:
要使 MicroK8s 故障域感知，请将一个整数关联到每个故障域并使用它更新 。 `/var/snap/microk8s/current/args/ha-conf` 需要在更新的节点中重新启动 MicroK8s （`microk8s.stop;microk8s.start`）。例如：

```auto
echo "failure-domain=42" > /var/snap/microk8s/current/args/ha-conf
microk8s.stop
microk8s.start
```

#### 检查状态

运行 status 命令：

```bash
microk8s status
```

从 MicroK8s 版本 1.19 开始，现在将通知您 HA 状态以及其他节点的地址和角色。例如：

```bash
microk8s is running
high-availability: yes
  datastore master nodes: 10.128.63.86:19001 10.128.63.166:19001 10.128.63.43:19001
  datastore standby nodes: none
```

### 使用 HA

Database maintenance involves a voting process through which a leader is elected. Apart from the voting nodes there are non-voting  nodes silently keeping a copy of the database. These nodes are on  standby to take over the position of a departing voter. 
HA 集群的所有节点都运行主控制平面。集群节点的子集（至少三个）维护 Kubernetes [dqlite](https://dqlite.io/) 数据库的副本。数据库维护涉及一个投票过程，通过该过程选举领导者。除了投票节点之外，还有非投票节点静默保留数据库的副本。这些节点处于待命状态，以接管离职选民的位置。最后，有些节点既不投票也不复制数据库。这些节点称为 `spare` 节点。综上所述，这三个节点角色是：

* **voters**            复制数据库，参与领袖选举
* **standby**         复制数据库，不参与 leader 选举
* **spare**              不复制数据库，不参与 leader 选举

集群形成、数据库同步、选民和领导者选举对管理员都是透明的。

HA 集群的当前状态状态显示为：

```bash
microk8s status
```

HA 检查报告的输出：

- 是否达到 HA 。
- voter 和 stand-by 节点。

由于 HA 集群的所有节点都运行主控制平面，因此 `microk8s *` 命令现在随处可用。如果其中一个节点崩溃，可以移动到任何其他节点并继续工作，而不会造成太大干扰。

几乎所有的 HA 集群管理对管理员都是透明的，并且需要最少的配置。管理员只能添加或删除节点。为了确保集群的运行状况，应考虑以下时间：

- 如果 leader 节点被不正常地“删除”，例如它崩溃并且再也没有回来，则集群最多需要 5 秒才能选出新的 leader 。
- 将非投票者提升为投票者最多需要 30 秒。当新节点进入集群或选民崩溃时，将进行此提升。

要正常删除节点，请首先在离开的节点上运行 `leave` 命令：

```bash
microk8s leave
```

该节点将在 Kubernetes 中标记为 'NotReady' （无法访问）。要完成对离开节点的删除，请在其余任何节点上发出以下命令：

```bash
microk8s remove-node <node>
```

In the case we are not able to call `microk8s leave` from the departing node, e.g. due to a node crash, we need to call `microk8s remove-node` with the `--force` flag:
如果我们无法从离开节点调用 `microk8s leave`，例如由于节点崩溃，我们需要使用 `--force` 标志调用 `microk8s remove-node`：

```auto
microk8s remove-node <node> --force
```

#### HA 集群上的附加组件

Certain add-ons download and “install” client binaries. These binaries will be  available only on the node the add-on was enabled from. For example, the helm client that gets installed with `microk8s enable helm` will be available only on the node the user issued the `microk8s enable` command.
某些附加组件会下载并 “install” 客户端二进制文件。这些二进制文件将仅在启用附加组件的节点上可用。例如，使用 `microk8s enable helm` 安装的 helm 客户端将仅在用户发出 `microk8s enable` 命令的节点上可用。

### 升级现有集群

如果有一个现有集群，则必须将所有节点刷新到至少 v1.19，例如：

```bash
sudo snap refresh microk8s --channel=1.19/stable
```

然后，需要在主节点上启用 HA 集群：

```bash
microk8s enable ha-cluster
```

为了建立 HA ，任何已经是集群中节点的计算机都需要退出并重新加入。

为此，循环遍历节点以排出、删除和重新加入它们：

```bash
microk8s kubectl drain <node> --ignore-daemonsets
```

在节点计算机上，强制它离开集群：

```bash
microk8s leave
```

然后使用 `microk8s enable ha-cluster` 启用 HA，并分别在主节点和节点上发出 `microk8s add-node` 和 `microk8s join` 将节点重新加入集群。

### 基于 etcd 的 HA 怎么样？

MicroK8s ships the upstream Kubernetes so an etcd HA setup is also possible, see the upstream documentation on how this can be achieved: [Kubernetes HA topology docs](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/)
MicroK8s 附带上游 Kubernetes，因此也可以进行 etcd HA 设置，有关如何实现此目的，请参阅上游文档：Kubernetes HA 拓扑文档
 The etcd approach is more involved and outside the scope of this  document. Overall you will need to maintain your own etcd HA cluster.  You will then need to configure the API server and flannel to point to  that etcd. Finally you will need to provide a load balancer in front of  the nodes acting as masters and configure the workers to reach the  masters through the load-balanced endpoint.
etcd 方法涉及更多，超出了本文档的范围。总的来说，需要维护自己的 etcd HA 集群。然后，您需要配置 API 服务器和 flannel  以指向该 etcd。最后，您需要在充当主节点的节点前面提供负载均衡器，并将工作线程配置为通过负载均衡终端节点访问主节点。

## 从丢失的仲裁中恢复                                                         

默认情况下，三节点 MicroK8s 集群会自动变为高可用性 （HA）。在 HA 模式下，默认数据存储 （dqlite） 实现基于 [Raft](https://raft.github.io/)  的协议，其中当选的领导者持有数据库的最终副本。在正常情况下，数据库的副本由另外两个节点维护。如果您永久丢失了用作数据库节点的大多数集群成员（例如，如果您有一个三节点集群，但丢失了其中两个节点），则该集群将变得不可用。但是，如果至少有一个数据库节点幸存下来，您将能够通过以下手动步骤恢复集群。

**Note:**This process does not  recover any data you have in PVs on the lost nodes.
**注意：**以下恢复过程仅适用于使用 MicroK8s 的默认 （dqlite） 数据存储的集群。此过程不会恢复丢失节点上 PV 中的任何数据。

### 在所有节点上停止 dqlite

停止 MicroK8s 的命令如下：

```bash
microk8s stop
```

You must also make sure the lost nodes that used to form the cluster will  not come back alive again. Any lost nodes that can be reinstated will  have to re-join the cluster with the `microk8s add-node` and `microk8s join` process (see the [documentation on clusters](https://microk8s.io/docs/clustering)).
还必须确保用于形成集群的丢失节点不会再次恢复活动状态。任何可以恢复的丢失节点都必须使用 `microk8s add-node` 和 `microk8s join` 过程重新加入集群（请参阅[集群文档](https://microk8s.io/docs/clustering)）。

### 备份数据库

Dqlite 将数据和配置文件存储在 `/var/snap/microk8s/current/var/kubernetes/backend/` 下。要制作当前状态的安全副本，请登录到一个幸存的节点并创建 dqlite 目录的 tarball：

```bash
tar -cvf backup.tar /var/snap/microk8s/current/var/kubernetes/backend
```

### 设置数据库集群的新状态

Under `/var/snap/microk8s/current/var/kubernetes/backend` the file `cluster.yaml` reflects the state of the cluster as dqlite sees it.
在文件下 `/var/snap/microk8s/current/var/kubernetes/backend` ，`cluster.yaml` 反映了 dqlite 所看到的集群状态。编辑此文件以删除丢失的节点，只留下可用的节点。例如，假设一个具有节点 `10.211.205.12`、`10.211.205.253` 和 `10.211.205.221` 的三节点集群，其中 `10.211.205.12` 和 `10.211.205.221` 丢失。在这种情况下，`cluster.yaml` 将如下所示：

```yaml
- Address: 10.211.205.122:19001
  ID: 3297041220608546238
  Role: 0
- Address: 10.211.205.253:19001
  ID: 9373968242441247628
  Role: 0
- Address: 10.211.205.221:19001
  ID: 3349965773726029294
  Role: 0
```

通过删除丢失的节点 `10.211.205.122` 和 `10.211.205.221` ，`cluster.yaml` 应只留下 `10.211.205.253` 条目。在此示例中，我们转换为单个节点。您可以选择在新集群中包含更多节点。

```yaml
- Address: 10.211.205.253:19001
  ID: 9373968242441247628
  Role: 0
```

### 重新配置 dqlite

MicroK8s 附带一个用于节点重新配置的 dqlite 客户端实用程序。

要运行的命令是：

```bash
sudo /snap/microk8s/current/bin/dqlite \
  -s 127.0.0.1:19001 \
  -c /var/snap/microk8s/current/var/kubernetes/backend/cluster.crt \
  -k /var/snap/microk8s/current/var/kubernetes/backend/cluster.key \
  k8s ".reconfigure /var/snap/microk8s/current/var/kubernetes/backend/ /var/snap/microk8s/current/var/kubernetes/backend/cluster.yaml"
```

该 `/snap/microk8s/current/bin/dqlite` 实用程序需要使用 `sudo` 调用，并采用以下参数：

- the endpoint to the (now stopped) dqlite service. We have used `-s 127.0.0.1:19001` for this endpoint in the example above.
  （现已停止）Dqlite 服务的终端节点。在上面的示例中，我们为此终端节点使用了 `-s 127.0.0.1：19001`。
- 访问数据库所需的私钥和公钥。这些键通过 `-c` 和 `-k` 参数传递，位于 dqlite 保存数据库的目录中。
- 数据库的名称。对于 MicroK8s，数据库为 `k8s`。
- 在这种情况下要执行的作是 “reconfigure”
- the path to the database we want to reconfigure is the current database under `/var/snap/microk8s/current/var/kubernetes/backend`
  我们要重新配置的数据库的路径是当前数据库 `/var/snap/microk8s/current/var/kubernetes/backend` 
- the end cluster configuration we want to recreate is reflected in the `cluster.yaml` we edited in the previous step.
  我们想要重新创建的 End Cluster 配置反映在我们上一步编辑的 `cluster.yaml` 中。

### 更新其余的集群节点

Copy the `cluster.yaml`, `snapshot-abc-abc-abc`, `snapshot-abc-abc-abc.meta` and segment files `00000abcxx-00000abcxx`) from the node where you ran the `reconfigure` command in the previous step on all other nodes mentioned in the `cluster.yaml` file.
从您在上一步中运行 `reconfigure` 命令的节点复制 `cluster.yaml`、`snapshot-abc-abc-abc`、`snapshot-abc-abc-abc.meta` 和分段文件 `00000abcxx-00000abcxx`） 在 `cluster.yaml` 文件中提到的所有其他节点上。

Further, on all nodes create an `info.yaml` that is in line with the `info.yaml` file you created previously.
此外，在所有节点上创建一个与您之前创建的 `info.yaml` 文件一致的 `info.yaml`。

> *WARNING*: Make sure to delete any leftover `snapshot-abc-abc-abc`, `snapshot-abc-abc-abc.meta`, segment (`00000abcxx-000000abcxx`, `open-abc`) and `metadata{1,2}` files that it contains. This is important otherwise the nodes will fail to cleanly rejoin the node.
> *警告*：确保删除它包含的所有剩余`快照-abc-abc-abc、``snapshot-abc-abc-abc.meta`、分段（`00000abcxx-000000abcxx`、`open-abc`）和`元数据{1,2}`文件。这一点很重要，否则节点将无法完全重新加入节点。

### 重启 MicroK8s 服务

现在应该可以通过以下方式使集群重新联机：

```bash
microk8s start
```

### 从 Kubernetes 中清理丢失的节点

丢失的节点已在 Kubernetes 中注册，但应在以下位置报告为 `NotReady`：

```bash
microk8s kubectl get no
```

要删除丢失的节点，请使用：

```bash
microk8s remove-node <node name>
```

### 恢复 HA

当 MicroK8s 集群中有三个或更多节点时，将重新获得高可用性。如果已恢复原始故障节点或创建新节点，则可以将这些节点加入集群以恢复高可用性。