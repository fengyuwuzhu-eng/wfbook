# 安装

[TOC]

## CentOS

```text
yum install openvswitch
systemctl enable openvswitch
systemctl start openvswitch
```

>  如果当前软件源中没有openvswitch，可以通过[阿里云官方镜像站](https://link.zhihu.com/?target=https%3A//developer.aliyun.com/packageSearch%3Fword%3Dopenvswitch)下载和操作系统版本对应的rpm包到本地再安装。 示例命令： `yum localinstall openvswitch-2.9.0-3.el7.x86_64.rpm`
>  

## 源码安装

在通用 Linux、FreeBSD 或 NetBSD 主机上构建和安装 Open vSwitch。

### 获取 Open vSwitch 源码

Open vSwitch 源代码的规范位置是其 Git 存储库，可以使用以下命令将其克隆到名为 “ovs” 的目录中：

```bash
git clone https://github.com/openvswitch/ovs.git
```

克隆存储库后，最初签出的 “主” 分支将退出。这是一般发展的正确分支。另一方面，如果要构建特定的已发布版本，可以通过从 “ovs” 目录运行如下命令来检查它：

```bash
git checkout v2.7.0
```

The repository also has a branch for each release series.  For example, to obtain the latest fixes in the Open vSwitch 2.7.x release series, which might include bug fixes that have not yet been in any released version, you can check it out from the “ovs” directory with:
存储库还为每个发布系列提供了一个分支。例如，要获取 Open vSwitch 2.7.x 版本系列中的最新修复程序（其中可能包括任何已发布版本中尚未出现的错误修复程序），可以从“ovs”目录中签出：

```
git checkout origin/branch-2.7
```

If you do not want to use Git, you can also obtain tarballs for Open vSwitch release versions via http://openvswitch.org/download/, or download a ZIP file for any snapshot from the web interface at https://github.com/openvswitch/ovs.
如果您不想使用 Git，还可以通过 http://openvswitch.org/download/ 获取 Open vSwitch 发行版的压缩包，或从  https://github.com/openvswitch/ovs 的 Web 界面下载任何快照的 ZIP 文件。



## Build Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#build-requirements) 构建要求 ¶

To compile the userspace programs in the Open vSwitch distribution, you will need the following software:
要编译 Open vSwitch 发行版中的用户空间程序，您将需要以下软件：

- GNU make GNU 制作

- A C compiler, such as:
  C 编译器，例如：

  - GCC 4.6 or later. GCC 4.6 或更高版本。
  - Clang 3.4 or later. Clang 3.4 或更高版本。
  - MSVC 2013. Refer to [Open vSwitch on Windows](https://docs.openvswitch.org/en/latest/intro/install/windows/) for additional Windows build instructions.
    MSVC 2013 年。有关其他 Windows 构建说明，请参阅在 Windows 上打开 vSwitch。

  While OVS may be compatible with other compilers, optimal support for atomic operations may be missing, making OVS very slow (see `lib/ovs-atomic.h`).
  虽然 OVS 可能与其他编译器兼容，但可能缺少对原子操作的最佳支持，从而使 OVS 非常慢（请参阅 `lib/ovs-atomic.h` ）。

- libssl, from OpenSSL, is optional but recommended if you plan to connect the Open vSwitch to an OpenFlow controller. libssl is required to establish confidentiality and authenticity in the connections from an Open vSwitch to an OpenFlow controller. If libssl is installed, then Open vSwitch will automatically build with support for it.
  OpenSSL 中的 libssl 是可选的，但如果您计划将 Open vSwitch 连接到 OpenFlow 控制器，则建议使用。需要 libssl 在从 Open vSwitch 到 OpenFlow 控制器的连接中建立机密性和真实性。如果安装了 libssl，则 Open vSwitch  将自动构建并支持它。

- libcap-ng, written by Steve Grubb, is optional but recommended. It is required to run OVS daemons as a non-root user with dropped root privileges. If libcap-ng is installed, then Open vSwitch will automatically build with support for it.
  由 Steve Grubb 编写的 libcap-ng 是可选的，但值得推荐。需要以具有已删除的 root 权限的非 root 用户身份运行 OVS 守护程序。如果安装了 libcap-ng，则 Open vSwitch 将自动构建并支持它。

- Python 3.6 or later. Python 3.6 或更高版本。

- Unbound library, from http://www.unbound.net, is optional but recommended if you want to enable ovs-vswitchd and other utilities to use DNS names when specifying OpenFlow and OVSDB remotes. If unbound library is already installed, then Open vSwitch will automatically build with support for it. The environment variable OVS_RESOLV_CONF can be used to specify DNS server configuration file (the default file on Linux is /etc/resolv.conf), and environment variable OVS_UNBOUND_CONF can be used to specify the configuration file for unbound.
  http://www.unbound.net 中的未绑定库是可选的，但如果要在指定 OpenFlow 和 OVSDB 远程时启用 ovs-vswitchd 和其他实用程序使用 DNS  名称，则建议使用。如果已安装未绑定库，则 Open vSwitch 将自动构建支持该库。环境变量 OVS_RESOLV_CONF 可用于指定  DNS 服务器配置文件（Linux 上的默认文件为 /etc/resolv.conf），环境变量 OVS_UNBOUND_CONF  可用于指定未绑定的配置文件。

On Linux, you may choose to compile the kernel module that comes with the Open vSwitch distribution or to use the kernel module built into the Linux kernel (version 3.3 or later). See the [FAQ](https://docs.openvswitch.org/en/latest/faq/) question “What features are not available in the Open vSwitch kernel datapath that ships as part of the upstream Linux kernel?” for more information on this trade-off. You may also use the userspace-only implementation, at some cost in features and performance. Refer to [Open vSwitch without Kernel Support](https://docs.openvswitch.org/en/latest/intro/install/userspace/) for details.
在 Linux 上，您可以选择编译 Open vSwitch 发行版附带的内核模块，也可以使用 Linux 内核（版本 3.3  或更高版本）中内置的内核模块。有关此权衡的更多信息，请参阅常见问题解答问题“作为上游 Linux 内核的一部分提供的 Open vSwitch  内核数据路径中不提供哪些功能？您也可以使用仅限用户空间的实现，但在功能和性能方面会付出一些代价。有关详细信息，请参阅在没有内核支持的情况下打开  vSwitch。

To compile the kernel module on Linux, you must also install the following:
要在 Linux 上编译内核模块，还必须安装以下内容：

- A supported Linux kernel version.
  受支持的 Linux 内核版本。

  For optional support of ingress policing, you must enable kernel configuration options `NET_CLS_BASIC`, `NET_SCH_INGRESS`, and `NET_ACT_POLICE`, either built-in or as modules. `NET_CLS_POLICE` is obsolete and not needed.)
  要获得对入口策略的可选支持，必须启用内核配置选项 `NET_CLS_BASIC` 、 `NET_SCH_INGRESS` 和 `NET_ACT_POLICE` ，无论是内置的还是作为模块的。 `NET_CLS_POLICE` 已过时且不需要。

  On kernels before 3.11, the `ip_gre` module, for GRE tunnels over IP (`NET_IPGRE`), must not be loaded or compiled in.
  在 3.11 之前的内核上，不得加载或编译用于 IP （ `NET_IPGRE` ） 上的 GRE 隧道的 `ip_gre` 模块。

  To configure HTB or HFSC quality of service with Open vSwitch, you must enable the respective configuration options.
  要使用 Open vSwitch 配置 HTB 或 HFSC 服务质量，您必须启用相应的配置选项。

  To use Open vSwitch support for TAP devices, you must enable `CONFIG_TUN`.
  要对 TAP 设备使用 Open vSwitch 支持，必须启用 `CONFIG_TUN` 。

- To build a kernel module, you need the same version of GCC that was used to build that kernel.
  要构建内核模块，您需要与构建该内核时使用的相同版本的 GCC。

- A kernel build directory corresponding to the Linux kernel image the module is to run on. Under Debian and Ubuntu, for example, each linux-image package containing a kernel binary has a corresponding linux-headers package with the required build infrastructure.
  与运行模块的 Linux 内核映像相对应的内核构建目录。例如，在 Debian 和 Ubuntu 下，每个包含内核二进制文件的 linux-image 软件包都有一个相应的 linux-headers 软件包，其中包含所需的构建基础结构。

If you are working from a Git tree or snapshot (instead of from a distribution tarball), or if you modify the Open vSwitch build system or the database schema, you will also need the following software:
如果使用 Git 树或快照（而不是从分发压缩包）工作，或者修改 Open vSwitch 构建系统或数据库架构，则还需要以下软件：

- Autoconf version 2.63 or later.
  Autoconf 版本 2.63 或更高版本。
- Automake version 1.10 or later.
  Automake 版本 1.10 或更高版本。
- libtool version 2.4 or later. (Older versions might work too.)
  libtool 版本 2.4 或更高版本。（旧版本也可能有效。

The datapath tests for userspace and Linux datapaths also rely upon:
用户空间和 Linux 数据路径的数据路径测试还依赖于：

- pyftpdlib. Version 1.2.0 is known to work. Earlier versions should also work.
  pyftpdlib 中。已知版本 1.2.0 有效。早期版本也应该可以工作。
- GNU wget. Version 1.16 is known to work. Earlier versions should also work.
  GNU wget。已知版本 1.16 有效。早期版本也应该可以工作。
- netcat. Several common implementations are known to work.
  网猫。已知有几种常见的实现是有效的。
- curl. Version 7.47.0 is known to work. Earlier versions should also work.
  卷曲。已知版本 7.47.0 有效。早期版本也应该可以工作。
- tftpy. Version 0.6.2 is known to work. Earlier versions should also work.
  TFTPY中。已知版本 0.6.2 有效。早期版本也应该可以工作。
- netstat.  Available from various distro specific packages
  netstat 中。可从各种发行版特定软件包中获取

The ovs-vswitchd.conf.db(5) manpage will include an E-R diagram, in formats other than plain text, only if you have the following:
仅当满足以下条件时，ovs-vswitchd.conf.db（5） 手册页将包含除纯文本以外的格式的 E-R 图：

- dot from graphviz (http://www.graphviz.org/).
  来自 graphviz 的点 （ http://www.graphviz.org/）。

If you are going to extensively modify Open vSwitch, consider installing the following to obtain better warnings:
如果要广泛修改 Open vSwitch，请考虑安装以下内容以获得更好的警告：

- “sparse” version 0.6.2 or later (https://git.kernel.org/pub/scm/devel/sparse/sparse.git/).
  “sparse”版本 0.6.2 或更高版本 （ https://git.kernel.org/pub/scm/devel/sparse/sparse.git/）。
- GNU make. GNU 制作。
- clang, version 3.4 or later
  Clang，版本 3.4 或更高版本
- flake8 (for Python code) flake8（用于 Python 代码）
- the python packages listed in “python/test_requirements.txt” (compatible with pip). If they are installed, the pytest-based Python unit tests will be run.
  “python/test_requirements.txt”中列出的 python 包（与 pip 兼容）。如果安装了它们，将运行基于 pytest 的 Python 单元测试。

You may find the ovs-dev script found in `utilities/ovs-dev.py` useful.
您可能会发现 ovs-dev 脚本很有 `utilities/ovs-dev.py` 用。



## Installation Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#installation-requirements) 安装要求 ¶

The machine you build Open vSwitch on may not be the one you run it on. To simply install and run Open vSwitch you require the following software:
您构建 Open vSwitch 的计算机可能不是运行它的计算机。要简单地安装并运行 Open vSwitch，您需要以下软件：

- Shared libraries compatible with those used for the build.
  与用于构建的共享库兼容的共享库。
- On Linux, if you want to use the kernel-based datapath (which is the most common use case), then a kernel with a compatible kernel module.  This can be a kernel module built with Open vSwitch (e.g. in the previous step), or the kernel module that accompanies Linux 3.3 and later.  Open vSwitch features and performance can vary based on the module and the kernel.  Refer to [Releases](https://docs.openvswitch.org/en/latest/faq/releases/) for more information.
  在 Linux 上，如果要使用基于内核的数据路径（这是最常见的用例），则使用具有兼容内核模块的内核。这可以是使用 Open vSwitch  构建的内核模块（例如在上一步中），也可以是 Linux 3.3 及更高版本附带的内核模块。Open vSwitch  的功能和性能可能因模块和内核而异。有关详细信息，请参阅版本。
- For optional support of ingress policing on Linux, the “tc” program from iproute2 (part of all major distributions and available at https://wiki.linuxfoundation.org/networking/iproute2).
  对于 Linux 上的入口监管的可选支持，iproute2 中的“tc”程序（所有主要发行版的一部分，可在 https://wiki.linuxfoundation.org/networking/iproute2 上获得）。
- Python 3.6 or later. Python 3.6 或更高版本。

On Linux you should ensure that `/dev/urandom` exists. To support TAP devices, you must also ensure that `/dev/net/tun` exists.
在 Linux 上，您应该确保它 `/dev/urandom` 存在。若要支持 TAP 设备，还必须确保它 `/dev/net/tun` 存在。



## Bootstrapping[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#bootstrapping) 引导 ¶

This step is not needed if you have downloaded a released tarball. If you pulled the sources directly from an Open vSwitch Git tree or got a Git tree snapshot, then run boot.sh in the top source directory to build the “configure” script:
如果您已下载已发布的压缩包，则不需要此步骤。如果您直接从 Open vSwitch Git 树中提取源代码或获取 Git 树快照，请在顶部源目录中运行 boot.sh 以构建“configure”脚本：

```
$ ./boot.sh
```



## Configuring[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#configuring) 配置 ¶

Configure the package by running the configure script. You can usually invoke configure without any arguments. For example:
通过运行配置脚本来配置包。通常可以在没有任何参数的情况下调用 configure。例如：

```
$ ./configure
```

By default all files are installed under `/usr/local`. Open vSwitch also expects to find its database in `/usr/local/etc/openvswitch` by default. If you want to install all files into, e.g., `/usr` and `/var` instead of `/usr/local` and `/usr/local/var` and expect to use `/etc/openvswitch` as the default database directory, add options as shown here:
默认情况下，所有文件都安装在 `/usr/local` 下。默认情况下，Open vSwitch 还希望找到 `/usr/local/etc/openvswitch` 其数据库。如果要将所有文件安装到（例如）中 `/var` ， `/usr` 而不是 `/usr/local` and `/usr/local/var` 和 期望 `/etc/openvswitch` 用作默认数据库目录，请添加选项，如下所示：

```
$ ./configure --prefix=/usr --localstatedir=/var --sysconfdir=/etc
```

Note 注意

Open vSwitch installed with packages like .rpm (e.g. via `yum install` or `rpm -ivh`) and .deb (e.g. via `apt-get install` or `dpkg -i`) use the above configure options.
使用 .rpm（例如 via `yum install` 或 ）和 .deb（例如 via `apt-get install` 或 `dpkg -i` `rpm -ivh` ）等软件包安装的 Open vSwitch 使用上述配置选项。

By default, static libraries are built and linked against. If you want to use shared libraries instead:
默认情况下，将生成静态库并与之链接。如果要改用共享库：

```
$ ./configure --enable-shared
```

To use a specific C compiler for compiling Open vSwitch user programs, also specify it on the configure command line, like so:
要使用特定的 C 编译器来编译 Open vSwitch 用户程序，还要在 configure 命令行中指定它，如下所示：

```
$ ./configure CC=gcc-4.2
```

To use ‘clang’ compiler:
要使用“clang”编译器：

```
$ ./configure CC=clang
```

To supply special flags to the C compiler, specify them as `CFLAGS` on the configure command line. If you want the default CFLAGS, which include `-g` to build debug symbols and `-O2` to enable optimizations, you must include them yourself. For example, to build with the default CFLAGS plus `-mssse3`, you might run configure as follows:
若要向 C 编译器提供特殊标志，请 `CFLAGS` 在 configure 命令行上指定它们。如果需要默认的 CFLAGS（包括 `-g` 用于生成调试符号和 `-O2` 启用优化），则必须自行包含它们。例如，要使用默认的 CFLAGS plus `-mssse3` 进行构建，您可以按如下方式运行 configure：

```
$ ./configure CFLAGS="-g -O2 -mssse3"
```

For efficient hash computation special flags can be passed to leverage built-in intrinsics. For example on X86_64 with SSE4.2 instruction set support, CRC32 intrinsics can be used by passing `-msse4.2`:
为了实现高效的哈希计算，可以传递特殊标志以利用内置内部函数。例如，在支持 SSE4.2 指令集的X86_64上，可以通过传递 `-msse4.2` 以下命令来使用 CRC32 内部函数：

```
$ ./configure CFLAGS="-g -O2 -msse4.2"`
```

Also builtin popcnt instruction can be used to speedup the counting of the bits set in an integer. For example on X86_64 with POPCNT support, it can be enabled by passing `-mpopcnt`:
此外，内置的 popcnt 指令可用于加快整数中设置的位数的计数。例如，在支持 POPCNT 的X86_64上，可以通过传递以下命令 `-mpopcnt` 来启用它：

```
$ ./configure CFLAGS="-g -O2 -mpopcnt"`
```

If you are on a different processor and don’t know what flags to choose, it is recommended to use `-march=native` settings:
如果您使用的是其他处理器，并且不知道要选择哪些标志，建议使用 `-march=native` 设置：

```
$ ./configure CFLAGS="-g -O2 -march=native"
```

With this, GCC will detect the processor and automatically set appropriate flags for it. This should not be used if you are compiling OVS outside the target machine.
有了这个，GCC 将检测处理器并自动为其设置适当的标志。如果要在目标计算机外部编译 OVS，则不应使用此选项。

Note 注意

CFLAGS are not applied when building the Linux kernel module. Custom CFLAGS for the kernel module are supplied using the `EXTRA_CFLAGS` variable when running make. For example:
构建 Linux 内核模块时不应用 CFLAGS。内核模块的自定义 CFLAGS 在运行 make 时使用变量 `EXTRA_CFLAGS` 提供。例如：

```
$ make EXTRA_CFLAGS="-Wno-error=date-time"
```

If you are a developer and want to enable Address Sanitizer for debugging purposes, at about a 2x runtime cost, you can add `-fsanitize=address -fno-omit-frame-pointer -fno-common` to CFLAGS.  For example:
如果您是开发人员，并且想要启用地址清理程序进行调试，则可以以大约 2 倍的运行时成本添加到 `-fsanitize=address -fno-omit-frame-pointer -fno-common` CFLAGS。例如：

```
$ ./configure CFLAGS="-g -O2 -fsanitize=address -fno-omit-frame-pointer -fno-common"
```

If you plan to do much Open vSwitch development, you might want to add `--enable-Werror`, which adds the `-Werror` option to the compiler command line, turning warnings into errors. That makes it impossible to miss warnings generated by the build. For example:
如果您计划进行大量 Open vSwitch 开发，则可能需要添加 `--enable-Werror` ，这会将 `-Werror` 该选项添加到编译器命令行，从而将警告转换为错误。这样就不可能错过构建生成的警告。例如：

```
$ ./configure --enable-Werror
```

If you’re building with GCC, then, for improved warnings, install `sparse` (see “Prerequisites”) and enable it for the build by adding `--enable-sparse`.  Use this with `--enable-Werror` to avoid missing both compiler and `sparse` warnings, e.g.:
如果使用 GCC 进行构建，则为了改进警告，请安装 `sparse` （请参阅“先决条件”）并通过添加 `--enable-sparse` .将其与避免丢失编译器和 `sparse` 警告一起 `--enable-Werror` 使用，例如：

```
$ ./configure --enable-Werror --enable-sparse
```

To build with gcov code coverage support, add `--enable-coverage`:
要使用 gcov 代码覆盖率支持进行构建，请添加 `--enable-coverage` ：

```
$ ./configure --enable-coverage
```

The configure script accepts a number of other options and honors additional environment variables. For a full list, invoke configure with the `--help` option:
configure 脚本接受许多其他选项，并遵循其他环境变量。有关完整列表，请使用以下 `--help` 选项调用 configure：

```
$ ./configure --help
```

You can also run configure from a separate build directory. This is helpful if you want to build Open vSwitch in more than one way from a single source directory, e.g. to try out both GCC and Clang builds, or to build kernel modules for more than one Linux version. For example:
您也可以从单独的构建目录运行 configure。如果您想从单个源目录以多种方式构建 Open vSwitch，例如尝试 GCC 和 Clang 构建，或者为多个 Linux 版本构建内核模块，这将非常有用。例如：

```
$ mkdir _gcc && (cd _gcc && ./configure CC=gcc)
$ mkdir _clang && (cd _clang && ./configure CC=clang)
```

Under certain loads the ovsdb-server and other components perform better when using the jemalloc memory allocator, instead of the glibc memory allocator. If you wish to link with jemalloc add it to LIBS:
在某些负载下，ovsdb-server 和其他组件在使用 jemalloc 内存分配器而不是 glibc 内存分配器时性能更好。如果您想与 jemalloc 链接，请将其添加到 LIBS：

```
$ ./configure LIBS=-ljemalloc
```

Note 注意

Linking Open vSwitch with the jemalloc shared library may not work as expected in certain operating system development environments. You can override the automatic compiler decision to avoid possible linker issues by passing `-fno-lto` or `-fno-builtin` flag since the jemalloc override standard built-in memory allocation functions such as malloc, calloc, etc. Both options can solve possible jemalloc linker issues with pros and cons for each case, feel free to choose the path that appears best to you. Disabling LTO flag example:
在某些操作系统开发环境中，将 Open vSwitch 与 jemalloc 共享库链接可能无法按预期工作。您可以通过传递 `-fno-lto` 或 `-fno-builtin` 标记来覆盖自动编译器决策，以避免可能的链接器问题，因为 jemalloc 会覆盖标准内置内存分配函数，例如 malloc、calloc  等。这两个选项都可以解决可能的 jemalloc 链接器问题，每种情况都有利弊，请随意选择最适合您的路径。禁用 LTO 标志示例：

```
$ ./configure LIBS=-ljemalloc CFLAGS="-g -O2 -fno-lto"
```

Disabling built-in flag example:
禁用内置标志示例：

```
$ ./configure LIBS=-ljemalloc CFLAGS="-g -O2 -fno-builtin"
```



## Building[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#building) 构建 ¶

1. Run GNU make in the build directory, e.g.:
   在构建目录中运行 GNU make，例如：

   ```
   $ make
   ```

   or if GNU make is installed as “gmake”:
   或者如果 GNU make 安装为“gmake”：

   ```
   $ gmake
   ```

   If you used a separate build directory, run make or gmake from that directory, e.g.:
   如果您使用了单独的构建目录，请从该目录运行 make 或 gmake，例如：

   ```
   $ make -C _gcc
   $ make -C _clang
   ```

   Note 注意

   Some versions of Clang and ccache are not completely compatible. If you see unusual warnings when you use both together, consider disabling ccache.
   某些版本的 Clang 和 ccache 并不完全兼容。如果在同时使用两者时看到异常警告，请考虑禁用 ccache。

2. Consider running the testsuite. Refer to [Testing](https://docs.openvswitch.org/en/latest/topics/testing/) for instructions.
   请考虑运行测试套件。有关说明，请参阅测试。

3. Run `make install` to install the executables and manpages into the running system, by default under `/usr/local`:
   运行 `make install` 以将可执行文件和手册页安装到正在运行的系统中，默认情况下位于 `/usr/local` ：

   ```
   $ make install
   ```



## Starting[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#starting) 开始¶

On Unix-alike systems, such as BSDs and Linux, starting the Open vSwitch suite of daemons is a simple process.  Open vSwitch includes a shell script, and helpers, called ovs-ctl which automates much of the tasks for starting and stopping ovsdb-server, and ovs-vswitchd.  After installation, the daemons can be started by using the ovs-ctl utility.  This will take care to setup initial conditions, and start the daemons in the correct order.  The ovs-ctl utility is located in ‘$(pkgdatadir)/scripts’, and defaults to ‘/usr/local/share/openvswitch/scripts’.  An example after install might be:
在类 Unix 系统（如 BSD 和 Linux）上，启动 Open vSwitch 守护进程套件是一个简单的过程。Open vSwitch  包括一个 shell 脚本和称为 ovs-ctl 的帮助程序，它可以自动执行启动和停止 ovsdb-server 和 ovs-vswitched 的大部分任务。安装后，可以使用 ovs-ctl 实用程序启动守护程序。这将注意设置初始条件，并以正确的顺序启动守护程序。ovs-ctl  实用程序位于 '$（pkgdatadir）/scripts' 中，默认为  '/usr/local/share/openvswitch/scripts'。安装后的示例可能是：

```
$ export PATH=$PATH:/usr/local/share/openvswitch/scripts
$ ovs-ctl start
```

Additionally, the ovs-ctl script allows starting / stopping the daemons individually using specific options.  To start just the ovsdb-server:
此外，ovs-ctl 脚本允许使用特定选项单独启动/停止守护进程。要仅启动 ovsdb-server，请执行以下操作：

```
$ export PATH=$PATH:/usr/local/share/openvswitch/scripts
$ ovs-ctl --no-ovs-vswitchd start
```

Likewise, to start just the ovs-vswitchd:
同样，要启动 ovs-vswitchd：

```
$ export PATH=$PATH:/usr/local/share/openvswitch/scripts
$ ovs-ctl --no-ovsdb-server start
```

Refer to ovs-ctl(8) for more information on ovs-ctl.
有关 ovs-ctl 的更多信息，请参见 ovs-ctl（8）。

In addition to using the automated script to start Open vSwitch, you may wish to manually start the various daemons. Before starting ovs-vswitchd itself, you need to start its configuration database, ovsdb-server. Each machine on which Open vSwitch is installed should run its own copy of ovsdb-server. Before ovsdb-server itself can be started, configure a database that it can use:
除了使用自动脚本启动 Open vSwitch 之外，您可能还希望手动启动各种守护程序。在启动 ovs-vswitchd 本身之前，您需要启动其配置数据库  ovsdb-server。安装了 Open vSwitch 的每台计算机都应运行自己的 ovsdb-server 副本。在启动  ovsdb-server 本身之前，请配置一个可以使用的数据库：

```
$ mkdir -p /usr/local/etc/openvswitch
$ ovsdb-tool create /usr/local/etc/openvswitch/conf.db \
    vswitchd/vswitch.ovsschema
```

Configure ovsdb-server to use database created above, to listen on a Unix domain socket, to connect to any managers specified in the database itself, and to use the SSL configuration in the database:
将 ovsdb-server 配置为使用上面创建的数据库、侦听 Unix 域套接字、连接到数据库本身中指定的任何管理器，以及使用数据库中的 SSL 配置：

```
$ mkdir -p /usr/local/var/run/openvswitch
$ ovsdb-server --remote=punix:/usr/local/var/run/openvswitch/db.sock \
    --remote=db:Open_vSwitch,Open_vSwitch,manager_options \
    --private-key=db:Open_vSwitch,SSL,private_key \
    --certificate=db:Open_vSwitch,SSL,certificate \
    --bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert \
    --pidfile --detach --log-file
```

Note 注意

If you built Open vSwitch without SSL support, then omit `--private-key`, `--certificate`, and `--bootstrap-ca-cert`.)
如果构建了不支持 SSL 的 Open vSwitch，则省略 `--private-key` 、 `--certificate` 和 `--bootstrap-ca-cert` 。

Initialize the database using ovs-vsctl. This is only necessary the first time after you create the database with ovsdb-tool, though running it at any time is harmless:
使用 ovs-vsctl 初始化数据库。这仅在使用 ovsdb-tool 创建数据库后第一次需要，尽管随时运行它是无害的：

```
$ ovs-vsctl --no-wait init
```

Start the main Open vSwitch daemon, telling it to connect to the same Unix domain socket:
启动主 Open vSwitch 守护程序，告诉它连接到同一个 Unix 域套接字：

```
$ ovs-vswitchd --pidfile --detach --log-file
```

## Starting OVS in container[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#starting-ovs-in-container) 在容器中启动 OVS ¶

For ovs vswitchd, we need to load ovs kernel modules on host.
对于 ovs vswitchd，我们需要在主机上加载 ovs 内核模块。

Hence, OVS containers kernel version needs to be same as that of host kernel.
因此，OVS容器内核版本需要与主机内核版本相同。

Export following variables in .env  and place it under project root:
在 .env 中导出以下变量并将其放在项目根目录下：

```
$ OVS_BRANCH=<BRANCH>
$ OVS_VERSION=<VERSION>
$ DISTRO=<LINUX_DISTRO>
$ KERNEL_VERSION=<LINUX_KERNEL_VERSION>
$ GITHUB_SRC=<GITHUB_URL>
$ DOCKER_REPO=<REPO_TO_PUSH_IMAGE>
```

To build ovs modules:
要构建 ovs 模块，请执行以下操作：

```
$ cd utilities/docker
$ make build
```

Compiled Modules will be tagged with docker image
编译后的模块将使用 docker 镜像进行标记

To Push ovs modules:
要推送 ovs 模块，请执行以下操作：

```
$ make push
```

OVS docker image will be pushed to specified docker repo.
OVS docker 镜像将被推送到指定的 docker 存储库。

Start ovsdb-server using below command:
使用以下命令启动 ovsdb-server：

```
$ docker run -itd --net=host --name=ovsdb-server \
  <docker_repo>:<tag> ovsdb-server
```

Start ovs-vswitchd with privileged mode as it needs to load kernel module in host using below command:
使用特权模式启动 ovs-vswitchd，因为它需要使用以下命令在主机中加载内核模块：

```
$ docker run -itd --net=host --name=ovs-vswitchd \
  --volumes-from=ovsdb-server -v /lib:/lib --privileged \
  <docker_repo>:<tag> ovs-vswitchd
```

Note 注意

The debian docker file uses ubuntu 16.04 as a base image for reference.
debian docker 文件使用 ubuntu 16.04 作为基础镜像作为参考。

User can use any other base image for debian, e.g. u14.04, etc.
用户可以使用任何其他 Debian 基础映像，例如 u14.04 等。

RHEL based docker build support needs to be added.
需要添加基于 RHEL 的 docker 构建支持。

## Validating[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#validating) 验证 ¶

At this point you can use ovs-vsctl to set up bridges and other Open vSwitch features.  For example, to create a bridge named `br0` and add ports `eth0` and `vif1.0` to it:
此时，您可以使用 ovs-vsctl 设置网桥和其他 Open vSwitch 功能。例如，要创建一个名为 `br0` and add ports `eth0` and `vif1.0` to it 的网桥：

```
$ ovs-vsctl add-br br0
$ ovs-vsctl add-port br0 eth0
$ ovs-vsctl add-port br0 vif1.0
```

Refer to ovs-vsctl(8) for more details. You may also wish to refer to [Testing](https://docs.openvswitch.org/en/latest/topics/testing/) for information on more generic testing of OVS.
更多细节请参考 ovs-vsctl（8）。您可能还希望参考测试，以获取有关 OVS 的更通用测试的信息。

When using ovs in container, exec to container to run above commands:
在容器中使用 ovs 时，exec to container 以运行以下命令：

```
$ docker exec -it <ovsdb-server/ovs-vswitchd> /bin/bash
```

## Upgrading[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#upgrading) 升级 ¶

When you upgrade Open vSwitch from one version to another you should also upgrade the database schema:
将 Open vSwitch 从一个版本升级到另一个版本时，还应升级数据库架构：

Note 注意

The following manual steps may also be accomplished by using ovs-ctl to stop and start the daemons after upgrade.  The ovs-ctl script will automatically upgrade the schema.
升级后，还可以使用 ovs-ctl 停止和启动守护程序来完成以下手动步骤。ovs-ctl 脚本将自动升级架构。

1. Stop the Open vSwitch daemons, e.g.:
   停止打开 vSwitch 守护程序，例如：

   ```
   $ kill `cd /usr/local/var/run/openvswitch && cat ovsdb-server.pid ovs-vswitchd.pid`
   ```

2. Install the new Open vSwitch release by using the same configure options as was used for installing the previous version. If you do not use the same configure options, you can end up with two different versions of Open vSwitch executables installed in different locations.
   使用与安装以前版本相同的配置选项安装新的 Open vSwitch 版本。如果不使用相同的配置选项，则最终可能会在不同位置安装两个不同版本的 Open vSwitch 可执行文件。

3. Upgrade the database, in one of the following two ways:
   通过以下两种方式之一升级数据库：

   - If there is no important data in your database, then you may delete the database file and recreate it with ovsdb-tool, following the instructions under “Building and Installing Open vSwitch for Linux, FreeBSD or NetBSD”.
     如果您的数据库中没有重要数据，那么您可以删除数据库文件并使用 ovsdb-tool 重新创建它，按照“构建和安装适用于 Linux、FreeBSD 或 NetBSD 的 Open vSwitch”中的说明。

   - If you want to preserve the contents of your database, back it up first, then use `ovsdb-tool convert` to upgrade it, e.g.:
     如果要保留数据库的内容，请先备份它，然后用于 `ovsdb-tool convert` 升级它，例如：

     ```
     $ ovsdb-tool convert /usr/local/etc/openvswitch/conf.db \
         vswitchd/vswitch.ovsschema
     ```

4. Start the Open vSwitch daemons as described under [Starting](https://docs.openvswitch.org/en/latest/intro/install/general/#starting) above.
   启动打开 vSwitch 守护程序，如上文“开始”中所述。

## Hot Upgrading[¶](https://docs.openvswitch.org/en/latest/intro/install/general/#hot-upgrading) 热升级 ¶

Upgrading Open vSwitch from one version to the next version with minimum disruption of traffic going through the system that is using that Open vSwitch needs some considerations:
将 Open vSwitch 从一个版本升级到下一个版本，同时最大限度地减少通过使用该 Open vSwitch 的系统的流量中断，需要注意一些事项：

1. If the upgrade only involves upgrading the userspace utilities and daemons of Open vSwitch, make sure that the new userspace version is compatible with the previously loaded kernel module.
   如果升级仅涉及升级 Open vSwitch 的用户空间实用程序和守护程序，请确保新的用户空间版本与之前加载的内核模块兼容。
2. An upgrade of userspace daemons means that they have to be restarted. Restarting the daemons means that the OpenFlow flows in the ovs-vswitchd daemon will be lost. One way to restore the flows is to let the controller re-populate it. Another way is to save the previous flows using a utility like ovs-ofctl and then re-add them after the restart. Restoring the old flows is accurate only if the new Open vSwitch interfaces retain the old ‘ofport’ values.
   用户空间守护程序的升级意味着必须重新启动它们。重新启动守护进程意味着 ovs-vswitchd 守护进程中的 OpenFlow 流将丢失。还原流的一种方法是让控制器重新填充它。另一种方法是使用像  ovs-ofctl 这样的实用程序保存以前的流，然后在重新启动后重新添加它们。仅当新的 Open vSwitch  接口保留旧的“ofport”值时，还原旧流才准确。
3. When the new userspace daemons get restarted, they automatically flush the old flows setup in the kernel. This can be expensive if there are hundreds of new flows that are entering the kernel but userspace daemons are busy setting up new userspace flows from either the controller or an utility like ovs-ofctl. Open vSwitch database provides an option to solve this problem through the `other_config:flow-restore-wait` column of the `Open_vSwitch` table. Refer to the ovs-vswitchd.conf.db(5) manpage for details.
   当新的用户空间守护进程重新启动时，它们会自动刷新内核中的旧流设置。如果有数百个新流进入内核，但用户空间守护进程正忙于从控制器或 ovs-ofctl 等实用程序设置新的用户空间流，这可能会很昂贵。Open vSwitch 数据库提供了一个选项，通过 `Open_vSwitch` 表 `other_config:flow-restore-wait` 的列来解决此问题。有关详细信息，请参见 ovs-vswitchd.conf.db（5） 手册页。
4. If the upgrade also involves upgrading the kernel module, the old kernel module needs to be unloaded and the new kernel module should be loaded. This means that the kernel network devices belonging to Open vSwitch is recreated and the kernel flows are lost. The downtime of the traffic can be reduced if the userspace daemons are restarted immediately and the userspace flows are restored as soon as possible.
   如果升级还涉及升级内核模块，则需要卸载旧内核模块并加载新内核模块。这意味着将重新创建属于 Open vSwitch 的内核网络设备，并且内核流将丢失。如果立即重新启动用户空间守护程序并尽快恢复用户空间流，则可以减少流量的停机时间。
5. When upgrading ovs running in container on host that is managed by ovn, simply stop the docker container, remove and re-run with new docker image that has newer ovs version.
   在由 ovn 管理的主机上升级在容器中运行的 ovs 时，只需停止 docker 容器，删除并使用具有较新 ovs 版本的新 docker 映像重新运行即可。
6. When running ovs in container, if ovs is used in bridged mode where management interface is managed by ovs, docker restart will result in loss of network connectivity. Hence, make sure to delete the bridge mapping of physical interface from ovs, upgrade ovs via docker and then add back the interface to ovs bridge. This mapping need not be deleted in case of multi nics if management interface is not managed by ovs.
   在容器中运行 ovs 时，如果在桥接模式下使用 ovs，管理接口由 ovs 管理，docker 重启将导致网络连接丢失。因此，请确保从 ovs  中删除物理接口的桥接映射，通过 docker 升级 ovs，然后将接口重新添加到 ovs 桥接。如果管理接口不是由 ovs  管理的，则在多网卡的情况下无需删除此映射。

The ovs-ctl utility’s `restart` function only restarts the userspace daemons, makes sure that the ‘ofport’ values remain consistent across restarts, restores userspace flows using the ovs-ofctl utility and also uses the `other_config:flow-restore-wait` column to keep the traffic downtime to the minimum. The ovs-ctl utility’s `force-reload-kmod` function does all of the above, but also replaces the old kernel module with the new one. Open vSwitch startup scripts for Debian and RHEL use ovs-ctl’s functions and it is recommended that these functions be used for other software platforms too.
ovs-ctl 实用程序 `restart` 的功能仅重新启动用户空间守护程序，确保“ofport”值在重新启动时保持一致，使用 ovs-ofctl 实用程序恢复用户空间流，并使用 `other_config:flow-restore-wait` 列将流量停机时间降至最低。ovs-ctl 实用程序 `force-reload-kmod` 的函数可以完成上述所有操作，但也会用新内核模块替换旧内核模块。Open vSwitch for Debian 和 RHEL 的启动脚本使用 ovs-ctl 的函数，建议将这些函数也用于其他软件平台。

### Open vSwitch on NetBSD[¶](https://docs.openvswitch.org/en/latest/intro/install/netbsd/#open-vswitch-on-netbsd) 在 NetBSD 上打开 vSwitch ¶

On NetBSD, you might want to install requirements from pkgsrc.  In that case, you need at least the following packages.
在 NetBSD 上，您可能希望从 pkgsrc 安装需求。在这种情况下，您至少需要以下软件包。

- automake 自动制作
- libtool-base libtool-base（库工具基地）
- gmake
- python37 蟒蛇37

Some components have additional requirements. Refer to [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/) for more information.
某些组件有其他要求。有关详细信息，请参阅在 Linux、FreeBSD 和 NetBSD 上打开 vSwitch。

Assuming you are running NetBSD/amd64 7.0.2, you can download and install pre-built binary packages as the following:
假设您运行的是 NetBSD/amd64 7.0.2，您可以下载并安装预构建的二进制软件包，如下所示：

```
$ PKG_PATH=http://ftp.netbsd.org/pub/pkgsrc/packages/NetBSD/amd64/7.0.2/All/
$ export PKG_PATH
$ pkg_add automake libtool-base gmake python37 pkg_alternatives
```

Note 注意

You might get some warnings about minor version mismatch. These can be safely ignored.
您可能会收到一些有关次要版本不匹配的警告。这些可以安全地忽略。

NetBSD’s `/usr/bin/make` is not GNU make.  GNU make is installed as `/usr/pkg/bin/gmake` by the above mentioned `gmake` package.
NetBSD `/usr/bin/make` 不是 GNU 制造的。GNU make 是按照 `/usr/pkg/bin/gmake` 上面提到的 `gmake` 软件包安装的。

As all executables installed with pkgsrc are placed in `/usr/pkg/bin/` directory, it might be a good idea to add it to your PATH. Or install OVS by `gmake` and `gmake install`.
由于所有随 pkgsrc 一起安装的可执行文件都放在目录中 `/usr/pkg/bin/` ，因此将其添加到您的 PATH 中可能是个好主意。或者通过 `gmake` 和 `gmake install` 安装 OVS。

Open vSwitch on NetBSD is currently “userspace switch” implementation in the sense described in [Open vSwitch without Kernel Support](https://docs.openvswitch.org/en/latest/intro/install/userspace/) and [Porting Open vSwitch to New Software or Hardware](https://docs.openvswitch.org/en/latest/topics/porting/).
NetBSD 上的 Open vSwitch 目前是“用户空间交换机”实现，在 Open vSwitch without Kernel Support 和  Porting Open vSwitch to New Software or Hardware 中进行了描述。

# Open vSwitch on Windows[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#open-vswitch-on-windows) 在 Windows 上打开 vSwitch ¶



## Build Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#build-requirements) 构建要求 ¶

Open vSwitch on Linux uses autoconf and automake for generating Makefiles.  It will be useful to maintain the same build system while compiling on Windows too.  One approach is to compile Open vSwitch in a MinGW environment that contains autoconf and automake utilities and then use Visual C++ as a compiler and linker.
Linux 上的 Open vSwitch 使用 autoconf 和 automake 来生成 Makefile。在 Windows  上编译时维护相同的构建系统也很有用。一种方法是在包含 autoconf 和 automake 实用程序的 MinGW 环境中编译 Open  vSwitch，然后使用 Visual C++ 作为编译器和链接器。

The following explains the steps in some detail.
下面将详细介绍这些步骤。

- Mingw 明婷

  Install Mingw on a Windows machine by following the instructions on [mingw.org](http://www.mingw.org/wiki/Getting_Started).
  按照 mingw.org 上的说明在 Windows 计算机上安装 Mingw。

  This should install mingw at `C:\Mingw` and msys at `C:\Mingw\msys`.  Add `C:\MinGW\bin` and `C:\Mingw\msys\1.0\bin` to PATH environment variable of Windows.
  这应该安装 mingw at `C:\Mingw` 和 msys `C:\Mingw\msys` at 。将 和 `C:\Mingw\msys\1.0\bin` 添加到 `C:\MinGW\bin` Windows 的 PATH 环境变量。

  You can either use the MinGW installer or the command line utility `mingw-get` to install both the base packages and additional packages like automake and autoconf(version 2.68).
  您可以使用 MinGW 安装程序或命令行实用程序 `mingw-get` 来安装基本软件包和其他软件包，如 automake 和 autoconf（版本 2.68）。

  Also make sure that `/mingw` mount point exists. If its not, please add/create the following entry in `/etc/fstab`:
  此外， `/mingw` 请确保安装点存在。如果不是，请在以下 `/etc/fstab` 位置添加/创建以下条目：

  ```
  'C:/MinGW /mingw'.
  ```

- Python 3.6 or later.
  Python 3.6 或更高版本。

  Install the latest Python 3.x from python.org and verify that its path is part of Windows’ PATH environment variable. We require that you have pypiwin32 library installed. The library can be installed via pip command:
  从 python.org 安装最新的 Python 3.x，并验证其路径是否是 Windows 的 PATH 环境变量的一部分。我们要求您安装 pypiwin32 库。该库可以通过 pip 命令安装：

  > ```
  > $ pip install pypiwin32
  > ```

- Visual Studio Visual Studio的

  You will need at least Visual Studio 2013 (update 4) to compile userspace binaries.  In addition to that, if you want to compile the kernel module you will also need to install Windows Driver Kit (WDK) 8.1 Update or later. To generate the Windows installer you need [WiX Toolset](https://wixtoolset.org/) and also be able to build the kernel module.
  至少需要 Visual Studio 2013（更新 4）才能编译用户空间二进制文件。除此之外，如果要编译内核模块，还需要安装 Windows  驱动程序工具包 （WDK） 8.1 更新或更高版本。要生成 Windows 安装程序，您需要 WiX 工具集，并且还能够构建内核模块。

  We recommend using the latest Visual Studio version together with the latest WDK installed.
  建议将最新的 Visual Studio 版本与安装的最新 WDK 一起使用。

  It is important to get the Visual Studio related environment variables and to have the $PATH inside the bash to point to the proper compiler and linker. One easy way to achieve this for VS2013 is to get into the “VS2013 x86 Native Tools Command Prompt” (in a default installation of Visual Studio 2013 this can be found under the following location: `C:\Program Files (x86)\Microsoft Visual Studio 12.0\Common7\Tools\Shortcuts`) and through it enter into the bash shell available from msys by typing `bash --login`.
  获取与 Visual Studio 相关的环境变量，并在 bash 中$PATH指向正确的编译器和链接器非常重要。在 VS2013  中实现此目的的一种简单方法是进入“VS2013 x86 本机工具命令提示符”（在 Visual Studio 2013  的默认安装中，可以在以下位置找到： `C:\Program Files (x86)\Microsoft Visual Studio 12.0\Common7\Tools\Shortcuts` ），然后通过它通过键入 `bash --login` 进入 msys 提供的 bash shell。

  There is support for generating 64 bit binaries too.  To compile under x64, open the “VS2013 x64 Native Tools Command Prompt” (if your current running OS is 64 bit) or “VS2013 x64 Cross Tools Command Prompt” (if your current running OS is not 64 bit) instead of opening its x86 variant.  This will point the compiler and the linker to their 64 bit equivalent.
  还支持生成 64 位二进制文件。若要在 x64 下编译，请打开“VS2013 x64 本机工具命令提示符”（如果当前运行的操作系统为 64  位）或“VS2013 x64 跨工具命令提示符”（如果当前运行的操作系统不是 64 位），而不是打开其 x86  变体。这会将编译器和链接器指向它们的 64 位等效项。

  If after the above step, a `which link` inside MSYS’s bash says, `/bin/link.exe`, rename `/bin/link.exe` to something else so that the Visual studio’s linker is used. You should also see a ‘which sort’ report `/bin/sort.exe`.
  如果在上述步骤之后，MSYS `which link` 的 bash 内部会说 ， `/bin/link.exe` 请重命名为 `/bin/link.exe` 其他名称，以便使用 Visual Studio 的链接器。您还应该看到“哪种”报告 `/bin/sort.exe` 。

- PThreads4W

  For pthread support, install the library, dll and includes of PThreads4W project from [sourceware](https://sourceforge.net/projects/pthreads4w/) to a directory (e.g.: `C:/pthread`). You should add the PThreads4W’s dll path (e.g.: `C:\pthread\bin`) to the Windows’ PATH environment variable.
  对于 pthread 支持，请将 PThreads4W 项目的库、dll 和 includes 从源软件安装到目录（例如：）。 `C:/pthread` 您应该将 PThreads4W 的 dll 路径（例如： `C:\pthread\bin` ）添加到 Windows 的 PATH 环境变量中。

- OpenSSL OpenSSL的

  To get SSL support for Open vSwitch on Windows, you will need to install [OpenSSL for Windows](https://wiki.openssl.org/index.php/Binaries)
  要在 Windows 上获得对 Open vSwitch 的 SSL 支持，您需要安装适用于 Windows 的 OpenSSL

  Note down the directory where OpenSSL is installed (e.g.: `C:/OpenSSL-Win64`) for later use.
  记下安装 OpenSSL 的目录（例如： `C:/OpenSSL-Win64` ），以备后用。

Note 注意

Commands prefixed by `$` must be run in the Bash shell provided by MinGW. Open vSwitch commands, such as `ovs-dpctl` are shown running under the DOS shell (`cmd.exe`), as indicated by the `>` prefix, but will also run under Bash. The remainder, prefixed by `>`, are PowerShell commands and must be run in PowerShell.
以 为 `$` 前缀的命令必须在 MinGW 提供的 Bash shell 中运行。打开 vSwitch 命令，例如 `ovs-dpctl` 在 DOS shell （ `cmd.exe` ） 下运行，如 `>` 前缀所示，但也将在 Bash 下运行。其余部分（以 `>` 前缀）是 PowerShell 命令，必须在 PowerShell 中运行。

## Install Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#install-requirements) 安装要求 ¶

- Share network adaptors 共享网络适配器

  We require that you don’t disable the “Allow management operating system to share this network adapter” under ‘Virtual Switch Properties’ > ‘Connection type: External network’, in the Hyper-V virtual network switch configuration.
  我们要求您不要在 Hyper-V 虚拟网络交换机配置中禁用“虚拟交换机属性”>“连接类型：外部网络”下的“允许管理操作系统共享此网络适配器”。

- Checksum Offloads 校验和卸载

  While there is some support for checksum/segmentation offloads in software, this is still a work in progress. Till the support is complete we recommend disabling TX/RX offloads for both the VM’s as well as the Hyper-V.
  虽然软件中有一些对校验和/分段卸载的支持，但这仍然是一项正在进行的工作。在支持完成之前，我们建议禁用 VM 和 Hyper-V 的 TX/RX 卸载。

## Bootstrapping[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#bootstrapping) 引导 ¶

This step is not needed if you have downloaded a released tarball. If you pulled the sources directly from an Open vSwitch Git tree or got a Git tree snapshot, then run boot.sh in the top source directory to build the “configure” script:
如果您已下载已发布的压缩包，则不需要此步骤。如果您直接从 Open vSwitch Git 树中提取源代码或获取 Git 树快照，请在顶部源目录中运行 boot.sh 以构建“configure”脚本：

```
$ ./boot.sh
```



## Configuring[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#configuring) 配置 ¶

Configure the package by running the configure script.  You should provide some configure options to choose the right compiler, linker, libraries, Open vSwitch component installation directories, etc. For example:
通过运行配置脚本来配置包。您应该提供一些配置选项来选择正确的编译器、链接器、库、Open vSwitch 组件安装目录等。例如：

```
$ ./configure CC=./build-aux/cccl LD="$(which link)" \
    LIBS="-lws2_32 -lShlwapi -liphlpapi -lwbemuuid -lole32 -loleaut32" \
    --prefix="C:/openvswitch/usr" \
    --localstatedir="C:/openvswitch/var" \
    --sysconfdir="C:/openvswitch/etc" \
    --with-pthread="C:/pthread"
```

Note 注意

By default, the above enables compiler optimization for fast code.  For default compiler optimization, pass the `--with-debug` configure option.
默认情况下，上述方法为快速代码启用编译器优化。对于默认编译器优化，请传递 `--with-debug` configure 选项。

To configure with SSL support, add the requisite additional options:
要使用 SSL 支持进行配置，请添加必要的附加选项：

```
$ ./configure CC=./build-aux/cccl LD="`which link`"  \
    LIBS="-lws2_32 -lShlwapi -liphlpapi -lwbemuuid -lole32 -loleaut32" \
    --prefix="C:/openvswitch/usr" \
    --localstatedir="C:/openvswitch/var"
    --sysconfdir="C:/openvswitch/etc" \
    --with-pthread="C:/pthread" \
    --enable-ssl --with-openssl="C:/OpenSSL-Win64"
```

Finally, to the kernel module also:
最后，对内核模块也：

```
$ ./configure CC=./build-aux/cccl LD="`which link`" \
    LIBS="-lws2_32 -lShlwapi -liphlpapi -lwbemuuid -lole32 -loleaut32" \
    --prefix="C:/openvswitch/usr" \
    --localstatedir="C:/openvswitch/var" \
    --sysconfdir="C:/openvswitch/etc" \
    --with-pthread="C:/pthread" \
    --enable-ssl --with-openssl="C:/OpenSSL-Win64" \
    --with-vstudiotarget="<target type>" \
    --with-vstudiotargetver="<target versions>"
```

Possible values for `<target type>` are: `Debug` and `Release` Possible values for `<target versions>` is a comma separated list of target versions to compile among: `Win8,Win8.1,Win10`
可能的 `<target type>` 值是： `Debug` 和 `Release` 可能的值是 `<target versions>` 要在以下之间编译的目标版本的逗号分隔列表： `Win8,Win8.1,Win10` 

Note 注意

You can directly use the Visual Studio 2013 IDE to compile the kernel datapath. Open the ovsext.sln file in the IDE and build the solution.
可以直接使用 Visual Studio 2013 IDE 编译内核数据路径。在 IDE 中打开ovsext.sln文件并生成解决方案。

Refer to [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/) for information on additional configuration options.
有关其他配置选项的信息，请参阅在 Linux、FreeBSD 和 NetBSD 上打开 vSwitch。



## Building[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#building) 构建 ¶

Once correctly configured, building Open vSwitch on Windows is similar to building on Linux, FreeBSD, or NetBSD.
正确配置后，在 Windows 上构建 Open vSwitch 类似于在 Linux、FreeBSD 或 NetBSD 上构建。

1. Run make for the ported executables in the top source directory, e.g.:
   对顶部源目录中的移植可执行文件运行 make，例如：

   ```
   $ make
   ```

   For faster compilation, you can pass the `-j` argument to make.  For example, to run 4 jobs simultaneously, run `make -j4`.
   为了加快编译速度，可以传递 `-j` 参数 make。例如，若要同时运行 4 个作业，请运行 `make -j4` .

   Note 注意

   MSYS 1.0.18 has a bug that causes parallel make to hang. You can overcome this by downgrading to MSYS 1.0.17.  A simple way to downgrade is to exit all MinGW sessions and then run the below command from MSVC developers command prompt.:
   MSYS 1.0.18 有一个 bug，会导致 parallel make 挂起。您可以通过降级到 MSYS 1.0.17 来克服此问题。降级的简单方法是退出所有 MinGW 会话，然后从 MSVC 开发人员命令提示符运行以下命令。

   ```
   > mingw-get upgrade msys-core-bin=1.0.17-1
   ```

2. To run all the unit tests in Open vSwitch, one at a time:
   要在 Open vSwitch 中运行所有单元测试，一次运行一个：

   ```
   $ make check
   ```

   To run all the unit tests in Open vSwitch, up to 8 in parallel:
   要在 Open vSwitch 中并行运行所有单元测试，最多可运行 8 个单元测试：

   ```
   $ make check TESTSUITEFLAGS="-j8"
   ```

3. To install all the compiled executables on the local machine, run:
   若要在本地计算机上安装所有已编译的可执行文件，请运行：

   ```
   $ make install
   ```

> Note 注意
>
> This will install the Open vSwitch executables in `C:/openvswitch`.  You can add `C:\openvswitch\usr\bin` and `C:\openvswitch\usr\sbin` to Windows’ PATH environment variable for easy access.
> 这将在 `C:/openvswitch` 中安装 Open vSwitch 可执行文件。您可以向 Windows 的 PATH 环境变量添加 `C:\openvswitch\usr\bin` 和 `C:\openvswitch\usr\sbin` 以便于访问。

### The Kernel Module[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#the-kernel-module) 内核模块 ¶

If you are building the kernel module, you will need to copy the below files to the target Hyper-V machine.
如果要生成内核模块，则需要将以下文件复制到目标 Hyper-V 计算机。

- `./datapath-windows/x64/Win8.1Debug/package/ovsext.inf`
- `./datapath-windows/x64/Win8.1Debug/package/OVSExt.sys`
- `./datapath-windows/x64/Win8.1Debug/package/ovsext.cat`
- `./datapath-windows/misc/install.cmd`
- `./datapath-windows/misc/uninstall.cmd`

Note 注意

The above path assumes that the kernel module has been built using Windows DDK 8.1 in Debug mode. Change the path appropriately, if a different WDK has been used.
上述路径假定内核模块是在调试模式下使用 Windows DDK 8.1 构建的。如果使用了其他 WDK，请相应更改路径。

Now run `./uninstall.cmd` to remove the old extension. Once complete, run `./install.cmd` to insert the new one.  For this to work you will have to turn on `TESTSIGNING` boot option or ‘Disable Driver Signature Enforcement’ during boot.  The following commands can be used:
现在运行 `./uninstall.cmd` 以删除旧扩展。完成后，运行 `./install.cmd` 以插入新的。为此，您必须在启动过程中打开 `TESTSIGNING` 启动选项或“禁用驱动程序签名强制执行”。可以使用以下命令：

```
> bcdedit /set LOADOPTIONS DISABLE_INTEGRITY_CHECKS
> bcdedit /set TESTSIGNING ON
> bcdedit /set nointegritychecks ON
```

Note 注意

You may have to restart the machine for the settings to take effect.
您可能需要重新启动计算机才能使设置生效。

In the Virtual Switch Manager configuration you can enable the Open vSwitch Extension on an existing switch or create a new switch.  If you are using an existing switch, make sure to enable the “Allow Management OS” option for VXLAN to work (covered later).
在虚拟交换机管理器配置中，您可以在现有交换机上启用 Open vSwitch 扩展或创建新交换机。如果您使用的是现有交换机，请确保启用“允许管理操作系统”选项，以便 VXLAN 正常工作（稍后介绍）。

The command to create a new switch named ‘OVS-Extended-Switch’ using a physical NIC named ‘Ethernet 1’ is:
使用名为“以太网 1”的物理网卡创建名为“OVS-Extended-Switch”的新交换机的命令为：

```
PS > New-VMSwitch "OVS-Extended-Switch" -NetAdapterName "Ethernet 1"
```

Note 注意

You can obtain the list of physical NICs on the host using ‘Get-NetAdapter’ command.
您可以使用“Get-NetAdapter”命令获取主机上的物理网卡列表。

In the properties of any switch, you should now see “Open vSwitch Extension” under ‘Extensions’.  Click the check box to enable the extension. An alternative way to do the same is to run the following command:
在任何交换机的属性中，您现在应该在“扩展”下看到“打开 vSwitch 扩展”。单击该复选框以启用扩展。执行相同操作的另一种方法是运行以下命令：

```
PS > Enable-VMSwitchExtension "Open vSwitch Extension" OVS-Extended-Switch
```

Note 注意

If you enabled the extension using the command line, a delay of a few seconds has been observed for the change to be reflected in the UI.  This is not a bug in Open vSwitch.
如果使用命令行启用扩展，则会观察到几秒钟的延迟，更改才会反映在 UI 中。这不是 Open vSwitch 中的错误。

### Generate the Windows installer[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#generate-the-windows-installer) 生成 Windows 安装程序 ¶

To generate the Windows installler run the following command from the top source directory:
若要生成 Windows 安装程序，请从顶部源目录运行以下命令：

```
$ make windows_installer
```

Note 注意

This will generate the Windows installer in the following location (relative to the top source directory): windows/ovs-windows-installer/bin/Release/OpenvSwitch.msi
这将在以下位置（相对于顶部源目录）生成 Windows 安装程序：windows/ovs-windows-installer/bin/Release/OpenvSwitch.msi

## Starting[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#starting) 开始¶

Important 重要

The following steps assume that you have installed the Open vSwitch utilities in the local machine via ‘make install’.
以下步骤假定您已通过“make install”在本地计算机中安装了 Open vSwitch 实用程序。

Before starting ovs-vswitchd itself, you need to start its configuration database, ovsdb-server. Each machine on which Open vSwitch is installed should run its own copy of ovsdb-server. Before ovsdb-server itself can be started, configure a database that it can use:
在启动 ovs-vswitchd 本身之前，您需要启动其配置数据库 ovsdb-server。安装了 Open vSwitch  的每台计算机都应运行自己的 ovsdb-server 副本。在启动 ovsdb-server 本身之前，请配置一个可以使用的数据库：

```
> ovsdb-tool create C:\openvswitch\etc\openvswitch\conf.db \
    C:\openvswitch\usr\share\openvswitch\vswitch.ovsschema
```

Configure ovsdb-server to use database created above and to listen on a Unix domain socket:
将 ovsdb-server 配置为使用上面创建的数据库并侦听 Unix 域套接字：

```
> ovsdb-server -vfile:info --remote=punix:db.sock --log-file \
    --pidfile --detach
```

Note 注意

The logfile is created at `C:/openvswitch/var/log/openvswitch/`
日志文件创建于 `C:/openvswitch/var/log/openvswitch/` 

Initialize the database using ovs-vsctl. This is only necessary the first time after you create the database with ovsdb-tool, though running it at any time is harmless:
使用 ovs-vsctl 初始化数据库。这仅在使用 ovsdb-tool 创建数据库后第一次需要，尽管随时运行它是无害的：

```
> ovs-vsctl --no-wait init
```

Tip

If you would later like to terminate the started ovsdb-server, run:
如果以后要终止已启动的 ovsdb-server，请运行：

```
> ovs-appctl -t ovsdb-server exit
```

Start the main Open vSwitch daemon, telling it to connect to the same Unix domain socket:
启动主 Open vSwitch 守护程序，告诉它连接到同一个 Unix 域套接字：

```
> ovs-vswitchd -vfile:info --log-file --pidfile --detach
```

Tip

If you would like to terminate the started ovs-vswitchd, run:
如果要终止已启动的 ovs-vswitchd，请运行：

```
> ovs-appctl exit
```

Note 注意

The logfile is created at `C:/openvswitch/var/log/openvswitch/`
日志文件创建于 `C:/openvswitch/var/log/openvswitch/` 

## Validating[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#validating) 验证 ¶

At this point you can use ovs-vsctl to set up bridges and other Open vSwitch features.
此时，您可以使用 ovs-vsctl 设置网桥和其他 Open vSwitch 功能。

### Add bridges[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-bridges) 添加桥接 ¶

Let’s start by creating an integration bridge, `br-int` and a PIF bridge, `br-pif`:
让我们从创建一个集成桥 `br-int` 接和一个 PIF 桥接开始： `br-pif` 

```
> ovs-vsctl add-br br-int
> ovs-vsctl add-br br-pif
```

Note 注意

There’s a known bug that running the ovs-vsctl command does not terminate. This is generally solved by having ovs-vswitchd running.  If you face the issue despite that, hit Ctrl-C to terminate ovs-vsctl and check the output to see if your command succeeded.
运行 ovs-vsctl 命令不会终止存在一个已知错误。这通常通过运行 ovs-vswitchd 来解决。如果您仍然遇到问题，请按 Ctrl-C 终止 ovs-vsctl 并检查输出以查看您的命令是否成功。

Validate that ports are added by dumping from both ovs-dpctl and ovs-vsctl:
验证是否通过从 ovs-dpctl 和 ovs-vsctl 转储来添加端口：

```
> ovs-dpctl show
system@ovs-system:
        lookups: hit:0 missed:0 lost:0
        flows: 0
        port 2: br-pif (internal)     <<< internal port on 'br-pif' bridge
        port 1: br-int (internal)     <<< internal port on 'br-int' bridge

> ovs-vsctl show
a56ec7b5-5b1f-49ec-a795-79f6eb63228b
    Bridge br-pif
        Port br-pif
            Interface br-pif
                type: internal
    Bridge br-int
        Port br-int
            Interface br-int
                type: internal
```

Note 注意

There’s a known bug that the ports added to OVSDB via ovs-vsctl don’t get to the kernel datapath immediately, ie. they don’t show up in the output of `ovs-dpctl show` even though they show up in output of `ovs-vsctl show`. In order to workaround this issue, restart ovs-vswitchd. (You can terminate ovs-vswitchd by running `ovs-appctl exit`.)
有一个已知的错误，即通过 ovs-vsctl 添加到 OVSDB 的端口不会立即到达内核数据路径，即。即使它们出现在 的 `ovs-vsctl show` 输出中，它们也不会出现在 的 `ovs-dpctl show` 输出中。要解决此问题，请重新启动 ovs-vswitchd。（您可以通过运行 `ovs-appctl exit` 来终止 ovs-vswitchd 。

### Add physicals NICs (PIF)[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-physicals-nics-pif) 添加物理网卡 （PIF） ¶

Now, let’s add the physical NIC and the internal port to `br-pif`. In OVS for Hyper-V, we use the name of the adapter on top of which the Hyper-V virtual switch was created, as a special name to refer to the physical NICs connected to the Hyper-V switch, e.g. if we created the Hyper-V virtual switch on top of the adapter named `Ethernet0`, then in OVS we use that name (`Ethernet0`) as a special name to refer to that adapter.
现在，让我们将物理 NIC 和内部端口添加到 `br-pif` .在 OVS for Hyper-V 中，我们使用在其上创建 Hyper-V 虚拟交换机的适配器的名称作为特殊名称来引用连接到 Hyper-V 交换机的物理 NIC，例如，如果我们在名为 `Ethernet0` 的适配器上创建了 Hyper-V 虚拟交换机，则在 OVS 中，我们使用该名称 （ `Ethernet0` ） 作为特殊名称来引用该适配器。

Note 注意

We assume that the OVS extension is enabled Hyper-V switch.
我们假设 OVS 扩展已启用 Hyper-V 交换机。

Internal ports are the virtual adapters created on the Hyper-V switch using the `ovs-vsctl add-br <bridge>` command. By default they are created under the following rule “<name of bridge>” and the adapters are disabled. One needs to enable them and set the corresponding values to it to make them IP-able.
内部端口是使用命令 `ovs-vsctl add-br <bridge>` 在 Hyper-V 交换机上创建的虚拟适配器。默认情况下，它们是在以下规则“<网桥名称>下创建的，并且适配器处于禁用状态。需要启用它们并为其设置相应的值以使其具有 IP 功能。

As a whole example, if we issue the following in a powershell console:
作为整个示例，如果我们在 powershell 控制台中发出以下命令：

```
PS > Get-NetAdapter | select Name,InterfaceDescription
Name                   InterfaceDescription
----                   --------------------
Ethernet1              Intel(R) PRO/1000 MT Network Connection
br-pif                 Hyper-V Virtual Ethernet Adapter #2
Ethernet0              Intel(R) PRO/1000 MT Network Connection #2
br-int                 Hyper-V Virtual Ethernet Adapter #3

PS > Get-VMSwitch
Name     SwitchType NetAdapterInterfaceDescription
----     ---------- ------------------------------
external External   Intel(R) PRO/1000 MT Network Connection #2
```

We can see that we have a switch(external) created upon adapter name ‘Ethernet0’ with the internal ports under name ‘br-pif’ and ‘br-int’. Thus resulting into the following ovs-vsctl commands:
我们可以看到，我们在适配器名称“Ethernet0”上创建了一个交换机（外部），其内部端口名为“br-pif”和“br-int”。因此，会产生以下 ovs-vsctl 命令：

```
> ovs-vsctl add-port br-pif Ethernet0
```

Dumping the ports should show the additional ports that were just added:
转储端口应显示刚刚添加的其他端口：

```
> ovs-dpctl show
system@ovs-system:
        lookups: hit:0 missed:0 lost:0
        flows: 0
        port 2: br-pif (internal)               <<< internal port
                                                    adapter on
                                                    Hyper-V switch
        port 1: br-int (internal)               <<< internal port
                                                    adapter on
                                                    Hyper-V switch
        port 3: Ethernet0                       <<< Physical NIC

> ovs-vsctl show
a56ec7b5-5b1f-49ec-a795-79f6eb63228b
    Bridge br-pif
        Port br-pif
            Interface br-pif
                type: internal
        Port "Ethernet0"
            Interface "Ethernet0"
    Bridge br-int
        Port br-int
            Interface br-int
                type: internal
```

### Add virtual interfaces (VIFs)[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-virtual-interfaces-vifs) 添加虚拟接口 （VIF） ¶

Adding VIFs to Open vSwitch is a two step procedure.  The first step is to assign a ‘OVS port name’ which is a unique name across all VIFs on this Hyper-V.  The next step is to add the VIF to the ovsdb using its ‘OVS port name’ as key.
将 VIF 添加到 Open vSwitch 的过程分为两步。第一步是分配一个“OVS 端口名称”，这是此 Hyper-V 上所有 VIF 的唯一名称。下一步是使用其“OVS 端口名称”作为密钥将 VIF 添加到 ovsdb。

First, assign a unique ‘OVS port name’ to the VIF. The VIF needs to have been disconnected from the Hyper-V switch before assigning a ‘OVS port name’ to it. In the example below, we assign a ‘OVS port name’ called `ovs-port-a` to a VIF on a VM `VM1`.  By using index 0 for `$vnic`, the first VIF of the VM is being addressed.  After assigning the name `ovs-port-a`, the VIF is connected back to the Hyper-V switch with name `OVS-HV-Switch`, which is assumed to be the Hyper-V switch with OVS extension enabled.:
首先，为 VIF 分配一个唯一的“OVS 端口名称”。在为VIF分配“OVS端口名称”之前，需要已断开与Hyper-V交换机的连接。在下面的示例中，我们将调用 `ovs-port-a` 的“OVS 端口名称”分配给 VM 上的 `VM1` VIF。通过对 `$vnic` 使用索引 0 for ，正在对 VM 的第一个 VIF 进行寻址。分配名称 `ovs-port-a` 后，VIF 将连接回名称为 `OVS-HV-Switch` 的 Hyper-V 交换机，该交换机假定为启用了 OVS 扩展的 Hyper-V 交换机。

```
PS > import-module .\datapath-windows\misc\OVS.psm1
PS > $vnic = Get-VMNetworkAdapter <Name of the VM>
PS > Disconnect-VMNetworkAdapter -VMNetworkAdapter $vnic[0]
PS > $vnic[0] | Set-VMNetworkAdapterOVSPort -OVSPortName ovs-port-a
PS > Connect-VMNetworkAdapter -VMNetworkAdapter $vnic[0] \
      -SwitchName OVS-Extended-Switch
```

Next, add the VIFs to `br-int`:
接下来，将 VIF 添加到 `br-int` ：

```
> ovs-vsctl add-port br-int ovs-port-a
```

Dumping the ports should show the additional ports that were just added:
转储端口应显示刚刚添加的其他端口：

```
> ovs-dpctl show
system@ovs-system:
        lookups: hit:0 missed:0 lost:0
        flows: 0
        port 4: ovs-port-a
        port 2: br-pif (internal)
        port 1: br-int (internal
        port 3: Ethernet0

> ovs-vsctl show
4cd86499-74df-48bd-a64d-8d115b12a9f2
    Bridge br-pif
        Port "vEthernet (external)"
            Interface "vEthernet (external)"
        Port "Ethernet0"
            Interface "Ethernet0"
        Port br-pif
            Interface br-pif
                type: internal
    Bridge br-int
        Port br-int
            Interface br-int
                type: internal
        Port "ovs-port-a"
            Interface "ovs-port-a"
```

### Add multiple NICs to be managed by OVS[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-multiple-nics-to-be-managed-by-ovs) 添加多个网卡由OVS管理 ¶

To leverage support of multiple NICs into OVS, we will be using the MSFT cmdlets for forwarding team extension. More documentation about them can be found at [technet](https://technet.microsoft.com/en-us/library/jj553812(v=wps.630).aspx).
为了利用对 OVS 的多个 NIC 的支持，我们将使用 MSFT cmdlet 转发团队扩展。有关它们的更多文档，请访问 technet。

For example, to set up a switch team combined from `Ethernet0 2` and `Ethernet1 2` named `external`:
例如，要设置一个组合自 `Ethernet0 2` 并 `Ethernet1 2` 命名为 `external` ：

```
PS > Get-NetAdapter
Name                      InterfaceDescription
----                      --------------------
br-int                    Hyper-V Virtual Ethernet Adapter #3
br-pif                    Hyper-V Virtual Ethernet Adapter #2
Ethernet3 2               Intel(R) 82574L Gigabit Network Co...#3
Ethernet2 2               Intel(R) 82574L Gigabit Network Co...#4
Ethernet1 2               Intel(R) 82574L Gigabit Network Co...#2
Ethernet0 2               Intel(R) 82574L Gigabit Network Conn...

PS > New-NetSwitchTeam -Name external -TeamMembers "Ethernet0 2","Ethernet1 2"

PS > Get-NetSwitchTeam
Name    : external
Members : {Ethernet1 2, Ethernet0 2}
```

This will result in a new adapter bound to the host called `external`:
这将导致绑定到主机的新适配器，称为 `external` ：

```
PS > Get-NetAdapter
Name                      InterfaceDescription
----                      --------------------
br-test                   Hyper-V Virtual Ethernet Adapter #4
br-pif                    Hyper-V Virtual Ethernet Adapter #2
external                  Microsoft Network Adapter Multiplexo...
Ethernet3 2               Intel(R) 82574L Gigabit Network Co...#3
Ethernet2 2               Intel(R) 82574L Gigabit Network Co...#4
Ethernet1 2               Intel(R) 82574L Gigabit Network Co...#2
Ethernet0 2               Intel(R) 82574L Gigabit Network Conn...
```

Next we will set up the Hyper-V VMSwitch on the new adapter `external`:
接下来，我们将在新适配器上设置 Hyper-V VMSwitch `external` ：

```
PS > New-VMSwitch -Name external -NetAdapterName external \
     -AllowManagementOS $false
```

Under OVS the adapters under the team `external`, `Ethernet0 2` and `Ethernet1 2`, can be added either under a bond device or separately.
在 OVS 下，组 `external` 和 `Ethernet0 2` `Ethernet1 2` 下的适配器可以添加到绑定设备下或单独添加。

The following example shows how the bridges look with the NICs being separated:
以下示例显示了网桥在网卡分离后的外观：

```
> ovs-vsctl show
6cd9481b-c249-4ee3-8692-97b399dd29d8
    Bridge br-test
        Port br-test
            Interface br-test
                type: internal
        Port "Ethernet1 2"
            Interface "Ethernet1 2"
    Bridge br-pif
        Port "Ethernet0 2"
            Interface "Ethernet0 2"
        Port br-pif
            Interface br-pif
                type: internal
```

### Add patch ports and configure VLAN tagging[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-patch-ports-and-configure-vlan-tagging) 添加补丁端口并配置 VLAN 标记 ¶

The Windows Open vSwitch implementation support VLAN tagging in the switch. Switch VLAN tagging along with patch ports between `br-int` and `br-pif` is used to configure VLAN tagging functionality between two VMs on different Hyper-Vs.  To start, add a patch port from `br-int` to `br-pif`:
Windows Open vSwitch 实现支持交换机中的 VLAN 标记。在 `br-int` 不同 Hyper-V 上的两个 VM 之间切换 VLAN 标记以及修补程序端口，用于 `br-pif` 配置 VLAN 标记功能。首先，将补丁端口从 `br-int` ： `br-pif` 

```
> ovs-vsctl add-port br-int patch-to-pif
> ovs-vsctl set interface patch-to-pif type=patch \
    options:peer=patch-to-int
```

Add a patch port from `br-pif` to `br-int`:
添加一个补丁 `br-pif` 端口： `br-int` 

```
> ovs-vsctl add-port br-pif patch-to-int
> ovs-vsctl set interface patch-to-int type=patch \
    options:peer=patch-to-pif
```

Re-Add the VIF ports with the VLAN tag:
重新添加带有 VLAN 标记的 VIF 端口：

```
> ovs-vsctl add-port br-int ovs-port-a tag=900
> ovs-vsctl add-port br-int ovs-port-b tag=900
```

### Add tunnels[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-tunnels) 添加隧道 ¶

1. IPv4 tunnel, e.g.: IPv4 隧道，例如：

   The Windows Open vSwitch implementation support VXLAN and STT tunnels. To add tunnels. For example, first add the tunnel port between 172.168.201.101 <->172.168.201.102:
   Windows Open vSwitch 实现支持 VXLAN 和 STT 隧道。添加隧道。例如，首先在 172.168.201.101 <->172.168.201.102 之间添加隧道端口：

   ```
     > ovs-vsctl add-port br-int tun-1
     > ovs-vsctl set Interface tun-1 type=<port-type>
     > ovs-vsctl set Interface tun-1 options:local_ip=172.168.201.101
     > ovs-vsctl set Interface tun-1 options:remote_ip=172.168.201.102
     > ovs-vsctl set Interface tun-1 options:in_key=flow
     > ovs-vsctl set Interface tun-1 options:out_key=flow
   
   ...and the tunnel port between 172.168.201.101 <-> 172.168.201.105:
   ```

   ```
   > ovs-vsctl add-port br-int tun-2
   > ovs-vsctl set Interface tun-2 type=<port-type>
   > ovs-vsctl set Interface tun-2 options:local_ip=172.168.201.102
   > ovs-vsctl set Interface tun-2 options:remote_ip=172.168.201.105
   > ovs-vsctl set Interface tun-2 options:in_key=flow
   > ovs-vsctl set Interface tun-2 options:out_key=flow
   
   Where ``<port-type>`` is one of: ``stt`` or ``vxlan``
   ```

   Note 注意

   Any patch ports created between br-int and br-pif MUST be deleted prior to adding tunnels.
   在添加隧道之前，必须删除在 br-int 和 br-pif 之间创建的任何补丁端口。

2. IPv6 tunnel, e.g.: IPv6 隧道，例如：

   To add IPV6 Geneve tunnels. For example, add the tunnel port between 5000::2 <-> 5000::9.
   添加 IPV6 Geneve 隧道。例如，在 5000：：2 <-> 5000：：9 之间添加隧道端口。

   ```
    > ovs-vsctl add-port br-int tun-3 -- set interface tun-3 type=Geneve \
      options:csum=true options:key=flow options:local_ip="5000::2"\
      options:remote_ip=flow
   
   add the tunnel port between 5000::2 <-> 5000::9
   
    > ovs-ofctl add-flow br-int "table=0,priority=100,ipv6,ipv6_src=6000::2 \
      actions=load:0x9->NXM_NX_TUN_IPV6_DST[0..63], \
      load:0x5000000000000000->NXM_NX_TUN_IPV6_DST[64..127], output:tun-3"
   
   add the specified flow from 6000::2 go via IPV6 Geneve tunnel
   ```

   Note 注意

   Till the checksum offload support is complete we recommend disabling TX/RX offloads for IPV6 on Windows VM.
   在校验和卸载支持完成之前，我们建议在 Windows VM 上禁用 IPV6 的 TX/RX 卸载。

### Add conntrack for ipv6[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#add-conntrack-for-ipv6) 为 ipv6 添加 conntrack ¶

The Windows Open vSwitch implementation support conntrack ipv6. To use the conntrack ipv6. Using the following commands. Take tcp6(replace Protocol to icmp6, udp6 to other protocol) for example.
Windows Open vSwitch 实现支持 conntrack ipv6。要使用 conntrack ipv6。使用以下命令。以 tcp6（将 Protocol 替换为 icmp6，将 udp6 替换为其他协议）为例。

```
normal scenario
Vif38(20::1, ofport:2)->Vif40(20:2, ofport:3)
Vif38Name="podvif38"
Vif40Name="podvif40"
Vif38Port=2
Vif38Address="20::1"
Vif40Port=3
Vif40Address="20::2"
Vif40MacAddressCli="00-15-5D-F0-01-0C"
Vif38MacAddressCli="00-15-5D-F0-01-0b"
Protocol="tcp6"
> netsh int ipv6 set neighbors $Vif38Name $Vif40Address \
  Vif40MacAddressCli
> netsh int ipv6 set neighbors $Vif40Name $Vif38Address \
  $Vif38MacAddressCli
> ovs-ofctl del-flows --strict br-int "table=0,priority=0"
> ovs-ofctl add-flow br-int "table=0,priority=1,ip6, \
  ipv6_dst=$Vif40Address,$Protocol,actions=ct(table=1)"
> ovs-ofctl add-flow br-int "table=0,priority=1,ip6, \
  ipv6_dst=$Vif38Address,$Protocol,actions=ct(table=1)"
> ovs-ofctl add-flow br-int "table=1,priority=1,ip6,ct_state=+new+trk, \
  $Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=1,priority=2,ip6, \
  ct_state=-new+rpl+trk,$Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=1,priority=1,ip6, \
  ct_state=+trk+est-new,$Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=2,priority=1,ip6, \
  ipv6_dst=$Vif38Address,$Protocol,actions=output:$Vif38Port"
> ovs-ofctl add-flow br-int "table=2,priority=1,ip6, \
  ipv6_dst=$Vif40Address,$Protocol,actions=output:$Vif40Port"
nat scenario
Vif38(20::1, ofport:2) -> nat address(20::9) -> Vif42(21::3, ofport:4)
Due to not construct flow to return neighbor mac address,
we set the neighbor mac address manually.
Vif38Name="podvif38"
Vif42Name="podvif42"
Vif38Ip="20::1"
Vif38Port=2
Vif42Port=4
NatAddress="20::9"
NatMacAddress="aa:bb:cc:dd:ee:ff"
NatMacAddressForCli="aa-bb-cc-dd-ee-ff"
Vif42Ip="21::3"
Vif38MacAddress="00:15:5D:F0:01:0B"
Vif38MacAddressCli="00-15-5D-F0-01-0B"
Vif42MacAddress="00:15:5D:F0:01:0D"
Protocol="tcp6"
> netsh int ipv6 set neighbors $Vif38Name $NatAddress \
  $NatMacAddressForCli
> netsh int ipv6 set neighbors $Vif42Name $Vif38Ip \
  $Vif38MacAddressCli
> ovs-ofctl del-flows --strict br-int "table=0,priority=0"
> ovs-ofctl add-flow br-int "table=0, priority=2,ipv6, \
  dl_dst=$NatMacAddress,ct_state=-trk,$Protocol \
  actions=ct(table=1,zone=456,nat)"
> ovs-ofctl add-flow br-int "table=0, priority=1,ipv6,ct_state=-trk, \
  ip6,$Protocol actions=ct(nat, zone=456,table=1)"
> ovs-ofctl add-flow br-int "table=1, ipv6,in_port=$Vif38Port, \
  ipv6_dst=$NatAddress,$Protocol,ct_state=+trk+new, \
  actions=ct(commit,nat(dst=$Vif42Ip),zone=456, \
  exec(set_field:1->ct_mark)),mod_dl_src=$NatMacAddress, \
  mod_dl_dst=$Vif42MacAddress,output:$Vif42Port"
> ovs-ofctl add-flow br-int "table=1, ipv6,ct_state=+dnat,$Protocol, \
  action=resubmit(,2)"
> ovs-ofctl add-flow br-int "table=1, ipv6,ct_state=+trk+snat, \
  $Protocol, action=resubmit(,2)"
> ovs-ofctl add-flow br-int "table=2, ipv6,in_port=$Vif38Port, \
  ipv6_dst=$Vif42Ip,$Protocol, actions=mod_dl_src=$NatMacAddress, \
  mod_dl_dst=$Vif42MacAddress,output:$Vif42Port"
> ovs-ofctl add-flow br-int "table=2, ipv6,in_port=$Vif42Port, \
  ct_state=-new+est,ct_mark=1,ct_zone=456,$Protocol, \
  actions=mod_dl_src=$NatMacAddress,mod_dl_dst=$Vif38MacAddress, \
  output:$Vif38Port"
```

Ftp is a specific protocol, it contains an related flow, we need to match is related state.
Ftp 是一个特定的协议，它包含一个相关的流程，我们需要匹配的是相关的状态。

```
normal scenario
Vif38(20::1, ofport:2)->Vif40(20:2, ofport:3)
Vif38Name="podvif70"
Vif40Name="Ethernet1"
Vif38Port=2
Vif38Address="20::88"
Vif40Port=3
Vif40Address="20::45"
Vif40MacAddressCli="00-50-56-98-9d-97"
Vif38MacAddressCli="00-15-5D-F0-01-0B"
Protocol="tcp6"
> netsh int ipv6 set neighbors $Vif38Name $Vif40Address $Vif40MacAddressCli
> netsh int ipv6 set neighbors $Vif42Name $Vif38Ip $Vif38MacAddressCli
> ovs-ofctl del-flows br-int --strict "table=0,priority=0"
> ovs-ofctl add-flow br-int "table=0,priority=1,$Protocol
  actions=ct(table=1)"
> ovs-ofctl add-flow br-int "table=1,priority=1,tp_dst=21, $Protocol,\
  actions=ct(commit,table=2,alg=ftp)"
> ovs-ofctl add-flow br-int "table=1,priority=1,tp_src=21, $Protocol,\
  actions=ct(commit,table=2,alg=ftp)"
> ovs-ofctl add-flow br-int "table=1,priority=1, ct_state=+new+trk+rel,\
  $Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=1,priority=1, \
  ct_state=-new+trk+est+rel,$Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=2,priority=1,ip6,\
  ipv6_dst=$Vif38Address,$Protocol,actions=output:$Vif38Port"
> ovs-ofctl add-flow br-int "table=2,priority=1,ip6,\
  ipv6_dst=$Vif40Address,$Protocol,actions=output:$Vif40Port"
nat scenario
Vif38(20::1, ofport:2) -> nat address(20::9) -> Vif42(21::3, ofport:4)
Due to not construct flow to return neighbor mac address, we set the
neighbor mac address manually
Vif38Name="podvif70"
Vif42Name="Ethernet1"
Vif38Ip="20::88"
Vif38Port=2
Vif42Port=3
NatAddress="20::9"
NatMacAddress="aa:bb:cc:dd:ee:ff"
NatMacAddressForCli="aa-bb-cc-dd-ee-ff"
Vif42Ip="21::3"
Vif38MacAddress="00:15:5D:F0:01:14"
Vif38MacAddressCli="00-15-5D-F0-01-14"
Vif42MacAddress="00:50:56:98:9d:97"
Protocol="tcp6"
netsh int ipv6 set neighbors $Vif38Name $NatAddress $NatMacAddressForCli
netsh int ipv6 set neighbors $Vif42Name $Vif38Ip $Vif38MacAddressCli
> ovs-ofctl del-flows br-int --strict "table=0,priority=0"
> ovs-ofctl add-flow br-int "table=0,priority=2,ipv6,ipv6_dst=$NatAddress,\
  ct_state=-trk,$Protocol actions=ct(table=1,zone=456)"
> ovs-ofctl add-flow br-int "table=0,priority=1,ipv6,ipv6_dst=$Vif38Ip,\
  ct_state=-trk,ip6,$Protocol actions=ct(zone=456,table=1)"
> ovs-ofctl add-flow br-int "table=1,priority=2,ipv6,in_port=$Vif38Port,\
  ipv6_dst=$NatAddress,ct_state=+trk-rel,tp_dst=21,$Protocol \
  actions=ct(commit,alg=ftp,nat(dst=$Vif42Ip),zone=456, \
  exec(set_field:1->ct_mark)),mod_dl_src=$NatMacAddress,\
  mod_dl_dst=$Vif42MacAddress,output:$Vif42Port"
> ovs-ofctl add-flow br-int "table=1,priority=1,ipv6,ct_state=+trk-rel,\
  ipv6_dst=$Vif38Ip,$Protocol,action=ct(nat,alg=ftp,zone=456,table=2)"
> ovs-ofctl add-flow br-int "table=1,ipv6,ct_state=+trk+rel,\
  ipv6_dst=$NatAddress,$Protocol,\
  action=ct(table=2,commit,nat(dst=$Vif42Ip),\
  zone=456, exec(set_field:1->ct_mark))"
> ovs-ofctl add-flow br-int "table=1,ipv6,ct_state=+trk+rel,$Protocol,\
  ipv6_dst=$Vif38Ip, action=ct(nat,zone=456,table=2)"
> ovs-ofctl add-flow br-int "table=2,ipv6,ipv6_dst=$Vif42Ip,$Protocol,\
  actions=mod_dl_src=$NatMacAddress, mod_dl_dst=$Vif42MacAddress,\
  output:$Vif42Port"
> ovs-ofctl add-flow br-int "table=2,ipv6,ipv6_dst=$Vif38Ip,\
  ct_state=-new+est,ct_mark=1,ct_zone=456,$Protocol,\
  actions=mod_dl_src=$NatMacAddress,mod_dl_dst=$Vif38MacAddress,\
  output:$Vif38Port"
> ovs-ofctl add-flow br-int "table=2,ipv6,ipv6_dst=$Vif38Ip,\
  ct_state=+new,ct_mark=1,ct_zone=456,$Protocol,\
  actions=mod_dl_src=$NatMacAddress,\
  mod_dl_dst=$Vif38MacAddress, output:$Vif38Port"
```

Tftp same with ftp, it also contains a related connection, we could use following follow test the tftp connection.
Tftp 和 ftp 一样，它也包含一个相关的连接，我们可以使用以下 tftp 连接进行以下测试。

```
normal scenario
Vif38Name="podvif70"
Vif40Name="Ethernet1"
Vif38Port=2
Vif38Address="20::88"
Vif40Port=3
Vif40Address="20::45"
Vif40MacAddressCli="00-50-56-98-9d-97"
Vif38MacAddressCli="00-15-5D-F0-01-14"
Protocol="udp6"
netsh int ipv6 set neighbors $Vif38Name $Vif40Address $Vif40MacAddressCli
netsh int ipv6 set neighbors $Vif40Name $Vif38Address $Vif38MacAddressCli
> ovs-ofctl del-flows br-int --strict "table=0,priority=0"
> ovs-ofctl add-flow br-int "table=0,priority=1,$Protocol,
  ipv6_src=$Vif38Address actions=ct(table=1)"
> ovs-ofctl add-flow br-int "table=0,priority=1,$Protocol,
  ipv6_src=$Vif40Address actions=ct(table=1)"
> ovs-ofctl add-flow br-int "table=1,priority=1,ct_state=+new+trk-est,
  tp_dst=69,$Protocol,udp6 actions=ct(commit,alg=tftp,table=2)"
> ovs-ofctl add-flow br-int "table=1,priority=1,ct_state=-new+trk+est-rel,\
  udp6 $Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=1,priority=1,ct_state=-new+trk+est+rel,\
  $Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=1,priority=1,ct_state=+new+trk+rel,\
  $Protocol,actions=ct(commit,table=2)"
> ovs-ofctl add-flow br-int "table=2,priority=1,ip6,\
  ipv6_dst=$Vif38Address,$Protocol,actions=output:$Vif38Port"
> ovs-ofctl add-flow br-int "table=2,priority=1,ip6,\
  ipv6_dst=$Vif40Address,$Protocol,actions=output:$Vif40Port"
nat scenario
Vif38Name="podvif70"
Vif42Name="Ethernet1"
Vif38Ip="20::88"
Vif38Port=2
Vif42Port=3
NatAddress="20::9"
NatMacAddress="aa:bb:cc:dd:ee:ff"
NatMacAddressForCli="aa-bb-cc-dd-ee-ff"
Vif42Ip="21::3"
Vif38MacAddress="00:15:5D:F0:01:14"
Vif38MacAddressCli="00-15-5D-F0-01-14"
Vif42MacAddress="00:50:56:98:9d:97"
Protocol="ip6"
netsh int ipv6 set neighbors $Vif38Name $NatAddress $NatMacAddressForCli
netsh int ipv6 set neighbors $Vif42Name $Vif38Ip $Vif38MacAddressCli
> ovs-ofctl del-flows br-int --strict "table=0,priority=0"
> ovs-ofctl add-flow br-int "table=0,priority=2,ipv6,\
  dl_dst=$NatMacAddress,ct_state=-trk,$Protocol \
  actions=ct(table=1,zone=456)"
> ovs-ofctl add-flow br-int "table=0,priority=1,ipv6,ct_state=-trk,ip6,\
  $Protocol actions=ct(table=1,zone=456)"
> ovs-ofctl add-flow br-int "table=1,in_port=$Vif38Port,\
  ipv6_dst=$NatAddress,ct_state=+trk+new-rel,$Protocol,udp6\
  actions=ct(commit,alg=tftp,nat(dst=$Vif42Ip),zone=456,\
  exec(set_field:1->ct_mark)),mod_dl_src=$NatMacAddress,\
  mod_dl_dst=$Vif42MacAddress,output:$Vif42Port"
> ovs-ofctl add-flow br-int "table=1,ipv6,in_port=$Vif42Port,\
  ipv6_dst=$Vif38Ip,ct_state=+trk+rel-rpl,$Protocol\
  actions=ct(commit,nat(src=$NatAddress),zone=456,\
  exec(set_field:1->ct_mark)),mod_dl_src=$NatMacAddress,\
  mod_dl_dst=$Vif38MacAddress,output:$Vif38Port"
> ovs-ofctl add-flow br-int "table=1,ipv6,ct_state=+trk+rel+est+rpl,\
  $Protocol,action=ct(nat,table=2,zone=456)"
> ovs-ofctl add-flow br-int "table=2,ipv6,in_port=$Vif38Port,\
  ct_state=+rel+dnat,ipv6_dst=$Vif42Ip,$Protocol,\
  actions=mod_dl_src=$NatMacAddress,mod_dl_dst=$Vif42MacAddress,\
  output:$Vif42Port"
> ovs-ofctl add-flow br-int "table=2,ipv6,in_port=$Vif42Port,\
  ct_state=-new+est,$Protocol,actions=mod_dl_src=$NatMacAddress,\
  mod_dl_dst=$Vif38MacAddress,output:$Vif38Port"
```

Note 注意

Till the checksum offload support is complete we recommend disabling TX/RX offloads for IPV6 on Windows VM.
在校验和卸载支持完成之前，我们建议在 Windows VM 上禁用 IPV6 的 TX/RX 卸载。

## Windows Services[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#windows-services) Windows 服务 ¶

Open vSwitch daemons come with support to run as a Windows service. The instructions here assume that you have installed the Open vSwitch utilities and daemons via `make install`.
Open vSwitch 守护程序支持作为 Windows 服务运行。此处的说明假定您已通过 `make install` 安装了 Open vSwitch 实用程序和守护程序。

To start, create the database:
首先，创建数据库：

```
> ovsdb-tool create C:/openvswitch/etc/openvswitch/conf.db \
    "C:/openvswitch/usr/share/openvswitch/vswitch.ovsschema"
```

Create the ovsdb-server service and start it:
创建 ovsdb-server 服务并启动它：

```
> sc create ovsdb-server \
    binpath="C:/openvswitch/usr/sbin/ovsdb-server.exe \
    C:/openvswitch/etc/openvswitch/conf.db \
    -vfile:info --log-file --pidfile \
    --remote=punix:db.sock --service --service-monitor"
> sc start ovsdb-server
```

Tip

One of the common issues with creating a Windows service is with mungled paths.  You can make sure that the correct path has been registered with the Windows services manager by running:
创建 Windows 服务的常见问题之一是路径混乱。您可以通过运行以下命令来确保已向 Windows 服务管理器注册正确的路径：

```
> sc qc ovsdb-server
```

Check that the service is healthy by running:
通过运行以下命令来检查服务是否正常：

```
> sc query ovsdb-server
```

Initialize the database:
初始化数据库：

```
> ovs-vsctl --no-wait init
```

Create the ovs-vswitchd service and start it:
创建 ovs-vswitchd 服务并启动它：

```
> sc create ovs-vswitchd \
    binpath="C:/openvswitch/usr/sbin/ovs-vswitchd.exe \
    --pidfile -vfile:info --log-file  --service --service-monitor"
> sc start ovs-vswitchd
```

Check that the service is healthy by running:
通过运行以下命令来检查服务是否正常：

```
> sc query ovs-vswitchd
```

To stop and delete the services, run:
若要停止和删除服务，请运行：

```
> sc stop ovs-vswitchd
> sc stop ovsdb-server
> sc delete ovs-vswitchd
> sc delete ovsdb-server
```

## Windows CI Service[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#windows-ci-service) Windows CI 服务 ¶

[AppVeyor](https://www.appveyor.com) provides a free Windows autobuild service for open source projects.  Open vSwitch has integration with AppVeyor for continuous build.  A developer can build test his changes for Windows by logging into appveyor.com using a github account, creating a new project by linking it to his development repository in github and triggering a new build.
AppVeyor 为开源项目提供免费的 Windows 自动构建服务。Open vSwitch 与 AppVeyor 集成，可进行持续构建。开发人员可以通过使用 github 帐户登录 appveyor.com、通过将新项目链接到 github 中的开发存储库来创建新项目并触发新版本来针对  Windows 构建测试他的更改。

## TODO[¶](https://docs.openvswitch.org/en/latest/intro/install/windows/#todo) 待办事项 ¶

- Investigate the working of sFlow on Windows and re-enable the unit tests.
  调查 sFlow 在 Windows 上的工作情况，并重新启用单元测试。
- Investigate and add the feature to provide QoS.
  调查并添加功能以提供 QoS。
- Sign the driver. 对驱动程序进行签名。

​            [Open vSwitch without Kernel Support ›
在没有](https://docs.openvswitch.org/en/latest/intro/install/userspace/)

# Open vSwitch without Kernel Support[¶](https://docs.openvswitch.org/en/latest/intro/install/userspace/#open-vswitch-without-kernel-support) 在没有内核支持的情况下打开 vSwitch ¶

Open vSwitch can operate, at a cost in performance, entirely in userspace, without assistance from a kernel module.  This file explains how to install Open vSwitch in such a mode.
Open vSwitch 可以完全在用户空间中运行，但性能会降低，而无需内核模块的帮助。本文件说明如何在这种模式下安装 Open vSwitch。

This version of Open vSwitch should be built manually with `configure` and `make`.  Debian packaging for Open vSwitch is also included, but it has not been recently tested, and so Debian packages are not a recommended way to use this version of Open vSwitch.
此版本的 Open vSwitch 应使用 `configure` 和 `make` 手动构建。Open vSwitch 的 Debian 软件包也包括在内，但最近还没有经过测试，因此不推荐使用 Debian 软件包来使用此版本的 Open vSwitch。

Warning 警告

The userspace-only mode of Open vSwitch without DPDK is considered experimental. It has not been thoroughly tested.
不带 DPDK 的 Open vSwitch 的仅限用户空间模式被视为实验性模式。它尚未经过彻底测试。

## Building and Installing[¶](https://docs.openvswitch.org/en/latest/intro/install/userspace/#building-and-installing) 构建和安装 ¶

The requirements and procedure for building, installing, and configuring Open vSwitch are the same as those given in [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/). You may omit configuring, building, and installing the kernel module, and the related requirements.
构建、安装和配置 Open vSwitch 的要求和过程与 Linux、FreeBSD 和 NetBSD 上的 Open vSwitch 中给出的要求和过程相同。您可以省略内核模块的配置、构建和安装以及相关要求。

On Linux, the userspace switch additionally requires the kernel TUN/TAP driver to be available, either built into the kernel or loaded as a module.  If you are not sure, check for a directory named `/sys/class/misc/tun`.  If it does not exist, then attempt to load the module with `modprobe tun`.
在 Linux 上，用户空间开关还需要内核 TUN/TAP 驱动程序可用，要么内置到内核中，要么作为模块加载。如果不确定，请检查名为 `/sys/class/misc/tun` 的目录。如果它不存在，则尝试使用 `modprobe tun` .

The tun device must also exist as `/dev/net/tun`.  If it does not exist, then create `/dev/net` (if necessary) with `mkdir /dev/net`, then create `/dev/net/tun` with `mknod /dev/net/tun c 10 200`.
tun 设备也必须以 `/dev/net/tun` .如果它不存在，则使用 create `/dev/net` （如有必要） ， `mkdir /dev/net` 然后使用 create `/dev/net/tun` with `mknod /dev/net/tun c 10 200` .

On FreeBSD and NetBSD, the userspace switch additionally requires the kernel tap(4) driver to be available, either built into the kernel or loaded as a module.
在 FreeBSD 和 NetBSD 上， 用户空间开关还要求内核 tap（4） 驱动程序可用， 要么内置在内核中，要么作为模块加载。

## Using the Userspace Datapath with ovs-vswitchd[¶](https://docs.openvswitch.org/en/latest/intro/install/userspace/#using-the-userspace-datapath-with-ovs-vswitchd) 将用户空间数据路径与 ovs-vswitchd 一起使用 ¶

To use ovs-vswitchd in userspace mode, create a bridge with `datapath_type=netdev` in the configuration database.  For example:
要在用户空间模式下使用 ovs-vswitchd，请在配置数据库中创建一个网 `datapath_type=netdev` 桥。例如：

```
$ ovs-vsctl add-br br0
$ ovs-vsctl set bridge br0 datapath_type=netdev
$ ovs-vsctl add-port br0 eth0
$ ovs-vsctl add-port br0 eth1
$ ovs-vsctl add-port br0 eth2
```

ovs-vswitchd will create a TAP device as the bridge’s local interface, named the same as the bridge, as well as for each configured internal interface.
ovs-vswitchd 将创建一个 TAP 设备作为网桥的本地接口，其名称与网桥相同，以及每个配置的内部接口。

Currently, on FreeBSD, the functionality required for in-band control support is not implemented.  To avoid related errors, you can disable the in-band support with the following command:
目前， 在 FreeBSD 上， 没有实现带内控制支持所需的功能。为避免相关错误，您可以使用以下命令禁用带内支持：

```
$ ovs-vsctl set bridge br0 other_config:disable-in-band=true
```

## Firewall Rules[¶](https://docs.openvswitch.org/en/latest/intro/install/userspace/#firewall-rules) 防火墙规则 ¶

On Linux, when a physical interface is in use by the userspace datapath, packets received on the interface still also pass into the kernel TCP/IP stack. This can cause surprising and incorrect behavior.  You can use “iptables” to avoid this behavior, by using it to drop received packets.  For example, to drop packets received on eth0:
在 Linux 上，当用户空间数据路径使用物理接口时，在接口上接收的数据包仍会传递到内核 TCP/IP  堆栈中。这可能会导致令人惊讶和不正确的行为。您可以使用“iptables”来避免此行为，方法是使用它来丢弃收到的数据包。例如，要丢弃在 eth0 上接收的数据包：

```
$ iptables -A INPUT -i eth0 -j DROP
$ iptables -A FORWARD -i eth0 -j DROP
```

## Other Settings[¶](https://docs.openvswitch.org/en/latest/intro/install/userspace/#other-settings) 其他设置 ¶

On NetBSD, depending on your network topology and applications, the following configuration might help.  See sysctl(7).:
在 NetBSD 上，根据您的网络拓扑和应用程序，以下配置可能会有所帮助。参见 sysctl（7）。

```
$ sysctl -w net.inet.ip.checkinterface=1
```

## Reporting Bugs[¶](https://docs.openvswitch.org/en/latest/intro/install/userspace/#reporting-bugs) 报告错误 ¶

Report problems to [bugs@openvswitch.org](mailto:bugs@openvswitch.org).
向 bugs@openvswitch.org 报告问题。

# Open vSwitch with DPDK[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#open-vswitch-with-dpdk) 使用 DPDK 打开 vSwitch ¶

This document describes how to build and install Open vSwitch using a DPDK datapath. Open vSwitch can use the DPDK library to operate entirely in userspace.
本文档介绍如何使用DPDK数据路径构建和安装Open vSwitch。Open vSwitch 可以使用 DPDK 库完全在用户空间中运行。

Important 重要

The [releases FAQ](https://docs.openvswitch.org/en/latest/faq/releases/) lists support for the required versions of DPDK for each version of Open vSwitch. If building OVS and DPDK outside of the main build tree users should consult this list first.
版本常见问题解答列出了每个 Open vSwitch 版本对所需 DPDK 版本的支持。如果在主构建树之外构建 OVS 和 DPDK，用户应首先查阅此列表。

## Build requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#build-requirements) 构建要求 ¶

In addition to the requirements described in [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/), building Open vSwitch with DPDK will require the following:
除了在 Linux、FreeBSD 和 NetBSD 上打开 vSwitch 中描述的要求外，使用 DPDK 构建 Open vSwitch 还需要满足以下条件：

- DPDK 23.11

- A [DPDK supported NIC](https://doc.dpdk.org/guides-23.11/nics/index.html) 支持 DPDK 的 NIC

  Only required when physical ports are in use
  仅在使用物理端口时才需要

- A suitable kernel 合适的内核

  On Linux Distros running kernel version >= 3.0, only IOMMU needs to enabled via the grub cmdline, assuming you are using **VFIO**. For older kernels, ensure the kernel is built with `UIO`, `HUGETLBFS`, `PROC_PAGE_MONITOR`, `HPET`, `HPET_MMAP` support. If these are not present, it will be necessary to upgrade your kernel or build a custom kernel with these flags enabled.
  在运行内核版本 >= 3.0 的 Linux 发行版上，假设您使用的是 VFIO，则只需要通过 grub cmdline 启用 IOMMU。对于较旧的内核，请确保内核是使用 `UIO` 、 `HUGETLBFS` 、 `PROC_PAGE_MONITOR` 、 `HPET` `HPET_MMAP` 支持构建的。如果这些标志不存在，则需要升级内核或构建启用了这些标志的自定义内核。

Detailed system requirements can be found at [DPDK requirements](https://doc.dpdk.org/guides-23.11/linux_gsg/sys_reqs.html).
详细的系统要求可以在 DPDK 要求中找到。



## Installing[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#installing) 安装 ¶

### Install DPDK[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#install-dpdk) 安装 DPDK ¶

1. Download the [DPDK sources](http://dpdk.org/rel), extract the file and set `DPDK_DIR`:
   下载 DPDK 源代码，解压文件并设置 `DPDK_DIR` ：

   ```
   $ cd /usr/src/
   $ wget https://fast.dpdk.org/rel/dpdk-23.11.tar.xz
   $ tar xf dpdk-23.11.tar.xz
   $ export DPDK_DIR=/usr/src/dpdk-23.11
   $ cd $DPDK_DIR
   ```

2. Configure and install DPDK using Meson
   使用 Meson 配置和安装 DPDK

   Build and install the DPDK library:
   生成并安装 DPDK 库：

   ```
   $ export DPDK_BUILD=$DPDK_DIR/build
   $ meson build
   $ ninja -C build
   $ sudo ninja -C build install
   $ sudo ldconfig
   ```

   Check if libdpdk can be found by pkg-config:
   检查 pkg-config 是否可以找到 libdpdk：

   ```
   $ pkg-config --modversion libdpdk
   ```

   The above command should return the DPDK version installed. If not found, export the path to the installed DPDK libraries:
   上述命令应返回已安装的 DPDK 版本。如果未找到，请将路径导出到已安装的 DPDK 库：

   ```
   $ export PKG_CONFIG_PATH=/path/to/installed/".pc" file/for/DPDK
   ```

   For example, On Fedora 32:
   例如，在 Fedora 32 上：

   ```
   $ export PKG_CONFIG_PATH=/usr/local/lib64/pkgconfig
   ```

   Detailed information can be found at [DPDK documentation](https://doc.dpdk.org/guides-23.11/linux_gsg/build_dpdk.html).
   有关详细信息，请参阅 DPDK 文档。

3. (Optional) Configure and export the DPDK shared library location
   （可选）配置和导出 DPDK 共享库位置

   Since DPDK is built both as static and shared library by default, no extra configuration is required for the build.
   由于 DPDK 默认同时构建为静态库和共享库，因此构建不需要额外的配置。

   Exporting the path to library is not necessary if the DPDK libraries are system installed. For libraries installed using a prefix, export the path to this library:
   如果系统安装了 DPDK 库，则不需要将路径导出到库。对于使用前缀安装的库，请导出此库的路径：

   ```
   $ export LD_LIBRARY_PATH=/path/to/installed/DPDK/libraries
   ```

   Note 注意

   Minor performance loss is expected when using OVS with a shared DPDK library compared to a static DPDK library.
   与静态 DPDK 库相比，将 OVS 与共享 DPDK 库一起使用时，预计性能会略有下降。

### Install OVS[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#install-ovs) 安装 OVS ¶

OVS can be installed using different methods.  For OVS to use DPDK, it has to be configured to build against the DPDK library (`--with-dpdk`).
可以使用不同的方法安装 OVS。要使 OVS 使用 DPDK，必须将其配置为针对 DPDK 库 （ `--with-dpdk` ） 进行构建。

Note 注意

This section focuses on generic recipe that suits most cases. For distribution specific instructions, refer to one of the more relevant guides.
本节重点介绍适合大多数情况的通用配方。有关特定于发行版的说明，请参阅更相关的指南之一。

1. Ensure the standard OVS requirements, described in [Build Requirements](https://docs.openvswitch.org/en/latest/intro/install/general/#general-build-reqs), are installed
   确保安装了生成要求中所述的标准 OVS 要求

2. Bootstrap, if required, as described in [Bootstrapping](https://docs.openvswitch.org/en/latest/intro/install/general/#general-bootstrapping)
   Bootstrap（如果需要），如 Bootstrapping 中所述

3. Configure the package using the `--with-dpdk` flag:
   使用 `--with-dpdk` 以下标志配置包：

   If OVS must consume DPDK static libraries (also equivalent to `--with-dpdk=yes` ):
   如果 OVS 必须使用 DPDK 静态库（也等效于 `--with-dpdk=yes` ）：

   ```
   $ ./configure --with-dpdk=static
   ```

   If OVS must consume DPDK shared libraries:
   如果 OVS 必须使用 DPDK 共享库：

   ```
   $ ./configure --with-dpdk=shared
   ```

   Note 注意

   While `--with-dpdk` is required, you can pass any other configuration option described in [Configuring](https://docs.openvswitch.org/en/latest/intro/install/general/#general-configuring).
   虽然 `--with-dpdk` 是必需的，但您可以传递配置中描述的任何其他配置选项。

   It is strongly recommended to build OVS with at least `-msse4.2` and `-mpopcnt` optimization flags. If these flags are not enabled, the AVX512 optimized DPCLS implementation is not available in the resulting binary. For technical details see the subtable registration code in the `lib/dpif-netdev-lookup.c` file.
   强烈建议至少 `-msse4.2` 使用优化标志来 `-mpopcnt` 构建 OVS。如果未启用这些标志，则生成的二进制文件中不提供 AVX512 优化的 DPCLS 实现。有关技术详细信息， `lib/dpif-netdev-lookup.c` 请参阅文件中的子表注册代码。

   An example that enables the AVX512 optimizations is:
   启用 AVX512 优化的示例如下：

   ```
   $ ./configure --with-dpdk=static CFLAGS="-Ofast -msse4.2 -mpopcnt"
   ```

4. Build and install OVS, as described in [Building](https://docs.openvswitch.org/en/latest/intro/install/general/#general-building)
   生成并安装 OVS，如构建中所述

Additional information can be found in [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/).
更多信息可以在 Linux 上的 Open vSwitch、FreeBSD 和 NetBSD 中找到。

Note 注意

If you are running using the Fedora or Red Hat package, the Open vSwitch daemon will run as a non-root user.  This implies that you must have a working IOMMU.  Visit the [RHEL README](https://github.com/openvswitch/ovs/blob/main/rhel/README.RHEL.rst) for additional information.
如果您使用 Fedora 或 Red Hat 软件包运行，则 Open vSwitch 守护程序将以非 root 用户身份运行。这意味着您必须有一个有效的 IOMMU。有关更多信息，请访问 RHEL 自述文件。

#### Possible issues when enabling AVX512[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#possible-issues-when-enabling-avx512) 启用 AVX512 时可能出现的问题 ¶

The enabling of ISA optimized builds requires build-system support. Certain versions of the assembler provided by binutils is known to have AVX512 assembling issues. The binutils versions affected are 2.30 and 2.31. As many distros backport fixes to previous versions of a package, checking the version output of `as -v` can err on the side of disabling AVX512. To remedy this, the OVS build system uses a build-time check to see if `as` will correctly assemble the AVX512 code. The output of a good version when running the `./configure` step of the build process is as follows:
启用 ISA 优化版本需要构建系统支持。已知 binutils 提供的某些版本的汇编程序存在 AVX512 汇编问题。受影响的 binutils 版本为 2.30 和 2.31。由于许多发行版向后移植到软件包的先前版本，因此检查版本 `as -v` 输出可能会在禁用 AVX512 时出错。为了解决这个问题，OVS 构建系统使用构建时检查来查看是否 `as` 正确组装 AVX512 代码。运行生成过程 `./configure` 步骤时，良好版本的输出如下：

```
$ checking binutils avx512 assembler checks passing... yes
```

If a bug is detected in the binutils assembler, it would indicate `no`. Build an updated binutils, or request a backport of this binutils fix commit `2069ccaf8dc28ea699bd901fdd35d90613e4402a` to fix the issue.
如果在 binutils 汇编器中检测到错误，则指示 `no` .构建更新的 binutils，或请求此 binutils 修复提交 `2069ccaf8dc28ea699bd901fdd35d90613e4402a` 的向后移植以修复此问题。

## Setup[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#setup) 设置 ¶

### Setup Hugepages[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#setup-hugepages) 设置 Hugepages ¶

Allocate a number of 2M Huge pages:
分配一些 2M 大页面：

- For persistent allocation of huge pages, write to hugepages.conf file in /etc/sysctl.d:
  对于大页面的持久分配，请写入 /etc/sysctl.d 中的 hugepages.conf 文件：

  ```
  $ echo 'vm.nr_hugepages=2048' > /etc/sysctl.d/hugepages.conf
  ```

- For run-time allocation of huge pages, use the `sysctl` utility:
  对于大页面的运行时分配，请使用以下 `sysctl` 实用程序：

  ```
  $ sysctl -w vm.nr_hugepages=N  # where N = No. of 2M huge pages
  ```

To verify hugepage configuration:
要验证 hugepage 配置，请执行以下操作：

```
$ grep HugePages_ /proc/meminfo
```

Mount the hugepages, if not already mounted by default:
如果默认情况下尚未挂载 hugepages，则挂载：

```
$ mount -t hugetlbfs none /dev/hugepages
```

Note 注意

The amount of hugepage memory required can be affected by various aspects of the datapath and device configuration. Refer to [DPDK Device Memory Models](https://docs.openvswitch.org/en/latest/topics/dpdk/memory/) for more details.
所需的 hugepage 内存量可能会受到数据路径和设备配置的各个方面的影响。有关详细信息，请参阅 DPDK 设备内存型号。



### Setup DPDK devices using VFIO[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#setup-dpdk-devices-using-vfio) 使用 VFIO 设置 DPDK 设备 ¶

VFIO is preferred to the UIO driver when using recent versions of DPDK. VFIO support required support from both the kernel and BIOS. For the former, kernel version > 3.6 must be used. For the latter, you must enable VT-d in the BIOS and ensure this is configured via grub. To ensure VT-d is enabled via the BIOS, run:
使用最新版本的 DPDK 时，VFIO 优先于 UIO 驱动程序。VFIO 支持需要内核和 BIOS 的支持。对于前者，必须使用 > 3.6  内核版本。对于后者，您必须在 BIOS 中启用 VT-d，并确保通过 grub 进行配置。要确保通过 BIOS 启用 VT-d，请运行：

```
$ dmesg | grep -e DMAR -e IOMMU
```

If VT-d is not enabled in the BIOS, enable it now.
如果 BIOS 中未启用 VT-d，请立即启用它。

To ensure VT-d is enabled in the kernel, run:
要确保在内核中启用 VT-d，请运行：

```
$ cat /proc/cmdline | grep iommu=pt
$ cat /proc/cmdline | grep intel_iommu=on
```

If VT-d is not enabled in the kernel, enable it now.
如果内核中未启用 VT-d，请立即启用它。

Once VT-d is correctly configured, load the required modules and bind the NIC to the VFIO driver:
正确配置 VT-d 后，加载所需的模块并将 NIC 绑定到 VFIO 驱动程序：

```
$ modprobe vfio-pci
$ /usr/bin/chmod a+x /dev/vfio
$ /usr/bin/chmod 0666 /dev/vfio/*
$ $DPDK_DIR/usertools/dpdk-devbind.py --bind=vfio-pci eth1
$ $DPDK_DIR/usertools/dpdk-devbind.py --status
```

### Setup OVS[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#setup-ovs) 设置 OVS ¶

Open vSwitch should be started as described in [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/) with the exception of ovs-vswitchd, which requires some special configuration to enable DPDK functionality. DPDK configuration arguments can be passed to ovs-vswitchd via the `other_config` column of the `Open_vSwitch` table. At a minimum, the `dpdk-init` option must be set to either `true` or `try`. For example:
Open vSwitch 应该按照 Linux 上 Open vSwitch、FreeBSD 和 NetBSD 中所述启动，但 ovs-vswitchd 除外，它需要一些特殊配置才能启用 DPDK 功能。DPDK 配置参数可以通过 `Open_vSwitch` 表 `other_config` 的列传递给 ovs-vswitchd。该 `dpdk-init` 选项至少必须设置为 或 `true` `try` 。例如：

```
$ export PATH=$PATH:/usr/local/share/openvswitch/scripts
$ export DB_SOCK=/usr/local/var/run/openvswitch/db.sock
$ ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true
$ ovs-ctl --no-ovsdb-server --db-sock="$DB_SOCK" start
```

There are many other configuration options, the most important of which are listed below. Defaults will be provided for all values not explicitly set.
还有许多其他配置选项，下面列出了其中最重要的选项。将为所有未显式设置的值提供默认值。

- `dpdk-init`

  Specifies whether OVS should initialize and support DPDK ports. This field can either be `true` or `try`. A value of `true` will cause the ovs-vswitchd process to abort on initialization failure. A value of `try` will imply that the ovs-vswitchd process should continue running even if the EAL initialization fails. 指定 OVS 是否应初始化和支持 DPDK 端口。此字段可以是 `true` 或 `try` 。值 `true` 将导致 ovs-vswitchd 进程在初始化失败时中止。值 将 `try` 意味着即使 EAL 初始化失败，ovs-vswitchd 进程也应继续运行。

- `dpdk-lcore-mask`

  Specifies the CPU cores on which dpdk lcore threads should be spawned and expects hex string (eg ‘0x123’). 指定应在其上生成 dpdk lcore 线程的 CPU 内核，并期望十六进制字符串（例如“0x123”）。

- `dpdk-socket-mem`

  Comma separated list of memory to pre-allocate from hugepages on specific sockets. If not specified, this option will not be set by default. DPDK default will be used instead. 逗号分隔的内存列表，要从特定套接字上的大页面中预先分配。如果未指定，则默认情况下不会设置此选项。将改用 DPDK 默认值。

- `dpdk-hugepage-dir`

  Directory where hugetlbfs is mounted hugetlbfs 挂载的目录

- `vhost-sock-dir`

  Option to set the path to the vhost-user unix socket files. 用于设置 vhost-user unix 套接字文件路径的选项。

If allocating more than one GB hugepage, you can configure the amount of memory used from any given NUMA nodes. For example, to use 1GB from NUMA node 0 and 0GB for all other NUMA nodes, run:
如果分配多个 GB hugepage，则可以配置从任何给定 NUMA 节点使用的内存量。例如，若要将 NUMA 节点 0 中的 1GB 用于所有其他 NUMA 节点，请运行：

```
$ ovs-vsctl --no-wait set Open_vSwitch . \
    other_config:dpdk-socket-mem="1024,0"
```

or:

```
$ ovs-vsctl --no-wait set Open_vSwitch . \
    other_config:dpdk-socket-mem="1024"
```

Note 注意

Changing any of these options requires restarting the ovs-vswitchd application
更改其中任何一个选项都需要重新启动 ovs-vswitchd 应用程序

See the section `Performance Tuning` for important DPDK customizations.
有关重要的 DPDK 自定义项，请参阅该部分 `Performance Tuning` 。

## Validating[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#validating) 验证 ¶

DPDK support can be confirmed by validating the `dpdk_initialized` boolean value from the ovsdb.  A value of `true` means that the DPDK EAL initialization succeeded:
可以通过验证 ovsdb 中的 `dpdk_initialized` 布尔值来确认 DPDK 支持。值 为 表示 `true` DPDK EAL 初始化成功：

```
$ ovs-vsctl get Open_vSwitch . dpdk_initialized
true
```

Additionally, the library version linked to ovs-vswitchd can be confirmed with either the ovs-vswitchd logs, or by running either of the commands:
此外，可以通过 ovs-vswitchd 日志或运行任一命令来确认链接到 ovs-vswitchd 的库版本：

```
$ ovs-vswitchd --version
ovs-vswitchd (Open vSwitch) 2.9.0
DPDK 17.11.0
$ ovs-vsctl get Open_vSwitch . dpdk_version
"DPDK 17.11.0"
```

At this point you can use ovs-vsctl to set up bridges and other Open vSwitch features. Seeing as we’ve configured DPDK support, we will use DPDK-type ports. For example, to create a userspace bridge named `br0` and add two `dpdk` ports to it, run:
此时，您可以使用 ovs-vsctl 设置网桥和其他 Open vSwitch 功能。由于我们已经配置了 DPDK 支持，我们将使用 DPDK 类型的端口。例如，要创建名为 `br0` 的用户空间网桥并向其添加两个 `dpdk` 端口，请运行：

```
$ ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev
$ ovs-vsctl add-port br0 myportnameone -- set Interface myportnameone \
    type=dpdk options:dpdk-devargs=0000:06:00.0
$ ovs-vsctl add-port br0 myportnametwo -- set Interface myportnametwo \
    type=dpdk options:dpdk-devargs=0000:06:00.1
```

DPDK devices will not be available for use until a valid dpdk-devargs is specified.
在指定有效的 dpdk-devargs 之前，DPDK 设备将不可用。

Refer to ovs-vsctl(8) and [Using Open vSwitch with DPDK](https://docs.openvswitch.org/en/latest/howto/dpdk/) for more details.
有关更多详细信息，请参考 ovs-vsctl（8） 和 将 Open vSwitch 与 DPDK 配合使用。

## Performance Tuning[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#performance-tuning) 性能调优 ¶

To achieve optimal OVS performance, the system can be configured and that includes BIOS tweaks, Grub cmdline additions, better understanding of NUMA nodes and apt selection of PCIe slots for NIC placement.
为了实现最佳的 OVS 性能，可以对系统进行配置，其中包括 BIOS 调整、Grub cmdline 添加、更好地了解 NUMA 节点以及为 NIC 放置选择合适的 PCIe 插槽。

Note 注意

This section is optional. Once installed as described above, OVS with DPDK will work out of the box.
此部分是可选的。如上所述安装后，带有 DPDK 的 OVS 将开箱即用。

### Recommended BIOS Settings[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#recommended-bios-settings) 推荐的 BIOS 设置 ¶

| Setting                                                      | Value       |
| ------------------------------------------------------------ | ----------- |
| C3 Power State C3 电源状态                                   | Disabled    |
| C6 Power State C6 电源状态                                   | Disabled    |
| MLC Streamer                                                 | Enabled     |
| MLC Spatial Prefetcher MLC 空间预取器                        | Enabled     |
| DCU Data Prefetcher DCU 数据预取器                           | Enabled     |
| DCA                                                          | Enabled     |
| CPU Power and Performance CPU 功率和性能                     | Performance |
| Memory RAS and Performance Config -> NUMA optimized 内存 RAS 和性能配置 -> NUMA 优化 | Enabled     |

### PCIe Slot Selection[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#pcie-slot-selection) PCIe插槽选择 ¶

The fastpath performance can be affected by factors related to the placement of the NIC, such as channel speeds between PCIe slot and CPU or the proximity of PCIe slot to the CPU cores running the DPDK application. Listed below are the steps to identify right PCIe slot.
快速路径性能可能会受到与 NIC 放置相关的因素的影响，例如 PCIe 插槽和 CPU 之间的通道速度或 PCIe 插槽与运行 DPDK 应用程序的 CPU 内核的接近程度。下面列出了识别正确PCIe插槽的步骤。

1. Retrieve host details using `dmidecode`. For example:
   使用 `dmidecode` 检索主机详细信息。例如：

   ```
   $ dmidecode -t baseboard | grep "Product Name"
   ```

2. Download the technical specification for product listed, e.g: S2600WT2
   下载所列产品的技术规格，例如：S2600WT2

3. Check the Product Architecture Overview on the Riser slot placement, CPU sharing info and also PCIe channel speeds
   查看产品架构概述，了解转接卡插槽位置、CPU 共享信息以及 PCIe 通道速度

   For example: On S2600WT, CPU1 and CPU2 share Riser Slot 1 with Channel speed between CPU1 and Riser Slot1 at 32GB/s, CPU2 and Riser Slot1 at 16GB/s. Running DPDK app on CPU1 cores and NIC inserted in to Riser card Slots will optimize OVS performance in this case.
   例如：在S2600WT上，CPU1 和 CPU2 共享转接卡插槽 1，CPU1 和转接卡插槽 1 之间的通道速度为 32GB/s，CPU2 和转接卡插槽 1 之间的通道速度为  16GB/s。在这种情况下，在 CPU1 内核上运行 DPDK 应用程序并将 NIC 插入转接卡插槽将优化 OVS 性能。

4. Check the Riser Card #1 - Root Port mapping information, on the available slots and individual bus speeds. In S2600WT slot 1, slot 2 has high bus speeds and are potential slots for NIC placement.
   检查转接卡 #1 - 根端口映射信息，了解可用插槽和各个总线速度。在插槽 1 S2600WT，插槽 2 具有较高的总线速度，并且是用于放置 NIC 的潜在插槽。

### Advanced Hugepage Setup[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#advanced-hugepage-setup) 高级 Hugepage 设置 ¶

Allocate and mount 1 GB hugepages.
分配并挂载 1 GB 大页面。

- For persistent allocation of huge pages, add the following options to the kernel bootline:
  要永久分配大页面，请将以下选项添加到内核引导行：

  ```
  default_hugepagesz=1GB hugepagesz=1G hugepages=N
  ```

  For platforms supporting multiple huge page sizes, add multiple options:
  对于支持多个大页面大小的平台，请添加多个选项：

  ```
  default_hugepagesz=<size> hugepagesz=<size> hugepages=N
  ```

  where: 哪里：

  - `N`

    number of huge pages requested 请求的大页面数

  - `size`

    huge page size with an optional suffix `[kKmMgG]` 带有可选后缀 `[kKmMgG]` 的巨大页面大小

- For run-time allocation of huge pages:
  对于大页面的运行时分配：

  ```
  $ echo N > /sys/devices/system/node/nodeX/hugepages/hugepages-1048576kB/nr_hugepages
  ```

  where: 哪里：

  - `N`

    number of huge pages requested 请求的大页面数

  - `X`

    NUMA Node NUMA 节点

  Note 注意

  For run-time allocation of 1G huge pages, Contiguous Memory Allocator (`CONFIG_CMA`) has to be supported by kernel, check your Linux distro.
  对于 1G 大页面的运行时分配，内核必须支持连续内存分配器 （ `CONFIG_CMA` ），请检查您的 Linux 发行版。

Now mount the huge pages, if not already done so:
现在挂载大页面，如果还没有这样做的话：

```
$ mount -t hugetlbfs -o pagesize=1G none /dev/hugepages
```

### Isolate Cores[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#isolate-cores) 隔离核心 ¶

The `isolcpus` option can be used to isolate cores from the Linux scheduler. The isolated cores can then be used to dedicatedly run HPC applications or threads.  This helps in better application performance due to zero context switching and minimal cache thrashing. To run platform logic on core 0 and isolate cores between 1 and 19 from scheduler, add  `isolcpus=1-19` to GRUB cmdline.
该 `isolcpus` 选项可用于将内核与 Linux 调度程序隔离开来。然后，隔离内核可用于专门运行 HPC 应用程序或线程。由于零上下文切换和最小的缓存抖动，这有助于提高应用程序性能。要在内核 0 上运行平台逻辑，并将 1 到 19 之间的内核与调度程序隔离开来，请添加到 `isolcpus=1-19` GRUB cmdline。

Note 注意

It has been verified that core isolation has minimal advantage due to mature Linux scheduler in some circumstances.
已经验证，在某些情况下，由于成熟的 Linux 调度程序，核心隔离的优势很小。

### Compiler Optimizations[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#compiler-optimizations) 编译器优化 ¶

The default compiler optimization level is `-O2`. Changing this to more aggressive compiler optimization such as `-O3 -march=native` with gcc (verified on 5.3.1) can produce performance gains though not significant. `-march=native` will produce optimized code on local machine and should be used when software compilation is done on Testbed.
默认编译器优化级别为 `-O2` 。将其更改为更积极的编译器优化，例如 `-O3 -march=native` gcc（在 5.3.1 上验证）可以产生性能提升，但并不显着。 `-march=native` 将在本地计算机上生成优化的代码，并且应该在 Testbed 上完成软件编译时使用。

### Multiple Poll-Mode Driver Threads[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#multiple-poll-mode-driver-threads) 多个轮询模式驱动线程 ¶

With pmd multi-threading support, OVS creates one pmd thread for each NUMA node by default, if there is at least one DPDK interface from that NUMA node added to OVS. However, in cases where there are multiple ports/rxq’s producing traffic, performance can be improved by creating multiple pmd threads running on separate cores. These pmd threads can share the workload by each being responsible for different ports/rxq’s. Assignment of ports/rxq’s to pmd threads is done automatically.
借助 pmd 多线程支持，如果至少有一个 DPDK 接口从该 NUMA 节点添加到 OVS，则默认情况下，OVS 会为每个 NUMA 节点创建一个  pmd 线程。但是，在有多个端口/rxq 产生流量的情况下，可以通过创建在不同内核上运行的多个 pmd 线程来提高性能。这些 pmd  线程可以通过每个线程负责不同的端口/rxq 来共享工作负载。将端口/rxq 分配给 pmd 线程是自动完成的。

A set bit in the mask means a pmd thread is created and pinned to the corresponding CPU core. For example, to run pmd threads on core 1 and 2:
掩码中的设置位表示创建 pmd 线程并将其固定到相应的 CPU 内核。例如，要在核心 1 和 2 上运行 pmd 线程：

```
$ ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x6
```

When using dpdk and dpdkvhostuser ports in a bi-directional VM loopback as shown below, spreading the workload over 2 or 4 pmd threads shows significant improvements as there will be more total CPU occupancy available:
在双向 VM 环回中使用 dpdk 和 dpdkvhostuser 端口时，如下所示，将工作负载分散到 2 或 4 个 pmd 线程上会显示出显著的改进，因为将有更多的总 CPU 占用率可用：

```
NIC port0 <-> OVS <-> VM <-> OVS <-> NIC port 1
```

Refer to ovs-vswitchd.conf.db(5) for additional information on configuration options.
有关配置选项的其他信息，请参阅 ovs-vswitchd.conf.db（5）。

### Affinity[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#affinity) 亲和力 ¶

For superior performance, DPDK pmd threads and Qemu vCPU threads need to have affinity set accordingly.
为了获得卓越的性能，DPDK pmd 线程和 Qemu vCPU 线程需要相应地设置关联性。

- PMD thread Affinity PMD 线程关联

  A poll mode driver (pmd) thread handles the I/O of all DPDK interfaces assigned to it. A pmd thread shall poll the ports for incoming packets, switch the packets and send to tx port.  A pmd thread is CPU bound, and needs to be have affinity set to isolated cores for optimum performance.  Even though a PMD thread may exist, the thread only starts consuming CPU cycles if there is at least one receive queue assigned to the pmd.
  轮询模式驱动程序 （pmd） 线程处理分配给它的所有 DPDK 接口的 I/O。pmd 线程应轮询端口以获取传入数据包，切换数据包并发送到 tx 端口。pmd  线程受 CPU 限制，需要将关联设置为隔离内核以获得最佳性能。即使可能存在 PMD 线程，该线程也只有在至少一个接收队列分配给 pmd  时才开始消耗 CPU 周期。

  Note 注意

  On NUMA systems, PCI devices are also local to a NUMA node.  Unbound rx queues for a PCI device will be assigned to a pmd on it’s local NUMA node if a non-isolated PMD exists on that NUMA node.  If not, the queue will be assigned to a non-isolated pmd on a remote NUMA node.  This will result in reduced maximum throughput on that device and possibly on other devices assigned to that pmd thread. If such a queue assignment is made a warning message will be logged: “There’s no available (non-isolated) pmd thread on numa node N. Queue Q on port P will be assigned to the pmd on core C (numa node N’). Expect reduced performance.”
  在 NUMA 系统上，PCI 设备也是 NUMA 节点的本地设备。如果 PCI 设备的本地 NUMA 节点上存在非隔离的 PMD，则该 PCI  设备的未绑定 rx 队列将分配给其本地 NUMA 节点上的 pmd。否则，队列将分配给远程 NUMA 节点上的非隔离  pmd。这将导致该设备上的最大吞吐量降低，并可能降低分配给该 pmd  线程的其他设备上的最大吞吐量。如果进行此类队列分配，将记录一条警告消息：“numa 节点 N 上没有可用的（非隔离）pmd 线程，端口 P  上的队列 Q 将分配给核心 C（numa 节点 N'）上的 pmd。预计性能会降低。

  Binding PMD threads to cores is described in the above section `Multiple Poll-Mode Driver Threads`.
  将 PMD 线程绑定到内核在上一节 `Multiple Poll-Mode Driver Threads` 中进行了描述。

- QEMU vCPU thread Affinity
  QEMU vCPU 线程关联

  A VM performing simple packet forwarding or running complex packet pipelines has to ensure that the vCPU threads performing the work has as much CPU occupancy as possible.
  执行简单数据包转发或运行复杂数据包管道的 VM 必须确保执行工作的 vCPU 线程具有尽可能多的 CPU 占用率。

  For example, on a multicore VM, multiple QEMU vCPU threads shall be spawned. When the DPDK `testpmd` application that does packet forwarding is invoked, the `taskset` command should be used to affinitize the vCPU threads to the dedicated isolated cores on the host system.
  例如，在多核 VM 上，应生成多个 QEMU vCPU 线程。调用执行数据包转发的 DPDK `testpmd` 应用程序时，应使用该 `taskset` 命令将 vCPU 线程关联到主机系统上的专用隔离内核。

### Enable HyperThreading[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#enable-hyperthreading) 启用超线程 ¶

With HyperThreading, or SMT, enabled, a physical core appears as two logical cores. SMT can be utilized to spawn worker threads on logical cores of the same physical core there by saving additional cores.
启用超线程（SMT）后，物理内核显示为两个逻辑内核。SMT 可用于通过保存额外的内核，在相同物理内核的逻辑内核上生成工作线程。

With DPDK, when pinning pmd threads to logical cores, care must be taken to set the correct bits of the `pmd-cpu-mask` to ensure that the pmd threads are pinned to SMT siblings.
使用 DPDK 时，将 pmd 线程固定到逻辑内核时，必须注意设置正确的位， `pmd-cpu-mask` 以确保 pmd 线程固定到 SMT 同级。

Take a sample system configuration, with 2 sockets, 2 * 10 core processors, HT enabled. This gives us a total of 40 logical cores. To identify the physical core shared by two logical cores, run:
以系统配置为例，具有 2 个插槽、2 * 10 核处理器、启用 HT。这为我们提供了总共 40 个逻辑内核。要标识两个逻辑内核共享的物理内核，请运行：

```
$ cat /sys/devices/system/cpu/cpuN/topology/thread_siblings_list
```

where `N` is the logical core number.
其中 `N` 是逻辑核心编号。

In this example, it would show that cores `1` and `21` share the same physical core. Logical cores can be specified in pmd-cpu-masks similarly to physical cores, as described in `Multiple Poll-Mode Driver Threads`.
在此示例中，它将显示内核 `1` 和 `21` 共享相同的物理内核。逻辑内核可以在 pmd-cpu-masks 中指定，类似于物理内核，如 中 `Multiple Poll-Mode Driver Threads` 所述。

### NUMA/Cluster-on-Die[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#numa-cluster-on-die)

Ideally inter-NUMA datapaths should be avoided where possible as packets will go across QPI and there may be a slight performance penalty when compared with intra NUMA datapaths. On Intel Xeon Processor E5 v3, Cluster On Die is introduced on models that have 10 cores or more.  This makes it possible to logically split a socket into two NUMA regions and again it is preferred where possible to keep critical datapaths within the one cluster.
理想情况下，应尽可能避免 NUMA 间数据路径，因为数据包将通过 QPI，并且与 NUMA 内数据路径相比，可能会有轻微的性能损失。在英特尔至强处理器 E5 v3  上，在具有 10 个或更多内核的型号上引入了 Cluster On Die。这样就可以在逻辑上将套接字拆分为两个 NUMA  区域，并且最好在可能的情况下将关键数据路径保留在一个集群中。

It is good practice to ensure that threads that are in the datapath are pinned to cores in the same NUMA area. e.g. pmd threads and QEMU vCPUs responsible for forwarding. If DPDK is built with `CONFIG_RTE_LIBRTE_VHOST_NUMA=y`, vHost User ports automatically detect the NUMA socket of the QEMU vCPUs and will be serviced by a PMD from the same node provided a core on this node is enabled in the `pmd-cpu-mask`. `libnuma` packages are required for this feature.
最好确保数据路径中的线程固定到同一 NUMA 区域中的核心。例如，负责转发的 pmd 线程和 QEMU vCPU。如果 DPDK 是使用 `CONFIG_RTE_LIBRTE_VHOST_NUMA=y` 构建的，则 vHost 用户端口会自动检测 QEMU vCPU 的 NUMA 套接字，并且如果在 中启用了此节点上的核心 `pmd-cpu-mask` ，则该节点上的 PMD 将提供服务。 `libnuma` 此功能需要软件包。

Binding PMD threads is described in the above section `Multiple Poll-Mode Driver Threads`.
绑定 PMD 线程在上一节 `Multiple Poll-Mode Driver Threads` 中进行了描述。

### DPDK Physical Port Rx Queues[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#dpdk-physical-port-rx-queues) DPDK 物理端口接收队列 ¶

```
$ ovs-vsctl set Interface <DPDK interface> options:n_rxq=<integer>
```

The above command sets the number of rx queues for DPDK physical interface. The rx queues are assigned to pmd threads on the same NUMA node in a round-robin fashion.
上述命令设置 DPDK 物理接口的 rx 队列数。rx 队列以循环方式分配给同一 NUMA 节点上的 pmd 线程。



### DPDK Physical Port Queue Sizes[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#dpdk-physical-port-queue-sizes) DPDK 物理端口队列大小 ¶

```
$ ovs-vsctl set Interface dpdk0 options:n_rxq_desc=<integer>
$ ovs-vsctl set Interface dpdk0 options:n_txq_desc=<integer>
```

The above command sets the number of rx/tx descriptors that the NIC associated with dpdk0 will be initialised with.
上述命令设置与 dpdk0 关联的 NIC 将用于初始化的 rx/tx 描述符的数量。

Different `n_rxq_desc` and `n_txq_desc` configurations yield different benefits in terms of throughput and latency for different scenarios. Generally, smaller queue sizes can have a positive impact for latency at the expense of throughput. The opposite is often true for larger queue sizes. Note: increasing the number of rx descriptors eg. to 4096  may have a negative impact on performance due to the fact that non-vectorised DPDK rx functions may be used. This is dependent on the driver in use, but is true for the commonly used i40e and ixgbe DPDK drivers.
对于不同的场景，不同的 `n_rxq_desc`  `n_txq_desc`  配置在吞吐量和延迟方面会产生不同的优势。通常，较小的队列大小可能会以牺牲吞吐量为代价对延迟产生积极影响。对于较大的队列大小，情况通常正好相反。注意：增加 rx 描述符的数量，例如。到 4096 可能会对性能产生负面影响，因为可能会使用非矢量化的 DPDK rx  函数。这取决于正在使用的驱动程序，但对于常用的 i40e 和 ixgbe DPDK 驱动程序也是如此。

### Exact Match Cache[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#exact-match-cache) 精确匹配缓存 ¶

Each pmd thread contains one Exact Match Cache (EMC). After initial flow setup in the datapath, the EMC contains a single table and provides the lowest level (fastest) switching for DPDK ports. If there is a miss in the EMC then the next level where switching will occur is the datapath classifier.  Missing in the EMC and looking up in the datapath classifier incurs a significant performance penalty.  If lookup misses occur in the EMC because it is too small to handle the number of flows, its size can be increased. The EMC size can be modified by editing the define `EM_FLOW_HASH_SHIFT` in `lib/dpif-netdev.c`.
每个 pmd 线程都包含一个完全匹配缓存 （EMC）。在数据路径中设置初始流后，EMC 将包含一个表，并为 DPDK  端口提供最低级别（最快）交换。如果 EMC 中出现遗漏，则发生切换的下一个级别是数据路径分类器。在 EMC  中缺失并在数据路径分类器中查找会导致严重的性能损失。如果 EMC 中由于太小而无法处理流数而发生查找未命中，则可以增加其大小。可以通过编辑 中 `lib/dpif-netdev.c` 的定义 `EM_FLOW_HASH_SHIFT` 来修改 EMC 大小。

As mentioned above, an EMC is per pmd thread. An alternative way of increasing the aggregate amount of possible flow entries in EMC and avoiding datapath classifier lookups is to have multiple pmd threads running.
如上所述，EMC 是每个 pmd 线程。在 EMC 中增加可能的流条目的聚合量并避免数据路径分类器查找的另一种方法是运行多个 pmd 线程。

### Rx Mergeable Buffers[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#rx-mergeable-buffers) Rx 可合并缓冲区 ¶

Rx mergeable buffers is a virtio feature that allows chaining of multiple virtio descriptors to handle large packet sizes. Large packets are handled by reserving and chaining multiple free descriptors together. Mergeable buffer support is negotiated between the virtio driver and virtio device and is supported by the DPDK vhost library.  This behavior is supported and enabled by default, however in the case where the user knows that rx mergeable buffers are not needed i.e. jumbo frames are not needed, it can be forced off by adding `mrg_rxbuf=off` to the QEMU command line options. By not reserving multiple chains of descriptors it will make more individual virtio descriptors available for rx to the guest using dpdkvhost ports and this can improve performance.
Rx 可合并缓冲区是一项 virtio 功能，它允许链接多个 virtio  描述符以处理大数据包大小。大数据包是通过保留多个免费描述符并将其链接在一起来处理的。可合并缓冲区支持在 virtio 驱动程序和 virtio  设备之间协商，并受 DPDK 虚拟主机库支持。默认情况下，此行为受支持并启用，但是，如果用户知道不需要 rx  可合并缓冲区，即不需要巨型帧，则可以通过 `mrg_rxbuf=off` 向 QEMU 命令行添加选项来强制关闭它。通过不保留多个描述符链，它将使更多单独的 virtio 描述符可用于使用 dpdkvhost 端口的 rx 给客户机，这可以提高性能。

### Output Packet Batching[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#output-packet-batching) 输出数据包批处理 ¶

To make advantage of batched transmit functions, OVS collects packets in intermediate queues before sending when processing a batch of received packets. Even if packets are matched by different flows, OVS uses a single send operation for all packets destined to the same output port.
为了利用批量传输功能，OVS在处理一批接收的数据包时，会在发送前在中间队列中收集数据包。即使数据包由不同的流匹配，OVS 也会对发往同一输出端口的所有数据包使用单个发送操作。

Furthermore, OVS is able to buffer packets in these intermediate queues for a configurable amount of time to reduce the frequency of send bursts at medium load levels when the packet receive rate is high, but the receive batch size still very small. This is particularly beneficial for packets transmitted to VMs using an interrupt-driven virtio driver, where the interrupt overhead is significant for the OVS PMD, the host operating system and the guest driver.
此外，当数据包接收速率较高但接收批大小仍然非常小时，OVS 能够在这些中间队列中缓冲数据包，以降低中等负载水平下发送突发的频率。这对于使用中断驱动的 virtio 驱动程序传输到 VM  的数据包特别有用，其中 OVS PMD、主机操作系统和客户机驱动程序的中断开销很大。

The `tx-flush-interval` parameter can be used to specify the time in microseconds OVS should wait between two send bursts to a given port (default is `0`). When the intermediate queue fills up before that time is over, the buffered packet batch is sent immediately:
该 `tx-flush-interval` 参数可用于指定 OVS 在两次发送到给定端口的突发之间应等待的时间（以微秒为单位）（默认为 `0` ）。当中间队列在该时间结束之前填满时，将立即发送缓冲的数据包批处理：

```
$ ovs-vsctl set Open_vSwitch . other_config:tx-flush-interval=50
```

This parameter influences both throughput and latency, depending on the traffic load on the port. In general lower values decrease latency while higher values may be useful to achieve higher throughput.
此参数会影响吞吐量和延迟，具体取决于端口上的流量负载。通常，较低的值可减少延迟，而较高的值可能有助于实现更高的吞吐量。

Low traffic (`packet rate < 1 / tx-flush-interval`) should not experience any significant latency or throughput increase as packets are forwarded immediately.
低流量 （ `packet rate < 1 / tx-flush-interval` ） 不应出现任何显著的延迟或吞吐量增加，因为数据包会立即转发。

At intermediate load levels (`1 / tx-flush-interval < packet rate < 32 / tx-flush-interval`) traffic should experience an average latency increase of up to `1 / 2 * tx-flush-interval` and a possible throughput improvement.
在中间负载级别 （ `1 / tx-flush-interval < packet rate < 32 / tx-flush-interval` ） 时，流量的平均延迟应增加， `1 / 2 * tx-flush-interval` 并可能提高吞吐量。

Very high traffic (`packet rate >> 32 / tx-flush-interval`) should experience the average latency increase equal to `32 / (2 * packet rate)`. Most send batches in this case will contain the maximum number of packets (`32`).
非常高的流量 （ `packet rate >> 32 / tx-flush-interval` ） 的平均延迟增加应等于 `32 / (2 * packet rate)` 。在这种情况下，大多数发送批次将包含最大数据包数 （ `32` ）。

A `tx-burst-interval` value of `50` microseconds has shown to provide a good performance increase in a `PHY-VM-PHY` scenario on `x86` system for interrupt-driven guests while keeping the latency increase at a reasonable level:
微秒 `tx-burst-interval` 值已显示，在系统上的 `PHY-VM-PHY`  `x86` 方案中，为中断驱动的客户机提供良好的性能提升，同时将延迟增加保持在合理的 `50` 水平：

> https://mail.openvswitch.org/pipermail/ovs-dev/2017-December/341628.html

Note 注意

Throughput impact of this option significantly depends on the scenario and the traffic patterns. For example: `tx-burst-interval` value of `50` microseconds shows performance degradation in `PHY-VM-PHY` with bonded PHY scenario while testing with `256 - 1024` packet flows:
此选项对吞吐量的影响很大程度上取决于方案和流量模式。例如： `tx-burst-interval` `50` 微秒值显示在使用 `256 - 1024` 数据包流进行测试时 `PHY-VM-PHY` ，绑定 PHY 场景中的性能下降：

> https://mail.openvswitch.org/pipermail/ovs-dev/2017-December/341700.html

The average number of packets per output batch can be checked in PMD stats:
可以在 PMD 统计信息中检查每个输出批次的平均数据包数：

```
$ ovs-appctl dpif-netdev/pmd-stats-show
```

## Limitations[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#limitations) 限制 ¶

- Network Interface Firmware requirements: Each release of DPDK is validated against a specific firmware version for a supported Network Interface. New firmware versions introduce bug fixes, performance improvements and new functionality that DPDK leverages. The validated firmware versions are available as part of the release notes for DPDK. It is recommended that users update Network Interface firmware to match what has been validated for the DPDK release.
  网络接口固件要求：DPDK 的每个版本都针对受支持的网络接口的特定固件版本进行验证。新固件版本引入了 DPDK 利用的错误修复、性能改进和新功能。经验证的固件版本作为  DPDK 发行说明的一部分提供。建议用户更新网络接口固件，以匹配已针对 DPDK 版本验证的固件。

  The latest list of validated firmware versions can be found in the [DPDK release notes](https://doc.dpdk.org/guides-23.11/rel_notes/release_23_11.html).
  可在 DPDK 发行说明中找到已验证固件版本的最新列表。

- Upper bound MTU: DPDK device drivers differ in how the L2 frame for a given MTU value is calculated e.g. i40e driver includes 2 x vlan headers in MTU overhead, em driver includes 1 x vlan header, ixgbe driver does not include a vlan  header in overhead. Currently it is not possible for OVS DPDK to know what upper bound MTU value is supported for a given device. As such OVS DPDK must provision for the case where the L2 frame for a given MTU includes 2 x vlan headers. This reduces the upper bound MTU value for devices that do not include vlan headers in their L2 frames by 8 bytes e.g. ixgbe devices upper bound MTU is reduced from 9710 to 9702. This work around is temporary and is expected to be removed once a method is provided by DPDK to query the upper bound MTU value for a given device.
  上限 MTU：DPDK 设备驱动程序在计算给定 MTU 值的 L2 帧的方式上有所不同，例如 i40e 驱动程序在 MTU 开销中包括 2 个  VLAN 标头，em 驱动程序包括 1 个 VLAN 标头，ixgbe 驱动程序在开销中不包括 VLAN 标头。目前，OVS DPDK  无法知道给定设备支持的上限 MTU 值。因此，OVS DPDK 必须针对给定 MTU 的 L2 帧包含 2 个 VLAN  标头的情况进行配置。这会将 L2 帧中不包含 VLAN 标头的设备的上限 MTU 值减少 8 个字节，例如 ixgbe 设备的上限 MTU 从  9710 减少到 9702。此变通办法是临时的，一旦 DPDK 提供方法来查询给定设备的上限 MTU 值，就会将其删除。

- Flow Control: When using i40e devices (Intel(R) 700 Series) it is recommended to set Link State Change detection to interrupt mode. Otherwise it has been observed that using the default polling mode, flow control changes may not be applied, and flow control states will not be reflected correctly. The issue is under investigation, this is a temporary work around.
  流量控制：使用 i40e 设备（英特尔® 700 系列）时，建议将链路状态更改检测设置为中断模式。否则，已观察到使用默认轮询模式时，可能无法应用流控制更改，并且无法正确反映流控制状态。该问题正在调查中，这是一个临时解决方法。

  For information about setting Link State Change detection, refer to [Link State Change (LSC) detection configuration](https://docs.openvswitch.org/en/latest/topics/dpdk/phy/#lsc-detection).
  有关设置链路状态更改检测的信息，请参阅链路状态更改 （LSC） 检测配置。

## Reporting Bugs[¶](https://docs.openvswitch.org/en/latest/intro/install/dpdk/#reporting-bugs) 报告错误 ¶

Report problems to [bugs@openvswitch.org](mailto:bugs@openvswitch.org).
向 bugs@openvswitch.org 报告问题。

# Open vSwitch with AF_XDP[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#open-vswitch-with-af-xdp) 使用 AF_XDP 打开 vSwitch ¶

This document describes how to build and install Open vSwitch using AF_XDP netdev.
本文档介绍如何使用 netdev 构建和安装 Open vSwitch AF_XDP。

Warning 警告

The AF_XDP support of Open vSwitch is considered ‘experimental’.
Open vSwitch 的AF_XDP支持被认为是“实验性的”。

## Introduction[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#introduction) 简介 ¶

AF_XDP, Address Family of the eXpress Data Path, is a new Linux socket type built upon the eBPF and XDP technology.  It is aims to have comparable performance to DPDK but cooperate better with existing kernel’s networking stack.  An AF_XDP socket receives and sends packets from an eBPF/XDP program attached to the netdev, by-passing a couple of Linux kernel’s subsystems. As a result, AF_XDP socket shows much better performance than AF_PACKET. For more details about AF_XDP, please see linux kernel’s Documentation/networking/af_xdp.rst
AF_XDP，即 eXpress 数据路径的地址系列，是一种基于 eBPF 和 XDP 技术构建的新 Linux 套接字类型。它旨在具有与 DPDK  相当的性能，但与现有内核的网络堆栈更好地协作。AF_XDP套接字从附加到 netdev 的 eBPF/XDP 程序接收和发送数据包，绕过  Linux 内核的几个子系统。因此，AF_XDP套接字的性能比AF_PACKET要好得多。有关 AF_XDP 的更多详细信息，请参阅 linux 内核的 Documentation/networking/af_xdp.rst

## AF_XDP Netdev[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#af-xdp-netdev)

OVS has a couple of netdev types, i.e., system, tap, or dpdk.  The AF_XDP feature adds a new netdev types called “afxdp”, and implement its configuration, packet reception, and transmit functions.  Since the AF_XDP socket, called xsk, operates in userspace, once ovs-vswitchd receives packets from xsk, the afxdp netdev re-uses the existing userspace dpif-netdev datapath.  As a result, most of the packet processing happens at the userspace instead of linux kernel.
OVS 有几种 netdev 类型，即 system、tap 或 dpdk。AF_XDP功能增加了一个名为“afxdp”的新 netdev  类型，并实现了其配置、数据包接收和发送功能。由于 AF_XDP 套接字（称为 xsk）在用户空间中运行，因此一旦 ovs-vswitchd  收到来自 xsk 的数据包，afxdp netdev 就会重用现有的用户空间 dpif-netdev  数据路径。因此，大多数数据包处理发生在用户空间而不是 linux 内核。

```
          |   +-------------------+
          |   |    ovs-vswitchd   |<-->ovsdb-server
          |   +-------------------+
          |   |      ofproto      |<-->OpenFlow controllers
          |   +--------+-+--------+
          |   | netdev | |ofproto-|
userspace |   +--------+ |  dpif  |
          |   | afxdp  | +--------+
          |   | netdev | |  dpif  |
          |   +---||---+ +--------+
          |       ||     |  dpif- |
          |       ||     | netdev |
          |_      ||     +--------+
                  ||
           _  +---||-----+--------+
          |   | AF_XDP prog +     |
   kernel |   |   xsk_map         |
          |_  +--------||---------+
                       ||
                    physical
                       NIC
```

## Build requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#build-requirements) 构建要求 ¶

In addition to the requirements described in [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/), building Open vSwitch with AF_XDP will require the following:
除了 Linux 上的 Open vSwitch、FreeBSD 和 NetBSD 中描述的要求外，使用 AF_XDP 构建 Open vSwitch 还需要满足以下条件：

- `libbpf` and `libxdp` (if version of `libbpf` if higher than `0.6`).
   `libbpf` 和 `libxdp` （if 的 `libbpf` if 版本高于 `0.6` ）。

- Linux kernel XDP support, with the following options (required)
  Linux 内核 XDP 支持，具有以下选项（必需）

  - CONFIG_BPF=y
  - CONFIG_BPF_SYSCALL=y
  - CONFIG_XDP_SOCKETS=y

- The following optional Kconfig options are also recommended, but not required:
  还建议使用以下可选的 Kconfig 选项，但不是必需的：

  - CONFIG_BPF_JIT=y (Performance)
    CONFIG_BPF_JIT=y （性能）
  - CONFIG_HAVE_BPF_JIT=y (Performance)
    CONFIG_HAVE_BPF_JIT=y （性能）
  - CONFIG_XDP_SOCKETS_DIAG=y (Debugging)
    CONFIG_XDP_SOCKETS_DIAG=y（调试）

- If you’re building your own kernel, be sure that you’re installing kernel headers too.  For example, with the following command:
  如果您正在构建自己的内核，请确保您也安装了内核头文件。例如，使用以下命令：

  ```
  make headers_install INSTALL_HDR_PATH=/usr
  ```

- If you’re using kernel from the distribution, be sure that corresponding kernel headers package installed.
  如果您使用的是发行版中的内核，请确保安装了相应的内核头文件包。

- Once your AF_XDP-enabled kernel is ready, if possible, run **./xdpsock -r -N -z -i <your device>** under linux/samples/bpf. This is an OVS independent benchmark tools for AF_XDP. It makes sure your basic kernel requirements are met for AF_XDP.
  启用AF_XDP的内核准备就绪后，如果可能，请在 linux/samples/bpf 下运行 ./xdpsock -r -N -z -i <您的设备>。这是一个独立的 OVS AF_XDP基准测试工具。它确保满足AF_XDP的基本内核要求。

## Installing[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#installing) 安装 ¶

For OVS to use AF_XDP netdev, it has to be configured with LIBBPF support.
要使 OVS 使用 AF_XDP netdev，必须配置 LIBBPF 支持。

First, install `libbpf` and `libxdp`.  For example, on Fedora these libraries along with development headers can be obtained by installing `libbpf-devel` and `libxdp-devel` packages.  For Ubuntu that will be `libbpf-dev` package with additional `libxdp-dev` on Ubuntu 22.10 or later.
首先，安装 `libbpf` 和 `libxdp` .例如，在 Fedora 上，这些库以及开发头文件可以通过安装 `libbpf-devel` 和 `libxdp-devel` 打包来获得。对于 Ubuntu，将在 Ubuntu 22.10 或更高版本上与附加 `libbpf-dev`  `libxdp-dev` 软件一起打包。

Next, ensure the standard OVS requirements are installed and bootstrap/configure the package:
接下来，确保安装了标准 OVS 要求并引导/配置包：

```
./boot.sh && ./configure --enable-afxdp
```

`--enable-afxdp` here is optional, but it will ensure that all dependencies are available at the build time.
 `--enable-afxdp` 这里是可选的，但它将确保所有依赖项在生成时都可用。

Finally, build and install OVS:
最后，构建并安装 OVS：

```
make && make install
```

To kick start end-to-end autotesting:
要启动端到端自动测试，请执行以下操作：

```
uname -a # make sure having 5.0+ kernel
ethtool --version # make sure ethtool is installed
make check-afxdp TESTSUITEFLAGS='1'
```

Note 注意

Not all test cases pass at this time. Currently all cvlan tests are skipped due to kernel issues.
目前并非所有测试用例都通过。目前，由于内核问题，所有 cvlan 测试都被跳过。

If a test case fails, check the log at:
如果测试用例失败，请在以下位置检查日志：

```
cat \
tests/system-afxdp-testsuite.dir/<test num>/system-afxdp-testsuite.log
```

## Setup AF_XDP netdev[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#setup-af-xdp-netdev) netdev AF_XDP设置 ¶

Before running OVS with AF_XDP, make sure the libbpf and libnuma are set-up right:
在使用 AF_XDP 运行 OVS 之前，请确保 libbpf 和 libnuma 设置正确：

```
ldd vswitchd/ovs-vswitchd
```

Open vSwitch should be started using userspace datapath as described in [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/):
Open vSwitch 应该使用用户空间数据路径启动，如 Linux 上的 Open vSwitch 和 NetBSD 中所述：

```
ovs-vswitchd ...
ovs-vsctl -- add-br br0 -- set Bridge br0 datapath_type=netdev
```

Make sure your device driver support AF_XDP, netdev-afxdp supports the following additional options (see `man ovs-vswitchd.conf.db` for more details):
请确保您的设备驱动程序支持 AF_XDP，netdev-afxdp 支持以下附加选项（有关详细信息，请参阅 `man ovs-vswitchd.conf.db` ）：

> - `xdp-mode`: `best-effort`, `native-with-zerocopy`, `native` or `generic`.  Defaults to `best-effort`, i.e. best of supported modes, so in most cases you don’t need to change it.
>    `xdp-mode` ： `best-effort` ， `native-with-zerocopy` ， `native` 或 `generic` .默认为 `best-effort` ，即支持的最佳模式，因此在大多数情况下，您不需要更改它。
> - `use-need-wakeup`: default `true` if libbpf supports it, otherwise `false`.
>    `use-need-wakeup` ：如果 libbpf 支持它，则默认 `true` ，否则 `false` 。

For example, to use 1 PMD (on core 4) on 1 queue (queue 0) device, configure these options: `pmd-cpu-mask`, `pmd-rxq-affinity`, and `n_rxq`:
例如，要在 1 个队列（队列 0）设备上使用 1 个 PMD（在核心 4 上），请配置以下选项： `pmd-cpu-mask` 、 `pmd-rxq-affinity` 和 `n_rxq` ：

```
ethtool -L enp2s0 combined 1
ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x10
ovs-vsctl add-port br0 enp2s0 -- set interface enp2s0 type="afxdp" \
                                 other_config:pmd-rxq-affinity="0:4"
```

Or, use 4 pmds/cores and 4 queues by doing:
或者，通过执行以下操作来使用 4 个 pmds/内核和 4 个队列：

```
ethtool -L enp2s0 combined 4
ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x36
ovs-vsctl add-port br0 enp2s0 -- set interface enp2s0 type="afxdp" \
  options:n_rxq=4 other_config:pmd-rxq-affinity="0:1,1:2,2:3,3:4"
```

Note 注意

`pmd-rxq-affinity` is optional. If not specified, system will auto-assign. `n_rxq` equals `1` by default.
 `pmd-rxq-affinity` 是可选的。如果未指定，系统将自动分配。 `n_rxq` 默认情况下等 `1` 于。

To validate that the bridge has successfully instantiated, you can use the:
若要验证网桥是否已成功实例化，可以使用：

```
ovs-vsctl show
```

Should show something like:
应该显示类似的东西：

```
Port "ens802f0"
 Interface "ens802f0"
    type: afxdp
    options: {n_rxq="1"}
```

Otherwise, enable debugging by:
否则，请通过以下方式启用调试：

```
ovs-appctl vlog/set netdev_afxdp::dbg
```

To check which XDP mode was chosen by `best-effort`, you can look for `xdp-mode` in the output of `ovs-vsctl get interface INT status:xdp-mode`:
要检查 `best-effort` 选择了哪种 XDP 模式，您可以在以下 `ovs-vsctl get interface INT status:xdp-mode` 输出中查找 `xdp-mode` ：

```
# ovs-vsctl get interface ens802f0 status:xdp-mode
"native-with-zerocopy"
```

## References[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#references) 参考资料 ¶

Most of the design details are described in the paper presented at Linux Plumber 2018, “Bringing the Power of eBPF to Open vSwitch”[1], section 4, and slides[2][4]. “The Path to DPDK Speeds for AF XDP”[3] gives a very good introduction about AF_XDP current and future work.
大多数设计细节在 Linux Plumber 2018 上发表的论文“将 eBPF 的强大功能引入 Open vSwitch”[1]、第 4  节和幻灯片[2][4] 中进行了描述。“AF XDP 的 DPDK 速度之路”[3] 很好地介绍了AF_XDP当前和未来的工作。

[1] http://vger.kernel.org/lpc_net2018_talks/ovs-ebpf-afxdp.pdf

[2] http://vger.kernel.org/lpc_net2018_talks/ovs-ebpf-lpc18-presentation.pdf

[3] http://vger.kernel.org/lpc_net2018_talks/lpc18_paper_af_xdp_perf-v2.pdf

[4] https://ovsfall2018.sched.com/event/IO7p/fast-userspace-ovs-with-afxdp

## Performance Tuning[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#performance-tuning) 性能调优 ¶

The name of the game is to keep your CPU running in userspace, allowing PMD to keep polling the AF_XDP queues without any interferences from kernel.
游戏的名称是保持您的 CPU 在用户空间中运行，允许 PMD 继续轮询AF_XDP队列，而不会受到内核的任何干扰。

1. Make sure everything is in the same NUMA node (memory used by AF_XDP, pmd running cores, device plug-in slot)
   确保所有内容都位于同一个 NUMA 节点中（AF_XDP使用的内存、运行内核的 pmd、设备插件插槽）
2. Isolate your CPU by doing isolcpu at grub configure.
   通过在 grub configure 执行 isolcpu 来隔离您的 CPU。
3. IRQ should not set to pmd running core.
   IRQ 不应设置为 pmd 运行核心。
4. The Spectre and Meltdown fixes increase the overhead of system calls.
   Spectre 和 Meltdown 修复增加了系统调用的开销。

### Debugging performance issue[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#debugging-performance-issue) 调试性能问题 ¶

While running the traffic, use linux perf tool to see where your cpu spends its cycle:
在运行流量时，请使用 linux perf 工具查看 cpu 的周期：

```
cd bpf-next/tools/perf
make
./perf record -p `pidof ovs-vswitchd` sleep 10
./perf report
```

Measure your system call rate by doing:
通过执行以下操作来测量系统调用速率：

```
pstree -p `pidof ovs-vswitchd`
strace -c -p <your pmd's PID>
```

Or, use OVS pmd tool:
或者，使用 OVS pmd 工具：

```
ovs-appctl dpif-netdev/pmd-stats-show
```

## Example Script[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#example-script) 示例脚本 ¶

Below is a script using namespaces and veth peer:
下面是一个使用命名空间和 veth peer 的脚本：

```
#!/bin/bash
ovs-vswitchd --no-chdir --pidfile -vvconn -vofproto_dpif -vunixctl \
  --disable-system --detach \
ovs-vsctl -- add-br br0 -- set Bridge br0 \
  protocols=OpenFlow10,OpenFlow11,OpenFlow12,OpenFlow13,OpenFlow14 \
  fail-mode=secure datapath_type=netdev

ip netns add at_ns0
ovs-appctl vlog/set netdev_afxdp::dbg

ip link add p0 type veth peer name afxdp-p0
ip link set p0 netns at_ns0
ip link set dev afxdp-p0 up
ovs-vsctl add-port br0 afxdp-p0 -- \
  set interface afxdp-p0 external-ids:iface-id="p0" type="afxdp"

ip netns exec at_ns0 sh << NS_EXEC_HEREDOC
ip addr add "10.1.1.1/24" dev p0
ip link set dev p0 up
NS_EXEC_HEREDOC

ip netns add at_ns1
ip link add p1 type veth peer name afxdp-p1
ip link set p1 netns at_ns1
ip link set dev afxdp-p1 up

ovs-vsctl add-port br0 afxdp-p1 -- \
  set interface afxdp-p1 external-ids:iface-id="p1" type="afxdp"
ip netns exec at_ns1 sh << NS_EXEC_HEREDOC
ip addr add "10.1.1.2/24" dev p1
ip link set dev p1 up
NS_EXEC_HEREDOC

ip netns exec at_ns0 ping -i .2 10.1.1.2
```

## Limitations/Known Issues[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#limitations-known-issues) 限制/已知问题 ¶

1. No QoS support because AF_XDP netdev by-pass the Linux TC layer. A possible work-around is to use OpenFlow meter action.
   不支持 QoS，因为 netdev 绕过 Linux TC 层AF_XDP。一种可能的解决方法是使用 OpenFlow 仪表操作。
2. Most of the tests are done using i40e single port. Multiple ports and also ixgbe driver also needs to be tested.
   大多数测试都是使用 i40e 单端口完成的。还需要测试多个端口和 ixgbe 驱动程序。
3. No latency test result (TODO items)
   无延迟测试结果（TODO 项）
4. Due to limitations of current upstream kernel, various offloading (vlan, cvlan) is not working over virtual interfaces (i.e. veth pair). Also, TCP is not working over virtual interfaces (veth) in generic XDP mode. Some more information and possible workaround available [here](https://github.com/cilium/cilium/issues/3077#issuecomment-430801467) . For TAP interfaces generic mode seems to work fine (TCP works) and even could provide better performance than native mode in some cases.
   由于当前上游内核的限制，各种卸载（vlan、cvlan）无法在虚拟接口（即 veth  对）上工作。此外，TCP在通用XDP模式下无法通过虚拟接口（veth）工作。此处提供了更多信息和可能的解决方法。对于TAP接口，通用模式似乎工作正常（TCP有效），在某些情况下甚至可以提供比本机模式更好的性能。

## PVP using tap device[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#pvp-using-tap-device) 使用分流器的 PVP ¶

Assume you have enp2s0 as physical nic, and a tap device connected to VM. First, start OVS, then add physical port:
假设你有 enp2s0 作为物理网卡，并且有一个连接到 VM 的分流器设备。首先，启动 OVS，然后添加物理端口：

```
ethtool -L enp2s0 combined 1
ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x10
ovs-vsctl add-port br0 enp2s0 -- set interface enp2s0 type="afxdp" \
  options:n_rxq=1 other_config:pmd-rxq-affinity="0:4"
```

Start a VM with virtio and tap device:
使用 virtio 启动 VM 并点击 device：

```
qemu-system-x86_64 -hda ubuntu1810.qcow \
  -m 4096 \
  -cpu host,+x2apic -enable-kvm \
  -device virtio-net-pci,mac=00:02:00:00:00:01,netdev=net0,mq=on,vectors=10,mrg_rxbuf=on,rx_queue_size=1024 \
  -netdev type=tap,id=net0,vhost=on,queues=8 \
  -object memory-backend-file,id=mem,size=4096M,mem-path=/dev/hugepages,share=on \
  -numa node,memdev=mem -mem-prealloc -smp 2
```

Create OpenFlow rules: 创建 OpenFlow 规则：

```
ovs-vsctl add-port br0 tap0 -- set interface tap0
ovs-ofctl del-flows br0
ovs-ofctl add-flow br0 "in_port=enp2s0, actions=output:tap0"
ovs-ofctl add-flow br0 "in_port=tap0, actions=output:enp2s0"
```

Inside the VM, use xdp_rxq_info to bounce back the traffic:
在 VM 内部，使用 xdp_rxq_info 来反弹流量：Inside the VM， use  to bounceback the traffic：

```
./xdp_rxq_info --dev ens3 --action XDP_TX
```

## PVP using vhostuser device[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#pvp-using-vhostuser-device) 使用 vhostuser 设备的 PVP ¶

First, build OVS with DPDK and AFXDP:
首先，使用 DPDK 和 AFXDP 构建 OVS：

```
./configure  --enable-afxdp --with-dpdk=shared|static
make -j4 && make install
```

Create a vhost-user port from OVS:
从 OVS 创建 vhost-user 端口：

```
ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true
ovs-vsctl -- add-br br0 -- set Bridge br0 datapath_type=netdev \
  other_config:pmd-cpu-mask=0xfff
ovs-vsctl add-port br0 vhost-user-1 \
  -- set Interface vhost-user-1 type=dpdkvhostuserclient \
      options:vhost-server-path=/tmp/vhost-user-1
```

Start VM using vhost-user mode:
使用 vhost-user 模式启动 VM：

```
qemu-system-x86_64 -hda ubuntu1810.qcow \
 -m 4096 \
 -cpu host,+x2apic -enable-kvm \
 -chardev socket,id=char1,path=/tmp/vhost-user-1,server \
 -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce,queues=4 \
 -device virtio-net-pci,mac=00:00:00:00:00:01,netdev=mynet1,mq=on,vectors=10 \
 -object memory-backend-file,id=mem,size=4096M,mem-path=/dev/hugepages,share=on \
 -numa node,memdev=mem -mem-prealloc -smp 2
```

Setup the OpenFlow ruls:
设置 OpenFlow 规则：

```
ovs-ofctl del-flows br0
ovs-ofctl add-flow br0 "in_port=enp2s0, actions=output:vhost-user-1"
ovs-ofctl add-flow br0 "in_port=vhost-user-1, actions=output:enp2s0"
```

Inside the VM, use xdp_rxq_info to drop or bounce back the traffic:
在 VM 内部，使用 xdp_rxq_info 丢弃或反弹流量：Inside the VM， use  to drop or bounce back the traffic：

```
./xdp_rxq_info --dev ens3 --action XDP_DROP
./xdp_rxq_info --dev ens3 --action XDP_TX
```

## PCP container using veth[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#pcp-container-using-veth) 使用 veth 的 PCP 容器 ¶

Create namespace and veth peer devices:
创建命名空间并访问对等设备：

```
ip netns add at_ns0
ip link add p0 type veth peer name afxdp-p0
ip link set p0 netns at_ns0
ip link set dev afxdp-p0 up
ip netns exec at_ns0 ip link set dev p0 up
```

Attach the veth port to br0 (linux kernel mode):
将 veth 端口附加到 br0（linux 内核模式）：

```
ovs-vsctl add-port br0 afxdp-p0 -- set interface afxdp-p0
```

Or, use AF_XDP: 或者，使用AF_XDP：

```
ovs-vsctl add-port br0 afxdp-p0 -- set interface afxdp-p0 type="afxdp"
```

Setup the OpenFlow rules:
设置 OpenFlow 规则：

```
ovs-ofctl del-flows br0
ovs-ofctl add-flow br0 "in_port=enp2s0, actions=output:afxdp-p0"
ovs-ofctl add-flow br0 "in_port=afxdp-p0, actions=output:enp2s0"
```

In the namespace, run drop or bounce back the packet:
在命名空间中，运行 drop 或 bounceback 数据包：

```
ip netns exec at_ns0 ./xdp_rxq_info --dev p0 --action XDP_DROP
ip netns exec at_ns0 ./xdp_rxq_info --dev p0 --action XDP_TX
```

## Bug Reporting[¶](https://docs.openvswitch.org/en/latest/intro/install/afxdp/#bug-reporting) Bug 报告 ¶

Please report problems to [dev@openvswitch.org](mailto:dev@openvswitch.org).
请向 dev@openvswitch.org 报告问题。

## 软件包安装

# Distributions packaging Open vSwitch[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#distributions-packaging-open-vswitch) 发行版打包 Open vSwitch ¶

This document lists various popular distributions packaging Open vSwitch. Open vSwitch is packaged by various distributions for multiple platforms and architectures.
本文档列出了各种流行的发行版打包Open vSwitch。Open vSwitch 由适用于多个平台和架构的各种发行版打包。

Note 注意

The packaged version available with distributions may not be latest Open vSwitch release.
发行版提供的打包版本可能不是最新的 Open vSwitch 版本。

## Debian / Ubuntu[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#debian-ubuntu)

You can use `apt-get` or `aptitude` to install the .deb packages and must be superuser.
您可以使用 `apt-get` 或 `aptitude` 安装 .deb 包，并且必须是超级用户。

1. Debian and Ubuntu has `openvswitch-switch` and `openvswitch-common` packages that includes the core userspace components of the switch.  Extra packages for documentation, ipsec, pki, VTEP and Python support are also available.  The Open vSwitch kernel datapath is maintained as part of the upstream kernel available in the distribution.
2. Debian 和 Ubuntu 具有 `openvswitch-switch` 包含交换机核心用户空间组件的 `openvswitch-common` 软件包。还提供用于文档、ipsec、pki、VTEP 和 Python 支持的额外软件包。Open vSwitch 内核数据路径作为发行版中可用的上游内核的一部分进行维护。
3. For fast userspace switching, Open vSwitch with DPDK support is bundled in the package `openvswitch-switch-dpdk`.
4. 为了快速切换用户空间，包 `openvswitch-switch-dpdk` 中捆绑了支持 DPDK 的 Open vSwitch。

## Fedora[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#fedora) Fedora 函数 ¶

Fedora provides `openvswitch`, `openvswitch-devel`, `openvswitch-test` and `openvswitch-debuginfo` rpm packages. You can install `openvswitch` package in minimum installation. Use `yum` or `dnf` to install the rpm packages and must be superuser.
Fedora 提供了 `openvswitch` 、 `openvswitch-devel` 和 `openvswitch-test` `openvswitch-debuginfo` rpm 软件包。您可以在最小安装中安装 `openvswitch` 软件包。使用 `yum` 或 `dnf` 安装 rpm 软件包，并且必须是超级用户。

## Red Hat[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#red-hat) 红帽 ¶

RHEL distributes `openvswitch` rpm package that supports kernel datapath. DPDK accelerated Open vSwitch can be installed using `openvswitch-dpdk` package.
RHEL 分发支持内核数据路径的 `openvswitch` rpm 包。DPDK 加速的 Open vSwitch 可以使用 `openvswitch-dpdk` 软件包安装。

## OpenSuSE[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#opensuse)

OpenSUSE provides `openvswitch`, `openvswitch-switch` rpm packages. Also `openvswitch-dpdk` and `openvswitch-dpdk-switch` can be installed for Open vSwitch using DPDK accelerated datapath.
OpenSUSE 提供了 `openvswitch` 、 `openvswitch-switch` rpm 软件包。 `openvswitch-dpdk-switch` 也可以 `openvswitch-dpdk` 使用 DPDK 加速数据路径为 Open vSwitch 安装。

# Debian Packaging for Open vSwitch[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#debian-packaging-for-open-vswitch) Open vSwitch 的 Debian 打包 ¶

This document describes how to build Debian packages for Open vSwitch. To install Open vSwitch on Debian without building Debian packages, refer to [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/) instead.
本文介绍如何构建 Open vSwitch 的 Debian 软件包。要在不构建 Debian 软件包的情况下在 Debian 上安装 Open vSwitch，请参考 Linux 上的 Open vSwitch、FreeBSD 和 NetBSD。

Note 注意

These instructions should also work on Ubuntu and other Debian derivative distributions.
这些指令也应该适用于 Ubuntu 和其他 Debian 衍生发行版。

## Before You Begin[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#before-you-begin) 开始之前 ¶

Before you begin, consider whether you really need to build packages yourself. Debian “wheezy” and “sid”, as well as recent versions of Ubuntu, contain pre-built Debian packages for Open vSwitch. It is easier to install these than to build your own. To use packages from your distribution, skip ahead to “Installing .deb Packages”, below.
在开始之前，请考虑是否真的需要自己构建包。Debian “wheezy” 和 “sid” 以及最新版本的 Ubuntu 包含用于 Open vSwitch 的预构建 Debian  软件包。安装这些比构建自己的更容易。要使用发行版中的软件包，请跳到下面的“安装.deb软件包”。

## Building Open vSwitch Debian packages[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#building-open-vswitch-debian-packages) 构建 Open vSwitch Debian 软件包 ¶

You may build from an Open vSwitch distribution tarball or from an Open vSwitch Git tree with these instructions.
您可以按照这些说明从 Open vSwitch 发行版 tarball 或 Open vSwitch Git 树进行构建。

You do not need to be the superuser to build the Debian packages.
您不需要成为超级用户来构建 Debian 软件包。

1. Install the “build-essential” and “fakeroot” packages. For example:
   安装 “build-essential” 和 “fakeroot” 软件包。例如：

   ```
   $ apt-get install build-essential fakeroot
   ```

2. Obtain and unpack an Open vSwitch source distribution and `cd` into its top level directory.
   获取 Open vSwitch 源分发并将其 `cd` 解压缩到其顶级目录中。

3. Install the build dependencies listed under “Build-Depends:” near the top of `debian/control.in`. You can install these any way you like, e.g.  with `apt-get install`.
   安装 “Build-Depends：” 下列出的 `debian/control.in` 构建依赖项。您可以以任何您喜欢的方式安装它们，例如使用 `apt-get install` .

4. Prepare the package source.
   准备包源。

   If you want to build the package with DPDK support execute the following command:
   如果要生成支持 DPDK 的包，请执行以下命令：

   ```
   $ ./boot.sh && ./configure --with-dpdk=shared && make debian
   ```

   If not: 如果不是：

   ```
   $ ./boot.sh && ./configure && make debian
   ```

Check your work by running `dpkg-checkbuilddeps` in the top level of your OVS directory. If you’ve installed all the dependencies properly, `dpkg-checkbuilddeps` will exit without printing anything. If you forgot to install some dependencies, it will tell you which ones.
通过在 OVS 目录的顶层运行 `dpkg-checkbuilddeps` 来检查您的工作。如果您已正确安装所有依赖项， `dpkg-checkbuilddeps` 则将退出而不打印任何内容。如果你忘记安装一些依赖项，它会告诉你哪些依赖项。

1. Build the package: 构建包：

   ```
   $ make debian-deb
   ```

1. The generated .deb files will be in the parent directory of the Open vSwitch source distribution.
   生成的.deb文件将位于 Open vSwitch 源分发的父目录中。

## Installing .deb Packages[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#installing-deb-packages) 安装 .deb 软件包 ¶

These instructions apply to installing from Debian packages that you built yourself, as described in the previous section.  In this case, use a command such as `dpkg -i` to install the .deb files that you build.  You will have to manually install any missing dependencies.
这些指示信息适用于从您自己构建的 Debian 软件包进行安装，如上一节所述。在这种情况下，请使用命令（例如 `dpkg -i` 安装您生成的.deb文件）。您必须手动安装任何缺少的依赖项。

You can also use these instruction to install from packages provided by Debian or a Debian derivative distribution such as Ubuntu.  In this case, use a program such as `apt-get` or `aptitude` to download and install the provided packages.  These programs will also automatically download and install any missing dependencies.
您也可以使用这些指令从 Debian 提供的软件包或 Debian 衍生发行版（如 Ubuntu）进行安装。在这种情况下，请使用 `apt-get` or `aptitude` 等程序下载并安装提供的软件包。这些程序还将自动下载并安装任何缺少的依赖项。

Important 重要

You must be superuser to install Debian packages.
您必须是超级用户才能安装 Debian 软件包。

1. Start by installing an Open vSwitch kernel module. See `debian/openvswitch-switch.README.Debian` for the available options.
   首先安装 Open vSwitch 内核模块。请参阅 `debian/openvswitch-switch.README.Debian` 可用选项。
2. Install the `openvswitch-switch` and `openvswitch-common` packages. These packages include the core userspace components of the switch.
   安装 `openvswitch-switch` and `openvswitch-common` 包。这些软件包包括交换机的核心用户空间组件。

Open vSwitch `.deb` packages not mentioned above are rarely useful. Refer to their individual package descriptions to find out whether any of them are useful to you.
上面未提及的打开的 vSwitch `.deb` 软件包很少有用。请参阅他们的单独软件包描述，以了解其中是否有任何对您有用。

## Reporting Bugs[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#reporting-bugs) 报告错误 ¶

Report problems to [bugs@openvswitch.org](mailto:bugs@openvswitch.org).
向 bugs@openvswitch.org 报告问题。

# Fedora, RHEL 7.x Packaging for Open vSwitch[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#fedora-rhel-7-x-packaging-for-open-vswitch) Fedora、RHEL 7.x 打包 Open vSwitch ¶

This document provides instructions for building and installing Open vSwitch RPM packages on a Fedora Linux host. Instructions for the installation of Open vSwitch on a Fedora Linux host without using RPM packages can be found in the [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/).
本文档提供了在 Fedora Linux 主机上构建和安装 Open vSwitch RPM 软件包的说明。在不使用 RPM 软件包的情况下在 Fedora  Linux 主机上安装 Open vSwitch 的说明可以在 Linux、FreeBSD 和 NetBSD 上的 Open vSwitch  中找到。

These instructions have been tested with Fedora 23, and are also applicable for RHEL 7.x and its derivatives, including CentOS 7.x and Scientific Linux 7.x.
这些指令已经过 Fedora 23 的测试，也适用于 RHEL 7.x 及其衍生产品，包括 CentOS 7.x 和 Scientific Linux 7.x。

## Build Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#build-requirements) 构建要求 ¶

You will need to install all required packages to build the RPMs. Newer distributions use `dnf` but if it’s not available, then use `yum` instructions.
您需要安装所有必需的软件包来构建 RPM。 较新的发行版使用 `dnf` ，但如果它不可用，请使用 `yum` 说明。

The command below will install RPM tools and generic build dependencies. And (optionally) include these packages: libcap-ng libcap-ng-devel dpdk-devel.
下面的命令将安装 RPM 工具和通用构建依赖项。并且（可选）包含以下软件包：libcap-ng libcap-ng-devel dpdk-devel。

DNF: DNF：

```
$ dnf install @'Development Tools' rpm-build dnf-plugins-core
```

YUM: 百 胜：

```
$ yum install @'Development Tools' rpm-build yum-utils
```

Then it is necessary to install Open vSwitch specific build dependencies. The dependencies are listed in the SPEC file, but first it is necessary to replace the VERSION tag to be a valid SPEC.
然后，需要安装特定于 Open vSwitch 的构建依赖项。依赖项列在 SPEC 文件中，但首先需要将 VERSION 标记替换为有效的 SPEC。

The command below will create a temporary SPEC file:
以下命令将创建一个临时 SPEC 文件：

```
$ sed -e 's/@VERSION@/0.0.1/' rhel/openvswitch-fedora.spec.in \
  > /tmp/ovs.spec
```

And to install specific dependencies, use the corresponding tool below. For some of the dependencies on RHEL you may need to add two additional repositories to help yum-builddep, e.g.:
要安装特定的依赖项，请使用下面的相应工具。对于 RHEL 上的某些依赖项，您可能需要添加两个额外的存储库来帮助 yum-builddep，例如：

```
$ subscription-manager repos --enable=rhel-7-server-extras-rpms
$ subscription-manager repos --enable=rhel-7-server-optional-rpms
```

or for RHEL 8:
或者对于 RHEL 8：

```
$ subscription-manager repos \
  --enable=codeready-builder-for-rhel-8-x86_64-rpms
```

DNF: DNF：

```
$ dnf builddep /tmp/ovs.spec
```

YUM: 百 胜：

```
$ yum-builddep /tmp/ovs.spec
```

Once that is completed, remove the file `/tmp/ovs.spec`.
完成后，删除文件 `/tmp/ovs.spec` 。

## Bootstrapping[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#bootstrapping) 引导 ¶

Refer to [Bootstrapping](https://docs.openvswitch.org/en/latest/intro/install/general/#general-bootstrapping). 请参阅引导。

## Configuring[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#configuring) 配置 ¶

Refer to [Configuring](https://docs.openvswitch.org/en/latest/intro/install/general/#general-configuring). 请参阅配置。

## Building[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#building) 构建 ¶

### User Space RPMs[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#user-space-rpms) 用户空间 RPM ¶

To build Open vSwitch user-space RPMs, execute the following from the directory in which ./configure was executed:
要构建 Open vSwitch 用户空间 RPM，请从执行 ./configure 的目录执行以下命令：

```
$ make rpm-fedora
```

This will create the RPMs openvswitch, python3-openvswitch, openvswitch-test, openvswitch-devel and openvswitch-debuginfo.
这将创建 RPM openvswitch、python3-openvswitch、openvswitch-test、openvswitch-devel 和 openvswitch-debuginfo。

To enable DPDK support in the openvswitch package, the `--with dpdk` option can be added:
要在 openvswitch 软件包中启用 DPDK 支持，可以添加以下 `--with dpdk` 选项：

```
$ make rpm-fedora RPMBUILD_OPT="--with dpdk --without check"
```

To enable AF_XDP support in the openvswitch package, the `--with afxdp` option can be added:
要在 openvswitch 软件包中启用AF_XDP支持，可以添加以下 `--with afxdp` 选项：

```
$ make rpm-fedora RPMBUILD_OPT="--with afxdp --without check"
```

You can also have the above commands automatically run the Open vSwitch unit tests.  This can take several minutes.
您还可以让上述命令自动运行 Open vSwitch 单元测试。这可能需要几分钟时间。

```
$ make rpm-fedora RPMBUILD_OPT="--with check"
```

## Installing[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#installing) 安装 ¶

RPM packages can be installed by using the command `rpm -i`. Package installation requires superuser privileges.
可以使用命令 `rpm -i` 安装 RPM 软件包。软件包安装需要超级用户权限。

In most cases only the openvswitch RPM will need to be installed. The python3-openvswitch, openvswitch-test, openvswitch-devel, and openvswitch-debuginfo RPMs are optional unless required for a specific purpose.
在大多数情况下，只需要安装 openvswitch RPM。python3-openvswitch、openvswitch-test、openvswitch-devel 和 openvswitch-debuginfo RPM 是可选的，除非特定用途需要。

Refer to the [RHEL README](https://github.com/openvswitch/ovs/blob/main/rhel/README.RHEL.rst) for additional usage and configuration information.
有关其他用法和配置信息，请参阅 RHEL 自述文件。

## Reporting Bugs[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#reporting-bugs) 报告错误 ¶

Report problems to [bugs@openvswitch.org](mailto:bugs@openvswitch.org).
向 bugs@openvswitch.org 报告问题。

# RHEL 5.6, 6.x Packaging for Open vSwitch[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#rhel-5-6-6-x-packaging-for-open-vswitch) 面向 Open vSwitch 的 RHEL 5.6、6.x 打包 ¶

This document describes how to build and install Open vSwitch on a Red Hat Enterprise Linux (RHEL) host.  If you want to install Open vSwitch on a generic Linux host, refer to [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/) instead.
本文档介绍如何在 Red Hat Enterprise Linux （RHEL） 主机上构建和安装 Open vSwitch。如果要在通用 Linux  主机上安装 Open vSwitch，请参考 Open vSwitch on Linux、FreeBSD 和 NetBSD。

We have tested these instructions with RHEL 5.6 and RHEL 6.0.
我们已经在 RHEL 5.6 和 RHEL 6.0 中测试了这些指令。

For RHEL 7.x (or derivatives, such as CentOS 7.x), you should follow the instructions in the [Fedora, RHEL 7.x Packaging for Open vSwitch](https://docs.openvswitch.org/en/latest/intro/install/fedora/).  The Fedora spec files are used for RHEL 7.x.
对于 RHEL 7.x（或衍生产品，如 CentOS 7.x），您应该按照 Fedora， RHEL 7.x Packaging for Open vSwitch 中的说明进行操作。Fedora 规范文件用于 RHEL 7.x。



## Prerequisites[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#prerequisites) 先决条件 ¶

You may build from an Open vSwitch distribution tarball or from an Open vSwitch Git tree.
您可以从 Open vSwitch 发行版 tarball 或 Open vSwitch Git 树进行构建。

The default RPM build directory, `_topdir`, has five directories in the top-level.
默认的 RPM 构建目录 `_topdir` ，在顶层有五个目录。

- BUILD/ 建/

  where the software is unpacked and built 解压缩和构建软件的地方

- RPMS/ 转速/

  where the newly created binary package files are written 写入新创建的二进制包文件的位置

- SOURCES/ 来源/

  contains the original sources, patches, and icon files 包含原始源、补丁和图标文件

- SPECS/ 规格/

  contains the spec files for each package to be built 包含要构建的每个包的规范文件

- SRPMS/

  where the newly created source package files are written 写入新创建的源包文件的位置

Before you begin, note the RPM sources directory on your version of RHEL.  The command `rpmbuild --showrc` will show the configuration for each of those directories. Alternatively, the command `rpm --eval '%{_topdir}'` shows the current configuration for the top level directory and the command `rpm --eval '%{_sourcedir}'` does the same for the sources directory. On RHEL 5, the default RPM `_topdir` is `/usr/src/redhat` and the default RPM sources directory is `/usr/src/redhat/SOURCES`. On RHEL 6, the default `_topdir` is `$HOME/rpmbuild` and the default RPM sources directory is `$HOME/rpmbuild/SOURCES`.
在开始之前，请注意 RHEL 版本上的 RPM 源目录。该命令 `rpmbuild --showrc` 将显示每个目录的配置。或者，该命令 `rpm --eval '%{_topdir}'` 显示顶级目录的当前配置，该命令 `rpm --eval '%{_sourcedir}'` 对源目录执行相同的操作。在 RHEL 5 上，默认 RPM `_topdir` 为 `/usr/src/redhat` ，默认 RPM 源目录为 `/usr/src/redhat/SOURCES` 。在 RHEL 6 上，默认 `_topdir` 值为 `$HOME/rpmbuild` ，默认 RPM 源目录为 `$HOME/rpmbuild/SOURCES` 。

## Build Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#build-requirements) 构建要求 ¶

You will need to install all required packages to build the RPMs. The command below will install RPM tools and generic build dependencies:
您将需要安装所有必需的软件包来构建 RPM。以下命令将安装 RPM 工具和通用构建依赖项：

```
$ yum install @'Development Tools' rpm-build yum-utils
```

Then it is necessary to install Open vSwitch specific build dependencies. The dependencies are listed in the SPEC file, but first it is necessary to replace the VERSION tag to be a valid SPEC.
然后，需要安装特定于 Open vSwitch 的构建依赖项。依赖项列在 SPEC 文件中，但首先需要将 VERSION 标记替换为有效的 SPEC。

The command below will create a temporary SPEC file:
以下命令将创建一个临时 SPEC 文件：

```
$ sed -e 's/@VERSION@/0.0.1/' rhel/openvswitch.spec.in > /tmp/ovs.spec
```

And to install specific dependencies, use yum-builddep tool:
要安装特定的依赖项，请使用 yum-builddep 工具：

```
$ yum-builddep /tmp/ovs.spec
```

Once that is completed, remove the file `/tmp/ovs.spec`.
完成后，删除文件 `/tmp/ovs.spec` 。

If python3-sphinx package is not available in your version of RHEL, you can install it via pip with ‘pip install sphinx’.
如果 python3-sphinx 软件包在您的 RHEL 版本中不可用，您可以通过 pip 和 'pip install sphinx' 安装它。

Open vSwitch requires python 3.6 or newer which is not available in older distributions. In the case of RHEL 6.x and its derivatives, one option is to install python34 from [EPEL](https://fedoraproject.org/wiki/EPEL).
Open vSwitch 需要 Python 3.6 或更高版本，这在旧发行版中不可用。对于 RHEL 6.x 及其衍生产品，一种选择是从 EPEL 安装 python34。



## Bootstrapping and Configuring[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#bootstrapping-and-configuring) 引导和配置 ¶

If you are building from a distribution tarball, skip to [Building](https://docs.openvswitch.org/en/latest/intro/install/rhel/#rhel-building). If not, you must be building from an Open vSwitch Git tree.  Determine what version of Autoconf is installed (e.g. run `autoconf --version`).  If it is not at least version 2.63, then you must upgrade or use another machine to build the packages.
如果要从分发压缩包构建，请跳到构建。否则，您必须从 Open vSwitch Git 树进行构建。确定安装的 Autoconf 版本（例如运行 `autoconf --version` ）。如果它不是至少 2.63 版，则必须升级或使用另一台计算机来构建包。

Assuming all requirements have been met, build the tarball by running:
假设已满足所有要求，请通过运行以下命令来构建压缩包：

```
$ ./boot.sh
$ ./configure
$ make dist
```

You must run this on a machine that has the tools listed in [Build Requirements](https://docs.openvswitch.org/en/latest/intro/install/general/#general-build-reqs) as prerequisites for building from a Git tree. Afterward, proceed with the rest of the instructions using the distribution tarball.
您必须在具有生成要求中列出的工具作为从 Git 树生成的先决条件的计算机上运行此操作。之后，使用分发压缩包继续执行其余说明。

Now you have a distribution tarball, named something like `openvswitch-x.y.z.tar.gz`.  Copy this file into the RPM sources directory, e.g.:
现在你有一个分布压缩包，命名为 `openvswitch-x.y.z.tar.gz` .将此文件复制到 RPM 源目录中，例如：

```
$ cp openvswitch-x.y.z.tar.gz $HOME/rpmbuild/SOURCES
```

### Broken `build` symlink[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#broken-build-symlink) 符号链接损坏 `build` ¶

Some versions of the RHEL 6 kernel-devel package contain a broken `build` symlink.  If you are using such a version, you must fix the problem before continuing.
某些版本的 RHEL 6 kernel-devel 软件包包含损坏 `build` 的符号链接。如果您使用的是此类版本，则必须先解决问题，然后才能继续。

To find out whether you are affected, run:
若要了解您是否受到影响，请运行：

```
$ cd /lib/modules/<version>
$ ls -l build/
```

where `<version>` is the version number of the RHEL 6 kernel.
其中 `<version>` 是 RHEL 6 内核的版本号。

Note 注意

The trailing slash in the final command is important.  Be sure to include it.
最终命令中的尾部斜杠很重要。请务必将其包括在内。

If the `ls` command produces a directory listing, your kernel-devel package is OK.  If it produces a `No such file or directory` error, your kernel-devel package is buggy.
如果该 `ls` 命令生成目录列表，则您的 kernel-devel 软件包是正常的。如果它产生 `No such file or directory` 错误，则您的 kernel-devel 包有问题。

If your kernel-devel package is buggy, then you can fix it with:
如果你的 kernel-devel 软件包有问题，那么你可以用以下方法修复它：

```
$ cd /lib/modules/<version>
$ rm build
$ ln -s /usr/src/kernels/<target> build
```

where `<target>` is the name of an existing directory under `/usr/src/kernels`, whose name should be similar to `<version>` but may contain some extra parts.  Once you have done this, verify the fix with the same procedure you used above to check for the problem.
其中 `<target>` 是 下的 `/usr/src/kernels` 现有目录的名称，其名称应与 `<version>` 相似，但可能包含一些额外的部分。完成此操作后，请使用上面用于检查问题的相同过程验证修复程序。



## Building[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#building) 构建 ¶

You should have a distribution tarball named something like openvswitch-x.y.z.tar.gz.  Copy this file into the RPM sources directory:
您应该有一个名为 openvswitch-x.y.z.tar.gz 的分发压缩包。将此文件复制到 RPM 源目录中：

```
$ cp openvswitch-x.y.z.tar.gz $HOME/rpmbuild/SOURCES
```

Make another copy of the distribution tarball in a temporary directory.  Then unpack the tarball and `cd` into its root:
在临时目录中制作另一个分发压缩包副本。然后打开压缩包并 `cd` 进入其根部：

```
$ tar xzf openvswitch-x.y.z.tar.gz
$ cd openvswitch-x.y.z
```

### Userspace[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#userspace) 用户空间 ¶

To build Open vSwitch userspace, run:
要构建 Open vSwitch 用户空间，请运行：

```
$ rpmbuild -bb rhel/openvswitch.spec
```

This produces two RPMs: “openvswitch” and “openvswitch-debuginfo”.
这将产生两个 RPM：“openvswitch”和“openvswitch-debuginfo”。

The above command automatically runs the Open vSwitch unit tests.  To disable the unit tests, run:
上述命令会自动运行 Open vSwitch 单元测试。若要禁用单元测试，请运行：

```
$ rpmbuild -bb --without check rhel/openvswitch.spec
```

Note 注意

If the build fails with `configure: error: source dir /lib/modules/2.6.32-279.el6.x86_64/build doesn't exist` or similar, then the kernel-devel package is missing or buggy.
如果构建失败 `configure: error: source dir /lib/modules/2.6.32-279.el6.x86_64/build doesn't exist` 或类似，则 kernel-devel 包丢失或有错误。



## Red Hat Network Scripts Integration[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#red-hat-network-scripts-integration) 红帽网络脚本集成 ¶

A RHEL host has default firewall rules that prevent any Open vSwitch tunnel traffic from passing through. If a user configures Open vSwitch tunnels like Geneve, GRE, VXLAN, LISP etc., they will either have to manually add iptables firewall rules to allow the tunnel traffic or add it through a startup script Refer to the “enable-protocol” command in the ovs-ctl(8) manpage for more information.
RHEL 主机具有默认防火墙规则，可防止任何 Open vSwitch 隧道流量通过。如果用户配置了 Open vSwitch 隧道，如  Geneve、GRE、VXLAN、LISP 等，他们必须手动添加 iptables 防火墙规则以允许隧道流量，或者通过启动脚本添加它  有关更多信息，请参见 ovs-ctl（8） 手册页中的 “enable-protocol” 命令。

In addition, simple integration with Red Hat network scripts has been implemented.  Refer to [README.RHEL.rst](https://github.com/openvswitch/ovs/blob/main/rhel/README.RHEL.rst) in the source tree or /usr/share/doc/openvswitch/README.RHEL.rst in the installed openvswitch package for details.
此外，还实现了与 Red Hat 网络脚本的简单集成。请参阅自述文件。RHEL.rst 或 /usr/share/doc/openvswitch/README 中。RHEL.rst，了解详细信息。

## Reporting Bugs[¶](https://docs.openvswitch.org/en/latest/intro/install/rhel/#reporting-bugs) 报告错误 ¶

Report problems to [bugs@openvswitch.org](mailto:bugs@openvswitch.org).
向 bugs@openvswitch.org 报告问题。

# Bash command-line completion scripts[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#bash-command-line-completion-scripts) Bash 命令行补全脚本 ¶

There are two completion scripts available: `ovs-appctl-bashcomp.bash` and `ovs-vsctl-bashcomp.bash`.
有两个完成脚本可用： `ovs-appctl-bashcomp.bash` 和 `ovs-vsctl-bashcomp.bash` 。

## ovs-appctl-bashcomp[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#ovs-appctl-bashcomp)

`ovs-appctl-bashcomp.bash` adds bash command-line completion support for `ovs-appctl`, `ovs-dpctl`, `ovs-ofctl` and `ovsdb-tool` commands.
 `ovs-appctl-bashcomp.bash` 添加了对 `ovs-appctl` 、 `ovs-dpctl` 和 `ovs-ofctl` `ovsdb-tool` 命令的 bash 命令行补全支持。

### Features[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#features) 功能 ¶

- Display available completion or complete on unfinished user input (long option, subcommand, and argument).
  显示可用的完成或未完成的用户输入（长选项、子命令和参数）的完成。
- Subcommand hints 子命令提示
- Convert between keywords like `bridge`, `port`, `interface`, or `dp` and the available record in ovsdb.
  在 `bridge` 、 `port` 、 `interface` 或 `dp` 等关键字和 ovsdb 中的可用记录之间进行转换。

### Limitations[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#limitations) 限制 ¶

- Only supports a small set of important keywords (`dp`, `datapath`, `bridge`, `switch`, `port`, `interface`, `iface`).
  仅支持一小部分重要关键字（ `dp` ， `datapath` ， `bridge` ， `switch` ， `port` `iface` ， `interface` ， ）。

- Does not support parsing of nested options. For example:
  不支持分析嵌套选项。例如：

  ```
  $ ovsdb-tool create [db [schema]]
  ```

- Does not support expansion on repeated argument. For example:
  不支持对重复参数的扩展。例如：

  ```
  $ ovs-dpctl show [dp...]).
  ```

- Only supports matching on long options, and only in the format `--option [arg]`. Do not use `--option=[arg]`.
  仅支持对长选项进行匹配，并且仅支持 `--option [arg]` 格式为 。请勿使用 `--option=[arg]` .

## ovs-vsctl-bashcomp[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#ovs-vsctl-bashcomp)

`ovs-vsctl-bashcomp.bash` adds Bash command-line completion support for `ovs-vsctl` command.
 `ovs-vsctl-bashcomp.bash` 添加了对 `ovs-vsctl` 命令的 Bash 命令行补全支持。

### Features[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#id1) 功能 ¶

- Display available completion and complete on user input for global/local options, command, and argument.
  显示全局/本地选项、命令和参数的用户输入的可用完成和完成。
- Query database and expand keywords like `table`, `record`, `column`, or `key`, to available completions.
  查询数据库并将关键字（如 `table` 、 `record` 、 `column` 或 `key` ）展开为可用的补全项。
- Deal with argument relations like ‘one and more’, ‘zero or one’.
  处理诸如“一和多”、“零或一”之类的参数关系。
- Complete multiple `ovs-vsctl` commands cascaded via `--`.
  完成通过 `--` 级联的多个 `ovs-vsctl` 命令。

### Limitations[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#id2) 限制 ¶

Completion of very long `ovs-vsctl` commands can take up to several seconds.
完成很长 `ovs-vsctl` 的命令可能需要几秒钟的时间。

## Usage[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#usage) 用法 ¶

The bashcomp scripts should be placed at `/etc/bash_completion.d/` to be available for all bash sessions.  Running `make install` will place the scripts to `$(sysconfdir)/bash_completion.d/`, thus, the user should specify `--sysconfdir=/etc` at configuration.  If OVS is installed from packages, the scripts will automatically be placed inside `/etc/bash_completion.d/`.
bashcomp 脚本应放置在 `/etc/bash_completion.d/` 以便可用于所有 bash 会话。运行 `make install` 会将脚本放置到 `$(sysconfdir)/bash_completion.d/` ，因此，用户应在配置时指定 `--sysconfdir=/etc` 。如果从软件包安装 OVS，则脚本将自动放置在 `/etc/bash_completion.d/` .

If you just want to run the scripts in one bash, you can remove them from `/etc/bash_completion.d/` and run the scripts via `. ovs-appctl-bashcomp.bash` or `. ovs-vsctl-bashcomp.bash`.
如果您只想在一个 bash 中运行脚本，则可以从中删除它们 `/etc/bash_completion.d/` 并通过 `. ovs-appctl-bashcomp.bash` 或 `. ovs-vsctl-bashcomp.bash` 运行脚本。

## Tests[¶](https://docs.openvswitch.org/en/latest/intro/install/bash-completion/#tests) 测试 ¶

Unit tests are added in `tests/completion.at` and integrated into autotest framework.  To run the tests, just run `make check`.
单元测试被添加到自动测试框架中 `tests/completion.at` 并集成到自动测试框架中。要运行测试，只需运行 `make check` 。

# Open vSwitch Documentation[¶](https://docs.openvswitch.org/en/latest/intro/install/documentation/#open-vswitch-documentation) 打开 vSwitch 文档 ¶

This document describes how to build the OVS documentation for use offline. A continuously updated, online version can be found at [docs.openvswitch.org](http://docs.openvswitch.org).
本文档介绍如何构建 OVS 文档以供脱机使用。可以在 docs.openvswitch.org 上找到持续更新的在线版本。

Note 注意

These instructions provide information on building the documentation locally. For information on writing documentation, refer to [Documentation Style](https://docs.openvswitch.org/en/latest/internals/contributing/documentation-style/)
这些说明提供了有关在本地构建文档的信息。有关编写文档的信息，请参阅文档样式

## Build Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/documentation/#build-requirements) 构建要求 ¶

As described in the [Documentation Style](https://docs.openvswitch.org/en/latest/internals/contributing/documentation-style/), the Open vSwitch documentation is written in reStructuredText and built with Sphinx. A detailed guide on installing Sphinx in many environments is available on the [Sphinx website](http://www.sphinx-doc.org/en/master/usage/installation.html) but, for most Linux distributions, you can install with your package manager. For example, on Debian/Ubuntu run:
如文档样式中所述，Open vSwitch 文档是用 reStructuredText 编写的，并使用 Sphinx 构建。Sphinx 网站上提供了在许多环境中安装  Sphinx 的详细指南，但对于大多数 Linux 发行版，您可以使用包管理器进行安装。例如，在 Debian/Ubuntu 上运行：

```
$ sudo apt-get install python3-sphinx
```

Similarly, on RHEL/Fedora run:
同样，在 RHEL/Fedora 上运行：

```
$ sudo dnf install python3-sphinx
```

A `requirements.txt` is also provided in the `/Documentation`, should you wish to install using `pip`:
中也提供了 A `requirements.txt` ， `/Documentation` 如果您希望使用以下命令 `pip` 进行安装：

```
$ virtualenv .venv
$ source .venv/bin/activate
$ pip install -r Documentation/requirements.txt
```

## Configuring[¶](https://docs.openvswitch.org/en/latest/intro/install/documentation/#configuring) 配置 ¶

It’s unlikely that you’ll need to customize any aspect of the configuration. However, the `Documentation/conf.py` is the go-to place for all configuration. This file is well documented and further information is available on the [Sphinx website](http://www.sphinx-doc.org/en/master/config.html).
您不太可能需要自定义配置的任何方面。但是，这是 `Documentation/conf.py` 所有配置的首选位置。该文件有据可查，更多信息可在狮身人面像网站上找到。

## Building[¶](https://docs.openvswitch.org/en/latest/intro/install/documentation/#building) 构建 ¶

Once Sphinx is installed, the documentation can be built using the provided Makefile targets:
安装 Sphinx 后，可以使用提供的 Makefile 目标构建文档：

```
$ make docs-check
```

Important 重要

The `docs-check` target will fail if there are any syntax errors. However, it won’t catch more succinct issues such as style or grammar issues.  As a result, you should always inspect changes visually to ensure the result is as intended.
如果存在任何语法错误， `docs-check` 目标将失败。但是，它不会捕获更简洁的问题，例如样式或语法问题。因此，您应该始终目视检查更改，以确保结果符合预期。

Once built, documentation is available in the `/Documentation/_build` folder. Open the root `index.html` to browse the documentation.
构建后， `/Documentation/_build` 文档可在文件夹中找到。打开根目录 `index.html` 以浏览文档。



## Debian / Ubuntu[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#debian-ubuntu)

You can use `apt-get` or `aptitude` to install the .deb packages and must be superuser.

\1. Debian and Ubuntu has `openvswitch-switch` and `openvswitch-common` packages that includes the core userspace components of the switch.  Extra packages for documentation, ipsec, pki, VTEP and Python support are also available.  The Open vSwitch kernel datapath is maintained as part of the upstream kernel available in the distribution.

\2. For fast userspace switching, Open vSwitch with DPDK support is bundled in the package `openvswitch-switch-dpdk`.

## Fedora[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#fedora)

Fedora provides `openvswitch`, `openvswitch-devel`, `openvswitch-test` and `openvswitch-debuginfo` rpm packages. You can install `openvswitch` package in minimum installation. Use `yum` or `dnf` to install the rpm packages and must be superuser.

## Red Hat[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#red-hat)

RHEL distributes `openvswitch` rpm package that supports kernel datapath. DPDK accelerated Open vSwitch can be installed using `openvswitch-dpdk` package.

## OpenSuSE[¶](https://docs.openvswitch.org/en/latest/intro/install/distributions/#opensuse)

OpenSUSE provides `openvswitch`, `openvswitch-switch` rpm packages. Also `openvswitch-dpdk` and `openvswitch-dpdk-switch` can be installed for Open vSwitch using DPDK accelerated datapath.

# Debian Packaging for Open vSwitch[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#debian-packaging-for-open-vswitch)

This document describes how to build Debian packages for Open vSwitch. To install Open vSwitch on Debian without building Debian packages, refer to [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/) instead.

Note

These instructions should also work on Ubuntu and other Debian derivative distributions.

## Before You Begin[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#before-you-begin)

Before you begin, consider whether you really need to build packages yourself. Debian “wheezy” and “sid”, as well as recent versions of Ubuntu, contain pre-built Debian packages for Open vSwitch. It is easier to install these than to build your own. To use packages from your distribution, skip ahead to “Installing .deb Packages”, below.

## Building Open vSwitch Debian packages[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#building-open-vswitch-debian-packages)

You may build from an Open vSwitch distribution tarball or from an Open vSwitch Git tree with these instructions.

You do not need to be the superuser to build the Debian packages.

1. Install the “build-essential” and “fakeroot” packages. For example:

   ```
   $ apt-get install build-essential fakeroot
   ```

2. Obtain and unpack an Open vSwitch source distribution and `cd` into its top level directory.

3. Install the build dependencies listed under “Build-Depends:” near the top of `debian/control.in`. You can install these any way you like, e.g.  with `apt-get install`.

4. Prepare the package source.

   If you want to build the package with DPDK support execute the following command:

   ```
   $ ./boot.sh && ./configure --with-dpdk=shared && make debian
   ```

   If not:

   ```
   $ ./boot.sh && ./configure && make debian
   ```

Check your work by running `dpkg-checkbuilddeps` in the top level of your OVS directory. If you’ve installed all the dependencies properly, `dpkg-checkbuilddeps` will exit without printing anything. If you forgot to install some dependencies, it will tell you which ones.

1. Build the package:

   ```
   $ make debian-deb
   ```

1. The generated .deb files will be in the parent directory of the Open vSwitch source distribution.

## Installing .deb Packages[¶](https://docs.openvswitch.org/en/latest/intro/install/debian/#installing-deb-packages)

These instructions apply to installing from Debian packages that you built yourself, as described in the previous section.  In this case, use a command such as `dpkg -i` to install the .deb files that you build.  You will have to manually install any missing dependencies.

You can also use these instruction to install from packages provided by Debian or a Debian derivative distribution such as Ubuntu.  In this case, use a program such as `apt-get` or `aptitude` to download and install the provided packages.  These programs will also automatically download and install any missing dependencies.

Important

You must be superuser to install Debian packages.

1. Start by installing an Open vSwitch kernel module. See `debian/openvswitch-switch.README.Debian` for the available options.
2. Install the `openvswitch-switch` and `openvswitch-common` packages. These packages include the core userspace components of the switch.

Open vSwitch `.deb` packages not mentioned above are rarely useful. Refer to their individual package descriptions to find out whether any of them are useful to you.

# Fedora, RHEL 7.x Packaging for Open vSwitch[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#fedora-rhel-7-x-packaging-for-open-vswitch)

This document provides instructions for building and installing Open vSwitch RPM packages on a Fedora Linux host. Instructions for the installation of Open vSwitch on a Fedora Linux host without using RPM packages can be found in the [Open vSwitch on Linux, FreeBSD and NetBSD](https://docs.openvswitch.org/en/latest/intro/install/general/).

These instructions have been tested with Fedora 23, and are also applicable for RHEL 7.x and its derivatives, including CentOS 7.x and Scientific Linux 7.x.

## Build Requirements[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#build-requirements)

You will need to install all required packages to build the RPMs. Newer distributions use `dnf` but if it’s not available, then use `yum` instructions.

The command below will install RPM tools and generic build dependencies. And (optionally) include these packages: libcap-ng libcap-ng-devel dpdk-devel.

DNF:

```
$ dnf install @'Development Tools' rpm-build dnf-plugins-core
```

YUM:

```
$ yum install @'Development Tools' rpm-build yum-utils
```

Then it is necessary to install Open vSwitch specific build dependencies. The dependencies are listed in the SPEC file, but first it is necessary to replace the VERSION tag to be a valid SPEC.

The command below will create a temporary SPEC file:

```
$ sed -e 's/@VERSION@/0.0.1/' rhel/openvswitch-fedora.spec.in \
  > /tmp/ovs.spec
```

And to install specific dependencies, use the corresponding tool below. For some of the dependencies on RHEL you may need to add two additional repositories to help yum-builddep, e.g.:

```
$ subscription-manager repos --enable=rhel-7-server-extras-rpms
$ subscription-manager repos --enable=rhel-7-server-optional-rpms
```

or for RHEL 8:

```
$ subscription-manager repos \
  --enable=codeready-builder-for-rhel-8-x86_64-rpms
```

DNF:

```
$ dnf builddep /tmp/ovs.spec
```

YUM:

```
$ yum-builddep /tmp/ovs.spec
```

Once that is completed, remove the file `/tmp/ovs.spec`.

## Bootstraping[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#bootstraping)

Refer to [Bootstrapping](https://docs.openvswitch.org/en/latest/intro/install/general/#general-bootstrapping).

## Configuring[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#configuring)

Refer to [Configuring](https://docs.openvswitch.org/en/latest/intro/install/general/#general-configuring).

## Building[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#building)

### User Space RPMs[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#user-space-rpms)

To build Open vSwitch user-space RPMs, execute the following from the directory in which ./configure was executed:

```
$ make rpm-fedora
```

This will create the RPMs openvswitch, python3-openvswitch, openvswitch-test, openvswitch-devel and openvswitch-debuginfo.

To enable DPDK support in the openvswitch package, the `--with dpdk` option can be added:

```
$ make rpm-fedora RPMBUILD_OPT="--with dpdk --without check"
```

To enable AF_XDP support in the openvswitch package, the `--with afxdp` option can be added:

```
$ make rpm-fedora RPMBUILD_OPT="--with afxdp --without check"
```

You can also have the above commands automatically run the Open vSwitch unit tests.  This can take several minutes.

```
$ make rpm-fedora RPMBUILD_OPT="--with check"
```

## Installing[¶](https://docs.openvswitch.org/en/latest/intro/install/fedora/#installing)

RPM packages can be installed by using the command `rpm -i`. Package installation requires superuser privileges.

In most cases only the openvswitch RPM will need to be installed. The python3-openvswitch, openvswitch-test, openvswitch-devel, and openvswitch-debuginfo RPMs are optional unless required for a specific purpose.

Refer to the [RHEL README](https://github.com/openvswitch/ovs/blob/master/rhel/README.RHEL.rst) for additional usage and configuration information.